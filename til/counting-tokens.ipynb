{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "title: \"A prompt in Japanese uses 77% more tokens than in English\"\n",
    "date: 2025-06-27\n",
    "description-meta: \"Counting tokens in different languages, and their implications for cost.\"\n",
    "categories:\n",
    "  - til\n",
    "  - openai \n",
    "  - tiktoken \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI mentions in their documentation that [1 token corresponds to roughly 4 characters](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them). \n",
    "\n",
    "I was curious how this would work for different languages. So I took a small section of Paul Graham's [How to Do Great Work](https://www.paulgraham.com/greatwork.html) and translated it into different languages (English, Spanish, French, German, Japanese, Chinese, Hindi, Russian, and Portuguese), and counted the tokens.\n",
    "\n",
    "Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def read_text(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return file.read()\n",
    "\n",
    "text_en = read_text(\"../_extras/counting-tokens/en.md\")\n",
    "text_es = read_text(\"../_extras/counting-tokens/es.md\")\n",
    "text_fr = read_text(\"../_extras/counting-tokens/fr.md\")\n",
    "text_de = read_text(\"../_extras/counting-tokens/de.md\")\n",
    "text_jp = read_text(\"../_extras/counting-tokens/jp.md\")\n",
    "text_zh = read_text(\"../_extras/counting-tokens/zh.md\")\n",
    "text_hi = read_text(\"../_extras/counting-tokens/hi.md\")\n",
    "text_ru = read_text(\"../_extras/counting-tokens/ru.md\")\n",
    "text_pt = read_text(\"../_extras/counting-tokens/pt.md\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(tiktoken.encoding_for_model(\"gpt-4o\").encode(text))\n",
    "\n",
    "chars_count = {\n",
    "    \"en\": len(text_en),\n",
    "    \"es\": len(text_es),\n",
    "    \"fr\": len(text_fr),\n",
    "    \"de\": len(text_de),\n",
    "    \"jp\": len(text_jp),\n",
    "    \"zh\": len(text_zh),\n",
    "    \"hi\": len(text_hi),\n",
    "    \"ru\": len(text_ru),\n",
    "    \"pt\": len(text_pt),\n",
    "}\n",
    "\n",
    "tokens_count = {\n",
    "    \"en\": count_tokens(text_en),\n",
    "    \"es\": count_tokens(text_es),\n",
    "    \"fr\": count_tokens(text_fr),\n",
    "    \"de\": count_tokens(text_de),\n",
    "    \"jp\": count_tokens(text_jp),\n",
    "    \"zh\": count_tokens(text_zh),\n",
    "    \"hi\": count_tokens(text_hi),\n",
    "    \"ru\": count_tokens(text_ru),\n",
    "    \"pt\": count_tokens(text_pt),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reads the text from the file, and uses tiktoken to count the tokens. I also counted the number of characters in the text.\n",
    "\n",
    "Then I calculated the ratio of tokens to characters for each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en: 4.752314814814815, 2053 chars, 432 tokens\n",
      "es: 4.5602409638554215, 2271 chars, 498 tokens\n",
      "fr: 4.692844677137871, 2689 chars, 573 tokens\n",
      "de: 4.4586330935251794, 2479 chars, 556 tokens\n",
      "jp: 1.409387222946545, 1081 chars, 767 tokens\n",
      "zh: 1.3314500941619585, 707 chars, 531 tokens\n",
      "hi: 3.5104, 2194 chars, 625 tokens\n",
      "ru: 4.019434628975265, 2275 chars, 566 tokens\n",
      "pt: 4.631578947368421, 2200 chars, 475 tokens\n"
     ]
    }
   ],
   "source": [
    "for lang in [\"en\", \"es\", \"fr\", \"de\", \"jp\", \"zh\", \"hi\", \"ru\", \"pt\"]:\n",
    "    chars = chars_count[lang]\n",
    "    tokens = tokens_count[lang]\n",
    "    print(f\"{lang}: {chars / tokens}, {chars} chars, {tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some highlights from the results:\n",
    "\n",
    "- English is the most efficient language, with 4.75 tokens per character.\n",
    "- Mandarin Chinese is the least efficient language, with 1.33 tokens per character.\n",
    "- The same prompt in Japanese uses 77% more tokens than in English.\n",
    "- Languages that use a latin alphabet (English, Spanish, French, German, Portuguese) are more efficient than languages that use a non-latin alphabet (Japanese, Chinese, Hindi, Russian). However, Russian is the most efficient language that uses a non-latin alphabet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
