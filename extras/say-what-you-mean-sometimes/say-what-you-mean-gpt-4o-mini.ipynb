{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18dc8ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65549cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33194d54-582b-4b81-a195-8f1269a438a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T20:37:59.673511Z",
     "iopub.status.busy": "2024-11-17T20:37:59.673328Z",
     "iopub.status.idle": "2024-11-17T20:38:07.222035Z",
     "shell.execute_reply": "2024-11-17T20:38:07.221730Z",
     "shell.execute_reply.started": "2024-11-17T20:37:59.673491Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dcast/Documents/GitHub/blog/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "from asyncio import Semaphore\n",
    "from enum import Enum\n",
    "from functools import partial\n",
    "from textwrap import dedent\n",
    "from typing import Callable, Dict, List, Literal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from openai import AsyncOpenAI\n",
    "from openai.types.chat import ChatCompletion\n",
    "from pydantic import BaseModel, ConfigDict, Field\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini-2024-07-18\"\n",
    "USE_SAMPLE = False\n",
    "SAMPLE_SIZE = 5\n",
    "MAX_CONCURRENCY = 200\n",
    "\n",
    "client = wrap_openai(AsyncOpenAI())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7967c968",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39c919d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "\n",
    "class PromptType(Enum):\n",
    "    WITHOUT_STRUCTURED_OUTPUT = \"without_so\"\n",
    "    WITH_TOOL_CALLS = \"with_so_tool_calls\"\n",
    "    WITH_JSON_MODE = \"with_so_json_mode\"\n",
    "    WITH_STRICT_TOOL_CALLS = \"with_so_strict_tool_calls\"\n",
    "\n",
    "\n",
    "class ClientConfig(BaseModel):\n",
    "    name: str\n",
    "    col_name: str\n",
    "    score_col_name: str\n",
    "\n",
    "\n",
    "CONFIGS = [\n",
    "    ClientConfig(\n",
    "        name=PromptType.WITHOUT_STRUCTURED_OUTPUT.value,\n",
    "        col_name=f\"response_{PromptType.WITHOUT_STRUCTURED_OUTPUT.value}\",\n",
    "        score_col_name=f\"score_{PromptType.WITHOUT_STRUCTURED_OUTPUT.value}\",\n",
    "    ),\n",
    "    ClientConfig(\n",
    "        name=PromptType.WITH_TOOL_CALLS.value,\n",
    "        col_name=f\"response_{PromptType.WITH_TOOL_CALLS.value}\",\n",
    "        score_col_name=f\"score_{PromptType.WITH_TOOL_CALLS.value}\",\n",
    "    ),\n",
    "    ClientConfig(\n",
    "        name=PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "        col_name=f\"response_{PromptType.WITH_STRICT_TOOL_CALLS.value}\",\n",
    "        score_col_name=f\"score_{PromptType.WITH_STRICT_TOOL_CALLS.value}\",\n",
    "    ),\n",
    "    ClientConfig(\n",
    "        name=PromptType.WITH_JSON_MODE.value,\n",
    "        col_name=f\"response_{PromptType.WITH_JSON_MODE.value}\",\n",
    "        score_col_name=f\"score_{PromptType.WITH_JSON_MODE.value}\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eb319a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "\n",
    "class LLMEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        configs: List[ClientConfig],\n",
    "        create_prompt_fn: Callable,\n",
    "        parse_response_fn: Callable,\n",
    "        response_model: BaseModel,\n",
    "        concurrency: int = MAX_CONCURRENCY,\n",
    "    ):\n",
    "        self.configs = configs\n",
    "        self.create_prompt_fn = create_prompt_fn\n",
    "        self.parse_response_fn = parse_response_fn\n",
    "        self.response_model = response_model\n",
    "        self.concurrency = concurrency\n",
    "\n",
    "        self.error_counts: Dict[str, int] = {config.name: 0 for config in self.configs}\n",
    "        self.key_order_errors: Dict[str, int] = {\n",
    "            config.name: 0 for config in self.configs\n",
    "        }\n",
    "        self.key_missing_errors: Dict[str, int] = {\n",
    "            config.name: 0 for config in self.configs\n",
    "        }\n",
    "\n",
    "    def _create_tool_call_schema(\n",
    "        self,\n",
    "        strict: bool = False,\n",
    "    ) -> dict:\n",
    "        model_schema = self.response_model.model_json_schema()\n",
    "        if strict:\n",
    "            return {\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": model_schema[\"title\"],\n",
    "                    \"schema\": model_schema,\n",
    "                    \"strict\": True,\n",
    "                },\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": model_schema[\"title\"],\n",
    "                    \"description\": f\"Correctly extracted `{model_schema['title']}` with all the required parameters with correct types\",\n",
    "                    \"parameters\": model_schema,\n",
    "                },\n",
    "            }\n",
    "\n",
    "    @traceable(run_type=\"prompt\")\n",
    "    def create_prompt(\n",
    "        self,\n",
    "        question: str,\n",
    "        prompt_type: str,\n",
    "    ) -> List[dict]:\n",
    "        return self.create_prompt_fn(\n",
    "            question=question,\n",
    "            prompt_type=prompt_type,\n",
    "            response_model=self.response_model,\n",
    "        )\n",
    "\n",
    "    @traceable(run_type=\"parser\")\n",
    "    def parse_response(\n",
    "        self,\n",
    "        response: str,\n",
    "        prompt_type: str,\n",
    "    ) -> str | int:\n",
    "        if prompt_type in [\n",
    "            PromptType.WITH_JSON_MODE.value,\n",
    "            PromptType.WITH_TOOL_CALLS.value,\n",
    "            PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "        ]:\n",
    "            reasoning_index = response.find(\"reasoning\")\n",
    "            answer_index = response.find(\"answer\")\n",
    "\n",
    "            if reasoning_index == -1 or answer_index == -1:\n",
    "                self.key_missing_errors[prompt_type] += 1\n",
    "            elif reasoning_index > answer_index:\n",
    "                self.key_order_errors[prompt_type] += 1\n",
    "\n",
    "        return self.parse_response_fn(response, prompt_type)\n",
    "\n",
    "    @traceable(run_type=\"llm\")\n",
    "    async def call_llm(\n",
    "        self,\n",
    "        config: ClientConfig,\n",
    "        question: str,\n",
    "    ) -> ChatCompletion:\n",
    "        params = {\n",
    "            \"messages\": self.create_prompt(question=question, prompt_type=config.name),\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"timeout\": 120,\n",
    "        }\n",
    "\n",
    "        prompt_type_configs = {\n",
    "            PromptType.WITH_JSON_MODE.value: {\n",
    "                \"response_format\": {\"type\": \"json_object\"}\n",
    "            },\n",
    "            PromptType.WITH_TOOL_CALLS.value: {\n",
    "                \"tools\": [self._create_tool_call_schema(strict=False)]\n",
    "            },\n",
    "            PromptType.WITH_STRICT_TOOL_CALLS.value: {\n",
    "                \"response_format\": self._create_tool_call_schema(strict=True)\n",
    "            },\n",
    "        }\n",
    "\n",
    "        if config.name in prompt_type_configs:\n",
    "            params.update(prompt_type_configs[config.name])\n",
    "\n",
    "        completion = await client.chat.completions.create(**params)\n",
    "\n",
    "        if config.name == PromptType.WITH_TOOL_CALLS.value:\n",
    "            response_content = (\n",
    "                completion.choices[0].message.tool_calls[0].function.arguments\n",
    "            )\n",
    "        else:\n",
    "            response_content = completion.choices[0].message.content\n",
    "\n",
    "        return response_content\n",
    "\n",
    "    @traceable(run_type=\"chain\")\n",
    "    async def process_question(\n",
    "        self,\n",
    "        question: str,\n",
    "        config: ClientConfig,\n",
    "        semaphore: Semaphore,\n",
    "        max_attempts: int = 3,\n",
    "    ) -> str | int | None:\n",
    "        async with semaphore:\n",
    "            for _ in range(max_attempts):\n",
    "                try:\n",
    "                    answer = await self.call_llm(\n",
    "                        config=config,\n",
    "                        question=question,\n",
    "                    )\n",
    "                    parsed_answer = self.parse_response(answer, config.name)\n",
    "                    if not parsed_answer:\n",
    "                        self.error_counts[config.name] += 1\n",
    "                    return parsed_answer\n",
    "                except Exception:\n",
    "                    self.error_counts[config.name] += 1\n",
    "                    print(f\"{config.name}, {question[:10]}: Retrying...\")\n",
    "                    await asyncio.sleep(1)\n",
    "                    continue\n",
    "            print(\n",
    "                f\"{config.name}, {question[:10]}: Failed to process question after {max_attempts} attempts. Set answer to null.\"\n",
    "            )\n",
    "        return None\n",
    "\n",
    "    @traceable(run_type=\"chain\")\n",
    "    async def process_questions(\n",
    "        self,\n",
    "        run_name: str,\n",
    "        questions: List[dict],\n",
    "        config: ClientConfig,\n",
    "    ) -> List[str | int | None]:\n",
    "        semaphore = Semaphore(self.concurrency)\n",
    "        tasks = [\n",
    "            self.process_question(\n",
    "                question=question[\"question\"],\n",
    "                config=config,\n",
    "                semaphore=semaphore,\n",
    "            )\n",
    "            for question in questions\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        return results\n",
    "\n",
    "    def generate_outputs(self, questions: List[dict]) -> pd.DataFrame:\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"id\": [i for i in range(len(questions))],\n",
    "                \"question\": [question[\"question\"] for question in questions],\n",
    "                \"answer\": [question[\"answer\"] for question in questions],\n",
    "            }\n",
    "        )\n",
    "        for config in self.configs:\n",
    "            responses = asyncio.run(\n",
    "                self.process_questions(\n",
    "                    run_name=config.name,\n",
    "                    questions=questions,\n",
    "                    config=config,\n",
    "                )\n",
    "            )\n",
    "            df[config.col_name] = responses\n",
    "        return df\n",
    "\n",
    "    def evaluate_outputs(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df_copy = df.copy()\n",
    "        for config in self.configs:\n",
    "            df_copy[config.score_col_name] = (\n",
    "                df_copy[\"answer\"] == df_copy[config.col_name]\n",
    "            ) * 1\n",
    "        return df_copy\n",
    "\n",
    "    def calculate_confidence_intervals(\n",
    "        self, df: pd.DataFrame, conf_level: float = 0.95\n",
    "    ) -> None:\n",
    "        print(\n",
    "            f\"Calculating confidence intervals ({conf_level}) with {len(df)} observations:\"\n",
    "        )\n",
    "        for config in self.configs:\n",
    "            score_col = config.score_col_name\n",
    "            scores = df[score_col]\n",
    "\n",
    "            if len(scores) == 0:\n",
    "                print(f\"No scores available for {score_col}\")\n",
    "                continue\n",
    "\n",
    "            mean_score = scores.mean()\n",
    "            se_score = scores.std() / np.sqrt(len(scores))\n",
    "\n",
    "            z_score = stats.norm.ppf((1 + conf_level) / 2)\n",
    "            margin_error = z_score * se_score\n",
    "            ci = [\n",
    "                max(0.0, mean_score - margin_error),\n",
    "                min(1.0, mean_score + margin_error),\n",
    "            ]\n",
    "            print(\n",
    "                f\"{score_col} - Mean: {mean_score * 100:.2f}% CI: {ci[0] * 100:.2f}% - {ci[1] * 100:.2f}%\"\n",
    "            )\n",
    "        print()\n",
    "\n",
    "    def run_paired_t_test(self, df: pd.DataFrame) -> None:\n",
    "        scores = {}\n",
    "\n",
    "        for config in self.configs:\n",
    "            score_col = config.score_col_name\n",
    "            scores[score_col] = df[score_col] * 1\n",
    "\n",
    "        for score_col_1, score_col_2 in [\n",
    "            (\"score_without_so\", \"score_with_so_tool_calls\"),\n",
    "            (\"score_without_so\", \"score_with_so_strict_tool_calls\"),\n",
    "            (\"score_without_so\", \"score_with_so_json_mode\"),\n",
    "        ]:\n",
    "            if score_col_1 in scores and score_col_2 in scores:\n",
    "                t_stat, p_value = stats.ttest_rel(\n",
    "                    scores[score_col_1], scores[score_col_2]\n",
    "                )\n",
    "                print(f\"{score_col_1} vs {score_col_2}\")\n",
    "                print(f\"t-statistic: {t_stat}, p-value: {p_value}\")\n",
    "\n",
    "    def report_error_counts(self) -> None:\n",
    "        print(\"Error counts:\")\n",
    "        for config in self.configs:\n",
    "            name = config.name\n",
    "            errors = self.error_counts.get(name, 0)\n",
    "            key_order = self.key_order_errors.get(name, 0)\n",
    "            print(f\"- {name}: {errors} processing errors, {key_order} key order errors\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13cccb1",
   "metadata": {},
   "source": [
    "## GSM8K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656a5248",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ff85876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "\n",
    "class ResponseGSM8K(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\", title=\"Response\")\n",
    "    reasoning: str = Field(description=\"step by step reasoning about the answer\")\n",
    "    answer: int = Field(description=\"final answer\")\n",
    "\n",
    "\n",
    "def create_prompt_gsm8k(\n",
    "    question: str,\n",
    "    prompt_type: str,\n",
    "    response_model: ResponseGSM8K | None = None,\n",
    "    zero_shot: bool = False,\n",
    "):\n",
    "    if prompt_type in [\n",
    "        PromptType.WITH_JSON_MODE.value,\n",
    "        PromptType.WITH_TOOL_CALLS.value,\n",
    "        PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "    ]:\n",
    "        system_prompt = (\n",
    "            dedent(\"\"\"\n",
    "        You are an expert in solving grade school math tasks. You will be presented with a grade-school math word problem and be asked to solve it.\n",
    "\n",
    "        You will always respond with JSON matching the following schema:\n",
    "        \"\"\")\n",
    "            + json.dumps(response_model.model_json_schema(), indent=2)\n",
    "            + \"\"\"\\n\\nFirst, provide your step by step reasoning in the \"reasoning\" field. Then, in the \"answer\" field, provide an integer that corresponds to the correct answer to the question. Don't include any other text in the \"answer\" field.\"\"\"\n",
    "        )\n",
    "    else:\n",
    "        system_prompt = dedent(\"\"\"\n",
    "        You are an expert in solving grade school math tasks. You will be presented with a grade-school math word problem and be asked to solve it.\n",
    "        \n",
    "        You will always respond in the following format:\n",
    "        \n",
    "        <str, reasoning about the answer>\n",
    "        ANSWER: <int, final answer>\n",
    "        \n",
    "        First, provide your step by step reasoning. Then, in ANSWER, provide an integer that corresponds to the correct answer to the question. Don't include any other text in ANSWER.\n",
    "        \"\"\")\n",
    "\n",
    "    examples = [\n",
    "        (\n",
    "            \"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\",\n",
    "            \"There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6.\",\n",
    "            6,\n",
    "        ),\n",
    "        (\n",
    "            \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\",\n",
    "            \"There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5.\",\n",
    "            5,\n",
    "        ),\n",
    "        (\n",
    "            \"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\",\n",
    "            \"Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39.\",\n",
    "            39,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    if not zero_shot:\n",
    "        system_prompt += \"\\nExamples:\" if examples else \"\"\n",
    "        for i, (example_q, example_reason, example_ans) in enumerate(examples):\n",
    "            system_prompt += f\"\\n\\n**{i+1}**\\nQuestion: {example_q}\"\n",
    "            if prompt_type in [\n",
    "                PromptType.WITH_JSON_MODE.value,\n",
    "                PromptType.WITH_TOOL_CALLS.value,\n",
    "                PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "            ]:\n",
    "                response = (\n",
    "                    f'{{\"reasoning\": \"{example_reason}\", \"answer\": {example_ans}}}'\n",
    "                )\n",
    "            else:\n",
    "                response = f\"{example_reason}\\nANSWER: {example_ans}\"\n",
    "            system_prompt += f\"\\nAssistant Response:\\n{response}\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question: {question}\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "\n",
    "create_prompt_gsm8k_zero_shot = partial(create_prompt_gsm8k, zero_shot=True)\n",
    "\n",
    "\n",
    "def parse_response_gsm8k(response: str, prompt_type: str) -> int | None:\n",
    "    if prompt_type in [\n",
    "        PromptType.WITH_JSON_MODE.value,\n",
    "        PromptType.WITH_TOOL_CALLS.value,\n",
    "        PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "    ]:\n",
    "        return ResponseGSM8K.model_validate_json(response).answer\n",
    "    else:\n",
    "        cleaned_response = (\n",
    "            response.split(\"\\nANSWER:\")[1].replace(\",\", \"\").rstrip(\".\").strip()\n",
    "        )\n",
    "        return int(cleaned_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8bb4c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "evals = [\n",
    "    {\n",
    "        \"question\": d[\"question\"],\n",
    "        \"answer\": int(d[\"answer\"].split(\"#### \")[1].replace(\",\", \"\").strip()),\n",
    "    }\n",
    "    for d in dataset[\"test\"]\n",
    "]\n",
    "\n",
    "if USE_SAMPLE:\n",
    "    evals = evals[:SAMPLE_SIZE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0409fd08",
   "metadata": {},
   "source": [
    "### Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86cd8588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without_so, Tim has a : Retrying...\n",
      "without_so, Tim has a : Retrying...\n",
      "with_so_tool_calls, A new prog: Retrying...\n",
      "with_so_tool_calls, Toula went: Retrying...\n",
      "with_so_tool_calls, John runs : Retrying...\n",
      "with_so_tool_calls, Morisette : Retrying...\n",
      "with_so_tool_calls, Elaine ini: Retrying...\n",
      "with_so_tool_calls, Judy teach: Retrying...\n",
      "with_so_tool_calls, Mark is a : Retrying...\n",
      "with_so_tool_calls, Tom plants: Retrying...\n",
      "with_so_tool_calls, Mandy owes: Retrying...\n",
      "with_so_tool_calls, Gunter is : Retrying...\n",
      "with_so_tool_calls, Shiela bou: Retrying...\n",
      "with_so_tool_calls, John drive: Retrying...\n",
      "with_so_tool_calls, Janet had : Retrying...\n",
      "with_so_tool_calls, Twenty doz: Retrying...\n",
      "with_so_tool_calls, Jill gets : Retrying...\n",
      "with_so_tool_calls, Sasha noti: Retrying...\n",
      "with_so_tool_calls, Kalinda is: Retrying...\n",
      "with_so_tool_calls, According : Retrying...\n",
      "with_so_tool_calls, Artie has : Retrying...\n",
      "with_so_tool_calls, Maria buys: Retrying...\n",
      "with_so_tool_calls, Sue lives : Retrying...\n",
      "with_so_tool_calls, In a room,: Retrying...\n",
      "with_so_tool_calls, A new prog: Retrying...\n",
      "with_so_tool_calls, Morisette : Retrying...\n",
      "with_so_tool_calls, Annabelle : Retrying...\n",
      "with_so_tool_calls, Every day,: Retrying...\n",
      "with_so_tool_calls, Nancy is c: Retrying...\n",
      "with_so_tool_calls, Jan has th: Retrying...\n",
      "with_so_tool_calls, Nick is ch: Retrying...\n",
      "with_so_tool_calls, A train tr: Retrying...\n",
      "with_so_tool_calls, Millie dec: Retrying...\n",
      "with_so_tool_calls, Elaine ini: Retrying...\n",
      "with_so_tool_calls, 15 gallons: Retrying...\n",
      "with_so_tool_calls, A salesman: Retrying...\n",
      "with_so_tool_calls, A factory : Retrying...\n",
      "with_so_tool_calls, Ben has 4 : Retrying...\n",
      "with_so_tool_calls, Toby is re: Retrying...\n",
      "with_so_tool_calls, Gabriel an: Retrying...\n",
      "with_so_tool_calls, Cody and T: Retrying...\n",
      "with_so_tool_calls, Hannah nee: Retrying...\n",
      "with_so_tool_calls, Artie has : Retrying...\n",
      "with_so_tool_calls, Adrian's a: Retrying...\n",
      "with_so_tool_calls, Judy bough: Retrying...\n",
      "with_so_tool_calls, Oscar has : Retrying...\n",
      "with_so_tool_calls, A marketin: Retrying...\n",
      "with_so_tool_calls, Morisette : Retrying...\n",
      "with_so_tool_calls, Marilyn wa: Retrying...\n",
      "with_so_tool_calls, James gets: Retrying...\n",
      "with_so_tool_calls, Sasha noti: Retrying...\n",
      "with_so_tool_calls, Ruiz can m: Retrying...\n",
      "with_so_tool_calls, Sue lives : Retrying...\n",
      "with_so_tool_calls, Ophelia an: Retrying...\n",
      "with_so_tool_calls, Wayne and : Retrying...\n",
      "with_so_tool_calls, Morisette : Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Every 2 mi: Retrying...\n",
      "with_so_tool_calls, Rong has b: Retrying...\n",
      "with_so_tool_calls, Colby want: Retrying...\n",
      "with_so_tool_calls, A train tr: Retrying...\n",
      "with_so_tool_calls, Gunther, t: Retrying...\n",
      "with_so_tool_calls, An ice cre: Retrying...\n",
      "with_so_tool_calls, A salesman: Retrying...\n",
      "with_so_tool_calls, A pencil c: Retrying...\n",
      "with_so_tool_calls, Carl has f: Retrying...\n",
      "with_so_tool_calls, It's straw: Retrying...\n",
      "with_so_tool_calls, Peter has : Retrying...\n",
      "with_so_tool_calls, John decid: Retrying...\n",
      "with_so_tool_calls, Cecelia we: Retrying...\n",
      "with_so_tool_calls, 90 single : Retrying...\n",
      "with_so_tool_calls, Jack decid: Retrying...\n",
      "with_so_tool_calls, Kris is tr: Retrying...\n",
      "with_so_tool_calls, Toby is re: Retrying...\n",
      "with_so_tool_calls, Jake's fam: Retrying...\n",
      "with_so_tool_calls, A food tru: Retrying...\n",
      "with_so_tool_calls, Joe has tw: Retrying...\n",
      "with_so_tool_calls, Mr. Smith : Retrying...\n",
      "with_so_tool_calls, Adrian's a: Retrying...\n",
      "with_so_tool_calls, A loaf of : Retrying...\n",
      "with_so_tool_calls, Mason is o: Retrying...\n",
      "with_so_tool_calls, To make a : Retrying...\n",
      "with_so_tool_calls, Peter has : Retrying...\n",
      "with_so_tool_calls, Rose is ou: Retrying...\n",
      "with_so_tool_calls, Adam wants: Retrying...\n",
      "with_so_tool_calls, A farmer h: Retrying...\n",
      "with_so_tool_calls, Marilyn wa: Retrying...\n",
      "with_so_tool_calls, James gets: Retrying...\n",
      "with_so_tool_calls, Andy's car: Retrying...\n",
      "with_so_tool_calls, Tanya is t: Retrying...\n",
      "with_so_tool_calls, Maria invi: Retrying...\n",
      "with_so_tool_calls, 15 gallons: Retrying...\n",
      "with_so_tool_calls, Nate is fe: Retrying...\n",
      "with_so_tool_calls, Tim has a : Retrying...\n",
      "with_so_tool_calls, James gets: Retrying...\n",
      "with_so_tool_calls, James love: Retrying...\n",
      "with_so_tool_calls, John drink: Retrying...\n",
      "with_so_tool_calls, There are : Retrying...\n",
      "with_so_tool_calls, Rong has b: Retrying...\n",
      "with_so_tool_calls, A salesman: Retrying...\n",
      "with_so_tool_calls, Seven bott: Retrying...\n",
      "with_so_tool_calls, Peter has : Retrying...\n",
      "with_so_tool_calls, Mr. Smith : Retrying...\n",
      "with_so_tool_calls, Cayley mar: Retrying...\n",
      "with_so_tool_calls, An ice cre: Retrying...\n",
      "with_so_tool_calls, A salesman: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Alisa bike: Retrying...\n",
      "with_so_tool_calls, Anne purch: Retrying...\n",
      "with_so_tool_calls, Nani is 8 : Retrying...\n",
      "with_so_tool_calls, Adrian's a: Retrying...\n",
      "with_so_tool_calls, Milly need: Retrying...\n",
      "with_so_tool_calls, Emily can : Retrying...\n",
      "with_so_tool_calls, Amy is tak: Retrying...\n",
      "with_so_tool_calls, As Sally w: Retrying...\n",
      "with_so_tool_calls, A $2000 wa: Retrying...\n",
      "with_so_tool_calls, The price : Retrying...\n",
      "with_so_tool_calls, John visit: Retrying...\n",
      "with_so_tool_calls, Maria invi: Retrying...\n",
      "with_so_tool_calls, A new arca: Retrying...\n",
      "with_so_tool_calls, Billy is v: Retrying...\n",
      "with_so_tool_calls, Adrian's a: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, John drink: Retrying...\n",
      "with_so_tool_calls, 15 gallons: Retrying...\n",
      "with_so_tool_calls, Melanie's : Retrying...\n",
      "with_so_tool_calls, Seven bott: Retrying...\n",
      "with_so_tool_calls, James gets: Retrying...\n",
      "with_so_tool_calls, Mr. Smith : Retrying...\n",
      "with_so_tool_calls, 15 gallons: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, A regular : Retrying...\n",
      "with_so_tool_calls, A food tru: Retrying...\n",
      "with_so_tool_calls, The glee c: Retrying...\n",
      "with_so_tool_calls, Jack decid: Retrying...\n",
      "with_so_tool_calls, James gets: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Madeline a: Retrying...\n",
      "with_so_tool_calls, The cheese: Retrying...\n",
      "with_so_tool_calls, Mr. Smith : Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, James gets: Retrying...\n",
      "with_so_tool_calls, Rose bough: Retrying...\n",
      "with_so_tool_calls, Helen went: Retrying...\n",
      "with_so_tool_calls, Tim wanted: Retrying...\n",
      "with_so_tool_calls, Conor live: Retrying...\n",
      "with_so_tool_calls, Ryan’s all: Retrying...\n",
      "with_so_tool_calls, Billy is v: Retrying...\n",
      "with_so_tool_calls, Michael is: Retrying...\n",
      "with_so_tool_calls, Dexter has: Retrying...\n",
      "with_so_tool_calls, A custodia: Retrying...\n",
      "with_so_tool_calls, Randy has : Retrying...\n",
      "with_so_tool_calls, Emily can : Retrying...\n",
      "with_so_tool_calls, Bob spends: Retrying...\n",
      "with_so_tool_calls, James gets: Retrying...\n",
      "with_so_tool_calls, The cheese: Retrying...\n",
      "with_so_tool_calls, As Sally w: Retrying...\n",
      "with_so_tool_calls, Seven bott: Retrying...\n",
      "with_so_tool_calls, Ryan’s all: Retrying...\n",
      "with_so_tool_calls, The glee c: Retrying...\n",
      "with_so_tool_calls, Helen went: Retrying...\n",
      "with_so_tool_calls, James gets: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Conor live: Retrying...\n",
      "with_so_tool_calls, Tim wanted: Retrying...\n",
      "with_so_tool_calls, Rose bough: Retrying...\n",
      "with_so_tool_calls, Seven bott: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Billy is v: Retrying...\n",
      "with_so_tool_calls, Milly need: Retrying...\n",
      "with_so_tool_calls, Billy is v: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Ryan’s all: Retrying...\n",
      "with_so_tool_calls, Emily can : Retrying...\n",
      "with_so_tool_calls, Tim wanted: Retrying...\n",
      "with_so_tool_calls, Cayley mar: Retrying...\n",
      "with_so_tool_calls, Ryan’s all: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Emily can : Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Tim wanted: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Rose bough: Retrying...\n",
      "with_so_tool_calls, Rose bough: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Milly need: Retrying...\n",
      "with_so_tool_calls, Milly need: Failed to process question after 3 attempts. Set answer to null.\n",
      "Error counts:\n",
      "- without_so: 4 processing errors, 0 key order errors\n",
      "- with_so_tool_calls: 163 processing errors, 0 key order errors\n",
      "- with_so_strict_tool_calls: 4 processing errors, 0 key order errors\n",
      "- with_so_json_mode: 4 processing errors, 0 key order errors\n",
      "\n",
      "Calculating confidence intervals (0.95) with 1319 observations:\n",
      "score_without_so - Mean: 94.31% CI: 93.06% - 95.56%\n",
      "score_with_so_tool_calls - Mean: 92.12% CI: 90.66% - 93.57%\n",
      "score_with_so_strict_tool_calls - Mean: 93.48% CI: 92.15% - 94.81%\n",
      "score_with_so_json_mode - Mean: 93.33% CI: 91.98% - 94.68%\n",
      "\n",
      "score_without_so vs score_with_so_tool_calls\n",
      "t-statistic: 3.274759816778192, p-value: 0.0010849871190684998\n",
      "score_without_so vs score_with_so_strict_tool_calls\n",
      "t-statistic: 1.605474133770892, p-value: 0.10862905530180934\n",
      "score_without_so vs score_with_so_json_mode\n",
      "t-statistic: 1.6655985284844081, p-value: 0.09603104670118999\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "evaluator = LLMEvaluator(\n",
    "    configs=CONFIGS,\n",
    "    create_prompt_fn=create_prompt_gsm8k_zero_shot,\n",
    "    parse_response_fn=parse_response_gsm8k,\n",
    "    response_model=ResponseGSM8K,\n",
    ")\n",
    "\n",
    "df = evaluator.generate_outputs(evals)\n",
    "df_results = evaluator.evaluate_outputs(df)\n",
    "\n",
    "evaluator.report_error_counts()\n",
    "evaluator.calculate_confidence_intervals(df_results)\n",
    "evaluator.run_paired_t_test(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177a5287",
   "metadata": {},
   "source": [
    "### Few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1072353e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without_so, Johnny's d: Retrying...\n",
      "without_so, Johnny's d: Retrying...\n",
      "with_so_tool_calls, 90 single : Retrying...\n",
      "with_so_tool_calls, One dwarf : Retrying...\n",
      "with_so_tool_calls, Anthony is: Retrying...\n",
      "with_so_tool_calls, The cheese: Retrying...\n",
      "with_so_tool_calls, One dwarf : Retrying...\n",
      "Error counts:\n",
      "- without_so: 3 processing errors, 0 key order errors\n",
      "- with_so_tool_calls: 9 processing errors, 0 key order errors\n",
      "- with_so_strict_tool_calls: 3 processing errors, 0 key order errors\n",
      "- with_so_json_mode: 5 processing errors, 0 key order errors\n",
      "\n",
      "Calculating confidence intervals (0.95) with 1319 observations:\n",
      "score_without_so - Mean: 93.86% CI: 92.56% - 95.16%\n",
      "score_with_so_tool_calls - Mean: 92.72% CI: 91.32% - 94.12%\n",
      "score_with_so_strict_tool_calls - Mean: 92.95% CI: 91.57% - 94.33%\n",
      "score_with_so_json_mode - Mean: 93.25% CI: 91.90% - 94.61%\n",
      "\n",
      "score_without_so vs score_with_so_tool_calls\n",
      "t-statistic: 1.954921383153301, p-value: 0.0508034766994856\n",
      "score_without_so vs score_with_so_strict_tool_calls\n",
      "t-statistic: 1.5007118113626086, p-value: 0.1336696247888939\n",
      "score_without_so vs score_with_so_json_mode\n",
      "t-statistic: 1.088738597031588, p-value: 0.2764682100216826\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "# Few-shot\n",
    "evaluator = LLMEvaluator(\n",
    "    configs=CONFIGS,\n",
    "    create_prompt_fn=create_prompt_gsm8k,\n",
    "    parse_response_fn=parse_response_gsm8k,\n",
    "    response_model=ResponseGSM8K,\n",
    ")\n",
    "\n",
    "df = evaluator.generate_outputs(evals)\n",
    "df_results = evaluator.evaluate_outputs(df)\n",
    "\n",
    "evaluator.report_error_counts()\n",
    "evaluator.calculate_confidence_intervals(df_results)\n",
    "evaluator.run_paired_t_test(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9883e6",
   "metadata": {},
   "source": [
    "## Last Letter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dca014",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2ae3a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "\n",
    "class ResponseLastLetter(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\", title=\"Response\")\n",
    "    reasoning: str = Field(description=\"step by step reasoning about the answer\")\n",
    "    answer: str = Field(description=\"final answer\")\n",
    "\n",
    "\n",
    "def create_prompt_last_letter(\n",
    "    question: str,\n",
    "    prompt_type: str,\n",
    "    response_model: ResponseLastLetter | None = None,\n",
    "    zero_shot: bool = False,\n",
    "):\n",
    "    if prompt_type in [\n",
    "        PromptType.WITH_JSON_MODE.value,\n",
    "        PromptType.WITH_TOOL_CALLS.value,\n",
    "        PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "    ]:\n",
    "        system_prompt = (\n",
    "            dedent(\"\"\"\n",
    "        You are an expert in solving simple word puzzles using reasoning steps. Your specific task is going to be to take a list of 4 names and reason about the last letter of each. Then, you will concatenate the last letters into a word. \n",
    "          \n",
    "        You will always respond with JSON matching the following schema:\n",
    "        \"\"\")\n",
    "            + json.dumps(response_model.model_json_schema(), indent=2)\n",
    "            + \"\"\"\\n\\nFirst, provide your step by step reasoning in the \"reasoning\" field. Then, in the \"answer\" field, provide the final answer. Don't include any other text in the \"answer\" field.\"\"\"\n",
    "        )\n",
    "    else:\n",
    "        system_prompt = dedent(\"\"\"\n",
    "        You are an expert in solving simple word puzzles using reasoning steps. Your specific task is going to be to take a list of 4 names and reason about the last letter of each. Then, you will concatenate the last letters into a word. \n",
    "        \n",
    "        You will always respond in the following format:\n",
    "        \n",
    "        <str, reasoning about the answer>\n",
    "        ANSWER: <str, final answer>\n",
    "        \n",
    "        First, provide your step by step reasoning. Then, in ANSWER, provide the final answer. Don't include any other text in ANSWER.\n",
    "        \"\"\")\n",
    "\n",
    "    fewshot_examples = [\n",
    "        (\n",
    "            \"Ian Peter Bernard Stephen\",\n",
    "            \"The last letter of 'Ian' is 'N'. The last letter of 'Peter' is 'R'. The last letter of 'Bernard' is 'D'. The last letter of 'Stephen' is 'N'. Concatenating them is 'NRDN'.\",\n",
    "            \"NRDN\",\n",
    "        ),\n",
    "        (\n",
    "            \"Javier Dylan Christopher Joseph\",\n",
    "            \"The last letter of 'Javier' is 'R'. The last letter of 'Dylan' is 'N'. The last letter of 'Christopher' is 'R'. The last letter of 'Joseph' is 'H'. Concatenating them is 'RNRH'.\",\n",
    "            \"RNRH\",\n",
    "        ),\n",
    "        (\n",
    "            \"Anthony Elizabeth Carlos Jesus\",\n",
    "            \"The last letter of 'Anthony' is 'Y'. The last letter of 'Elizabeth' is 'H'. The last letter of 'Carlos' is 'S'. The last letter of 'Jesus' is 'S'. Concatenating them is 'YHSS'.\",\n",
    "            \"YHSS\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    if not zero_shot:\n",
    "        system_prompt += \"\\nExamples:\" if fewshot_examples else \"\"\n",
    "        for i, (example_q, example_reason, example_ans) in enumerate(fewshot_examples):\n",
    "            system_prompt += f\"\\n\\n**{i+1}**\\nQuestion: Take the last letters of the words in '{example_q}' and concatenate them.\"\n",
    "            if prompt_type in [\n",
    "                PromptType.WITH_JSON_MODE.value,\n",
    "                PromptType.WITH_TOOL_CALLS.value,\n",
    "                PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "            ]:\n",
    "                response = (\n",
    "                    f'{{\"reasoning\": \"{example_reason}\", \"answer\": \"{example_ans}\"}}'\n",
    "                )\n",
    "            else:\n",
    "                response = f\"{example_reason}\\nANSWER: {example_ans}\"\n",
    "            system_prompt += f\"\\nAssistant Response:\\n{response}\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question: {question}\",\n",
    "        },\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "\n",
    "create_prompt_last_letter_zero_shot = partial(create_prompt_last_letter, zero_shot=True)\n",
    "\n",
    "\n",
    "def parse_response_last_letter(response: str, prompt_type: str) -> str | None:\n",
    "    if prompt_type in [\n",
    "        PromptType.WITH_JSON_MODE.value,\n",
    "        PromptType.WITH_TOOL_CALLS.value,\n",
    "        PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "    ]:\n",
    "        return ResponseLastLetter.model_validate_json(response).answer.lower()\n",
    "    else:\n",
    "        return response.split(\"\\nANSWER:\")[1].rstrip(\".\").strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bfacd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "dataset = load_dataset(\"ChilleD/LastLetterConcat\")\n",
    "evals = [\n",
    "    {\"question\": d[\"question\"], \"answer\": d[\"answer\"].lower()} for d in dataset[\"test\"]\n",
    "]\n",
    "\n",
    "if USE_SAMPLE:\n",
    "    evals = evals[:SAMPLE_SIZE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2287951b",
   "metadata": {},
   "source": [
    "### Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d36bc930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with_so_tool_calls, Take the l: Retrying...\n",
      "Error counts:\n",
      "- without_so: 0 processing errors, 0 key order errors\n",
      "- with_so_tool_calls: 1 processing errors, 0 key order errors\n",
      "- with_so_strict_tool_calls: 0 processing errors, 0 key order errors\n",
      "- with_so_json_mode: 0 processing errors, 0 key order errors\n",
      "\n",
      "Calculating confidence intervals (0.95) with 150 observations:\n",
      "score_without_so - Mean: 87.33% CI: 81.99% - 92.67%\n",
      "score_with_so_tool_calls - Mean: 88.00% CI: 82.78% - 93.22%\n",
      "score_with_so_strict_tool_calls - Mean: 87.33% CI: 81.99% - 92.67%\n",
      "score_with_so_json_mode - Mean: 90.00% CI: 85.18% - 94.82%\n",
      "\n",
      "score_without_so vs score_with_so_tool_calls\n",
      "t-statistic: -0.2175238112187401, p-value: 0.8280977462400442\n",
      "score_without_so vs score_with_so_strict_tool_calls\n",
      "t-statistic: 0.0, p-value: 1.0\n",
      "score_without_so vs score_with_so_json_mode\n",
      "t-statistic: -1.0, p-value: 0.3189317446414372\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "evaluator = LLMEvaluator(\n",
    "    configs=CONFIGS,\n",
    "    create_prompt_fn=create_prompt_last_letter_zero_shot,\n",
    "    parse_response_fn=parse_response_last_letter,\n",
    "    response_model=ResponseLastLetter,\n",
    ")\n",
    "\n",
    "df = evaluator.generate_outputs(evals)\n",
    "df_results = evaluator.evaluate_outputs(df)\n",
    "\n",
    "evaluator.report_error_counts()\n",
    "evaluator.calculate_confidence_intervals(df_results)\n",
    "evaluator.run_paired_t_test(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9486e99",
   "metadata": {},
   "source": [
    "### Few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "042dfdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error counts:\n",
      "- without_so: 0 processing errors, 0 key order errors\n",
      "- with_so_tool_calls: 0 processing errors, 0 key order errors\n",
      "- with_so_strict_tool_calls: 0 processing errors, 0 key order errors\n",
      "- with_so_json_mode: 0 processing errors, 0 key order errors\n",
      "\n",
      "Calculating confidence intervals (0.95) with 150 observations:\n",
      "score_without_so - Mean: 92.00% CI: 87.64% - 96.36%\n",
      "score_with_so_tool_calls - Mean: 94.67% CI: 91.06% - 98.27%\n",
      "score_with_so_strict_tool_calls - Mean: 93.33% CI: 89.33% - 97.34%\n",
      "score_with_so_json_mode - Mean: 90.00% CI: 85.18% - 94.82%\n",
      "\n",
      "score_without_so vs score_with_so_tool_calls\n",
      "t-statistic: -1.6422035742213938, p-value: 0.10265670235545438\n",
      "score_without_so vs score_with_so_strict_tool_calls\n",
      "t-statistic: -0.9999999999999999, p-value: 0.31893174464143725\n",
      "score_without_so vs score_with_so_json_mode\n",
      "t-statistic: 1.3452570755027768, p-value: 0.1805860455827262\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "# Few-shot\n",
    "evaluator = LLMEvaluator(\n",
    "    configs=CONFIGS,\n",
    "    create_prompt_fn=create_prompt_last_letter,\n",
    "    parse_response_fn=parse_response_last_letter,\n",
    "    response_model=ResponseLastLetter,\n",
    ")\n",
    "\n",
    "df = evaluator.generate_outputs(evals)\n",
    "df_results = evaluator.evaluate_outputs(df)\n",
    "\n",
    "evaluator.report_error_counts()\n",
    "evaluator.calculate_confidence_intervals(df_results)\n",
    "evaluator.run_paired_t_test(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bd6588",
   "metadata": {},
   "source": [
    "## Shuffled Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a56678",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00b7f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "\n",
    "class ResponseShuffledObjects(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\", title=\"Response\")\n",
    "    reasoning: str = Field(description=\"reasoning about the answer\")\n",
    "    answer: Literal[\"A\", \"B\", \"C\", \"D\", \"E\"] = Field(description=\"final answer\")\n",
    "\n",
    "\n",
    "def create_prompt_shuffled_objects(\n",
    "    question,\n",
    "    prompt_type: str,\n",
    "    response_model: ResponseShuffledObjects | None = None,\n",
    "    zero_shot: bool = False,\n",
    "):\n",
    "    if prompt_type in [\n",
    "        PromptType.WITH_JSON_MODE.value,\n",
    "        PromptType.WITH_TOOL_CALLS.value,\n",
    "        PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "    ]:\n",
    "        system_prompt = (\n",
    "            dedent(\"\"\"\n",
    "        You are an expert in performing common sense tasks involving the ordering of a sequence of events.\n",
    "        Each question will present you with a sequence of events involving 5 people (switching objects, partners, positions, etc.). Your task is to determine the correct answer from the options provided.\n",
    "          \n",
    "        You will always respond with JSON matching the following schema:\n",
    "        \"\"\")\n",
    "            + json.dumps(response_model.model_json_schema(), indent=2)\n",
    "            + \"\"\"\\n\\nFirst, provide your reasoning in the \"reasoning\" field. Then, in the \"answer\" field, provide only the single letter representing the correct choice you are presented with. Don't include any other text in the \"answer\" field.\"\"\"\n",
    "        )\n",
    "    else:\n",
    "        system_prompt = dedent(\"\"\"\n",
    "        You are an expert in performing common sense tasks involving the ordering of a sequence of events.\n",
    "        Each question will present you with a sequence of events involving 5 people (switching objects, partners, positions, etc.). Your task is to determine the correct answer from the options provided.\n",
    "        \n",
    "        You will always respond in the following format:\n",
    "        \n",
    "        <str, reasoning about the answer>\n",
    "        ANSWER: <str, final answer>\n",
    "        \n",
    "        First, provide your step by step reasoning. Then, in ANSWER, provide only the single letter representing the correct choice you are presented with. Don't include any other text in ANSWER.\n",
    "        \"\"\")\n",
    "\n",
    "    fewshot_examples = [\n",
    "        (\n",
    "            \"Alice, Bob, Claire, Dave, and Eve are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Patrick, Bob is dancing with Sam, Claire is dancing with Jamie, Dave is dancing with Lola, and Eve is dancing with Melissa.\\nThroughout the song, the dancers often trade partners. First, Dave and Eve switch partners. Then, Dave and Alice switch partners. Then, Eve and Alice switch partners. Then, Claire and Bob switch partners. Finally, Dave and Alice switch partners. At the end of the dance, Alice is dancing with\\nOptions:\\n(A) Patrick\\n(B) Sam\\n(C) Jamie\\n(D) Lola\\n(E) Melissa\",\n",
    "            \"Dave and Eve switch partners, so Dave's partner is now Melissa and Eve's partner is now Lola. Then Dave and Alice switch partners so Dave's partner is now Patrick and Alice's partner is now Melissa. Then Eve and Alice switch partners so Eve's partner is now Melissa and Alice's partner is now Lola. Then Claire and Bob switch patners so Claire's partner is now Sam, and Bob's partner is now Jamie. Finally, Dave and Alice switch partners so Dave's new partner is Lola, and Alice's new partner is Patrick. Alice is dance in with Patrick, choice A.\",\n",
    "            \"A\",\n",
    "        ),\n",
    "        (\n",
    "            \"Alice, Bob, Claire, Dave, and Eve are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Ophelia, Bob is dancing with Jamie, Claire is dancing with Melissa, Dave is dancing with Rodrigo, and Eve is dancing with Patrick.\\nThroughout the song, the dancers often trade partners. First, Claire and Bob switch partners. Then, Claire and Eve switch partners. Then, Claire and Bob switch partners. Then, Eve and Dave switch partners. Finally, Claire and Alice switch partners. At the end of the dance, Alice is dancing with\\nOptions:\\n(A) Ophelia\\n(B) Jamie\\n(C) Melissa\\n(D) Rodrigo\\n(E) Patrick\",\n",
    "            \"Claire and Bob switch partners, so Claire's partner is now Jamie and Bob's partner is now Melissa. Then, Claire and Eve switch partners, so Claire's partner becomes Patrick and Eve's partner becomes Jamie. Next, Claire and Bob switch partners again, making Claire's partner Melissa and Bob's partner Patrick. After that, Eve and Dave switch partners, resulting in Eve's partner being Rodrigo and Dave's partner being Jamie. Finally, Claire and Alice switch partners, so Claire's partner is now Ophelia and Alice's partner becomes Melissa. Alice is dancing with Melissa, which is choice C.\",\n",
    "            \"C\",\n",
    "        ),\n",
    "        (\n",
    "            \"Alice, Bob, Claire, Dave, and Eve are friends and avid readers who occasionally trade books. At the start of the semester, they each buy one new book: Alice gets Catch-22, Bob gets Hound of the Baskervilles, Claire gets Frankenstein, Dave gets The Pearl, and Eve gets The Fellowship of the Ring.\\nAs the semester proceeds, they start trading around the new books. First, Eve and Alice swap books. Then, Alice and Claire swap books. Then, Alice and Bob swap books. Then, Dave and Alice swap books. Finally, Dave and Claire swap books. At the end of the semester, Dave has\\nOptions:\\n(A) Catch-22\\n(B) Hound of the Baskervilles\\n(C) Frankenstein\\n(D) The Pearl\\n(E) The Fellowship of the Ring\",\n",
    "            \"First, Eve and Alice swap, so Alice gets The Fellowship of the Ring and Eve gets Catch-22. Next, Alice and Claire swap, giving Claire The Fellowship of the Ring and Alice Frankenstein. Then, Alice and Bob swap, resulting in Bob holding Frankenstein and Alice having Hound of the Baskervilles. Dave and Alice then swap, so Dave takes Hound of the Baskervilles and Alice receives The Pearl. Finally, Dave and Claire swap books, which means Dave takes The Fellowship of the Ring from Claire. Therefore, at the end of all the swaps, Dave possesses The Fellowship of the Ring, making option E the correct answer.\",\n",
    "            \"E\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    if not zero_shot:\n",
    "        system_prompt += \"\\nExamples:\" if fewshot_examples else \"\"\n",
    "        for i, (example_q, example_reason, example_ans) in enumerate(fewshot_examples):\n",
    "            system_prompt += f\"\\n\\n**{i+1}**\\nQuestion: {example_q}\"\n",
    "\n",
    "            if prompt_type in [\n",
    "                PromptType.WITH_JSON_MODE.value,\n",
    "                PromptType.WITH_TOOL_CALLS.value,\n",
    "                PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "            ]:\n",
    "                response = (\n",
    "                    f'{{\"reasoning\": \"{example_reason}\", \"answer\": \"{example_ans}\"}}'\n",
    "                )\n",
    "            else:\n",
    "                response = f\"{example_reason}\\nANSWER: {example_ans}\"\n",
    "            system_prompt += f\"\\nAssistant Response:\\n{response}\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question: {question}\",\n",
    "        },\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "\n",
    "create_prompt_shuffled_objects_zero_shot = partial(\n",
    "    create_prompt_shuffled_objects, zero_shot=True\n",
    ")\n",
    "\n",
    "\n",
    "def parse_response_shuffled_objects(response: str, prompt_type: str) -> str:\n",
    "    if prompt_type in [\n",
    "        PromptType.WITH_JSON_MODE.value,\n",
    "        PromptType.WITH_TOOL_CALLS.value,\n",
    "        PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "    ]:\n",
    "        return ResponseShuffledObjects.model_validate_json(response).answer\n",
    "    else:\n",
    "        return response.split(\"\\nANSWER:\")[1].rstrip(\".\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11d87fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"openeval/BIG-Bench-Hard\", data_files=\"tracking_shuffled_objects_five_objects.json\"\n",
    ")\n",
    "evals = [\n",
    "    {\n",
    "        \"question\": d[\"input\"],\n",
    "        \"answer\": d[\"target\"].replace(\"(\", \"\").replace(\")\", \"\").strip(),\n",
    "    }\n",
    "    for d in dataset[\"train\"][\"examples\"][0][4:]  # first 3 are few-shot examples\n",
    "]\n",
    "\n",
    "if USE_SAMPLE:\n",
    "    evals = evals[:SAMPLE_SIZE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561f7214",
   "metadata": {},
   "source": [
    "### Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89cf3a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Alice, Bob: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Alice, Bob: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Alice, Bob: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Alice, Bob: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Failed to process question after 3 attempts. Set answer to null.\n",
      "Error counts:\n",
      "- without_so: 0 processing errors, 0 key order errors\n",
      "- with_so_tool_calls: 120 processing errors, 0 key order errors\n",
      "- with_so_strict_tool_calls: 0 processing errors, 0 key order errors\n",
      "- with_so_json_mode: 0 processing errors, 0 key order errors\n",
      "\n",
      "Calculating confidence intervals (0.95) with 246 observations:\n",
      "score_without_so - Mean: 95.12% CI: 92.42% - 97.82%\n",
      "score_with_so_tool_calls - Mean: 79.67% CI: 74.64% - 84.71%\n",
      "score_with_so_strict_tool_calls - Mean: 89.84% CI: 86.05% - 93.62%\n",
      "score_with_so_json_mode - Mean: 81.71% CI: 76.87% - 86.55%\n",
      "\n",
      "score_without_so vs score_with_so_tool_calls\n",
      "t-statistic: 5.46626632790313, p-value: 1.1303262854838998e-07\n",
      "score_without_so vs score_with_so_strict_tool_calls\n",
      "t-statistic: 2.438180390801807, p-value: 0.015472993332472403\n",
      "score_without_so vs score_with_so_json_mode\n",
      "t-statistic: 4.725319750946163, p-value: 3.874827774586541e-06\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "evaluator = LLMEvaluator(\n",
    "    configs=CONFIGS,\n",
    "    create_prompt_fn=create_prompt_shuffled_objects_zero_shot,\n",
    "    parse_response_fn=parse_response_shuffled_objects,\n",
    "    response_model=ResponseShuffledObjects,\n",
    ")\n",
    "\n",
    "df = evaluator.generate_outputs(evals)\n",
    "df_results = evaluator.evaluate_outputs(df)\n",
    "\n",
    "evaluator.report_error_counts()\n",
    "evaluator.calculate_confidence_intervals(df_results)\n",
    "evaluator.run_paired_t_test(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2607131",
   "metadata": {},
   "source": [
    "### Few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5951ae92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error counts:\n",
      "- without_so: 0 processing errors, 0 key order errors\n",
      "- with_so_tool_calls: 0 processing errors, 0 key order errors\n",
      "- with_so_strict_tool_calls: 0 processing errors, 0 key order errors\n",
      "- with_so_json_mode: 0 processing errors, 0 key order errors\n",
      "\n",
      "Calculating confidence intervals (0.95) with 246 observations:\n",
      "score_without_so - Mean: 92.68% CI: 89.42% - 95.94%\n",
      "score_with_so_tool_calls - Mean: 69.51% CI: 63.75% - 75.28%\n",
      "score_with_so_strict_tool_calls - Mean: 65.85% CI: 59.92% - 71.79%\n",
      "score_with_so_json_mode - Mean: 62.60% CI: 56.54% - 68.66%\n",
      "\n",
      "score_without_so vs score_with_so_tool_calls\n",
      "t-statistic: 7.122048231999389, p-value: 1.177479832106033e-11\n",
      "score_without_so vs score_with_so_strict_tool_calls\n",
      "t-statistic: 8.214453009163616, p-value: 1.2335890894979526e-14\n",
      "score_without_so vs score_with_so_json_mode\n",
      "t-statistic: 8.972741031204352, p-value: 7.740615855587701e-17\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "evaluator = LLMEvaluator(\n",
    "    configs=CONFIGS,\n",
    "    create_prompt_fn=create_prompt_shuffled_objects,\n",
    "    parse_response_fn=parse_response_shuffled_objects,\n",
    "    response_model=ResponseShuffledObjects,\n",
    ")\n",
    "\n",
    "df = evaluator.generate_outputs(evals)\n",
    "df_results = evaluator.evaluate_outputs(df)\n",
    "\n",
    "evaluator.report_error_counts()\n",
    "evaluator.calculate_confidence_intervals(df_results)\n",
    "evaluator.run_paired_t_test(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
