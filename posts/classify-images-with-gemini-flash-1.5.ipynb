{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Classifying images with Gemini Flash 1.5\"\n",
    "date: \"09/08/2024\"\n",
    "date-modified: last-modified\n",
    "description-meta: \"Learn how to use In-Context Learning (ICL) to classify images using Gemini Flash 1.5\"\n",
    "toc: true\n",
    "toc-depth: 3\n",
    "lightbox: true\n",
    "fig-cap-location: margin\n",
    "categories:\n",
    "  - mllms\n",
    "  - in-context-learning\n",
    "  - gemini\n",
    "author:\n",
    "  - name: Dylan Castillo\n",
    "    url: https://dylancastillo.co\n",
    "    affiliation: Iwana Labs\n",
    "    affiliation-url: https://iwanalabs.com\n",
    "    citation: true\n",
    "    comments:\n",
    "    utterances:\n",
    "    repo: dylanjcastillo/blog_comments\n",
    "    theme: dark-blue\n",
    "    issue-term: pathname\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most people think of [In-Context Learning (ICL)](https://arxiv.org/abs/2301.00234) — the ability of LLMs to learn from examples provided in the context — only as a component of RAG applications. \n",
    "\n",
    "I used to think of it that way too. Until I recently found out that Multimodal Large Language Models (MLLMs) with ICL can be used to more traditional ML tasks like image classification (fixed set) \n",
    "\n",
    "I was skeptical at first, but I was pleasantly surprised to see that it worked pretty well in the literature (see [here](https://arxiv.org/abs/2405.09798) and [here](https://arxiv.org/abs/2403.07407)) and in my own experiments.\n",
    "\n",
    "\n",
    "You shouldn't expect state-of-the-art results with it, but it can often give you pretty good results with very little effort and data.\n",
    "\n",
    "In this tutorial, you'll see how to use ICL to classify images using Gemini Flash 1.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Gemini Flash 1.5?\n",
    "\n",
    "You can use any MLLM for this task, but I like Gemini Flash 1.5 because:\n",
    "\n",
    "1. It's cheaper than [Gemini Pro 1.5](https://ai.google.dev/pricing), [GPT-4o](https://platform.openai.com/pricing), and [Sonnet 3.5](https://docs.anthropic.com/en/docs/build-with-claude/vision#calculate-image-costs). For an image of 512x512 pixels, Gemini Flash 1.5 is 50x cheaper than Gemini Pro 1.5, 5x to 16x cheaper than GPT-4o, and 26x cheaper than Sonnet 3.5[^longnote].\n",
    "2. It lets you use up to 3,000 images per request. By trial and error, I found that GPT-4o seems to have a hard limit at 250 images per request and Sonnet 3.5's documentation mentions a limit of 20 images per request.\n",
    "3. It works well enough for this task. If you really want to squeeze the last bit of performance out of your model, you can use a bigger model, but for the purposes of this tutorial, Gemini Flash 1.5 will do just fine.\n",
    "\n",
    "Regardless of the model you choose, this tutorial will be a good starting point for you to classify images using ICL.\n",
    "\n",
    "[^longnote]: Estimated costs as of September 8, 2024:\n",
    "\n",
    "    | Model | Cost (512x512 image) |\n",
    "    |-------|------------------------|\n",
    "    | Gemini Flash 1.5 | $0.000039 |\n",
    "    | Gemini Pro 1.5 | $0.0018 |\n",
    "    | GPT-4o | $0.000213 - $0.000638 |\n",
    "    | Sonnet 3.5 | $0.001047 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To follow this tutorial you'll need to:\n",
    "\n",
    "1. Sign up and generate an API key in [Google AI Studio](https://aistudio.google.com/app/apikey)\n",
    "2. Set the API key as an environment variable called `GEMINI_API_KEY`.\n",
    "3. Download [this dataset](https://www.kaggle.com/datasets/gpiosenka/butterfly-images40-species?resource=download) and save it to `data/`.\n",
    "4. Create a virtual environment and install the requirements:\n",
    "\n",
    "```bash\n",
    "python -m venv venv\n",
    "source venv/bin/activate\n",
    "pip install pandas numpy scikit-learn google-generativeai pillow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| output: false\n",
    "#| echo: false\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, you start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dcast/Documents/GitHub/blog/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| output: false\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import google.generativeai as genai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the usual popular libraries (e.g. `pandas`, `sklearn`), you'll need:\n",
    "\n",
    "- `google.generativeai` for interacting with the Gemini API\n",
    "- `PIL` for handling images\n",
    "- `sklearn` for calculating performance metrics\n",
    "\n",
    "Then, you'll need to configure the Gemini API client with your API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take the `GEMINI_API_KEY` environment variable and use it to authenticate your requests to the Gemini API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do a fair evaluation of the performance of the model, you'll split the dataset into training and testing sets.\n",
    "\n",
    "Training, in this context, means the process of providing the model with a set of images and asking it to learn from them at inference time. So it's not training in the traditional sense of updating the model's weights.\n",
    "\n",
    "You'll grab a random selection of `n_images_icl` images per class from the training set and use them to provide context to the model when making predictions on the test set. To measure the performance, you'll use all the available images from those classes in the testing set.\n",
    "\n",
    "This function will do just that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(train_dir, test_dir, selected_classes, n_images_icl=3):\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "\n",
    "    for class_id, class_name in enumerate(selected_classes):\n",
    "        train_class_dir = train_dir / class_name\n",
    "        test_class_dir = test_dir / class_name\n",
    "\n",
    "        if not train_class_dir.is_dir() or not test_class_dir.is_dir():\n",
    "            continue\n",
    "\n",
    "        # Train dataset\n",
    "        train_image_files = list(train_class_dir.glob(\"*.jpg\"))\n",
    "        selected_train_images = np.random.choice(\n",
    "            train_image_files,\n",
    "            size=min(n_images_icl, len(train_image_files)),\n",
    "            replace=False,\n",
    "        )\n",
    "        for img_path in selected_train_images:\n",
    "            train_data.append(\n",
    "                {\n",
    "                    \"image_path\": str(img_path),\n",
    "                    \"class_id\": f\"class_{class_id}\",\n",
    "                    \"class_name\": class_name,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Test dataset\n",
    "        test_image_files = list(test_class_dir.glob(\"*.jpg\"))\n",
    "        for img_path in test_image_files:\n",
    "            test_data.append(\n",
    "                {\n",
    "                    \"image_path\": str(img_path),\n",
    "                    \"class_id\": f\"class_{class_id}\",\n",
    "                    \"class_name\": class_name,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df_train = pd.DataFrame(train_data)\n",
    "    df_test = pd.DataFrame(test_data).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can read the data and create the datasets (using 15 classes and 1 image per class in the context):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/\"\n",
    "TRAIN_DIR = Path(DATA_DIR) / \"train\"\n",
    "TEST_DIR = Path(DATA_DIR) / \"test\"\n",
    "\n",
    "all_classes = list(os.listdir(TRAIN_DIR))\n",
    "selected_classes = np.random.choice(all_classes, size=15, replace=False)\n",
    "\n",
    "df_train, df_test = create_datasets(TRAIN_DIR, TEST_DIR, selected_classes=selected_classes, n_images_icl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini Flash 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you'll need to put together all the pieces you'll need to generate a response from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you'll start by defining a simple system prompt that will tell the model how to classify the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFIER_SYSTEM_PROMPT = \"\"\"You are an expert lepidopterist.\n",
    "\n",
    "Your task is to classify images of butterflies into one of the provided labels.\n",
    "\n",
    "Provide your output as a JSON object using this format:\n",
    "\n",
    "{\n",
    "    \"number_of_labeled_images\": <integer>,\n",
    "    \"output\": [\n",
    "        {\n",
    "            \"image_id\": <image id, integer, starts at 0>,\n",
    "            \"confidence\": <number between 0 and 10, the higher the more confident, integer>,\n",
    "            \"label\": <label of the correct butterfly species, string>\n",
    "        }, \n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "\n",
    "## Guidelines\n",
    "\n",
    "- ALWAYS produce valid JSON.\n",
    "- Generate ONLY a single prediction per input image.\n",
    "- The `number_of_labeled_images` MUST be the same as the number of input images.\n",
    "\n",
    "This is an example of a valid output:\n",
    "```\n",
    "{\n",
    "  \"number_of_labeled_images\": 5,\n",
    "  \"output\": [\n",
    "      {\n",
    "        \"image_id\": 0,\n",
    "        \"confidence\": 10,\n",
    "        \"correct_label\": \"class_B\"\n",
    "      },\n",
    "      {\n",
    "        \"image_id\": 1,\n",
    "        \"confidence\": 9,\n",
    "        \"correct_label\": \"class_C\"\n",
    "      },\n",
    "      {\n",
    "        \"image_id\": 2,\n",
    "        \"confidence\": 4,\n",
    "        \"correct_label\": \"class_A\"\n",
    "      },\n",
    "      {\n",
    "        \"image_id\": 3,\n",
    "        \"confidence\": 2,\n",
    "        \"correct_label\": \"class_B\"\n",
    "      },\n",
    "      {\n",
    "        \"image_id\": 4,\n",
    "        \"confidence\": 10,\n",
    "        \"correct_label\": \"class_C\"\n",
    "      }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prompt explains the task to the model: you're providing it with a set of labels with corresponding images, and a set of images that should be classified into one of those labels. The model needs to output a single label for each image.\n",
    "\n",
    "I included an additional field called `number_of_labeled_images` because I noticed that the model would often \"forget\" to include all the labels in the output, and this was a simple way to ensure that it did so.\n",
    "\n",
    "::: {.callout-note}\n",
    "Fun fact: I didn't know that *lepidopterist* was a word until I wrote this prompt.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can define and configure the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "  \"temperature\": 1,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"application/json\",\n",
    "}\n",
    "classification_model = genai.GenerativeModel(\n",
    "    \"gemini-1.5-flash\", \n",
    "    system_instruction=CLASSIFIER_SYSTEM_PROMPT, \n",
    "    generation_config=generation_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does a few things:\n",
    "\n",
    "- It sets the temperature to 1, which controls the randomness of the model's output.\n",
    "- It sets the max_output_tokens to 8192, which is the maximum number of tokens the model can output.\n",
    "- It sets the response_mime_type to \"application/json\", which tells the model to output JSON.\n",
    "- It sets the system_instruction to the prompt you defined earlier.\n",
    "- It uses `gemini-1.5-flash` as the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the context "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini has its own way of building the context or messages you pass to it. You can use these functions for that: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_images_message(df):\n",
    "    messages = [\"Possible labels:\"]\n",
    "    grouped = df.groupby('class_id')\n",
    "    for class_id, group in grouped:\n",
    "        for _, row in group.iterrows():\n",
    "            base64_img = Image.open(row[\"image_path\"])\n",
    "            messages.append(base64_img)\n",
    "        messages.append(f\"label: {class_id}\")\n",
    "    return messages\n",
    "    \n",
    "context_images_message = create_context_images_message(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you'll create a message with the context images and their corresponding labels. This is the \"training\" part of ICL.\n",
    "\n",
    "In `create_context_images_message`, you're iterating over the training dataset, grouping the images by class and appending the images and labels to the messages list.\n",
    "\n",
    "The resulting message will look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Possible labels:',\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224>,\n",
       " 'label: class_0',\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224>,\n",
       " 'label: class_1']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_images_message[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you'll create a message with the input images. This are the images for which the model will generate predictions.\n",
    "\n",
    "Simlar to the context images message, you're iterating over the test dataset and appending the images to the messages list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_images_message(df):\n",
    "    messages = [\"Input images:\"]\n",
    "    for i, image_path in enumerate(df.image_path):\n",
    "        base64_img = Image.open(image_path)\n",
    "        image_message = [\n",
    "            base64_img,\n",
    "            f\"input_image_id: {i}\",\n",
    "        ]\n",
    "        messages.extend(image_message)\n",
    "    messages.append(f\"Please correctly classify all {df.shape[0]} images.\")\n",
    "    return messages\n",
    "\n",
    "input_images_message = create_input_images_message(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting message will look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Input images:',\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224>,\n",
       " 'input_image_id: 0',\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224>,\n",
       " 'input_image_id: 1']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_images_message[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can combine the context images message and the input images message to create the contents you'll pass to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = context_images_message + input_images_message\n",
    "response = classification_model.generate_content(\n",
    "    contents=contents\n",
    ")\n",
    "response_json = json.loads(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It'll take a few seconds to run. But after that you'll have a JSON response with the model's predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image_id': 0, 'confidence': 10, 'label': 'class_7'},\n",
       " {'image_id': 1, 'confidence': 10, 'label': 'class_2'},\n",
       " {'image_id': 2, 'confidence': 10, 'label': 'class_4'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_json[\"output\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can calculate the accuracy and F1-score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7333\n",
      "F1-score: 0.7229\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(df_test, response_json):\n",
    "    predictions = [item['label'] for item in response_json['output']]\n",
    "    accuracy = accuracy_score(df_test.class_id, predictions)\n",
    "    f1 = f1_score(df_test.class_id, predictions, average='weighted')\n",
    "    return accuracy, f1\n",
    "\n",
    "accuracy, f1 = calculate_metrics(df_test, response_json)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a single image in the context per class, you should get an accuracy around 82% and F1-score around 77%. Which isn't bad, but you can probably do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using 3 images per class in the context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One quick way to improve the performance of the model is to use more images per class in the context. Try with 5 images per class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9067\n",
      "F1-score: 0.9013\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = create_datasets(TRAIN_DIR, TEST_DIR, selected_classes=selected_classes, n_images_icl=5)\n",
    "\n",
    "# Create the context and input messages\n",
    "context_images_message = create_context_images_message(df_train)\n",
    "input_images_message = create_input_images_message(df_test)\n",
    "contents = context_images_message + input_images_message\n",
    "\n",
    "\n",
    "# Generate the response\n",
    "response = classification_model.generate_content(\n",
    "    contents=contents\n",
    ")\n",
    "response_json = json.loads(response.text)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy, f1 = calculate_metrics(df_test, response_json)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this change, you should get an accuracy and F1-score around 90%. \n",
    "\n",
    "Not bad for something that you've built in less than 10 minutes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data leakage and baseline performance\n",
    "\n",
    "But before we finish, let's address the elephant in the room: data leakage.\n",
    "\n",
    "You might be thinking, \"MLLMs have been trained on a lot of data, so they already know a lot of the images in the dataset, which means that these results are inflated\".\n",
    "\n",
    "Which is a good point, and for that purpose I've done two things:\n",
    "\n",
    "1. Anonymize the names of the classes (e.g., `class_0` instead of `Sleepy Orange`), so that the model doesn't have any information about the actual labels. \n",
    "2. Run a quick experiment using a zero-shot model (i.e., without providing any context images) to see the model's performance.\n",
    "\n",
    "Here's the code for the zero-shot baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4800\n",
      "F1-score: 0.4619\n"
     ]
    }
   ],
   "source": [
    "possible_labels = \"Possible labels: \" + \", \".join(df_train.class_name.unique().tolist())\n",
    "class_name_to_id = dict(zip(df_train['class_name'], df_train['class_id']))\n",
    "\n",
    "response = classification_model.generate_content(\n",
    "    contents=[possible_labels] + input_images_message\n",
    ")\n",
    "response_json = json.loads(response.text)\n",
    "\n",
    "for item in response_json[\"output\"]:\n",
    "    item['label'] = class_name_to_id.get(item['label'], item['label'])\n",
    "\n",
    "accuracy, f1 = calculate_metrics(df_test, response_json)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should achieve a 48% accuracy and a 46% F1-score. Both significantly higher than the 6.66% you'd expect from random guessing, but still far from the 90%+ accuracy you obtained earlier.\n",
    "\n",
    "These results demonstrate that ICL can indeed enhance the model's performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all! \n",
    "\n",
    "You've just used Gemini Flash 1.5 and ICL to classify images. I still find it amazing that without any \"real\" training and with just a few minutes of work, you can achieve pretty good results in a non-trivial image classification task.\n",
    "\n",
    "This a mostly unexplored area, so there's a lot of room for trying out different ideas and seeing what works best. This tutorial is just a starting point.\n",
    "\n",
    "Hope you found it useful! Let me know if you have any questions in the comments below."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
