{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9548f597",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Synthetic data for RAG\"\n",
    "date: 2025-07-11\n",
    "date-modified: 2025-07-11\n",
    "description-meta: \"How to use synthetic data to build a RAG system\"\n",
    "categories:\n",
    "  - llm\n",
    "  - python\n",
    "  - rag \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2a708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58b9d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import random\n",
    "from textwrap import dedent\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import Client, traceable\n",
    "from pydantic import BaseModel\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a8c9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\n",
    "    \"../data/synthetic-data-rag/people-group/\", glob=\"**/*.md\", loader_cls=TextLoader\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2fe2a3",
   "metadata": {},
   "source": [
    "## Index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7df6e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_ef = OpenAIEmbeddingFunction(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "client = chromadb.PersistentClient()\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    \"gitlab-handbook\", embedding_function=openai_ef\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10523d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownTextSplitter\n",
    "\n",
    "text_splitter = MarkdownTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4o\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cc250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(ids, documents, metadatas, batch_size=100):\n",
    "    batches = []\n",
    "    for i in range(0, len(ids), batch_size):\n",
    "        batch_ids = ids[i : i + batch_size]\n",
    "        batch_documents = documents[i : i + batch_size]\n",
    "        batch_metadatas = metadatas[i : i + batch_size]\n",
    "        batches.append((batch_ids, batch_metadatas, batch_documents))\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3287eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [f\"{str(i)}\" for i in range(len(splits))]\n",
    "documents = [doc.page_content for doc in splits]\n",
    "metadatas = [doc.metadata for doc in splits]\n",
    "\n",
    "try:\n",
    "    client.get_collection(\"gitlab-handbook\")\n",
    "    print(\"Collection already exists, skipping creation.\")\n",
    "except Exception:\n",
    "    batches = create_batches(ids=ids, documents=documents, metadatas=metadatas)\n",
    "    for i, batch in enumerate(batches):\n",
    "        print(f\"Adding batch {i} of size {len(batch[0])}\")\n",
    "        collection.add(ids=batch[0], metadatas=batch[1], documents=batch[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0baae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk(BaseModel):\n",
    "    path: str\n",
    "    page_content: str\n",
    "\n",
    "\n",
    "@traceable\n",
    "def get_similar_chunks(chunk, n_results: int = 5) -> list[Chunk]:\n",
    "    results = collection.query(query_texts=[chunk.page_content], n_results=20)\n",
    "    random_chunks = random.sample(range(20), n_results)\n",
    "    docs = [results[\"documents\"][0][i] for i in random_chunks]\n",
    "    metadatas = [results[\"metadatas\"][0][i] for i in random_chunks]\n",
    "    return [Chunk(path=m[\"source\"], page_content=d) for d, m in zip(docs, metadatas)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa4ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chunks(chunks: list[Chunk]) -> str:\n",
    "    return \"\\n\".join(\n",
    "        [f\"*** Filepath: {chunk.path} ***\\n{chunk.page_content}\\n\" for chunk in chunks]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf323e",
   "metadata": {},
   "source": [
    "## Generate QA Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582a5b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_generate = dedent(\n",
    "    \"\"\"\n",
    "    You are a helpful assistant generating synthetic QA pairs for retrieval evaluation.\n",
    "\n",
    "    Given a target chunk of text and a set of confounding chunks, you must extract a specific, self-contained fact from the target chunk that is not included in the confounding chunks. Then write a question that is directly and unambiguously answered by that fact. The question should only be answered by the fact extracted from the target chunk (and not by any of the confounding chunks) but it should also use themes or terminology that is present in the confounding chunks.\n",
    "\n",
    "    Always respond with a JSON object with the following keys (in that exact order):\n",
    "    1. \"fact\": \"<the fact extracted from the target chunk>\",\n",
    "    2. \"confounding_terms\": \"<a list of terms or themes from the confounding chunks that are relevant to the question>\",\n",
    "    3. \"question\": \"<the question that is directly and unambiguously answered by the fact>\",\n",
    "    \n",
    "    You should write the questions as if you're an employee looking for information in the handbook. The question should be as realistic and natural as possible, reflecting the kind of queries an employee might actually make when searching for information in the handbook.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "user_prompt_generate = dedent(\n",
    "    \"\"\"\n",
    "    TARGET CHUNK:\n",
    "    {target_chunk}\n",
    "\n",
    "    CONFOUNDING CHUNKS:\n",
    "    {confounding_chunks} \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aab841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response(BaseModel):\n",
    "    fact: str\n",
    "    confounding_terms: list[str] = []\n",
    "    question: str\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=1)\n",
    "llm_with_structured_output = llm.with_structured_output(Response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d155b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "messages = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt_generate), (\"user\", user_prompt_generate)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248ec536",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "async def generate_qa_pair(random_chunk):\n",
    "    similar_chunks = get_similar_chunks(random_chunk)\n",
    "    compiled_messages = await messages.ainvoke(\n",
    "        {\n",
    "            \"target_chunk\": format_chunks([similar_chunks[0]]),\n",
    "            \"confounding_chunks\": format_chunks(similar_chunks[1:]),\n",
    "        }\n",
    "    )\n",
    "    output = await llm_with_structured_output.ainvoke(compiled_messages)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3cf7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_splits = [random.choice(splits) for _ in range(200)]\n",
    "tasks = [generate_qa_pair(random_split) for random_split in random_splits]\n",
    "qa_pairs = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ba9fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([qa_pair.dict() for qa_pair in qa_pairs])\n",
    "df.to_excel(\"qa_pairs.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38ffcc1",
   "metadata": {},
   "source": [
    "## Filtering QA pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeec315",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_filtering = dedent(\n",
    "    \"\"\"\n",
    "You are an AI assistant helping us curate a high-quality dataset of questions for evaluating an company's internal handbook. We have generated synthetic questions and need to filter out those that are unrealistic or not representative of typical user queries.\n",
    "\n",
    "Here are examples of realistic and unrealistic user queries we have manually rated:\n",
    "\n",
    "### Realistic Queries (Good Examples)\n",
    "\n",
    "* **Query:** \"What is the required process for creating a new learning hub for your team in Level Up at GitLab?\"\n",
    "    * **Explanation:** Very realistic user query. It's concise, information-seeking, and process-oriented.\n",
    "    * **Rating:** 5\n",
    "* **Query:** \"Where is the People Operations internal handbook hosted, and how can someone gain access to it?\"\n",
    "    * **Explanation:** Realistic query but might be a bit too detailed for a typical user.\n",
    "    * **Rating:** 4\n",
    "* **Query:** \"Who controls access to People Data in the data warehouse at GitLab, and what approvals are required for Analytics Engineers and Data Analysts to obtain access?\"\n",
    "    * **Explanation:** Seems reasonable but too lengthy for a typical user query. \n",
    "    * **Rating:** 3\n",
    "\n",
    "### Unrealistic Queries (Bad Examples)\n",
    "\n",
    "* **Query:** \"If a GitLab team member has been with the company for over 3 months and is interested in participating in the Onboarding Buddy Program, what should they do to express their interest?\"\n",
    "    * **Explanation:** Overly specific and unnatural. No real user would ask this.\n",
    "    * **Rating:** 1\n",
    "* **Query:** \"On what date did the 'Managing Burnout with Time Off with John Fitch' session occur as part of the FY21 Learning Speaker Series?\"\n",
    "    * **Explanation:** Irrelevant and overly specific. Not a typical user query. \n",
    "    * **Rating:** 2\n",
    "\n",
    "### Your Task\n",
    "\n",
    "For the following generated question, please:\n",
    "\n",
    "1.  Rate its realism as a typical user query for an internal handbook application on a scale of 1 to 5 (1 = Very Unrealistic, 3 = Neutral/Somewhat Realistic, 5 = Very Realistic).\n",
    "2.  Provide a brief explanation for your rating, comparing it to the examples above if helpful.\n",
    "\n",
    "### Output Format\n",
    "\n",
    "**Explanation:** `[Your brief explanation]`\n",
    "**Rating:** `[Your 1â€“5 rating]`\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "user_prompt_filtering = dedent(\"\"\"\n",
    "**Generated Question to Evaluate:**\n",
    "`{question_to_evaluate}`\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "class ResponseFiltering(BaseModel):\n",
    "    explanation: str\n",
    "    rating: int\n",
    "\n",
    "\n",
    "llm_with_structured_output_filtering = llm.with_structured_output(ResponseFiltering)\n",
    "\n",
    "messages_filtering = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt_filtering), (\"user\", user_prompt_filtering)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5b953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_review_qa_pair(qa_pair):\n",
    "    compiled_messages = await messages_filtering.ainvoke(\n",
    "        {\"question_to_evaluate\": qa_pair.question}\n",
    "    )\n",
    "    output = await llm_with_structured_output_filtering.ainvoke(compiled_messages)\n",
    "    return output\n",
    "\n",
    "\n",
    "async def review_qa_pairs(qa_pairs) -> list[ResponseFiltering]:\n",
    "    \"\"\"\n",
    "    Function to review the generated QA pairs.\n",
    "    This function will be used to evaluate the realism of the questions.\n",
    "    \"\"\"\n",
    "    tasks = [generate_review_qa_pair(qa_pair) for qa_pair in qa_pairs]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "\n",
    "results = await review_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a7e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_ratings = [\n",
    "    {\n",
    "        \"rating\": result.rating,\n",
    "        \"explanation\": result.explanation,\n",
    "        \"question\": qa_pair.question,\n",
    "        \"answer\": qa_pair.fact,\n",
    "    }\n",
    "    for (result, qa_pair) in zip(results, qa_pairs)\n",
    "    if result.rating >= 4\n",
    "]\n",
    "len(high_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1921d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_ratings = pd.DataFrame(\n",
    "    high_ratings, columns=[\"Rating\", \"Explanation\", \"Question\"]\n",
    ")\n",
    "\n",
    "df_high_ratings.to_excel(\"high_ratings.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa79d22",
   "metadata": {},
   "source": [
    "## Evaluating RAG system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdfe231",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "\n",
    "dataset = client.create_dataset(dataset_name=\"Gitlab Handbook QA Evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a18fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\": {\n",
    "            \"question\": h.question,\n",
    "        },\n",
    "        \"output\": {\n",
    "            \"answer\": h.answer,\n",
    "            \"chunk\": chunk\n",
    "        },\n",
    "    }\n",
    "    for h, chunk in zip(high_ratings, random_splits)\n",
    "]\n",
    "\n",
    "# client.create_examples(dataset_id=dataset.id, examples=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dd7702",
   "metadata": {},
   "source": [
    "## Generating more diverse QA pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07239efe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
