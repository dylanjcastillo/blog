{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Is Pydantic making your model dumber?\"\n",
    "date: \"11/10/2024\"\n",
    "date-modified: last-modified\n",
    "description-meta: \"\"\n",
    "toc: true\n",
    "toc-depth: 3\n",
    "lightbox: true\n",
    "fig-cap-location: margin\n",
    "categories:\n",
    "  - llm\n",
    "  - openai\n",
    "  - pydantic\n",
    "  - python\n",
    "author:\n",
    "  - name: Dylan Castillo\n",
    "    url: https://dylancastillo.co\n",
    "    affiliation: Iwana Labs\n",
    "    affiliation-url: https://iwanalabs.com\n",
    "citation: true\n",
    "comments:\n",
    "  utterances:\n",
    "    repo: dylanjcastillo/blog_comments\n",
    "    theme: dark-blue\n",
    "    issue-term: pathname\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dcast/Documents/GitHub/blog/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import difflib\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from asyncio import Semaphore\n",
    "from enum import Enum\n",
    "from itertools import permutations\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import instructor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from openai import AsyncOpenAI\n",
    "from openai.types.chat import ChatCompletion\n",
    "from pydantic import BaseModel, Field\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "langsmith_client = wrap_openai(AsyncOpenAI())\n",
    "instructor_client = instructor.from_openai(langsmith_client, mode=instructor.Mode.TOOLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path().absolute().parent / \"data\" / \"live_bench\"\n",
    "reasoning_dir = data_dir / \"reasoning\"\n",
    "math_dir = data_dir / \"math\"\n",
    "language_dir = data_dir / \"language\"\n",
    "\n",
    "df_reasoning = pd.read_json(reasoning_dir / \"updated_questions.jsonl\", lines=True)\n",
    "df_language = pd.read_json(language_dir / \"updated_questions.jsonl\", lines=True)\n",
    "df_math = pd.read_json(math_dir / \"updated_questions.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response(BaseModel):\n",
    "    reasoning: str = Field(description=\"Your reasoning explaining your answer.\")\n",
    "    answer: str = Field(description=\"Your answer, don't include any other text.\")\n",
    "\n",
    "\n",
    "class ProcessedResponse(BaseModel):\n",
    "    value: str\n",
    "    elapsed_time: float\n",
    "\n",
    "\n",
    "class PromptType(Enum):\n",
    "    WITHOUT_STRUCTURED_OUTPUT = \"without_structured_output\"\n",
    "    WITH_TOOL_CALLS = \"with_structured_output_tool_calls\"\n",
    "    WITH_JSON_MODE = \"with_structured_output_json_mode\"\n",
    "\n",
    "\n",
    "SYSTEM_MESSAGE_MAPPING = {\n",
    "    PromptType.WITHOUT_STRUCTURED_OUTPUT.value: (\n",
    "        \"You're a helpful assistant. You will help me answer a question.\"\n",
    "        \"\\nYou must respond using the following format:\"\n",
    "        \"\\nREASONING: <your reasoning here>\"\n",
    "        \"\\nANSWER: <your answer here, don't include any other text>\"\n",
    "    ),\n",
    "    PromptType.WITH_TOOL_CALLS.value: (\n",
    "        \"You're a helpful assistant. You will help me answer a question.\"\n",
    "    ),\n",
    "    PromptType.WITH_JSON_MODE.value: (\n",
    "        \"You're a helpful assistant. You will help me answer a question.\"\n",
    "        + \"\\nYou must respond using the following JSON schema:\"\n",
    "        + json.dumps(Response.model_json_schema())\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(\n",
    "    response: ChatCompletion | Response, response_type: PromptType\n",
    ") -> str:\n",
    "    if isinstance(response, Response):\n",
    "        return response.answer\n",
    "    elif (\n",
    "        isinstance(response, ChatCompletion)\n",
    "        and response_type == PromptType.WITHOUT_STRUCTURED_OUTPUT\n",
    "    ):\n",
    "        return response.choices[0].message.content.split(\"\\nANSWER:\")[1].strip()\n",
    "    elif (\n",
    "        isinstance(response, ChatCompletion)\n",
    "        and response_type == PromptType.WITH_JSON_MODE\n",
    "    ):\n",
    "        return Response.model_validate_json(response.choices[0].message.content).answer\n",
    "    raise ValueError(f\"Invalid response type: {type(response)}\")\n",
    "\n",
    "\n",
    "@traceable\n",
    "async def call_model(\n",
    "    client,\n",
    "    prompt_type: PromptType,\n",
    "    user_message: str,\n",
    "    timeout: int = 120,\n",
    ") -> Response:\n",
    "    params = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_MESSAGE_MAPPING[prompt_type.value]},\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "        ],\n",
    "        \"timeout\": timeout,\n",
    "    }\n",
    "    if prompt_type == PromptType.WITH_JSON_MODE:\n",
    "        params.update({\"response_format\": {\"type\": \"json_object\"}})\n",
    "    if prompt_type == PromptType.WITH_TOOL_CALLS:\n",
    "        params.update(\n",
    "            {\n",
    "                \"response_model\": Response,\n",
    "            }\n",
    "        )\n",
    "    response = await client.chat.completions.create(**params)\n",
    "    return parse_response(response, prompt_type)\n",
    "\n",
    "\n",
    "@traceable\n",
    "async def process_row(\n",
    "    row: pd.Series,\n",
    "    prompt_type: PromptType,\n",
    "    semaphore: Semaphore,\n",
    ") -> ProcessedResponse:\n",
    "    start_time = time.time()\n",
    "    async with semaphore:\n",
    "        for _ in range(3):\n",
    "            try:\n",
    "                answer = await call_model(\n",
    "                    client=(\n",
    "                        instructor_client\n",
    "                        if prompt_type == PromptType.WITH_TOOL_CALLS\n",
    "                        else langsmith_client\n",
    "                    ),\n",
    "                    prompt_type=prompt_type,\n",
    "                    user_message=f\"Question:\\n{row.updated_question}\",\n",
    "                )\n",
    "                return ProcessedResponse(\n",
    "                    value=answer,\n",
    "                    elapsed_time=time.time() - start_time,\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {row.name}: {e}\")\n",
    "                continue\n",
    "        raise Exception(f\"Failed to process row {row.name}, after 3 attempts\")\n",
    "\n",
    "\n",
    "@traceable\n",
    "async def process_df(\n",
    "    df: pd.DataFrame,\n",
    "    prompt_type: PromptType,\n",
    "    concurrency: int = 50,\n",
    ") -> List[ProcessedResponse]:\n",
    "    semaphore = Semaphore(concurrency)\n",
    "    tasks = [process_row(row, prompt_type, semaphore) for _, row in df.iterrows()]\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://github.com/LiveBench/LiveBench/blob/main/livebench/process_results/writing/plot_unscrambling/utils.py\n",
    "def levenshtein_distance(A, B):\n",
    "    N, M = len(A), len(B)\n",
    "    # Create an array of size NxM\n",
    "    dp = [[0 for i in range(M + 1)] for j in range(N + 1)]\n",
    "\n",
    "    # Base Case: When N = 0\n",
    "    for j in range(M + 1):\n",
    "        dp[0][j] = j\n",
    "    # Base Case: When M = 0\n",
    "    for i in range(N + 1):\n",
    "        dp[i][0] = i\n",
    "    # Transitions\n",
    "    for i in range(1, N + 1):\n",
    "        for j in range(1, M + 1):\n",
    "            if A[i - 1] == B[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(\n",
    "                    dp[i - 1][j],  # Insertion\n",
    "                    dp[i][j - 1],  # Deletion\n",
    "                    dp[i - 1][j - 1],  # Replacement\n",
    "                )\n",
    "\n",
    "    return dp[N][M]\n",
    "\n",
    "\n",
    "def plot_unscrambling_process_results(ground_truth: str, llm_answer: str) -> float:\n",
    "    gt_sentences = [s.strip() for s in ground_truth.split(\".\")]\n",
    "    ans_sentences = [s.strip() for s in llm_answer.split(\".\")]\n",
    "\n",
    "    gt_sentences = [s for s in gt_sentences if s]\n",
    "    ans_sentences = [s for s in ans_sentences if s]\n",
    "\n",
    "    ans_ordering = []\n",
    "    for x in gt_sentences:\n",
    "        best_match = difflib.get_close_matches(x, ans_sentences, n=1, cutoff=0.0)\n",
    "        if best_match:\n",
    "            ans_ordering.append(ans_sentences.index(best_match[0]))\n",
    "\n",
    "    n_sentences_gt = len(gt_sentences)\n",
    "    raw_distance = levenshtein_distance(list(range(len(gt_sentences))), ans_ordering)\n",
    "    score = 1 - (raw_distance / n_sentences_gt)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def evaluate_language_task(ground_truth: str, task_type: str, response: str):\n",
    "    if task_type == \"connections\":\n",
    "        objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in response.split(\",\")\n",
    "        ]\n",
    "        gt_objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in ground_truth.split(\",\")\n",
    "        ]\n",
    "\n",
    "        groups = [set(objects[i : i + 4]) for i in range(0, len(objects), 4)]\n",
    "        gt_groups = [set(gt_objects[i : i + 4]) for i in range(0, len(gt_objects), 4)]\n",
    "\n",
    "        max_correct = 0\n",
    "        for perm in permutations(groups):\n",
    "            correct_groups = sum(g1 == g2 for g1, g2 in zip(perm, gt_groups))\n",
    "            max_correct = max(max_correct, correct_groups)\n",
    "        return max_correct / len(gt_groups)\n",
    "    elif task_type == \"plot_unscrambling\":\n",
    "        return plot_unscrambling_process_results(ground_truth, response)\n",
    "    elif task_type == \"typos\":\n",
    "        return ground_truth in response\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid task type: {task_type}\")\n",
    "\n",
    "\n",
    "def evaluate_reasoning_task(ground_truth: str, task_type: str, response: str):\n",
    "    if task_type == \"web_of_lies_v2\":\n",
    "        response_objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in response.split(\",\")\n",
    "        ]\n",
    "        gt_objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in ground_truth.split(\",\")\n",
    "        ]\n",
    "        return response_objects == gt_objects\n",
    "    elif task_type in (\"spatial\", \"zebra_puzzle\"):\n",
    "        response = response.rstrip(\".\")\n",
    "        return ground_truth.lower().strip() == response.lower().strip()\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid task type: {task_type}\")\n",
    "\n",
    "\n",
    "def evaluate_math_task(ground_truth: str, task_type: str, response: str):\n",
    "    if task_type == \"olympiad\":\n",
    "        response_objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in response.split(\",\")\n",
    "        ]\n",
    "        gt_objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in ground_truth.split(\",\")\n",
    "        ]\n",
    "        return response_objects == gt_objects\n",
    "    elif task_type == \"math_comp\":\n",
    "        return ground_truth == response\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid task type: {task_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "def generate_outputs(df):\n",
    "    df_copy = df.copy()\n",
    "    responses_without_so = asyncio.run(\n",
    "        process_df(df_copy, PromptType.WITHOUT_STRUCTURED_OUTPUT)\n",
    "    )\n",
    "    responses_with_so_tool_calls = asyncio.run(\n",
    "        process_df(df_copy, PromptType.WITH_TOOL_CALLS)\n",
    "    )\n",
    "    responses_with_so_json_mode = asyncio.run(\n",
    "        process_df(df_copy, PromptType.WITH_JSON_MODE)\n",
    "    )\n",
    "    df_copy[\"response_without_so\"] = [r.value for r in responses_without_so]\n",
    "    df_copy[\"response_with_so_tool_calls\"] = [\n",
    "        r.value for r in responses_with_so_tool_calls\n",
    "    ]\n",
    "    df_copy[\"response_with_so_json_mode\"] = [\n",
    "        r.value for r in responses_with_so_json_mode\n",
    "    ]\n",
    "    df_copy[\"elapsed_time_without_so\"] = [r.elapsed_time for r in responses_without_so]\n",
    "    df_copy[\"elapsed_time_with_so_tool_calls\"] = [\n",
    "        r.elapsed_time for r in responses_with_so_tool_calls\n",
    "    ]\n",
    "    df_copy[\"elapsed_time_with_so_json_mode\"] = [\n",
    "        r.elapsed_time for r in responses_with_so_json_mode\n",
    "    ]\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def evaluate_outputs(df, evaluator):\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"is_correct_without_so\"] = df_copy.apply(\n",
    "        lambda row: evaluator(\n",
    "            row[\"ground_truth\"], row[\"task\"], row[\"response_without_so\"]\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    df_copy[\"is_correct_with_so_tool_calls\"] = df_copy.apply(\n",
    "        lambda row: evaluator(\n",
    "            row[\"ground_truth\"], row[\"task\"], row[\"response_with_so_tool_calls\"]\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    df_copy[\"is_correct_with_so_json_mode\"] = df_copy.apply(\n",
    "        lambda row: evaluator(\n",
    "            row[\"ground_truth\"], row[\"task\"], row[\"response_with_so_json_mode\"]\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reasoning_results = generate_outputs(df_reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reasoning_results = evaluate_outputs(df_reasoning_results, evaluate_reasoning_task)\n",
    "df_reasoning_results.to_csv(data_dir / \"reasoning\" / \"reasoning_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "df_reasoning_results.groupby(\"task\").agg(\n",
    "    n_questions=(\"question_id\", \"count\"),\n",
    "    accuracy_without_so=(\"is_correct_without_so\", \"mean\"),\n",
    "    accuracy_with_so_tool_calls=(\"is_correct_with_so_tool_calls\", \"mean\"),\n",
    "    accuracy_with_so_json_mode=(\"is_correct_with_so_json_mode\", \"mean\"),\n",
    "    elapsed_time_without_so_p50=(\n",
    "        \"elapsed_time_without_so\",\n",
    "        lambda x: np.percentile(x, 50),\n",
    "    ),\n",
    "    elapsed_time_with_so_tool_calls_p50=(\n",
    "        \"elapsed_time_with_so_tool_calls\",\n",
    "        lambda x: np.percentile(x, 50),\n",
    "    ),\n",
    "    elapsed_time_with_so_json_mode_p50=(\n",
    "        \"elapsed_time_with_so_json_mode\",\n",
    "        lambda x: np.percentile(x, 50),\n",
    "    ),\n",
    "    elapsed_time_without_so_p99=(\n",
    "        \"elapsed_time_without_so\",\n",
    "        lambda x: np.percentile(x, 99),\n",
    "    ),\n",
    "    elapsed_time_with_so_tool_calls_p99=(\n",
    "        \"elapsed_time_with_so_tool_calls\",\n",
    "        lambda x: np.percentile(x, 99),\n",
    "    ),\n",
    "    elapsed_time_with_so_json_mode_p99=(\n",
    "        \"elapsed_time_with_so_json_mode\",\n",
    "        lambda x: np.percentile(x, 99),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_language_results = generate_outputs(df_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_language_results = evaluate_outputs(df_language_results, evaluate_language_task)\n",
    "df_language_results.to_csv(data_dir / \"language\" / \"language_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "df_language_results.groupby(\"task\").agg(\n",
    "    n_questions=(\"question_id\", \"count\"),\n",
    "    accuracy_without_so=(\"is_correct_without_so\", \"mean\"),\n",
    "    accuracy_with_so_tool_calls=(\"is_correct_with_so_tool_calls\", \"mean\"),\n",
    "    accuracy_with_so_json_mode=(\"is_correct_with_so_json_mode\", \"mean\"),\n",
    "    elapsed_time_without_so_p50=(\n",
    "        \"elapsed_time_without_so\",\n",
    "        lambda x: np.percentile(x, 50),\n",
    "    ),\n",
    "    elapsed_time_with_so_tool_calls_p50=(\n",
    "        \"elapsed_time_with_so_tool_calls\",\n",
    "        lambda x: np.percentile(x, 50),\n",
    "    ),\n",
    "    elapsed_time_with_so_json_mode_p50=(\n",
    "        \"elapsed_time_with_so_json_mode\",\n",
    "        lambda x: np.percentile(x, 50),\n",
    "    ),\n",
    "    elapsed_time_without_so_p99=(\n",
    "        \"elapsed_time_without_so\",\n",
    "        lambda x: np.percentile(x, 99),\n",
    "    ),\n",
    "    elapsed_time_with_so_tool_calls_p99=(\n",
    "        \"elapsed_time_with_so_tool_calls\",\n",
    "        lambda x: np.percentile(x, 99),\n",
    "    ),\n",
    "    elapsed_time_with_so_json_mode_p99=(\n",
    "        \"elapsed_time_with_so_json_mode\",\n",
    "        lambda x: np.percentile(x, 99),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_math_results = generate_outputs(df_math)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_math_results = evaluate_outputs(df_math_results, evaluate_math_task)\n",
    "df_math_results.to_csv(data_dir / \"math\" / \"math_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "df_math_results.groupby(\"task\").agg(\n",
    "    n_questions=(\"question_id\", \"count\"),\n",
    "    accuracy_without_so=(\"is_correct_without_so\", \"mean\"),\n",
    "    accuracy_with_so_tool_calls=(\"is_correct_with_so_tool_calls\", \"mean\"),\n",
    "    accuracy_with_so_json_mode=(\"is_correct_with_so_json_mode\", \"mean\"),\n",
    "    elapsed_time_without_so_p50=(\n",
    "        \"elapsed_time_without_so\",\n",
    "        lambda x: np.percentile(x, 50),\n",
    "    ),\n",
    "    elapsed_time_with_so_tool_calls_p50=(\n",
    "        \"elapsed_time_with_so_tool_calls\",\n",
    "        lambda x: np.percentile(x, 50),\n",
    "    ),\n",
    "    elapsed_time_with_so_json_mode_p50=(\n",
    "        \"elapsed_time_with_so_json_mode\",\n",
    "        lambda x: np.percentile(x, 50),\n",
    "    ),\n",
    "    elapsed_time_without_so_p99=(\n",
    "        \"elapsed_time_without_so\",\n",
    "        lambda x: np.percentile(x, 99),\n",
    "    ),\n",
    "    elapsed_time_with_so_tool_calls_p99=(\n",
    "        \"elapsed_time_with_so_tool_calls\",\n",
    "        lambda x: np.percentile(x, 99),\n",
    "    ),\n",
    "    elapsed_time_with_so_json_mode_p99=(\n",
    "        \"elapsed_time_with_so_json_mode\",\n",
    "        lambda x: np.percentile(x, 99),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "def calculate_confidence_intervals(df):\n",
    "    mean_accuracy_without_so = df[\"is_correct_without_so\"].mean()\n",
    "    mean_accuracy_with_so_tool_calls = df[\"is_correct_with_so_tool_calls\"].mean()\n",
    "    mean_accuracy_with_so_json_mode = df[\"is_correct_with_so_json_mode\"].mean()\n",
    "\n",
    "    n = len(df)\n",
    "    se_without_so = df[\"is_correct_without_so\"].std() / np.sqrt(n)\n",
    "    se_with_so_tool_calls = df[\"is_correct_with_so_tool_calls\"].std() / np.sqrt(n)\n",
    "    se_with_so_json_mode = df[\"is_correct_with_so_json_mode\"].std() / np.sqrt(n)\n",
    "\n",
    "    ci_without_so = [\n",
    "        mean_accuracy_without_so - 1.96 * se_without_so,\n",
    "        mean_accuracy_without_so + 1.96 * se_without_so,\n",
    "    ]\n",
    "    ci_with_so_tool_calls = [\n",
    "        mean_accuracy_with_so_tool_calls - 1.96 * se_with_so_tool_calls,\n",
    "        mean_accuracy_with_so_tool_calls + 1.96 * se_with_so_tool_calls,\n",
    "    ]\n",
    "    ci_with_so_json_mode = [\n",
    "        mean_accuracy_with_so_json_mode - 1.96 * se_with_so_json_mode,\n",
    "        mean_accuracy_with_so_json_mode + 1.96 * se_with_so_json_mode,\n",
    "    ]\n",
    "\n",
    "    print(\n",
    "        f\"Response format without SO - Mean: {mean_accuracy_without_so * 100:.2f}% CI: {ci_without_so[0] * 100:.2f}% - {ci_without_so[1] * 100:.2f}%\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Response format with SO tool calls - Mean: {mean_accuracy_with_so_tool_calls * 100:.2f}% CI: {ci_with_so_tool_calls[0] * 100:.2f}% - {ci_with_so_tool_calls[1] * 100:.2f}%\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Response format with SO JSON mode - Mean: {mean_accuracy_with_so_json_mode * 100:.2f}% CI: {ci_with_so_json_mode[0] * 100:.2f}% - {ci_with_so_json_mode[1] * 100:.2f}%\"\n",
    "    )\n",
    "\n",
    "\n",
    "def run_paired_t_test(df):\n",
    "    accuracies_without_so = df[\"is_correct_without_so\"] * 1\n",
    "    accuracies_with_so_tool_calls = df[\"is_correct_with_so_tool_calls\"] * 1\n",
    "    accuracies_with_so_json_mode = df[\"is_correct_with_so_json_mode\"] * 1\n",
    "\n",
    "    t_stat_without_so_tool_calls, p_value_without_so_tool_calls = stats.ttest_rel(\n",
    "        accuracies_without_so, accuracies_with_so_tool_calls\n",
    "    )\n",
    "    print(\"Without SO vs With SO Tool Calls\")\n",
    "    print(\n",
    "        f\"t-statistic: {t_stat_without_so_tool_calls}, p-value: {p_value_without_so_tool_calls}\"\n",
    "    )\n",
    "\n",
    "    t_stat_without_so_json_mode, p_value_without_so_json_mode = stats.ttest_rel(\n",
    "        accuracies_without_so, accuracies_with_so_json_mode\n",
    "    )\n",
    "    print(\"Without SO vs With SO JSON Mode\")\n",
    "    print(\n",
    "        f\"t-statistic: {t_stat_without_so_json_mode}, p-value: {p_value_without_so_json_mode}\"\n",
    "    )\n",
    "\n",
    "    print(\"With SO Tool Calls vs With SO JSON Mode\")\n",
    "    t_stat_with_so_tool_calls_json_mode, p_value_with_so_tool_calls_json_mode = (\n",
    "        stats.ttest_rel(accuracies_with_so_tool_calls, accuracies_with_so_json_mode)\n",
    "    )\n",
    "    print(\n",
    "        f\"t-statistic: {t_stat_with_so_tool_calls_json_mode}, p-value: {p_value_with_so_tool_calls_json_mode}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_confidence_intervals(df_reasoning_results)\n",
    "run_paired_t_test(df_reasoning_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_confidence_intervals(df_language_results)\n",
    "run_paired_t_test(df_language_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_confidence_intervals(df_math_results)\n",
    "run_paired_t_test(df_math_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
