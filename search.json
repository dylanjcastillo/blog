[
  {
    "objectID": "posts/nlp-snippets-cluster-documents-using-word2vec.html",
    "href": "posts/nlp-snippets-cluster-documents-using-word2vec.html",
    "title": "How to Cluster Documents Using Word2Vec and K-means",
    "section": "",
    "text": "There are more sentiment analysis tutorials online than people doing sentiment analysis in their day jobs. Don’t get me wrong. I’m not saying those tutorials aren’t useful. I just want to highlight that supervised learning receives much more attention than any other Natural Language Processing (NLP) method.\nOddly enough, there’s a big chance that most of the text data you’ll use in your next projects won’t have ground truth labels. So supervised learning might not be a solution you can immediately apply to your data problems.\nWhat can you do then? Use unsupervised learning algorithms.\nIn this tutorial, you’ll learn to apply unsupervised learning to generate value from your text data. You’ll cluster documents by training a word embedding (Word2Vec) and applying the K-means algorithm.\nPlease be aware that the next sections focus on practical manners. You won’t find much theory in them besides brief definitions of relevant ideas.\nTo make the most of this tutorial, you should be familiar with these topics:\nLet’s get to it!"
  },
  {
    "objectID": "posts/nlp-snippets-cluster-documents-using-word2vec.html#how-to-cluster-documents",
    "href": "posts/nlp-snippets-cluster-documents-using-word2vec.html#how-to-cluster-documents",
    "title": "How to Cluster Documents Using Word2Vec and K-means",
    "section": "How to Cluster Documents",
    "text": "How to Cluster Documents\nYou can think of the process of clustering documents in three steps:\n\nCleaning and tokenizing data usually involves lowercasing text, removing non-alphanumeric characters, or stemming words.\nGenerating vector representations of the documents concerns the mapping of documents from words into numerical vectors—some common ways of doing this include using bag-of-words models or word embeddings.\nApplying a clustering algorithm on the document vectors requires selecting and applying a clustering algorithm to find the best possible groups using the document vectors. Some frequently used algorithms include K-means, DBSCAN, or Hierarchical Clustering.\n\nThat’s it! Now, you’ll see how that looks in practice."
  },
  {
    "objectID": "posts/nlp-snippets-cluster-documents-using-word2vec.html#sample-project-clustering-news-articles",
    "href": "posts/nlp-snippets-cluster-documents-using-word2vec.html#sample-project-clustering-news-articles",
    "title": "How to Cluster Documents Using Word2Vec and K-means",
    "section": "Sample Project: Clustering News Articles",
    "text": "Sample Project: Clustering News Articles\nIn this section, you’ll learn how to cluster documents by working through a small project. You’ll group news articles into categories using a dataset published by Szymon Janowski.\n\nSet Up Your Local Environment\nTo follow along with the tutorial examples, you’ll need to download the data and install a few libraries. You can do it by following these steps:\n\nClone the nlp-snippets repository locally.\nCreate a new virtual environment using venv or conda.\nActivate your new virtual environment.\nInstall the required libraries.\nStart a Jupyter notebook.\n\nIf you’re using venv, then you need to run these commands:\ngit clone https://github.com/dylanjcastillo/nlp-snippets.git\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements\njupyter notebook\nIf you’re using conda, then you need to run these commands:\ngit clone https://github.com/dylanjcastillo/nlp-snippets.git\nconda create --name venv\nconda activate venv\npip install -r requirements\njupyter notebook\nNext, open Jupyter Notebook. Then, create a new notebook in the root folder and set its name to clustering_word2vec.ipynb.\nBy now, your project structure should look like this:\nnlp-snippets/\n│\n├── clustering/\n│\n├── data/\n│\n├── ds_utils/\n│\n├── preprocessing/\n│\n├── venv/ # (If you're using venv)\n│\n├── clustering_word2vec.ipynb\n├── LICENSE\n├── README.md\n└── requirements.txt\nThis is your project’s structure. It includes these directories and files:\n\nclustering/: Examples of clustering text data using bag-of-words, training a word2vec model, and using a pretrained fastText embeddings.\ndata/: Data used for the clustering examples.\nds_utils/: Common utility functions used in the sample notebooks in the repository.\npreprocessing/: Frequently used code snippets for preprocessing text.\nvenv/: If you used venv, then this directory will contain the files related to your virtual environment.\nrequirements.txt: Libraries used in the examples provided.\nREADME and License: Information about the repository and its license.\n\nFor now, you’ll use the notebook you created (clustering_word2vec.ipynb) and the news dataset in data/. The notebooks in clustering/ and preprocessing/ include additional code snippets that might be useful for NLP tasks. You can review those on your own.\nIn the next section, you’ll create the whole pipeline from scratch. If you’d like to download the full and cleaner version of the code in the examples, go to the NLP Snippets repository.\nThat’s it for setup! Next, you’ll define your imports.\n\n\nImport the Required Libraries\nOnce you finish setting up your local environment, it’s time to start writing code in your notebook. Open clustering_word2vec.ipynb, and copy the following code in the first cell:\nimport os\nimport random\nimport re\nimport string\n\nimport nltk\nimport numpy as np\nimport pandas as pd\n\nfrom gensim.models import Word2Vec\n\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\n\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nnltk.download(\"stopwords\")\nnltk.download(\"punkt\")\n\nSEED = 42\nrandom.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nnp.random.seed(SEED)\nThese are the libraries you need for the sample project. Here’s what you do with each of them:\n\nos and random help you define a random seed to make the code deterministically reproducible.\nre and string provide you with easy ways to clean the data.\npandashelps you read the data.\nnumpyprovides you with linear algebra utilities you’ll use to evaluate results. Also, it’s used for setting a random seed to make the code deterministically reproducible.\ngensim makes it easy for you to train a word embedding from scratch using the Word2Vec class.\nnltkaids you in cleaning and tokenizing data through the word_tokenize method and the stopword list.\nsklearngives you an easy interface to the clustering model, MiniBatchKMeans, and the metrics to evaluate the quality of its results, silhouette_samples and silhouette_score.\n\nIn addition to importing the libraries, you download English stopwords using nltk.download(\"stopwords\"), you define SEED and set it as a random seed using numpy, random, and the PYTHONHASHSEED environment variable. This last step makes sure your code is reproducible across systems.\nRun this cell and make sure you don’t get any errors. In the next section, you’ll prepare your text data.\n\n\nClean and Tokenize Data\nAfter you import the required libraries, you need to read and preprocess the data you’ll use in your clustering algorithm. The preprocessing consists of cleaning and tokenizing the data. To do that, copy the following function in a new cell in your notebook:\ndef clean_text(text, tokenizer, stopwords):\n    \"\"\"Pre-process text and generate tokens\n\n    Args:\n        text: Text to tokenize.\n\n    Returns:\n        Tokenized text.\n    \"\"\"\n    text = str(text).lower()  # Lowercase words\n    text = re.sub(r\"\\[(.*?)\\]\", \"\", text)  # Remove [+XYZ chars] in content\n    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces in content\n    text = re.sub(r\"\\w+…|…\", \"\", text)  # Remove ellipsis (and last word)\n    text = re.sub(r\"(?&lt;=\\w)-(?=\\w)\", \" \", text)  # Replace dash between words\n    text = re.sub(\n        f\"[{re.escape(string.punctuation)}]\", \"\", text\n    )  # Remove punctuation\n\n    tokens = tokenizer(text)  # Get tokens from text\n    tokens = [t for t in tokens if not t in stopwords]  # Remove stopwords\n    tokens = [\"\" if t.isdigit() else t for t in tokens]  # Remove digits\n    tokens = [t for t in tokens if len(t) &gt; 1]  # Remove short tokens\n    return tokens\nThis code cleans and tokenizes a text input, using a predefined tokenizer and a list of stopwords. It helps you perform these operations:\n\nLine 10: Transform the input into a string and lowercase it.\nLine 11: Remove substrings like “[+300 chars]” I found while reviewing the data.\nLine 12: Remove multiple spaces, tabs, and line breaks.\nLine 13: Remove ellipsis characters.\nLines 14-17: Replace dashes between words with a space and remove punctuation.\nLines 19-20: Tokenize text and remove tokens using a list of stop words.\nLines 21-22: Remove digits and tokens whose length is too short.\n\nThen, in the next cell, copy the following code to read the data and apply that function to the text columns:\ncustom_stopwords = set(stopwords.words(\"english\") + [\"news\", \"new\", \"top\"])\ntext_columns = [\"title\", \"description\", \"content\"]\n\ndf_raw = pd.read_csv(\"data/news_data.csv\")\ndf = df_raw.copy()\ndf[\"content\"] = df[\"content\"].fillna(\"\")\n\nfor col in text_columns:\n    df[col] = df[col].astype(str)\n\n# Create text column based on title, description, and content\ndf[\"text\"] = df[text_columns].apply(lambda x: \" | \".join(x), axis=1)\ndf[\"tokens\"] = df[\"text\"].map(lambda x: clean_text(x, word_tokenize, custom_stopwords))\n\n# Remove duplicated after preprocessing\n_, idx = np.unique(df[\"tokens\"], return_index=True)\ndf = df.iloc[idx, :]\n\n# Remove empty values and keep relevant columns\ndf = df.loc[df.tokens.map(lambda x: len(x) &gt; 0), [\"text\", \"tokens\"]]\n\ndocs = df[\"text\"].values\ntokenized_docs = df[\"tokens\"].values\n\nprint(f\"Original dataframe: {df_raw.shape}\")\nprint(f\"Pre-processed dataframe: {df.shape}\")\nThis is how you read and preprocess the data. This code applies the cleaning function you defined earlier, removes duplicates and nulls, and drops irrelevant columns.\nYou apply these steps to a new data frame (df). It contains a column with the raw documents called text and another one with the preprocessed documents called tokens. You save the values of those columns into two variables, docs and tokenized_docs, to use in the next code snippets.\nIf you execute the two cells you defined, then you should get the following output:\nOriginal dataframe: (10437, 15)\nPre-processed dataframe: (9882, 2)\nNext, you’ll create document vectors using Word2Vec.\n\n\nGenerate Document Vectors\nAfter you’ve cleaned and tokenized the text, you’ll use the documents’ tokens to create vectors using Word2Vec. This process consists of two steps:\n\nTrain a Word2Vec model using the tokens you generated earlier. Alternatively, you could load a pre-trained Word2Vec model (I’ll also show you how to do it).\nGenerate a vector per document based on its individual word vectors.\n\nIn this section, you’ll go through these steps.\n\nTrain Word2Vec Model\nThe following code will help you train a Word2Vec model. Copy it into a new cell in your notebook:\nmodel = Word2Vec(sentences=tokenized_docs, vector_size=100, workers=1, seed=SEED)\nYou use this code to train a Word2Vec model based on your tokenized documents. For this example, you specified the following parameters in the Word2Vec class:\n\nsentences expects a list of lists with the tokenized documents.\nvector_size defines the size of the word vectors. In this case, you set it to 100.\nworkers defines how many cores you use for training. I set it to 1 to make sure the code is deterministically reproducible.\nseed sets the seed for random number generation. It’s set to the constant SEED you defined in the first cell.\n\nThere are other parameters you can tune when training the Word2Vec model. See gensim’s documentation if you’d like to learn more about them.\nNote: In many cases, you might want to use a pre-trained model instead of training one yourself. If that’s the case, gensim provides you with an easy way to access some of the most popular pre-trained word embeddings.\nYou can load a pre-trained Word2Vec model as follows:\nwv = api.load('word2vec-google-news-300')\nOne last thing, if you’re following this tutorial and decide to use a pre-trained model, you’ll need to replace model.wv by wv in the code snippets from here on. Otherwise, you’ll get an error.\nNext, run the cell you just created in your notebook. It might take a couple of minutes. After it’s done, you can validate that the results make sense by plotting the vectors or reviewing the similarity results for relevant words. You can do the latter by copying and running this code in a cell in your notebook:\nmodel.wv.most_similar(\"trump\")\nIf you run this code, then you’ll get this output:\n[('trumps', 0.988541841506958),\n ('president', 0.9746493697166443),\n ('donald', 0.9274922013282776),\n ('ivanka', 0.9203903079032898),\n ('impeachment', 0.9195784330368042),\n ('pences', 0.9152231812477112),\n ('avlon', 0.9148306846618652),\n ('biden', 0.9146010279655457),\n ('breitbart', 0.9144087433815002),\n ('vice', 0.9067237973213196)]\nThat’s it! You’ve trained your Word2Vec model, now, you’ll use it to generate document vectors.\n\n\nCreate Document Vectors from Word Embedding\nNow you’ll generate document vectors using the Word2Vec model you trained. The idea is straightforward. From the Word2Vec model, you’ll get numerical vectors per word in a document, so you need to find a way of generating a single vector out of them.\nFor short texts, a common approach is to use the average of the vectors. There’s no clear consensus on what will work well for longer texts. Though, using a weighted average of the vectors might help.\nThe following code will help you create a vector per document by averaging its word vectors. Create a new cell in your notebook and copy this code there:\ndef vectorize(list_of_docs, model):\n    \"\"\"Generate vectors for list of documents using a Word Embedding\n\n    Args:\n        list_of_docs: List of documents\n        model: Gensim's Word Embedding\n\n    Returns:\n        List of document vectors\n    \"\"\"\n    features = []\n\n    for tokens in list_of_docs:\n        zero_vector = np.zeros(model.vector_size)\n        vectors = []\n        for token in tokens:\n            if token in model.wv:\n                try:\n                    vectors.append(model.wv[token])\n                except KeyError:\n                    continue\n        if vectors:\n            vectors = np.asarray(vectors)\n            avg_vec = vectors.mean(axis=0)\n            features.append(avg_vec)\n        else:\n            features.append(zero_vector)\n    return features\n\nvectorized_docs = vectorize(tokenized_docs, model=model)\nlen(vectorized_docs), len(vectorized_docs[0])\nThis code will get all the word vectors of each document and average them to generate a vector per each document. Here’s what’s happening there:\n\nYou define the vectorize function that takes a list of documents and a gensim model as input, and generates a feature vector per document as output.\nYou apply the function to the documents’ tokens in tokenized_doc, using the Word2Vec model you trained earlier.\nYou print the length of the list of documents and the size of the generated vectors.\n\nNext, you’ll cluster the documents using Mini-batches K-means.\n\n\n\nCluster Documents Using (Mini-batches) K-means\nTo cluster the documents, you’ll use the Mini-batches K-means algorithm. This K-means variant uses random input data samples to reduce the time required during training. The upside is that it shares the same objective function with the original algorithm, so, in practice, the results are just a bit worse than K-means.\nIn the code snippet below, you can see the function you’ll use to create the clusters using Mini-batches K-means. Create a new cell in your notebook, and copy the following code there:\ndef mbkmeans_clusters(\n    X,\n    k,\n    mb,\n    print_silhouette_values,\n):\n    \"\"\"Generate clusters and print Silhouette metrics using MBKmeans\n\n    Args:\n        X: Matrix of features.\n        k: Number of clusters.\n        mb: Size of mini-batches.\n        print_silhouette_values: Print silhouette values per cluster.\n\n    Returns:\n        Trained clustering model and labels based on X.\n    \"\"\"\n    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n    print(f\"For n_clusters = {k}\")\n    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n    print(f\"Inertia:{km.inertia_}\")\n\n    if print_silhouette_values:\n        sample_silhouette_values = silhouette_samples(X, km.labels_)\n        print(f\"Silhouette values:\")\n        silhouette_values = []\n        for i in range(k):\n            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n            silhouette_values.append(\n                (\n                    i,\n                    cluster_silhouette_values.shape[0],\n                    cluster_silhouette_values.mean(),\n                    cluster_silhouette_values.min(),\n                    cluster_silhouette_values.max(),\n                )\n            )\n        silhouette_values = sorted(\n            silhouette_values, key=lambda tup: tup[2], reverse=True\n        )\n        for s in silhouette_values:\n            print(\n                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n            )\n    return km, km.labels_\nThis function creates the clusters using the Mini-batches K-means algorithm. It takes the following arguments:\n\n**X**: Matrix of features. In this case, it’s your vectorized documents.\n**k**:Number of clusters you’d like to create.\n**mb**: Size of mini-batches.\n**print_silhouette_values**: Defines if the Silhouette Coefficient is printed for each cluster. If you haven’t heard about this coefficient, don’t worry, you’ll learn about it in a bit!\n\nmbkmeans_cluster takes these arguments and returns the fitted clustering model and the labels for each document.\nRun the cell where you copied the function. Next, you’ll apply this function to your vectorized documents.\n\nDefinition of Clusters\nNow, you need to execute mbkmean_clusters providing it with the vectorized documents and the number of clusters. You’ll print the Silhouette Coefficients per cluster to review the quality of your clusters.\nCreate a new cell and copy this code there:\nclustering, cluster_labels = mbkmeans_clusters(\n    X=vectorized_docs,\n    k=50,\n    mb=500,\n    print_silhouette_values=True,\n)\ndf_clusters = pd.DataFrame({\n    \"text\": docs,\n    \"tokens\": [\" \".join(text) for text in tokenized_docs],\n    \"cluster\": cluster_labels\n})\nThis code will fit the clustering model, print the Silhouette Coefficient per cluster, and return the fitted model and the labels per cluster. It’ll also create a data frame you can use to review the results.\nThere are a few things to consider when setting the input arguments:\n\nprint_silhouette_values is straightforward. In this case, you set it to True to print the evaluation metric per cluster. This will help you review the results.\nmb depends on the size of your dataset. You need to ensure that it is not too small to avoid a significant impact on the quality of results and not too big to avoid making the execution too slow. In this case, you set it to 500 observations.\nk is trickier. In general, it involves a mix of qualitative analysis and quantitative metrics. After a few experiments on my side, I found that 50 seemed to work well. But that is more or less arbitrary.\n\nYou could use metrics like the Silhouette Coefficient for the quantitative evaluation of the number of clusters. This coefficient is an evaluation metric frequently used in problems where ground truth labels are unknown. It’s calculated using the mean intra-cluster distance and the mean nearest-cluster distance and goes from -1 to 1. Well-defined clusters result in positive values of this coefficient, while incorrect clusters will result in negative values. If you’d like to learn more about it, look at scikit-learn’s documentation.\nThe qualitative part generally requires you to have domain knowledge of the subject matter so you can sense-check your clustering algorithm’s results. In the next section, I’ll show you two approaches you can use to check your results qualitatively.\nAfter executing the cell you just created, the output should look like this:\nFor n_clusters = 50\nSilhouette coefficient: 0.11\nInertia:3568.342791047967\nSilhouette values:\n    Cluster 29: Size:50 | Avg:0.39 | Min:0.01 | Max: 0.59\n    Cluster 35: Size:30 | Avg:0.34 | Min:0.05 | Max: 0.54\n    Cluster 37: Size:58 | Avg:0.32 | Min:0.09 | Max: 0.51\n    Cluster 39: Size:81 | Avg:0.31 | Min:-0.05 | Max: 0.52\n    Cluster 27: Size:63 | Avg:0.28 | Min:0.02 | Max: 0.46\n    Cluster 6: Size:101 | Avg:0.27 | Min:0.02 | Max: 0.46\n    Cluster 24: Size:120 | Avg:0.26 | Min:-0.04 | Max: 0.46\n    Cluster 49: Size:65 | Avg:0.26 | Min:-0.03 | Max: 0.47\n    Cluster 47: Size:53 | Avg:0.23 | Min:0.01 | Max: 0.45\n    Cluster 22: Size:78 | Avg:0.22 | Min:-0.01 | Max: 0.43\n    Cluster 45: Size:38 | Avg:0.21 | Min:-0.07 | Max: 0.41\n...\nThis is the output of your clustering algorithm. The sizes and Silhouette Coefficients per cluster are the most relevant metrics. The clusters are printed by the value of the Silhouette coefficient in descending order. A higher score means denser – and thus better – clusters. In this case, you can see that clusters 29, 35, and 37 seem to be the top ones.\nNext, you’ll learn how to check what’s in each cluster.\n\n\nQualitative Review of Clusters\nThere are a few ways you can qualitatively analyze the results. During the earlier sections, our approach resulted in vector representations of tokens and documents, and vectors of the clusters’ centroids. You can find the most representative tokens and documents to analyze the results by looking for the vectors closest to the clusters’ centroids.\nHere’s how you obtain the most representative tokens per cluster:\nprint(\"Most representative terms per cluster (based on centroids):\")\nfor i in range(50):\n    tokens_per_cluster = \"\"\n    most_representative = model.wv.most_similar(positive=[clustering.cluster_centers_[i]], topn=5)\n    for t in most_representative:\n        tokens_per_cluster += f\"{t[0]} \"\n    print(f\"Cluster {i}: {tokens_per_cluster}\")\nFor the top clusters we identified earlier – 29, 35, and 37 – these are the results:\nCluster 29: noaa sharpie claim assertions forecasters\nCluster 35: eye lilinow path halts projected\nCluster 37: cnnpolitics complaint clinton pences whistleblower\nNext, we can do the same analysis with documents instead of tokens. This is how you find the most representative documents for cluster 29:\ntest_cluster = 29\nmost_representative_docs = np.argsort(\n    np.linalg.norm(vectorized_docs - clustering.cluster_centers_[test_cluster], axis=1)\n)\nfor d in most_representative_docs[:3]:\n    print(docs[d])\n    print(\"-------------\")\nAnd these are the 3 most representative documents in that cluster:\nDorian, Comey and Debra Messing: What Trump tweeted on Labor Day weekend | President Donald Trump axed his visit to Poland over the weekend to monitor Hurricane Dorian from Camp David with emergency management staff, but if the President's more than 120 tweets are any indication, he had more than just the storm on his mind. | Washington (CNN)President Donald Trump axed his visit to Poland over the weekend to monitor Hurricane Dorian from Camp David with emergency management staff, but if the President's more than 120 tweets are any indication, he had more than just the storm on hi… [+3027 chars]\n-------------\nRoss Must Resign If Report He Threatened NOAA Officials Is True: Democrat | As President Donald Trump claimed Hurricane Dorian could hit Alabama, the National Weather Service tweeted to correct the rumors. | Commerce Secretary Wilbur Ross is facing calls to resign over a report alleging that he threatened to fire top officials at NOAA for a tweet disputing President Donald Trump's claim that Hurricane Dorian would hit Alabama.\n\"If that story is true, and I don't… [+3828 chars]\n-------------\nFederal weather workers are furious at the NOAA's 'utterly disgusting' statement defending Trump's claim Hurricane Dorian would hit Alabama | Federal weather workers have reacted furiously to the National Oceanic and Atmospheric Administration's (NOAA) defence of US President Donald Trump's repeated assertions that Hurricane Dorian was set to hit Alabama. \"Never ever before has their management thr… | Federal weather workers have reacted furiously to the National Oceanic and Atmospheric Administration's (NOAA) defence of US President Donald Trump's repeated assertions that Hurricane Dorian was set to hit Alabama, saying they have been \"thrown under the bus… [+3510 chars]\nMost of the results seem to be related to a dispute between Donald Trump and the National Oceanic and Atmospheric Agency (NOAA). It was a famous controversy that people referred to as Sharpiegate.\nYou could also explore other approaches like generating word frequencies per cluster or reviewing random samples of documents per cluster."
  },
  {
    "objectID": "posts/nlp-snippets-cluster-documents-using-word2vec.html#other-approaches",
    "href": "posts/nlp-snippets-cluster-documents-using-word2vec.html#other-approaches",
    "title": "How to Cluster Documents Using Word2Vec and K-means",
    "section": "Other Approaches",
    "text": "Other Approaches\nThere are other approaches you could take to cluster text data like:\n\nUse a pre-trained word embedding instead of training your own. In this tutorial, you trained a Word2Vec model from scratch, but it’s very common to use a pre-trained model.\nGenerating feature vectors using a bag-of-words approach instead of word embeddings.\nReducing dimensionality of feature vectors. This is very useful if you use a bag-of-words approach.\nClustering documents using other algorithms like HDBSCAN or Hierarchical Clustering.\nUsing BERT sentence embeddings to generate the feature vectors. Or generating the topics with BERTopic."
  },
  {
    "objectID": "posts/nlp-snippets-cluster-documents-using-word2vec.html#conclusion",
    "href": "posts/nlp-snippets-cluster-documents-using-word2vec.html#conclusion",
    "title": "How to Cluster Documents Using Word2Vec and K-means",
    "section": "Conclusion",
    "text": "Conclusion\nWay to go! You just learned how to cluster documents using Word2Vec. You went through an end-to-end project, where you learned all the steps required for clustering a corpus of text.\nYou learned how to:\n\nPreprocess data to use with a Word2Vec model\nTrain a Word2Vec model\nUse quantitative metrics, such as the Silhouette score to evaluate the quality of your clusters.\nFind the most representative tokens and documents in your clusters\n\nI hope you find this tutorial useful. Shoot me a message if you have any questions!"
  },
  {
    "objectID": "posts/mind-reading-algorithms.html",
    "href": "posts/mind-reading-algorithms.html",
    "title": "Mind-reading Algorithms: An Introduction to Recommender Systems",
    "section": "",
    "text": "Tonight’s game plan: A hot meal, a warm bed, and a couple of hours of Netflix. Best of all, you are about to make it happen.\nYou put the key in the front door, turn the lock, and push the door open…\n…That last episode from Black Mirror was somewhat disturbing. Maybe I should give a chance to Better Call Saul tonight…\nYou are halfway through the front door…\n…Probably won’t be as good as Breaking Bad but… Wait, what’s that?\nThere is music playing nearby. You step back outside and listen. It’s coming from that new store next to your building. It is a song you’ve had in your mind for the last couple of days. There is a red carpet laid down in front of the store.¹\n“E-Store,” you whisper to yourself, reading what’s on the glowing red neon sign on top of the store. You hadn’t paid much attention to this place the few times you passed by.\nYou will do a quick inspection of what is on display. After all, a few minutes of window shopping hasn’t harmed anyone.\nYou enter the store and notice that it is warmly lit. There are various shelves with items and some clerks moving and sorting stuff out. There is a small stage at the center of the store with a couch and a tiny shelf.\nUpon closer examination, you noticed that there are only running shoes of your size and your favorite sports brand on the tiny shelf on the stage. You had been thinking about starting running again.\nIs it time I do something about it?\nYou select a pair of shoes and head to the cashier. On your way there, you spot a shelf with running shirts and pants². A glance at them reveals that some would make for a great outfit when combined with your new shoes. You’ll take a quick look.\nI cannot go out there looking like an amateur!\nWith your now complete running outfit, you resume your way to the cashier. While handing in the items to the clerk, you notice a poster of your favorite actors running a marathon³. They are wearing almost the same outfit you are about to buy. The only difference is that you are missing the smartwatch they are wearing.\nWhat are the odds?\nThe clerk smiles and points toward a smartwatch on display next to the cashier. You happily abide.\nAfter leaving the store, a few hours later, you start to reflect on your spending spree. Besides the outfit and smartwatch, you bought a pair of sunglasses, 1 kg of protein powder, a gym subscription, and a health plan.\nMaybe that bit of window shopping was not that harmless after all…\nWhat seems like a far-fetched story for brick-and-mortar stores is the bread-and-butter of many internet services. The strategies used by the story’s E(vil)-store to capture your attention, have an existing digital analogous you can check in the references of this article. Most of these techniques are part of a field referred to as Recommender Systems.\nNowadays, Recommenders are ubiquitous. Chances are that you are reading this article because of a suggestion generated by one. They are responsible for 35% of what users buy on Amazon⁴, 75% of what people watch on Netflix, and 70% of watch time on YouTube⁵.\nThese algorithms are so ingrained in our society that people have even started to get wary of their risks⁶. Anti-vaxxers, flat-earth proponents, and other conspiracy theorists have learned to manipulate these systems to recommended their content at disproportionately high rates. Those who argued for a New Enlightenment Era driven by the internet most likely did not have present-day YouTube in mind.\nSo far, this article is not helping in rehabilitating the Recommendation Systems’ image. But my goal is to focus on their positive side. Recommenders provide us with a valuable service: enable decision-making by decreasing uncertainty over choices. Furthermore, as we will see in the next sections, in a digital world of endless options, this is not an easy task.\nThrough this article I try to share an intuitive idea on the What, Why, and How of Recommenders. I am not aiming to cover implementation nor technical details. So beware if that is what you are looking for.\nIn a nutshell, if you need to explain what is a recommender to your boss, this article might help. Conversely, if you need to build a recommender for your boss, this article might help to distract him while you search for other articles!"
  },
  {
    "objectID": "posts/mind-reading-algorithms.html#recommendation-systems-101",
    "href": "posts/mind-reading-algorithms.html#recommendation-systems-101",
    "title": "Mind-reading Algorithms: An Introduction to Recommender Systems",
    "section": "Recommendation Systems 101",
    "text": "Recommendation Systems 101\nA Recommendation System or a Recommender is a set of techniques used for suggesting users the most suitable items based on their needs. This definition sounds simple, yet it conceals many details.\nIn the context of recommenders, an item is a very malleable idea. It could go from movies or songs in entertainment applications to possible love or mating partners in a dating app. Based on the items’ qualities, the recommender tries to guess which items are the most suitable to suggest to a given user. Thus, if you have a history of watching action movies, it’s fair to assume that you’ll prefer movies like Fast & Furious than the latest romantic drama on Netflix.\nSuitability is also a subjective matter. From a user perspective, you expect that a recommender provides you with the best possible option for your needs, as fast as possible, and paying the least. On the other hand, a business is trying to make a living, thus the way in which the recommender will provide suggestions will need to reflect that end. One can expect then, that the needs of both the users and businesses will sometimes clash.\nCharles Duhigg popularized a telling example of recommendations going too far. In the book, The Power of Habit, he points out the case of an angry father who found out his teenage daughter was pregnant through a targeted ad. The advertising company, using his daughter’s purchasing history, thought she could soon need baby clothes and sent coupons for it. The unsuspecting father received the mail and found the coupons. Shortly after complaining to a representative of the company, learn from his daughter that she was indeed pregnant.\nFor this article, I deliberately chose a definition of Recommender Systems that was not limited to software or computer systems. That is because these systems are not technical matters that only big technology companies can build. Moreover, they are not limited to the digital world or even to human affairs.\nHunter-gatherers civilizations needed to recommend to others the best foraging places for their survival. Kings had panels of ministers for suggesting courses of action in essential areas of government. Even in the animal kingdom, ants leave traces behind to suggest to others in the colony the best routes towards food⁷.\nIt’s in recent times that the use of Recommender Systems has extended to a wide range of digital services. In applications where the number of choices was excessive, it became necessary. The research into such systems started at Duke University in the seventies. Yet, it took two decades for the arrival of the first known software-based Recommender System, Tapestry. It was developed at Xerox Palo Alto Research Center (PARC) and published in the journal Communications of the ACM in 1992⁸.\n\n\n\nXerox PARC researchers\n\n\nXerox PARC researchers during an informal meeting⁹. Probably complaining about all the cat images filling their inboxes.\nUsing Tapestry, the Xerox PARC’s researchers, tried to handle all the unnecessary documents they were receiving due to the increasing use of electronic mail. This system employed people’s collaboration to tag documents based on their reactions. Then, those tags were used to create personal filters that reduced the amount of incoming documents per user. For instance, Alice could create filters to only receive documents tagged as Funny by Bob and Joe, and to receive documents tagged as Important from Mary¹⁰.\nBut how does an algorithm which started as a filter for documents became so rooted in our present-day digital services? That is what we will go through in the next section.\nFor the sake of simplicity, from now on we will focus solely on software-based Recommenders and will refer to them using the broad terms Recommendation Systems, Recommender Systems, and Recommenders.\n\nThe Problems with Small Bookshelves and Infinite Bookshelves\nImagine you are about to open a bookstore in your town. It feels like a terrible idea now that Amazon dominates the market. Even so, nobody will stop your entrepreneurial drive. You’ve already signed a lease for a small but well-located place and are also planning on offering your signature espresso to customers.\nA while ago, you received catalogs of books from a few publishing houses, and today you need to decide which books will fill the shelves. But, as you read through the first catalog, making a decision feels more and more daunting.\nShould I order the latest book by Paulo Coelho?…\nWhat about The Hunger Games series?…\nAnd the recently-published Memories of a Ranch Dresser Expert from my friend Derek¹¹?\nShelf space limits how many books you can have at a time. As you probably want to survive over the long-term, it is sensible to focus on the most popular books. Sorry, Derek…\nIn this case, caring for the individual desires of customers is impossible. You don’t have enough space for so many books. If you want to make money, you need to put on display what you know is on demand. Some clients will not find what they are looking for, but the majority will be happy just by buying the most popular offerings.\nNow, imagine that a few years have gone by. Your strategy is working like a charm. Customers are very happy with your selection of books and signature espresso. So much, that a major bookstore chain recently made a huge offer for your store. They want to name you as CEO to drive their newly established digital strategy.\nFinally, you don’t have to worry about limited shelf space anymore. The company’s homepage is an infinite and fully-customizable bookshelf. Also, you have access to an inventory as big as Amazon’s. You just need to figure out which books to show, out of the fifty million available, to each of the ten million customers you are expecting next month… Hmm…\nYou could stick to your previous strategy of showing each customer the most popular books on the homepage. Yet, millions of customers will have little interest in what you are showing to them. Besides, you will not exploit your vast inventory’s potential. The end result could be millions of angry customers and under-performing sales.\nAnother option could be to show all the available books on the homepage. Nonetheless, you are at risk of the Paradox of Choice. Humans, when faced with abundant choices, instead of feeling happier, get irritated, and anxious¹². Thus, you might end up with angrier customers and fewer sales.\nIt has been couple of weeks and the Board of Directors are already having second thoughts about your designation.\nDesperate, you step out of the office and into the rainy night. Looking at the skies, you scream, asking for a way out…\nSuddenly, your mobile phone vibrates. You spend a few seconds struggling to unlock your phone under the rain until you can read the notification.\n“Wondering what to watch next? We suggest Black Mirror: Bandersnatch” reads on a small banner from Netflix on your dripping-wet screen.\nUgh… Thanks, but right now is not the best of times…\nOr… Is it?\nThat is when recommendation systems step in. Usually, in a less dramatic manner.\nThere is a middle road between providing all possible choices to a user or generic choices to all users. It is possible to provide a few but well-thought suggestions to each user by using a recommender.\nFor that, you do not need to care about the popularity of books. You could match each customers’ interests with books’ attributes as genre, length, and author. For instance, you might find that a few customers would react better to The Lightbringer series instead of the Game of Thrones (GoT). In an online bookstore, that is something you can and need to care for. In a physical store, those same customers will most likely need to settle with the GoT series.\nThe difference in which customers demands are met in the online and physical world is referred to as The Long Tail. The figure below is a visual aid to understanding this phenomenon. Each bar on the horizontal axis represents an item. These bars are ordered in a decreasing manner by popularity (represented in the vertical axis).\n\n\n\nThe Long Tail phenomenon\n\n\nThe Long Tail. Physical stores define what they show to users by their shelf space limitations. Online stores use Recommenders to define what to show.\nThe bars to the left of the dotted vertical line are the items that a physical store may display given its space limitations. In contrast, an online store could display the entire range of items: the tail as well as the popular ones¹³. Recommenders are meant to solve the issue of displaying an excessive number of options in an online context.\nSo far we have seen what recommenders are and the problems they solve. Now we will review what are the different ways Recommenders generate suggestions."
  },
  {
    "objectID": "posts/mind-reading-algorithms.html#fantastic-recommenders-and-where-to-find-them",
    "href": "posts/mind-reading-algorithms.html#fantastic-recommenders-and-where-to-find-them",
    "title": "Mind-reading Algorithms: An Introduction to Recommender Systems",
    "section": "Fantastic Recommenders and Where To Find Them",
    "text": "Fantastic Recommenders and Where To Find Them\nBesides the What and Why of recommenders, it also makes sense to get an idea of how these systems are usually build. For that, we will review the standard six categories of recommenders¹⁴ and which technological companies have made use of them¹⁵:\n\nContent-based (CB): recommends items similar to the ones that the user has liked. For identifying the similarity, the recommender uses characteristics or features from the items. For a books’ recommender the algorithm could use genre, author, or book-length as features to recommend similar books. Used by: Facebook, and Amazon\nCollaborative filtering (CF): recommends the user items that other users with similar tastes liked in the past. The reasoning behind CF is that “two or more individuals sharing some similar interests in one area tend to get inclined towards similar items or products from some other areas too.” Used by: Amazon, Facebook, LinkedIn, and Twitter\nDemographic: recommends items based on the demographic profile of the user. These systems usually segment users following business-specific rules and generate recommendations based on those segments. Used by: eBay\nKnowledge-based: recommends items by matching explicit user’s needs to items’ features. For instance, you specify the number of bedrooms, floor space, and the website returns a list of the best matches of houses.\nCommunity-based: recommends items using the user’s friends’ preferences: Tell me who your friends are, and I’ll tell you what you like.\nHybrid: this type of recommender suggests items combining two or more of the previous techniques. A typical case is to combine a collaborative filtering approach with a content-based system. Used by: Amazon, and Netflix\n\nOut of these six types of recommenders, the first two, Content-based and Collaborative Filtering, are the most popular. There is ample material on both available online. Start there, if you would like to dig deeper into recommenders or build one yourself."
  },
  {
    "objectID": "posts/mind-reading-algorithms.html#closing-words",
    "href": "posts/mind-reading-algorithms.html#closing-words",
    "title": "Mind-reading Algorithms: An Introduction to Recommender Systems",
    "section": "Closing Words",
    "text": "Closing Words\nThis article started as a technical introduction to Recommender Systems. Yet, after a bit of research, I noticed there were already hundreds of articles with a similar goal.\nAs I was not very motivated to do the same, I made this Frankenstein article by mixing a bit of narrative and theory. I hope it was useful for understanding Recommenders and maybe gave you a pity laugh.\nWe are now in an era where these algorithms are shaping a significant part of our daily lives. We should care to understand what is behind what we see in our social media feeds, online shopping suggestions, and other digital services. This article tried to fill that gap in an accessible manner.\nI hope you enjoyed the article. Feel free to drop me a note if you have questions or comments."
  },
  {
    "objectID": "posts/mind-reading-algorithms.html#next-steps",
    "href": "posts/mind-reading-algorithms.html#next-steps",
    "title": "Mind-reading Algorithms: An Introduction to Recommender Systems",
    "section": "Next Steps",
    "text": "Next Steps\nFinally, if you want to learn more about Recommenders, I have a couple of suggestions for starting points:\nTheory\n\nMining Massive Datasets, Chapter 9\nRecommender Systems Handbook\n\nApplications\n\nBeginner’s Recommendation Systems with Python\nRecommendation System Based on PySpark\nBuilding A Collaborative Filtering Recommender System with TensorFlow\n\nDatasets\n\nMovieLens\nTMDB 5000\nRestaurant Data with Consumer Ratings"
  },
  {
    "objectID": "posts/mind-reading-algorithms.html#references",
    "href": "posts/mind-reading-algorithms.html#references",
    "title": "Mind-reading Algorithms: An Introduction to Recommender Systems",
    "section": "References",
    "text": "References\n[1] Mailchimp, What is Retargeting? (2019, date of access)\n[2] R. Reshef, Understanding Collaborative Filtering Approach (2015)\n[3] A. Chandrashekar, F. Amat, J. Basilico and T. Jebara, Netflix’s Artwork Personalization (2017)\n[4] I. MacKenzie, C. Meyer, and S. Noble, How Retailers Can Keep Up With Consumers (2013)\n[5] A. Rodriguez, YouTube’s recommendations drive 70% of what we watch (2018)\n[6] G. Chalot, Twitter Thread on YouTube’s Recommendations (2019)\n[7] R. Sharma, R. Singh, Evolution of Recommender Systems from Ancient Times to Modern Era: A Survey (2016)\n[8] R. Sharma, R. Singh, Evolution of Recommender Systems from Ancient Times to Modern Era: A Survey (2016)\n[9] Computer History, Xerox PARC (2019, date of access)\n[10] Huttner, Joseph, From Tapestry to SVD: A Survey of the Algorithms That Power Recommender Systems (2009)\n[11] The_Curly_Council, This is a profession I can see myself getting into (2013)\n[12] P. Hiebert, The Paradox Of Choice, 10 Years Later (2017)\n[13] J. Leskovec, A. Rajaraman, J. Ullman, Mining Massive Datasets, Chapter 9 (2014)\n[14] F. Ricci, L. Rokach, B. Shapira, Introduction to Recommender Systems Handbook, Chapter 1 (2011)\n[15] R. Sharma, R. Singh, Evolution of Recommender Systems from Ancient Times to Modern Era: A Survey (2016)"
  },
  {
    "objectID": "posts/4-ways-to-improve-your-plotly-graphs.html",
    "href": "posts/4-ways-to-improve-your-plotly-graphs.html",
    "title": "4 Ways To Improve Your Graphs Using Plotly",
    "section": "",
    "text": "These last weeks I’ve been working on an application using Dash and Plotly. These tools are great if you want to get something out quickly. But, as usual, there’s no magical make_beautiful_graphs parameter you can set to True by default.\nIf you want to have beautiful and customized visualizations in your application, you’ll need to spend some time playing around with Plotly’s extensive list of figures’ attributes. I wanted to have something better than the default looks, so I went through Plotly’s documentation, old code snippets I had, and Stack Overflow questions.\nThis is not the first time I found myself doing that. However, this time I decided to keep track of the things I frequently do when making graphs with Plotly. That way, I wouldn’t need to read the same documentation or browse the same Stack Overflow questions next time.\nIn this article I’m compiling a list of things I frequently do when building data visualizations using Plotly. Rest assured, I’ve been working in data-related positions for a while, so you will not find outrageous things like how to make 3D pie charts. These improvements are based on a sample of one, but I’ve frequently seen others applying similar ideas.\nI’m focusing on practical and simple improvements that apply to most of the basic charts: scatter plots, line charts, bar charts, and some statistical charts. Here you will find things like removing gridlines and not things like selecting the best colors for your 4D contour plot.\nFirst, I’ll do a brief introduction on how you build graphs using Plotly. Next, I’ll provide a list of improvements and the reasoning behind them. Last, I’ll give additional recommendations I’ve found useful when working with Plotly and other plotting libraries."
  },
  {
    "objectID": "posts/4-ways-to-improve-your-plotly-graphs.html#how-to-make-a-graph-using-plotly",
    "href": "posts/4-ways-to-improve-your-plotly-graphs.html#how-to-make-a-graph-using-plotly",
    "title": "4 Ways To Improve Your Graphs Using Plotly",
    "section": "How to Make a Graph Using Plotly",
    "text": "How to Make a Graph Using Plotly\nThere are three things you need to know about the inner workings of Plotly:\nFirst, making a graph in Plotly is essentially populating a Python dictionary. This dictionary is usually referred to as figure.\nSecond, there are two keys in the figure dictionary: layout and data. In layout, you define the looks of your graph like the typography, titles, and axes. In the data key, you set the values and information of the traces you’ll be plotting. That could be something like [1, 2, 3] for X, [5, 3, 9] for Y and bar chart type.\nFinally, once you populate the figure dictionary, it is serialized (transformed) into a JSON structure. This resulting data structure is then used by the Plotly JavaScript library to plot your chart.\nThat’s it.\nSo, how do you make a figure?\nThere are multiple ways to do it. The lowest-level approach is to use Python dictionaries, and the highest-level one is using the Plotly Express interface. I tend to use a mid-level interface called Figure Constructor. It’s easier to debug than using Python dictionaries, and it’s more flexible than Plotly Express.\nThe code for making a graph using the Figure Constructor looks as follows:\nimport plotly.graph_objects as go\nimport numpy as np\nnp.random.seed(42)\n\n# Simulate data\nreturns = np.random.normal(0.01, 0.2, 100)\nprice = 100 * np.exp(returns.cumsum())\ntime = np.arange(100)\n\n# Generate graph using Figure Constructor\nlayout = go.Layout(\n    title=\"Historic Prices\",\n    xaxis_title=\"time\",\n    yaxis_title=\"price\"\n)\n\nfig = go.Figure(\n    data=go.Scatter(x=time, y=price),\n    layout=layout\n)\nfig.show()\nHow to make make a line chart using Plotly\nThis is the resulting graph:\n\n\n\nBasic plot in Plotly\n\nFor the code snippets listed below, I used the Figure Constructor approach. You may need to adjust the code to make it work for your case if you are using a different interface.\nSo let’s get down to the meat and potatoes."
  },
  {
    "objectID": "posts/4-ways-to-improve-your-plotly-graphs.html#list-of-improvements",
    "href": "posts/4-ways-to-improve-your-plotly-graphs.html#list-of-improvements",
    "title": "4 Ways To Improve Your Graphs Using Plotly",
    "section": "List of Improvements",
    "text": "List of Improvements\nHere’s the list of things I usually do to improve Plotly graphs:\n\n#1: Remove gridlines and background color\n#2: Keep consistent colors across graphs\n#3: Use spikelines to compare data points\n#4: Remove floating menu, disable zoom and adjust click behavior\n\n\n#1: Remove gridlines and background color\nGridlines are lines that cross the chart to show axis divisions. They help the viewer visualize the value represented by an unlabeled data point fast. However, gridlines are not very useful when working with interactive graphs. You can hover over a data point and see its value. So more often than not, I remove gridlines when working with Plotly.\nHere’s how you can do it:\nimport plotly.graph_objects as go\nimport numpy as np\nnp.random.seed(42)\n\n# Simulate data\nreturns = np.random.normal(0.01, 0.2, 100)\nprice = 100 * np.exp(returns.cumsum())\ntime = np.arange(100)\n\nlayout = go.Layout(\n    title=\"Historic Prices\",\n    plot_bgcolor=\"#FFF\",  # Sets background color to white\n    xaxis=dict(\n        title=\"time\",\n        linecolor=\"#BCCCDC\",  # Sets color of X-axis line\n        showgrid=False  # Removes X-axis grid lines\n    ),\n    yaxis=dict(\n        title=\"price\",\n        linecolor=\"#BCCCDC\",  # Sets color of Y-axis line\n        showgrid=False,  # Removes Y-axis grid lines\n    )\n)\n\nfig = go.Figure(\n    data=go.Scatter(x=time, y=price),\n    layout=layout\n)\nfig.show()\nCode snippet to remove grid lines\nAnd this is how it looks:\n\n\n\nLine chart without gridlines\n\n\n\n#2: Keep consistent colors across graphs\nWhen working with categories, there are two things people usually like to do. First, they want to assign some specific colors to each group. For instance, if you are analyzing electoral results in the US, you probably want to use particular blue and red variations to identify the Democratic and Republican parties.\nSecond, you want this color to remain consistent across all the graphs you do. For example, if you are analyzing some real-world companies, you may want to use their distinctive colors to plot their prices but also when you analyze their returns.\nHere’s how you can do that using Plotly:\nimport plotly.graph_objects as go\nimport numpy as np\nnp.random.seed(42)\n\n# Simulate data\nreturns_A = np.random.normal(0.01, 0.2, 100)\nreturns_B = np.random.normal(0.01, 0.2, 100)\nreturns = np.append(returns_A, returns_B)\n\nprices_A = 100 * np.exp(returns_A.cumsum())\nprices_B = 100 * np.exp(returns_B.cumsum())\nprices = np.append(prices_A, prices_B)\n\ncompanies = [\"A\"] * 100 + [\"B\"] * 100\ntime = np.append(np.arange(100), np.arange(100))\n\ndf = pd.DataFrame({\n    \"company\": companies,\n    \"time\": time,\n    \"price\": prices,\n    \"returns\": returns\n})\n\n# Build graph\nCOLORS_MAPPER = {\n    \"A\": \"#38BEC9\",\n    \"B\": \"#D64545\"\n}\n\nlayout = go.Layout(\n    title=\"Performance of A vs. B\",\n    plot_bgcolor=\"#FFFFFF\",\n    barmode=\"stack\",\n    xaxis=dict(\n        domain=[0, 0.5],\n        title=\"time\",\n        linecolor=\"#BCCCDC\",\n    ),\n    yaxis=dict(\n        title=\"price\",\n        linecolor=\"#BCCCDC\"\n    ),\n    xaxis2=dict(\n        domain=[0.6, 1],\n        title=\"returns\",\n        linecolor=\"#BCCCDC\",\n    ),\n    yaxis2=dict(\n        anchor=\"x2\",\n        linecolor=\"#BCCCDC\"\n    )\n)\n\ndata = []\nfor company,col in COLORS_MAPPER.items():\n    time = df.loc[df.company == company, \"time\"]\n    price = df.loc[df.company == company, \"price\"]\n    returns = df.loc[df.company == company, \"returns\"]\n    line_chart = go.Scatter(\n        x=time,\n        y=price,\n        marker_color=col,  # Defines specific color for a trace\n        legendgroup=company,  # Groups traces belonging to the same group in the legend\n        name=company\n    )\n    histogram = go.Histogram(\n        x=returns,\n        marker_color=col,  # Defines specific color for a trace\n        legendgroup=company,  # Groups traces belonging to the same group in the legend\n        xaxis=\"x2\",\n        yaxis=\"y2\",\n        showlegend=False\n    )\n    data.append(line_chart)\n    data.append(histogram)\n\nfig = go.Figure(data=data, layout=layout)\nfig.show()\n\n\n\nConsistent colors across graphs\n\nThe snippet above allows you to keep consistent colors when working with multiple graphs that share the same categories. The critical part is the COLOR_MAPPER dictionary and its use when adding new traces. This dictionary is the mapping of the categories and colors you’ll be using across your charts.\nWhenever you add a trace to a graph, you can assign the right color to the marker_color attribute by getting it from the COLOR_MAPPER dictionary.\nThe resulting graph looks as follows:\nConsistent colors across graphs\n\n\n#3: Use spike lines to compare data points\nA spike line is a vertical or horizontal line that appears when hovering on data. It’s useful for comparing values in line charts and scatter plots. This is how you can add those using Plotly:\nimport plotly.graph_objects as go\nimport numpy as np\nnp.random.seed(42)\n\n# Simulate data\nreturns_A = np.random.normal(0.01, 0.2, 100)\nreturns_B = np.random.normal(0.01, 0.2, 100)\nreturns = np.append(returns_A, returns_B)\n\nprices_A = 100 * np.exp(returns_A.cumsum())\nprices_B = 100 * np.exp(returns_B.cumsum())\nprices = np.append(prices_A, prices_B)\n\ncompanies = [\"A\"] * 100 + [\"B\"] * 100\ntime = np.append(np.arange(100), np.arange(100))\n\ndf = pd.DataFrame({\n    \"company\": companies,\n    \"time\": time,\n    \"price\": prices,\n    \"returns\": returns\n})\n\n# Build graph\nlayout = go.Layout(\n    title=\"Performance of A vs. B\",\n    plot_bgcolor=\"#FFFFFF\",\n    hovermode=\"x\",\n    hoverdistance=100, # Distance to show hover label of data point\n    spikedistance=1000, # Distance to show spike\n    xaxis=dict(\n        title=\"time\",\n        linecolor=\"#BCCCDC\",\n        showspikes=True, # Show spike line for X-axis\n        # Format spike\n        spikethickness=2,\n        spikedash=\"dot\",\n        spikecolor=\"#999999\",\n        spikemode=\"across\",\n    ),\n    yaxis=dict(\n        title=\"price\",\n        linecolor=\"#BCCCDC\"\n    )\n)\n\ndata = []\nfor company in [\"A\", \"B\"]:\n    time = df.loc[df.company == company, \"time\"]\n    price = df.loc[df.company == company, \"price\"]\n    returns = df.loc[df.company == company, \"returns\"]\n    line_chart = go.Scatter(\n        x=time,\n        y=price,\n        name=company\n    )\n    data.append(line_chart)\n\nfig = go.Figure(data=data, layout=layout)\nfig.show()\nCode snippet to add spike lines to chart\nThis is the resulting graph:\n\n\n\nChart with spike lines\n\n\n\n#4: Remove floating menu, disable zoom and adjust click behavior\nI’m not too fond of the floating menu that Plotly adds to your chart by default. It makes graphs look cool, but I’ve rarely seen people using it. It has so many options that it’s just confusing for someone looking at a graph for the first time. Usually, I remove it.\nAlso, I like to re-define two other user interaction parameters. I prefer to limit the users’ ability to zoom in and change the behavior of clicking on a trace in the legend. In Plotly, by default, if you want to inspect a trace on its own, you have to double-click on the trace, instead of just clicking on it. That’s not very intuitive, so I tend to invert that behavior.\nThis is how you can apply those changes:\nimport plotly.graph_objects as go\nimport numpy as np\nnp.random.seed(42)\n\n# Simulate data\nreturns_A = np.random.normal(0.01, 0.2, 100)\nreturns_B = np.random.normal(0.01, 0.2, 100)\nreturns = np.append(returns_A, returns_B)\n\nprices_A = 100 * np.exp(returns_A.cumsum())\nprices_B = 100 * np.exp(returns_B.cumsum())\nprices = np.append(prices_A, prices_B)\n\ncompanies = [\"A\"] * 100 + [\"B\"] * 100\ntime = np.append(np.arange(100), np.arange(100))\n\ndf = pd.DataFrame({\n    \"company\": companies,\n    \"time\": time,\n    \"price\": prices,\n    \"returns\": returns\n})\n\n# Build graph\nlayout = go.Layout(\n    title=\"Performance of A vs. B\",\n    plot_bgcolor=\"#FFFFFF\",\n    legend=dict(\n        # Adjust click behavior\n        itemclick=\"toggleothers\",\n        itemdoubleclick=\"toggle\",\n    ),\n    xaxis=dict(\n        title=\"time\",\n        linecolor=\"#BCCCDC\",\n        fixedrange=True\n    ),\n    yaxis=dict(\n        title=\"price\",\n        linecolor=\"#BCCCDC\",\n        fixedrange=True\n    )\n)\n\ndata = []\nfor company in [\"A\", \"B\"]:\n    time = df.loc[df.company == company, \"time\"]\n    price = df.loc[df.company == company, \"price\"]\n    returns = df.loc[df.company == company, \"returns\"]\n    line_chart = go.Scatter(\n        x=time,\n        y=price,\n        name=company\n    )\n    data.append(line_chart)\n\nfig = go.Figure(data=data, layout=layout)\nfig.show(config={\"displayModeBar\": False, \"showTips\": False}) # Remove floating menu and unnecesary dialog box\nThis is the resulting graph:\n\n\n\nChart without floating menu, no-zoom and adjusted click behavior"
  },
  {
    "objectID": "posts/4-ways-to-improve-your-plotly-graphs.html#additional-recommendations",
    "href": "posts/4-ways-to-improve-your-plotly-graphs.html#additional-recommendations",
    "title": "4 Ways To Improve Your Graphs Using Plotly",
    "section": "Additional recommendations",
    "text": "Additional recommendations\nThere are three things that I’ve found useful for learning how to make better data visualizations:\n\nGet feedback from your audience: This is not always possible. But if you can do it, always prioritize getting input from those who will use your data visualizations. If you are working on a dashboard, the first thing you should do is understand what problem your dashboard solves. Then see users interacting with it. That has the highest ROI for your time.\nCheck out Storytelling with Data by Cole Knaflic: It’s a great book if you want to level-up your data visualization design skills. It provides a lot of practical advice and compelling use cases.\nPlotly’s Figure Reference: Get used to Plotly’s Figure Reference and documentation. You’ll be using it a lot. Though, there’s nothing to worry about. Plotly has great documentation!"
  },
  {
    "objectID": "posts/4-ways-to-improve-your-plotly-graphs.html#closing-words",
    "href": "posts/4-ways-to-improve-your-plotly-graphs.html#closing-words",
    "title": "4 Ways To Improve Your Graphs Using Plotly",
    "section": "Closing Words",
    "text": "Closing Words\nI hope you’ve find these ideas useful. There might be some things that do not resonate with you, or others that you feel are missing. If that’s the case, please let me know in the comments below. I’ll be happy to update this and add other valuable advice.\nIf you want to keep up-to-date with what I’m doing you can follow me on twitter."
  },
  {
    "objectID": "posts/python-pyscript-101.html",
    "href": "posts/python-pyscript-101.html",
    "title": "PyScript 101",
    "section": "",
    "text": "These past few days, I’ve been playing around with PyScript. I built an interactive cheat sheet for pandas and a cookbook of the most popular Python data visualization libraries.\nI’ve learned a few things about how it works, so I thought of putting together these notes in case others may find them useful. This article covers the minimum you should know about PyScript to build a simple web app with it.\nI deliberately left out topics that I didn’t find as useful or interesting. But if you want a more detailed introduction, you should read this great tutorial from Real Python.\nLet’s get to it!"
  },
  {
    "objectID": "posts/python-pyscript-101.html#whats-pyscript",
    "href": "posts/python-pyscript-101.html#whats-pyscript",
    "title": "PyScript 101",
    "section": "What’s PyScript?",
    "text": "What’s PyScript?\nPyScript is a framework that lets users create Python applications in the browser. It’s built on top of Pyodide, a CPython port to WebAssembly (WASM).\nPyScript’s main selling point is that you no longer need a server to use Python and its extensive number of libraries when building a web app. You can plug in PyScript, make a few adjustments, and use many Python libraries in your web app.\nPeople often compare PyScript with Dash and Streamlit. But PyScript works quite differently. It operates on the client side, so no server is required. You could host a web app that uses PyScript for free using services like GitHub Pages.\nDash and Streamlit, on the other hand, require a server, making apps built with these libraries more difficult to deploy and usually more expensive. Furthermore, the server bears the majority of the computational load."
  },
  {
    "objectID": "posts/python-pyscript-101.html#how-to-use-pyscript",
    "href": "posts/python-pyscript-101.html#how-to-use-pyscript",
    "title": "PyScript 101",
    "section": "How to Use PyScript",
    "text": "How to Use PyScript\nThe good thing about PyScript is that there isn’t any installation required. You just need to add it to the head of your HTML file.\nI’ll assume you have a file named index.html with this content for this and the following sections:\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n    &lt;head&gt;\n        &lt;meta charset=\"UTF-8\"&gt;\n        &lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"&gt;\n        &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n        &lt;title&gt;My Awesome app!&lt;/title&gt;\n    &lt;/head&gt;\n\n    &lt;body&gt;\n\n    &lt;/body&gt;\n&lt;/html&gt;\nThe code above is a minimal example we’ll use as a starting point. Next, I’ll show you how to use PyScript. You can do it in two ways:\n\nDownload a copy of the latest version of PyScript, and add the required files to head in your HTML document:\n\n&lt;head&gt;\n  ...\n  &lt;link rel=\"stylesheet\" href=\"path/to/pyscript.css\" /&gt;\n  &lt;script defer src=\"path/to/pyscript.js\"&gt;&lt;/script&gt;\n&lt;/head&gt;\n\nAdd the CDN version of PyScript’s JS and CSS files to head in your HTML document:\n\n&lt;head&gt;\n  ...\n  &lt;link rel=\"stylesheet\" href=\"https://pyscript.net/latest/pyscript.css\" /&gt;\n  &lt;script defer src=\"https://pyscript.net/latest/pyscript.js\"&gt;&lt;/script&gt;\n&lt;/head&gt;\nPyScript is still under heavy development. If you use the second method, your app may break when new versions are released.\nTo test your changes locally, use Live Server if you like working with VS Code or try any of these methods if you prefer other code editors."
  },
  {
    "objectID": "posts/python-pyscript-101.html#how-to-run-python-code-in-pyscript",
    "href": "posts/python-pyscript-101.html#how-to-run-python-code-in-pyscript",
    "title": "PyScript 101",
    "section": "How to Run Python Code in PyScript",
    "text": "How to Run Python Code in PyScript\nThere are two ways to run Python code in PyScript: using py-script and py-repl tags. These elements inside go inside body in your HTML document.\n\npy-script\nYou can run code using the py-script tag in two ways:\n\nWrap your code with a py-script tag.\n\n&lt;body&gt;\n  ...\n  &lt;py-script&gt;print(\"Hello, world!\")&lt;/py-script&gt;\n  ...\n&lt;/body&gt;\n\nLoad an external script using the src attribute from the py-script tag:\n\n&lt;body&gt;\n    ...\n    &lt;py-script src='path/to/script.py'&gt;&lt;/pyscript&gt;\n    ...\n&lt;/body&gt;\nThe latter is usually preferred to avoid having formatting issues. If you use an HTML auto-formatter, it may break the python code you put inside py-script.\n\n\npy-repl\nYou can also run code using an interactive interpreter, usually known as read–eval–print loop (REPL). Using a REPL makes it feel like you’re running code in a Jupyter notebook.\nYou can create a REPL with PyScript using the py-repl tag as follows:\n&lt;body&gt;\n  &lt;py-repl&gt;print(\"Hello, world!\")&lt;/py-repl&gt;\n&lt;/body&gt;\nIf you try using multiple py-script and py-repl in the same page, you may find some unexpected behavior in the output. I came across two issues related to this but it looks like they’ll get fixed soon."
  },
  {
    "objectID": "posts/python-pyscript-101.html#how-to-import-libraries",
    "href": "posts/python-pyscript-101.html#how-to-import-libraries",
    "title": "PyScript 101",
    "section": "How to Import Libraries",
    "text": "How to Import Libraries\nYou can make Python libraries available in your environment when your app loads by specifying them in the py-env tag as follows:\n&lt;head&gt;\n  ...\n  &lt;py-env&gt; - numpy - pandas - seaborn &lt;/py-env&gt;\n&lt;/head&gt;\nOr you can load them programmatically using micropip:\n&lt;py-script&gt;\n  async def main(): await micropip.install([\"numpy\", \"pandas\", \"seaborn]) await\n  loop.run_until_complete(main())\n&lt;/py-script&gt;"
  },
  {
    "objectID": "posts/python-pyscript-101.html#how-to-interact-with-html-elements",
    "href": "posts/python-pyscript-101.html#how-to-interact-with-html-elements",
    "title": "PyScript 101",
    "section": "How to Interact With HTML Elements",
    "text": "How to Interact With HTML Elements\nPyScript automatically imports a few things from JavaScript’s global namespace into your Python environment. This means that you can use use these elements without having to import them beforehand. The most important ones are window, document, and console.\nwindow, and document, let you read and make changes to elements from the Document Object Model (DOM). While console lets you interact with your browser’s integrated console.\nFor example, you could use document to select a specific element from the DOM:\n&lt;body&gt;\n  ...\n  &lt;py-script&gt;\n    # Select an element using its ID document.getElementById(\"input-code\") #\n    Select an element using its class document.querySelector(\".table\")\n  &lt;/py-script&gt;\n  ...\n&lt;/body&gt;\nOr usedocument to read attributes from elements and make changes to them, and print information using console:\n&lt;body&gt;\n  &lt;ul id=\"users-list\"&gt;\n    &lt;li&gt;Dylan&lt;/li&gt;\n    &lt;li&gt;John&lt;/li&gt;\n    &lt;li&gt;Paul&lt;/li&gt;\n    &lt;li&gt;Jane&lt;/li&gt;\n  &lt;/ul&gt;\n  &lt;py-script&gt;\n    users = [\"Dylan\", \"John\", \"Jane\"] ul = document.querySelectorAll(\"ul &gt; li\")\n    for li in ul: if li.innerHTML not in users: console.log(li.innerHTML)\n    li.remove()\n  &lt;/py-script&gt;\n&lt;/body&gt;\nYou could also make your Python code react to changes in the DOM. But that requires some magic to work because the web browser cannot natively handle Python functions when an event such as a click on a button happens.\nFor those cases, Pyodide lets you create proxies that bridge Python functions and JavaScript callbacks. You could use create_proxy to make your Python code react to changes as follows:\n&lt;body&gt;\n  &lt;input type=\"text\" name=\"\" id=\"input-text\" /&gt;\n  &lt;button type=\"submit\" id=\"button-submit\" for=\"input-text\"&gt;Submit!&lt;/button&gt;\n  &lt;div id=\"text-length\"&gt;&lt;/div&gt;\n  &lt;py-script&gt;\n    from pyodide import create_proxy input_button =\n    document.getElementById(\"button-submit\") def get_text_length(x):\n    document.getElementById(\"text-length\").innerHTML =\n    len(document.getElementById(\"input-text\").value)\n    input_button.addEventListener(\"click\", create_proxy(get_text_length))\n  &lt;/py-script&gt;\n&lt;/body&gt;\nThe code lets the user input a text and run the get_text_length Python function, after he or she clicks on the button on the screen. This function calculates the length of the text that the user entered.\nFinally, it’s worth mentioning that PyScript also provides you with the [Element](https://realpython.com/pyscript-python-in-browser/#pyscripts-adapter-for-javascript-proxy) class. It’s a basic interface that lets you select elements by ID and do certain operations on them. I found it a bit limited, which is why I didn’t cover it in this section."
  },
  {
    "objectID": "posts/python-pyscript-101.html#other-topics",
    "href": "posts/python-pyscript-101.html#other-topics",
    "title": "PyScript 101",
    "section": "Other Topics",
    "text": "Other Topics\nThere are many topics I didn’t cover in this article. So I wanted to provide some resources additional resources I’ve found useful:\n\nPyScript visual components: PyScript provides a few elements you can use to build the UI of your web app. The Real Python tutorial covers them.\nAccessing the file system: Accessing files is a bit tricky. John Hanley made very detailed tutorials about this topic.\nPyScript and plotly: Getting plotly graphs to work with PyScript is tricky. This article explains how to do it.\nHosting PyScript: If you don’t need to store data from your users in a server, you can simply host a static page. The easiest way to do that is using GitHub Pages. That’s what I’ve used in my projects."
  },
  {
    "objectID": "posts/python-pyscript-101.html#conclusion",
    "href": "posts/python-pyscript-101.html#conclusion",
    "title": "PyScript 101",
    "section": "Conclusion",
    "text": "Conclusion\nPyScript is an experimental Python framework that lets you run Python on the web browser. This article covers the basics of PyScript, and will show you how to:\n\nUse PyScript on a web page\nRun Python code using PyScript\nInteract with the DOM using PyScript\n\nIf you have any questions or feedback, let me know in the comments!"
  },
  {
    "objectID": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html",
    "href": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html",
    "title": "Semantic Search with OpenSearch, Cohere, and FastAPI",
    "section": "",
    "text": "Semantic search is a hot topic right now. The fast-paced progress of Large Language Models (LLMs), as well as the availability and quality of embeddings, the key technology behind semantic search, have piqued the interest of many people in this field.\nI’ve worked on a number of projects involving semantic search (before it was cool!), and have been closely following the progress in LLMs. So I decided to write a step-by-step tutorial that combined these two technologies.\nIn this tutorial, I’ll show you how to build a semantic search service using OpenSearch, Cohere, and FastAPI. You’ll create an app that lets users search through news articles to find the ones that are most relevant to their query.\nLet’s get started!"
  },
  {
    "objectID": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#prerequisites",
    "href": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#prerequisites",
    "title": "Semantic Search with OpenSearch, Cohere, and FastAPI",
    "section": "Prerequisites",
    "text": "Prerequisites\nThere are a few things you need to know to get the most out of this tutorial:\n\nWhat semantic search is.\nHow OpenSearch works.\nWhat LLMs are.\n\nDon’t feel discouraged if some of these concepts are new to you. A basic understanding of these topics should be enough to complete this tutorial.\nIn addition, you must install Docker and create an account at Cohere."
  },
  {
    "objectID": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#keyword-based-search-vs.-semantic-search",
    "href": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#keyword-based-search-vs.-semantic-search",
    "title": "Semantic Search with OpenSearch, Cohere, and FastAPI",
    "section": "Keyword-based Search vs. Semantic Search",
    "text": "Keyword-based Search vs. Semantic Search\nSearch engines have evolved over time to provide users with more relevant results. In the past, search engines relied on keyword matching to deliver these results. For example, if a user searched for “AI chatbot,” the search engine would find documents that included that phrase and show them based on a ranking system like PageRank.\nThis method worked well for finding results that contained specific keywords but fell short when users sought information that was related to, but not identical to, their initial query. For example, a search for “machine learning” might yield more relevant results if it also considered semantically similar terms such as “artificial intelligence” or “deep learning”.\nEnter semantic search. It is a more sophisticated method that takes into account factors like synonyms, user context, and concept relationships when generating search results. By considering these factors, this approach provides users with better sets of results."
  },
  {
    "objectID": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#architecting-a-semantic-search-service",
    "href": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#architecting-a-semantic-search-service",
    "title": "Semantic Search with OpenSearch, Cohere, and FastAPI",
    "section": "Architecting a Semantic Search Service",
    "text": "Architecting a Semantic Search Service\nAside from the data extraction pipeline, which I’m not including here, the semantic search service you’ll create has four parts:\n\nVectorizer: This takes care of creating numerical vectors, called embeddings, from the documents (news articles) in your dataset.\nIndexer: This adds the embeddings and the metadata such as URL, title, and author to the vector database.\nVector database: This is a database that stores and retrieves vectors representing documents.\nSearch client: This is a FastAPI-based backend service that processes the user’s query, vectorizes it, and searches the vector database for the most similar vectors.\n\nHere’s a diagram of all the components:\n\nArchitecture diagram\nNext, you’ll set up your local environment to run the project."
  },
  {
    "objectID": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#set-up-your-local-environment",
    "href": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#set-up-your-local-environment",
    "title": "Semantic Search with OpenSearch, Cohere, and FastAPI",
    "section": "Set Up Your Local Environment",
    "text": "Set Up Your Local Environment\nFollow these steps to set up your local environment:\n\nInstall Python 3.11.\nClone the repository with the sample app:\n\ngit clone https://github.com/dylanjcastillo/opensearch-cohere-semantic-search\n\nGo to the root folder of the project and create a virtual environment with the dependencies using venv and pip:\n\npython3.11 -m venv .venv && source .venv/bin/activate\npip install -r requirements.txt\nAssuming all went smoothly, you should have a virtual environment set up with the required libraries and the following project structure:\nopensearch-cohere-semantic-search\n├── LICENSE\n├── README.md\n├── data\n│   ├── news.csv\n│   ├── news_sample.csv\n│   └── news_sample_with_vectors.csv\n├── notebooks\n│   └── generate_sample.ipynb\n├── requirements.txt\n├── run_opensearch_docker.sh\n└── src\n│   ├── app.py\n│   ├── config.py\n│   ├── indexer.py\n│   └── vectorizer.py\n├── .env-example\n└── .venv/\nThe project is organized into several key files and directories, as described below\n\ndata/: This directory contains the project’s data. It contains the original dataset downloaded from Kaggle, and a sample, which you’ll use in the tutorial.\nrequirements.txt: This file contains a list of Python packages required by the project and their respective versions.\nrun_opensearch_docker.sh: This file contains a bash script used to run an OpenSearch cluster locally.\nsrc/app.py: This file contains the code of the FastAPI application.\nsrc/config.py: This file contains project configuration specifications such as Cohere’s API key (read from a .env file), the paths to the data, and the name of the index.\nsrc/indexer.py: This file contains the code you use to create an index and insert the documents in OpenSearch.\nsrc/vectorizer.py: This file contains the code to transform the input data into embeddings.\n.env-example: This file is an example of the environment variables you must provide.\n.venv/: This directory contains the project’s virtual environment.\n\nAll done! Let’s get going."
  },
  {
    "objectID": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#run-a-local-opensearch-cluster",
    "href": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#run-a-local-opensearch-cluster",
    "title": "Semantic Search with OpenSearch, Cohere, and FastAPI",
    "section": "Run a Local OpenSearch Cluster",
    "text": "Run a Local OpenSearch Cluster\nBefore we get into the code, you should start a local OpenSearch cluster. Open a new terminal, navigate to the project’s root folder, and run:\nsh run_opensearch_docker.sh\nThis will launch a local OpenSearch cluster. If everything went well, the terminal will show a long string of text. Keep the terminal open in the background and move on to the next step."
  },
  {
    "objectID": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#vectorize-the-articles",
    "href": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#vectorize-the-articles",
    "title": "Semantic Search with OpenSearch, Cohere, and FastAPI",
    "section": "Vectorize the Articles",
    "text": "Vectorize the Articles\nYou’ll start by transforming the news articles into vectors (embeddings). There are many approaches you could take such as using Word2Vec, Sentence-Transformers, or LLM-based embedding services. In this case, you’ll use Cohere.\nUse src/vectorizer.py for that:\nimport cohere\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom config import COHERE_API_KEY, NEWS_SAMPLE_DATASET, DATA\n\n\ndef main():\n    df = pd.read_csv(NEWS_SAMPLE_DATASET)\n    cohere_client = cohere.Client(COHERE_API_KEY)\n\n    model = \"small\"\n    batch_size = 96\n    batch = []\n    vectors = []\n\n    for _, row in tqdm(df.iterrows(), total=len(df)):\n        batch.append(row[\"text\"])\n\n        if len(batch) &gt;= batch_size:\n            response = cohere_client.embed(texts=batch, model=model)\n            vectors.append(response.embeddings)\n            batch = []\n\n    if len(batch) &gt; 0:\n        response = cohere_client.embed(texts=batch, model=model)\n        vectors.append(response.embeddings)\n        batch = []\n\n    df[\"vector\"] = [item for sublist in vectors for item in sublist]\n\n    df.to_csv(DATA / \"news_sample_with_vectors.csv\", index=False)\n\n\nif __name__ == \"__main__\":\n    main()\nThis code reads the news articles dataset, splits it into batches, and generates embeddings for each individual article. It works as follows:\n\nLines 1 to 5 import the required Python libraries and the configuration settings from config.py.\nLines 9 to 28 read the news articles sample, start the Cohere client, split the dataset into batches of 96 documents (as this is the maximum accepted by Cohere), and uses the client to get embeddings for each document.\nLines 30 to 32 create a new column in the DataFrame to store the vectors and save the new dataset into your filesystem.\n\nYou can run this script by opening a terminal in src and running:\npython vectorizer.py\nNext, you’ll create an index to store the embeddings."
  },
  {
    "objectID": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#index-the-vectors-and-metadata",
    "href": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#index-the-vectors-and-metadata",
    "title": "Semantic Search with OpenSearch, Cohere, and FastAPI",
    "section": "Index the Vectors and Metadata",
    "text": "Index the Vectors and Metadata\nAfter you’ve created embeddings of each article, you’ll store them, and their metadata (title, content, description), in an index in your OpenSearch cluster.\nYou can use src/indexer.py for that:\nimport pandas as pd\nfrom opensearchpy import OpenSearch, NotFoundError\nfrom config import NEWS_WITH_VECTORS_DATASET, INDEX_NAME\n\nfrom tqdm import tqdm\n\n\ndef main():\n    client = OpenSearch(\n        hosts=[{\"host\": \"localhost\", \"port\": 9200}],\n        http_auth=(\"admin\", \"admin\"),\n        use_ssl=True,\n        verify_certs=False,\n        ssl_assert_hostname=False,\n        ssl_show_warn=False,\n    )\n\n    df = pd.read_csv(NEWS_WITH_VECTORS_DATASET)\n\n    body = {\n        \"settings\": {\n            \"index\": {\"knn\": True},\n        },\n        \"mappings\": {\n            \"properties\": {\n                \"id\": {\"type\": \"integer\"},\n                \"title\": {\"type\": \"keyword\"},\n                \"content\": {\"type\": \"keyword\"},\n                \"description\": {\"type\": \"keyword\"},\n                \"embedding\": {\"type\": \"knn_vector\", \"dimension\": 1024},\n            }\n        },\n    }\n\n    try:\n        client.indices.delete(index=INDEX_NAME)\n    except NotFoundError:\n        pass\n    client.indices.create(INDEX_NAME, body=body)\n\n    for i, row in tqdm(df.iterrows(), total=len(df)):\n        embedding = [\n            float(x) for x in row[\"vector\"].replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")\n        ]\n        client.index(\n            index=INDEX_NAME,\n            body={\n                \"source_id\": i,\n                \"title\": row[\"title\"],\n                \"content\": row[\"content\"],\n                \"description\": row[\"description\"],\n                \"embedding\": embedding,\n            },\n        )\n\n    client.indices.refresh(index=INDEX_NAME)\n    print(\"Done\", client.cat.count(index=INDEX_NAME, format=\"json\"))\n\n\nif __name__ == \"__main__\":\n    main()\nThis code will create a new index in your OpenSearch cluster, and store the vectors and metadata in it. Here’s how it works:\n\nLines 1 to 5 import the required Python libraries and the predefined configuration settings from config.py.\nLines 9 to 16 start the OpenSearch client.\nLines 20 to 33 define the settings and mappings of the index you’ll create. You set \"knn\": True so that OpenSearch knows that you’ll be using the k-NN plugin to store and retrieve vectors. Very importantly, you also need to define the size of the vector in the mappings, based on the model you use. Cohere’s small embeddings generate vectors of 1024 dimensions.\nLines 35 to 54 create the index (and delete any previous ones), and add each document one by one. You index the id, title, description, and embedding for each document.\n\nYou can run this script by opening a terminal in src and running:\npython indexer.py\nSo far, you’ve created embeddings for each document and indexed them in your OpenSearch cluster. Next, you’ll run a search client to interact with them."
  },
  {
    "objectID": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#create-a-search-client",
    "href": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#create-a-search-client",
    "title": "Semantic Search with OpenSearch, Cohere, and FastAPI",
    "section": "Create a Search Client",
    "text": "Create a Search Client\nFinally, you’ll create a search client so that users can search the articles you indexed using FastAPI. It’ll let users provide a search term, and give them back the 10 most similar documents based on that term.\nThe code is available in src/app.py:\nimport cohere\nfrom fastapi import FastAPI\n\nfrom config import COHERE_API_KEY, INDEX_NAME\n\nfrom opensearchpy import OpenSearch\n\n\napp = FastAPI()\n\nopensearch_client = OpenSearch(\n    hosts=[{\"host\": \"localhost\", \"port\": 9200}],\n    http_auth=(\"admin\", \"admin\"),\n    use_ssl=True,\n    verify_certs=False,\n    ssl_assert_hostname=False,\n    ssl_show_warn=False,\n)\n\ncohere_client = cohere.Client(COHERE_API_KEY)\n\n\n@app.get(\"/\")\ndef index():\n    return {\"message\": \"Make a post request to /search to search through news articles\"}\n\n\n@app.post(\"/search\")\ndef search(query: str):\n    query_embedding = cohere_client.embed(texts=[query], model=\"small\").embeddings[0]\n\n    similar_news = opensearch_client.search(\n        index=INDEX_NAME,\n        body={\n            \"query\": {\"knn\": {\"embedding\": {\"vector\": query_embedding, \"k\": 10}}},\n        },\n    )\n    response = [\n        {\n            \"title\": r[\"_source\"][\"title\"],\n            \"description\": r[\"_source\"][\"description\"],\n            \"content\": r[\"_source\"][\"content\"],\n        }\n        for r in similar_news[\"hits\"][\"hits\"]\n    ]\n\n    return {\n        \"response\": response,\n    }\nThis code lets users search through the index. It works as follows:\n\nLines 1 to 6 import the required Python libraries, and the configuration defined in config.py.\nLines 9 to 20 initialize the FastAPI app, and the OpenSearch and Cohere clients.\nLines 23 to 25 define an endpoint that provides the user with a message explaining how to use the app if they make a GET request to “/”.\nLines 28 to 49 define a**/**search endpoint that accepts a query string parameter. It uses Cohere to generate an embedding from a query and then searches the OpenSearch index for the ten most similar documents. Finally, it formats the results as a user response.\n\nTo run the app, you can use uvicorn app:app --reload. You can test the app by opening your browser, navigating to localhost:8000/docs, and clicking on POST /search:\n\nFor instance, if you search for “Nicolas Maduro,” the current president of Venezuela who is widely regarded as a dictator. You’ll get results for articles about authoritarian governments or power abuses:\n\nThat’s it! If you want to know how to deploy this app, check out a previous article I wrote."
  },
  {
    "objectID": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#conclusion",
    "href": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#conclusion",
    "title": "Semantic Search with OpenSearch, Cohere, and FastAPI",
    "section": "Conclusion",
    "text": "Conclusion\nCongrats! You’ve built your own semantic search service. In this tutorial, you’ve learned:\n\nWhat is semantic search, and how it is different from keyword-based search.\nWhat are the main components of a semantic search service.\nHow to use Cohere to vectorize text data.\nHow to use OpenSearch to store embeddings.\n\nHope you found this tutorial useful. Let me know if you have any questions!\nAll the code for this tutorial is available on GitHub."
  },
  {
    "objectID": "posts/classify-images-with-gemini-flash-1.5.html",
    "href": "posts/classify-images-with-gemini-flash-1.5.html",
    "title": "Classifying images with Gemini Flash 1.5",
    "section": "",
    "text": "Most people think of In-Context Learning (ICL) — the ability of LLMs to learn from examples provided in the context — only as a component of RAG applications.\nI used to think of it that way too. Until I recently found out that Multimodal Large Language Models (MLLMs) with ICL can be used to perform more traditional ML tasks such as image classification.\nI was skeptical at first, but was surprised to see that it worked pretty well both in the literature (see here and here) and in my own experiments.\nYou shouldn’t expect state-of-the-art results with it, but it can often give you pretty good results with very little effort and data.\nIn this tutorial, I’ll show you how to use ICL to classify images using Gemini Flash 1.5."
  },
  {
    "objectID": "posts/classify-images-with-gemini-flash-1.5.html#why-gemini-flash-1.5",
    "href": "posts/classify-images-with-gemini-flash-1.5.html#why-gemini-flash-1.5",
    "title": "Classifying images with Gemini Flash 1.5",
    "section": "Why Gemini Flash 1.5?",
    "text": "Why Gemini Flash 1.5?\nYou can use any MLLM for this task, but I chose Gemini Flash 1.5 because:\n\nIt’s cheaper than Gemini Pro 1.5, GPT-4o, and Sonnet 3.5. For an image of 512x512 pixels, Gemini Flash 1.5 is 50x cheaper than Gemini Pro 1.5, 5x to 16x cheaper than GPT-4o, and 26x cheaper than Sonnet 3.51.\nIt lets you use up to 3,000 images per request. By trial and error, I found that GPT-4o seems to have a hard limit at 250 images per request and Sonnet 3.5’s documentation mentions a limit of 20 images per request.\nIt works well. If you really want to squeeze the last bit of performance out of your model, you can use a bigger model, but for the purposes of this tutorial, Gemini Flash 1.5 will do just fine.\n\nRegardless of the model you choose, this tutorial will be a good starting point for you to classify images using ICL."
  },
  {
    "objectID": "posts/classify-images-with-gemini-flash-1.5.html#prerequisites",
    "href": "posts/classify-images-with-gemini-flash-1.5.html#prerequisites",
    "title": "Classifying images with Gemini Flash 1.5",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo follow this tutorial you’ll need to:\n\nSign up and generate an API key in Google AI Studio.\nSet the API key as an environment variable called GEMINI_API_KEY.\nDownload this dataset and save it to data/.\nCreate a virtual environment and install the requirements:\n\npython -m venv venv\nsource venv/bin/activate\npip install pandas numpy scikit-learn google-generativeai pillow"
  },
  {
    "objectID": "posts/classify-images-with-gemini-flash-1.5.html#set-up",
    "href": "posts/classify-images-with-gemini-flash-1.5.html#set-up",
    "title": "Classifying images with Gemini Flash 1.5",
    "section": "Set up",
    "text": "Set up\nAs usual, you start by importing the necessary libraries:\n\nimport json\nimport os\nimport warnings\n\nimport google.generativeai as genai\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom PIL import Image\nfrom pathlib import Path\n\nwarnings.filterwarnings(\"ignore\")\n\nnp.random.seed(42)\n\nIn addition to the usual popular libraries (e.g. pandas, sklearn), you’ll need:\n\ngoogle.generativeai for interacting with the Gemini API\nPIL for handling images\nsklearn for calculating performance metrics\n\nThen, you’ll need to configure the Gemini API client with your API key:\n\ngenai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n\nThis will take the GEMINI_API_KEY environment variable and use it to authenticate your requests to the Gemini API."
  },
  {
    "objectID": "posts/classify-images-with-gemini-flash-1.5.html#read-data",
    "href": "posts/classify-images-with-gemini-flash-1.5.html#read-data",
    "title": "Classifying images with Gemini Flash 1.5",
    "section": "Read data",
    "text": "Read data\nTo make a fair evaluation of the model’s performance, you should split the dataset into separate training and testing sets. The training set is used to provide context or examples to the model during inference. The testing set, comprised of unseen images, is then used to measure the model’s performance.\nThis process is different from the traditional “training” process, where you update the model’s weights or parameters. Here, you’re only providing the model with a set of images and asking it to learn from them at inference time.\nThis function will help you create the datasets:\n\ndef create_datasets(train_dir, test_dir, selected_classes, n_images_icl=3):\n    train_data = []\n    test_data = []\n\n    for class_id, class_name in enumerate(selected_classes):\n        train_class_dir = train_dir / class_name\n        test_class_dir = test_dir / class_name\n\n        if not train_class_dir.is_dir() or not test_class_dir.is_dir():\n            continue\n\n        # Train dataset\n        train_image_files = list(train_class_dir.glob(\"*.jpg\"))\n        selected_train_images = np.random.choice(\n            train_image_files,\n            size=min(n_images_icl, len(train_image_files)),\n            replace=False,\n        )\n        for img_path in selected_train_images:\n            train_data.append(\n                {\n                    \"image_path\": str(img_path),\n                    \"class_id\": f\"class_{class_id}\",\n                    \"class_name\": class_name,\n                }\n            )\n\n        # Test dataset\n        test_image_files = list(test_class_dir.glob(\"*.jpg\"))\n        for img_path in test_image_files:\n            test_data.append(\n                {\n                    \"image_path\": str(img_path),\n                    \"class_id\": f\"class_{class_id}\",\n                    \"class_name\": class_name,\n                }\n            )\n\n    df_train = pd.DataFrame(train_data)\n    df_test = pd.DataFrame(test_data).sample(frac=1).reset_index(drop=True)\n\n    return df_train, df_test\n\nThis function will get a random selection of n_images_icl images per class from the train folder (that you’ll later use in the model’s context). For the testing set, which you’ll use to measure the model’s performance, you’ll use all the available images in the test folder from those classes.\nTo keep things simple, you’ll start by selecting 15 different classes and 1 image per class for the context (i.e., n_images_icl=1)\n\nDATA_DIR = \"../data/\"\nTRAIN_DIR = Path(DATA_DIR) / \"train\"\nTEST_DIR = Path(DATA_DIR) / \"test\"\n\nall_classes = list(os.listdir(TRAIN_DIR))\nselected_classes = np.random.choice(all_classes, size=15, replace=False)\n\ndf_train, df_test = create_datasets(TRAIN_DIR, TEST_DIR, selected_classes=selected_classes, n_images_icl=1)\n\nThere will be 15 classes with 1 image in the training set and 15 classes with 5 images in the testing set."
  },
  {
    "objectID": "posts/classify-images-with-gemini-flash-1.5.html#gemini-flash-1.5",
    "href": "posts/classify-images-with-gemini-flash-1.5.html#gemini-flash-1.5",
    "title": "Classifying images with Gemini Flash 1.5",
    "section": "Gemini Flash 1.5",
    "text": "Gemini Flash 1.5\nNext, you’ll need to define a system prompt and configure the model to use it.\n\nDefine prompt\nYou’ll use a system prompt that will tell the model how to classify the images and the format you want the output to be in:\n\nCLASSIFIER_SYSTEM_PROMPT = \"\"\"You are an expert lepidopterist.\n\nYour task is to classify images of butterflies into one of the provided labels.\n\nProvide your output as a JSON object using this format:\n\n{\n    \"number_of_labeled_images\": &lt;integer&gt;,\n    \"output\": [\n        {\n            \"image_id\": &lt;image id, integer, starts at 0&gt;,\n            \"confidence\": &lt;number between 0 and 10, the higher the more confident, integer&gt;,\n            \"label\": &lt;label of the correct butterfly species, string&gt;\n        }, \n        ...\n    ]\n}\n\n## Guidelines\n\n- ALWAYS produce valid JSON.\n- Generate ONLY a single prediction per input image.\n- The `number_of_labeled_images` MUST be the same as the number of input images.\n\nThis is an example of a valid output:\n```\n{\n  \"number_of_labeled_images\": 5,\n  \"output\": [\n      {\n        \"image_id\": 0,\n        \"confidence\": 10,\n        \"correct_label\": \"class_B\"\n      },\n      {\n        \"image_id\": 1,\n        \"confidence\": 9,\n        \"correct_label\": \"class_C\"\n      },\n      {\n        \"image_id\": 2,\n        \"confidence\": 4,\n        \"correct_label\": \"class_A\"\n      },\n      {\n        \"image_id\": 3,\n        \"confidence\": 2,\n        \"correct_label\": \"class_B\"\n      },\n      {\n        \"image_id\": 4,\n        \"confidence\": 10,\n        \"correct_label\": \"class_C\"\n      }\n  ]\n}\n```\n\"\"\".strip()\n\nThis prompt explains the task to the model. You’re providing it with a set of labels with corresponding images, and a set of images that should be classified into one of those labels. The model needs to output a single label for each image.\nI included an additional field called number_of_labeled_images because I noticed that the model would often “forget” to include all the labels in the output, and this was a simple way to ensure that it did so.\n\n\n\n\n\n\nNote\n\n\n\nFun fact: I didn’t know that lepidopterist was a word until I wrote this prompt.\n\n\n\n\nConfigure model\nThen, you can define and configure the model:\n\ngeneration_config = {\n  \"temperature\": 1,\n  \"max_output_tokens\": 8192,\n  \"response_mime_type\": \"application/json\",\n}\nclassification_model = genai.GenerativeModel(\n    \"gemini-1.5-flash\", \n    system_instruction=CLASSIFIER_SYSTEM_PROMPT, \n    generation_config=generation_config\n)\n\nThis sets up the model with the following configuration:\n\ntemperature=1: Controls the randomness of the model’s output.\nmax_output_tokens=8192: The maximum number of tokens the model can generate.\nresponse_mime_type=\"application/json\": Tells the model to produce JSON.\n\nIt also sets the system_instruction using the prompt you defined earlier and uses gemini-1.5-flash as the model.\n\n\nBuilding the context\nGemini has a slightly different way of building the messages (context) used by the model.\nMost providers have adjusted their API to match OpenAI’s messages format. Gemini, however, uses a list of strings and media files (if you’re including images).\nYou can use these functions for that:\n\ndef create_context_images_message(df):\n    messages = [\"Possible labels:\"]\n    grouped = df.groupby('class_id')\n    for class_id, group in grouped:\n        for _, row in group.iterrows():\n            base64_img = Image.open(row[\"image_path\"])\n            messages.append(base64_img)\n        messages.append(f\"label: {class_id}\")\n    return messages\n    \ncontext_images_message = create_context_images_message(df_train)\n\nFirst, you’ll create a message with the context images and their corresponding labels. This is the “training” part of ICL.\nIn create_context_images_message, you’re iterating over the training dataset, grouping the images by class and appending the images and labels to the messages list.\nThe resulting message will look something like this:\n\ncontext_images_message[:5]\n\n['Possible labels:',\n &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224&gt;,\n 'label: class_0',\n &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224&gt;,\n 'label: class_1']\n\n\nYou might have noticed that instead of the actual names of the classes, you’re using class_0, class_1, etc. This is because I want to make the model prediction as “fair” as possible, see the baseline performance section for more details.\nThen, you’ll create a message with the input images. This are the images for which the model will generate predictions.\nSimlar to the context images message, you’re iterating over the test dataset and appending the images to the messages list.\n\ndef create_input_images_message(df):\n    messages = [\"Input images:\"]\n    for i, image_path in enumerate(df.image_path):\n        base64_img = Image.open(image_path)\n        image_message = [\n            base64_img,\n            f\"input_image_id: {i}\",\n        ]\n        messages.extend(image_message)\n    messages.append(f\"Please correctly classify all {df.shape[0]} images.\")\n    return messages\n\ninput_images_message = create_input_images_message(df_test)\n\nThe resulting message will look something like this:\n\ninput_images_message[:5]\n\n['Input images:',\n &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224&gt;,\n 'input_image_id: 0',\n &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224&gt;,\n 'input_image_id: 1']\n\n\n\n\nResults\nNow, you can combine the context images message and the input images message to create the contents you’ll pass to the model:\n\ncontents = context_images_message + input_images_message\nresponse = classification_model.generate_content(\n    contents=contents\n)\nresponse_json = json.loads(response.text)\n\nIt’ll take a few seconds to run. But after that you’ll have a JSON response with the model’s predictions:\n\nresponse_json[\"output\"][:3]\n\n[{'image_id': 0, 'confidence': 10, 'label': 'class_7'},\n {'image_id': 1, 'confidence': 10, 'label': 'class_2'},\n {'image_id': 2, 'confidence': 10, 'label': 'class_4'}]\n\n\nThen, you can calculate the accuracy and F1-score to evaluate the model’s performance:\n\ndef calculate_metrics(df_test, response_json):\n    predictions = [item['label'] for item in response_json['output']]\n    accuracy = accuracy_score(df_test.class_id, predictions)\n    f1 = f1_score(df_test.class_id, predictions, average='weighted')\n    return accuracy, f1\n\naccuracy, f1 = calculate_metrics(df_test, response_json)\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1-score: {f1:.4f}\")\n\nAccuracy: 0.7333\nF1-score: 0.7229\n\n\nUsing a single image in the context per class, you should get an accuracy around 73% and F1-score around 72%.\nNot bad, but you can probably do better.\n\nUsing 5 images per class in the context\nOne quick way to improve the performance of the model is to use more images per class in the context. Try with 5 images per class:\n\ndf_train, df_test = create_datasets(TRAIN_DIR, TEST_DIR, selected_classes=selected_classes, n_images_icl=5)\n\n# Create the context and input messages\ncontext_images_message = create_context_images_message(df_train)\ninput_images_message = create_input_images_message(df_test)\ncontents = context_images_message + input_images_message\n\n# Generate the response\nresponse = classification_model.generate_content(\n    contents=contents\n)\nresponse_json = json.loads(response.text)\n\n# Calculate the metrics\naccuracy, f1 = calculate_metrics(df_test, response_json)\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1-score: {f1:.4f}\")\n\nAccuracy: 0.9067\nF1-score: 0.9013\n\n\nWith this change, you should get an accuracy and F1-score around 90%.\nNice gains in performance for such a small change!\n\n\n\nData leakage and baseline performance\nYou might be thinking, “MLLMs have been trained on a lot of data, so they already know a lot of the images in the dataset, which means that these results are inflated”.\nWhich is a good point, and for that purpose I’ve done two things:\n\nAnonymize the names of the classes (e.g., class_0 instead of Sleepy Orange), so that the model doesn’t have any information about the actual labels.\nRun a quick experiment using a zero-shot2 model without anonymizing the labels to see the model’s performance.\n\nHere’s the code for the zero-shot baseline and the results:\n\npossible_labels = \"Possible labels: \" + \", \".join(df_train.class_name.unique().tolist())\nclass_name_to_id = dict(zip(df_train['class_name'], df_train['class_id']))\n\nresponse = classification_model.generate_content(\n    contents=[possible_labels] + input_images_message\n)\nresponse_json = json.loads(response.text)\n\nfor item in response_json[\"output\"]:\n    item['label'] = class_name_to_id.get(item['label'], item['label'])\n\naccuracy, f1 = calculate_metrics(df_test, response_json)\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1-score: {f1:.4f}\")\n\nAccuracy: 0.4800\nF1-score: 0.4619\n\n\nYou should get a 48% accuracy and a 46% F1-score. Both significantly higher than the ~7% you’d expect from random guessing, but still far from the 90%+ accuracy you obtained earlier.\nThis demonstrates that ICL can indeed enhance the model’s performance."
  },
  {
    "objectID": "posts/classify-images-with-gemini-flash-1.5.html#conclusion",
    "href": "posts/classify-images-with-gemini-flash-1.5.html#conclusion",
    "title": "Classifying images with Gemini Flash 1.5",
    "section": "Conclusion",
    "text": "Conclusion\nThat’s all!\nI still find it amazing that without any “real” training and just a few minutes of work, you can achieve pretty good results in a non-trivial image classification task using ICL with Gemini Flash 1.5 (or most other MLLMs).\nThis is a mostly unexplored area. There’s a lot of room for trying out different ideas and seeing what works best. This tutorial is just a starting point.\nHope you found it useful! Let me know if you have any questions in the comments below."
  },
  {
    "objectID": "posts/classify-images-with-gemini-flash-1.5.html#footnotes",
    "href": "posts/classify-images-with-gemini-flash-1.5.html#footnotes",
    "title": "Classifying images with Gemini Flash 1.5",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEstimated costs as of September 8, 2024:\n\n\n\nModel\nCost (512x512 image)\n\n\n\n\nGemini Flash 1.5\n$0.000039\n\n\nGemini Pro 1.5\n$0.0018\n\n\nGPT-4o\n$0.000213 - $0.000638\n\n\nSonnet 3.5\n$0.001047\n\n\n\n↩︎\nThat is, without providing any context images.↩︎"
  },
  {
    "objectID": "posts/create-a-kamal-ready-vps-on-hetzner-using-terraform.html",
    "href": "posts/create-a-kamal-ready-vps-on-hetzner-using-terraform.html",
    "title": "Create a Kamal-ready VPS on Hetzner using Terraform",
    "section": "",
    "text": "These days, I deploy all my side projects using Kamal and GitHub Actions on Hetzner. Once you get the hang of it, it’s easy to maintain, fast, and cheap.\nYou can run your app with a database (SQLite), caching (Redis), background jobs (Celery), and SSL certificates (Let’s Encrypt) for roughly €5/month. Plus, if you feel the need, you can easily scale up to a more powerful Virtual Private Server (VPS).\nBut setting up a VPS with the right configuration takes a bit of time. You have to:\nI already had a small script to do most of these steps, but I wanted to automate it to a single command. So I created a Terraform script to do it for me.\nI took terraform-hetzner and modified it to work with a single VPS instance. My updated version is available here."
  },
  {
    "objectID": "posts/create-a-kamal-ready-vps-on-hetzner-using-terraform.html#set-up",
    "href": "posts/create-a-kamal-ready-vps-on-hetzner-using-terraform.html#set-up",
    "title": "Create a Kamal-ready VPS on Hetzner using Terraform",
    "section": "Set up",
    "text": "Set up\nFirst, set up an API key with read and write permissions in Hetzner Cloud.\nSecond, install terraform.\nThird, clone the repo:\ngit clone https://github.com/dylanjcastillo/terraform-kamal-single-vps\nFourth, create a terraform.tfvars file with the following variables:\nhetzner_api_key = \"your-api-key\"\nssh_vps_root_key = \"&lt;your-ssh-root-public-key&gt;\"\nssh_vps_kamal_key = \"&lt;your-ssh-kamal-public-key&gt;\"\nThe ssh_vps_root_key and ssh_vps_kamal_key are the public keys for the root and kamal users, respectively. You can generate them with the make generate-ssh-key USER_NAME=root or make generate-ssh-key USER_NAME=kamal commands I added to the repo.\nStore your SSH keys in a secure location. You’ll need them to access the VPS."
  },
  {
    "objectID": "posts/create-a-kamal-ready-vps-on-hetzner-using-terraform.html#run-the-script",
    "href": "posts/create-a-kamal-ready-vps-on-hetzner-using-terraform.html#run-the-script",
    "title": "Create a Kamal-ready VPS on Hetzner using Terraform",
    "section": "Run the script",
    "text": "Run the script\nOnce the terraform.tfvars file is set up, you can see what changes will be applied with the following command:\nterraform plan\nThis will show in detail what changes will be applied to create a Kamal-ready VPS. If you’re happy with it, you can apply the changes with the following command:\nterraform apply\nThis will create a VPS with the following configuration:\n\nUbuntu 22.04 LTS\n2 VCPU\n2 GB RAM\n40 GB SSD\n\nIt’ll cost you roughly €5/month and will be located in Nuremberg (Germany).\nIn addition, after the VPS is created, it will automatically:\n\nCreate a non-root user (kamal) with sudo privileges.\nInstall the required software (Git, Docker, curl, etc.)\nCreate a directory for Let’s Encrypt SSL certificates.\nCreate a firewall rule to allow HTTP, HTTPS, and SSH traffic.\nCreate a directory for the database (SQLite) and the cache (Redis) (db/ and data/)"
  },
  {
    "objectID": "posts/create-a-kamal-ready-vps-on-hetzner-using-terraform.html#customizing-the-script",
    "href": "posts/create-a-kamal-ready-vps-on-hetzner-using-terraform.html#customizing-the-script",
    "title": "Create a Kamal-ready VPS on Hetzner using Terraform",
    "section": "Customizing the script",
    "text": "Customizing the script\nYou can customize the script to fit your needs. Here are a couple of things you can change:\n\nChange the software to install\nIf you want to change the software to install, you can modify the packages section in cloudinit/vps.yml.\n\n\nRun other commands after the VPS is created\nIf you want to run other commands after the VPS is created, you can add them to the runcmd section in the cloudinit/vps.yml file.\n\n\nUse already existing firewall rules\nIf you want to use already existing firewall rules, you can modify how the firewalls are attached in cloud.tf. Take a look at this section of cloud.tf.\n\n\nUse a different server type, operating system, or region\nIf you want to use a different server type, operating system, or region, you can modify the server_type, region, operating_system variables in variables.tf."
  },
  {
    "objectID": "posts/create-a-kamal-ready-vps-on-hetzner-using-terraform.html#conclusion",
    "href": "posts/create-a-kamal-ready-vps-on-hetzner-using-terraform.html#conclusion",
    "title": "Create a Kamal-ready VPS on Hetzner using Terraform",
    "section": "Conclusion",
    "text": "Conclusion\nThis script is a great way to create a Kamal-ready VPS on Hetzner using Terraform. It’s easy to maintain, fast, and cheap.\nAll the code is available in the repo.\nHope you find this useful!"
  },
  {
    "objectID": "posts/opensearch-python.html",
    "href": "posts/opensearch-python.html",
    "title": "How to Use OpenSearch in Python",
    "section": "",
    "text": "OpenSearch is a free and open-source search platform created by Amazon that makes it easy to search and analyze data. Developers use it for search in apps, log analytics, data observability, data ingestion, and more.\nOpenSearch began with a lot of controversy. It is a fork of Elasticsearch and Kibana that was created in January 2021 due to a change in the license of Elastic’s products. Elastic changed its policy because it believed AWS was not using its products fairly. Both companies faced significant backlash as a result of this event.\nFollowing this conflict, AWS stopped providing clusters with the most recent version of Elasticsearch. The most recent version available is 7.10. Since then, OpenSearch has been the default option (currently on version 1.3).\nGiven the prevalence of AWS in many industries, it’s likely that you’ll end up using OpenSearch at some point. If you work in data, you should familiarize yourself with this technology.\nMany data scientists struggle to set up a local environment or understand how to interact with OpenSearch in Python, and there aren’t many resources available to help. This is why I created this tutorial.\nYou’ll learn how to:\nLet’s get to it!"
  },
  {
    "objectID": "posts/opensearch-python.html#prerequisites",
    "href": "posts/opensearch-python.html#prerequisites",
    "title": "How to Use OpenSearch in Python",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore you begin, a few things must be in place. Follow these steps:\n\nInstall Docker.\nDownload the data.\nCreate a virtual environment and install the required packages. You can create one with venv by running these commands in the terminal:\n\npython3 -m venv .venv\nsource .venv/bin/activate\npython3 -m pip install pandas==1.5.3 notebook==6.5.3 opensearch-py==2.2.0\nLet’s move on to the next section."
  },
  {
    "objectID": "posts/opensearch-python.html#run-a-local-opensearch-cluster",
    "href": "posts/opensearch-python.html#run-a-local-opensearch-cluster",
    "title": "How to Use OpenSearch in Python",
    "section": "Run a Local OpenSearch Cluster",
    "text": "Run a Local OpenSearch Cluster\nUsing Docker is the simplest method for running OpenSearch locally. Run the following command in a terminal to launch a single-node cluster:\ndocker run --rm -p 9200:9200 -p 9600:9600 -e \"discovery.type=single-node\" opensearchproject/opensearch:2.6.0\nYou will see a lot of text on your terminal after running this command. But it’s okay, don’t worry!\nLet me describe what the command mentioned above does:\n\n**docker run**: It’s the command you use to run an image inside -a container.\n**--rm**: This flag instructs Docker to clear the container’s contents and file system after it shuts down.\n**-p 9200:9200 -p 9600:9600** : This instructs Docker to open specific ports on the container’s network interface.\n**-e \"discovery.type=single-node\"**: This instructs Docker to set up a cluster using a single node."
  },
  {
    "objectID": "posts/opensearch-python.html#connect-to-your-cluster",
    "href": "posts/opensearch-python.html#connect-to-your-cluster",
    "title": "How to Use OpenSearch in Python",
    "section": "Connect to Your Cluster",
    "text": "Connect to Your Cluster\nCreate a new Jupyter Notebook, and run the following code, to connect to your newly created OpenSearch cluster.\nfrom opensearchpy import OpenSearch\n\nclient = OpenSearch(\n    hosts = [{\"host\": \"localhost\", \"port\": 9200}],\n    http_auth = (\"admin\", \"admin\"),\n    use_ssl = True,\n    verify_certs = False,\n    ssl_assert_hostname = False,\n    ssl_show_warn = False,\n)\nclient.info()\nThis will connect to your local cluster. You specify the following parameters:\n\n**hosts = [{\"host\": \"localhost\", \"port\": 9200}]**: this tells OpenSearch to connect to the cluster running in your local machine in port 9200.\n**http_auth = (\"admin\", \"admin\")**: this provides the username and password to use when connecting to the cluster. You’ll use admin for both as this is only for local development.\nuse_ssl=True: this tells the client to use an SSL connection. OpenSearch’s docker images use SSL connections by default, so you’ll need to set this to True even if you’re using self signed certificates.\n**verify_certs**, **ssl_assert_hostname**, and **ssl_show_warn**: The first two security parameters that you’ll set to false, as this is a cluster running locally only used for development purposes. The last parameter prevents the client from raising a warning when initiating the connection with the cluster.\n\nIf everything went well, your output should look like this:\nNext, you’ll read the data you’ll use in the tutorial."
  },
  {
    "objectID": "posts/opensearch-python.html#read-the-data",
    "href": "posts/opensearch-python.html#read-the-data",
    "title": "How to Use OpenSearch in Python",
    "section": "Read the Data",
    "text": "Read the Data\nYou’ll use pandas to read the dataset and get a sample of 5,000 observations. Feel free to use dataset if you want to, but not that if you do, it’ll take longer to index the data.\nimport pandas as pd\n\ndf = (\n    pd.read_csv(\"wiki_movie_plots_deduped.csv\")\n    .dropna()\n    .sample(5000, random_state=42)\n    .reset_index(drop=True)\n)\nNext, let’s create an index to store your data."
  },
  {
    "objectID": "posts/opensearch-python.html#create-an-index",
    "href": "posts/opensearch-python.html#create-an-index",
    "title": "How to Use OpenSearch in Python",
    "section": "Create an Index",
    "text": "Create an Index\nOpenSearch stores and represents your data using a data structure known as an inverted index. This data structure identifies the documents in which each unique word appears.\nThis inverted index is why OpenSearch can form very quick full-text searches.\nTo index documents, you must first create an index. Here’s how you do it:\nbody = {\n    \"mappings\":{\n        \"properties\": {\n            \"title\": {\"type\": \"text\", \"analyzer\": \"english\"},\n            \"ethnicity\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n            \"director\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n            \"cast\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n            \"genre\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n            \"plot\": {\"type\": \"text\", \"analyzer\": \"english\"},\n            \"year\": {\"type\": \"integer\"},\n            \"wiki_page\": {\"type\": \"keyword\"}\n        }\n    }\n}\nresponse = client.indices.create(\"movies\", body=body)\nThis code will create a new index called movies using the cluster you set up earlier.\nLines 1 to 14 define the request’s body, specifying configuration settings used when the index is created. The mapping, which tells the index how to store the documents, is the only specified setting in this case.\nThere are two ways to map data fields in OpenSearch: dynamic mapping and explicit mapping. With dynamic mapping, the engine automatically detects the data type for each field. With explicit mapping, you manually define the data type for each field. In this example, you’ll use the latter.\nNow you’ll start adding data to your index."
  },
  {
    "objectID": "posts/opensearch-python.html#add-data-to-your-index",
    "href": "posts/opensearch-python.html#add-data-to-your-index",
    "title": "How to Use OpenSearch in Python",
    "section": "Add Data to Your Index",
    "text": "Add Data to Your Index\nThere are two ways to add data to an index: client.index() and bulk(). client.index() lets you add one item at a time while bulk() lets you add multiple items simultaneously.\nYou can use any of the two methods to add data to your index:\n\nUsing client.index()\nHere’s how you use client.index() to index your data:\nfor i, row in df.iterrows():\n    body = {\n            \"title\": row[\"Title\"],\n            \"ethnicity\": row[\"Origin/Ethnicity\"],\n            \"director\": row[\"Director\"],\n            \"cast\": row[\"Cast\"],\n            \"genre\": row[\"Genre\"],\n            \"plot\": row[\"Plot\"],\n            \"year\": row[\"Release Year\"],\n            \"wiki_page\": row[\"Wiki Page\"]\n    }\n    client.index(index=\"movies\", id=i, body=body)\nThis code iterates through the rows of the dataset you read earlier and indexes the relevant information from each row using client.index(). You use three parameters of that method:\n\nindex=\"movies\": this tells OpenSearch which index to use to store the data, as you can have multiple indexes in a cluster.\nid=i: this is the document’s identifier when you add it to the index. In this case, you set it to be the row number.\ndocument=doc: this tells the engine what information it should store.\n\n\n\nUsing bulk()\nHere’s how you use bulk() to store your data:\nfrom opensearchpy.helpers import bulk\n\nbulk_data = []\nfor i,row in df.iterrows():\n    bulk_data.append(\n        {\n            \"_index\": \"movies\",\n            \"_id\": i,\n            \"_source\": {\n                \"title\": row[\"Title\"],\n                \"ethnicity\": row[\"Origin/Ethnicity\"],\n                \"director\": row[\"Director\"],\n                \"cast\": row[\"Cast\"],\n                \"genre\": row[\"Genre\"],\n                \"plot\": row[\"Plot\"],\n                \"year\": row[\"Release Year\"],\n                \"wiki_page\": row[\"Wiki Page\"],\n            }\n        }\n    )\nbulk(client, bulk_data)\nbulk() requires the same information as client.index(): the index’s name, the document’s ID, and the document itself. But instead of adding each item one by one, you must create a list of dictionaries with all the documents you want to add to the index. Then, you pass this information and the client to bulk().\nAfter you add the data, you can make sure it worked by counting the number of items in the index:\nclient.indices.refresh(index=\"movies\")\nclient.cat.count(index=\"movies\", format=\"json\")\nYour output should look like this:"
  },
  {
    "objectID": "posts/opensearch-python.html#search-your-data",
    "href": "posts/opensearch-python.html#search-your-data",
    "title": "How to Use OpenSearch in Python",
    "section": "Search Your Data",
    "text": "Search Your Data\nFinally, you’ll want to start running searches using your index. OpenSearch comes with a query domain-specific Language (DSL) that lets you tailor your searches to your needs.\nHere’s an example of a search that looks for movies starring Jack Nicholson but whose director isn’t Tim Burton:\nresp = client.search(\n    index=\"movies\",\n    body={\n        \"query\": {\n            \"bool\": {\n                \"must\": {\n                    \"match_phrase\": {\n                        \"cast\": \"jack nicholson\",\n                    }\n                },\n                \"filter\": {\"bool\": {\"must_not\": {\"match_phrase\": {\"director\": \"tim burton\"}}}},\n            },\n        },\n    }\n)\nresp\nWhen you run this code, you should get a very long response that looks something like this:\n\nThere are many ways to tailor your search queries. To learn more about it, check out the official documentation."
  },
  {
    "objectID": "posts/opensearch-python.html#delete-documents-from-the-index",
    "href": "posts/opensearch-python.html#delete-documents-from-the-index",
    "title": "How to Use OpenSearch in Python",
    "section": "Delete Documents From the Index",
    "text": "Delete Documents From the Index\nYou can use the following code to remove documents from the index:\nclient.delete(index=\"movies\", id=\"2500\")\nThe code above will delete the document with ID 2500 from the index movies."
  },
  {
    "objectID": "posts/opensearch-python.html#delete-an-index",
    "href": "posts/opensearch-python.html#delete-an-index",
    "title": "How to Use OpenSearch in Python",
    "section": "Delete an Index",
    "text": "Delete an Index\nFinally, if, for whatever reason, you’d like to delete an index (and all of its documents), here’s how you do it:\nclient.indices.delete(index='movies')"
  },
  {
    "objectID": "posts/opensearch-python.html#conclusion",
    "href": "posts/opensearch-python.html#conclusion",
    "title": "How to Use OpenSearch in Python",
    "section": "Conclusion",
    "text": "Conclusion\nThis tutorial taught you the basics of OpenSearch and how to use it in Python. OpenSearch is now the default offering in AWS, so if you’re a data professional, you should familiarize yourself with it.\nIn this tutorial, you’ve learned:\n\nHow to set up a local OpenSearch cluster\nHow to create an index and store data in it\nHow to search your data using OpenSearch\nHow to delete documents and an index\n\nIf you have any questions or feedback, let me know in the comments!\nAll the code for this tutorial is available on GitHub."
  },
  {
    "objectID": "posts/entrepreneurship-as-a-risk-management-strategy.html",
    "href": "posts/entrepreneurship-as-a-risk-management-strategy.html",
    "title": "Entrepreneurship as a Risk Management Strategy",
    "section": "",
    "text": "Entrepreneurship, contrary to popular belief, is a great risk management strategy. Thoughtful entrepreneurs take on similar risks as employees while getting a chance to win a much larger pot.\nAlso, it’s more fun!\nIf you’ve ever read a book about how to start a business, you’ll know that most business failures can be attributed to a few factors:\nAs a result, avoiding those mistakes should be your number one priority. This will increase your chances of success and ensure you stay in the entrepreneurship game long enough to give yourself a fair chance of winning.\nYou don’t have to have those three things perfectly covered before you begin. Along the way, you could learn how to build products and pick the right market.\nBut one thing is certain, starting a business without a financial safety net is like going to a pistol duel with a nail clipper. It might work, and stories will be written about you if you succeed, but there’s a good chance you’ll get your brains blown out.\nOnce you’ve taken steps to improve your odds of success and ensured that failure won’t mean financial ruin, there’s little downside to “taking the risk” of starting your own business. Allow me to explain.\nLet’s start with a favorable scenario. You start a business, and it succeeds. In the best case, you could reap most of the financial benefits because you own most of the company’s equity. This could mean life-changing money. While if you’re an employee at a successful company, you either own very little equity or none at all. Only extremely rare circumstances, such as being an early employee in a tech unicorn, could result in comparable results.\nTherefore, starting your own business will put you in a better position to build wealth than working for someone else. It is no surprise that surveys of wealthy citizens reveal that most of them make their income from businesses, not salaries.\nConsider the inverse scenario. You are the owner of a failing business. Suppose you funded your business with your savings rather than debt. In that case, the worst-case scenario is that you’ll lose some of your savings and the time you’ve invested in the company. But, since, in any case, you’ll likely work thirty to forty years of your life, getting out of the rat race for a few years and using some of your savings before retirement won’t make a huge difference in the end result.\nFurthermore, you would’ve gained valuable skills you can use in your next ventures or in a regular job. You could go back to full-time employment and potentially earn more money than before!\nThink about what would happen to an employee in the same situation (a failing business). That’s when you most need job security, but also the time when many learn that it doesn’t exist. A company fighting for survival will not hesitate to let you go (ask Coinbase). In the best-case scenario, you’ll be overworked because you’ll take on the work of those who were let go. In the worst-case scenario, you’ll have to return to the job market and look for another job, just like the failed entrepreneur.\nIn conclusion, if all goes well, the advantages of being an entrepreneur far outweigh the benefits of being an employee. If things don’t go as planned, being an entrepreneur is just slightly worse than being an employee. With similar risk profiles, entrepreneurship produces better results. Not bad, is it?\nAllow me to clarify. This article isn’t intended to convince you to start your own business. I’m writing it because these were some of the factors I considered before embarking on my entrepreneurial journey.\nRight now, I’m in the early stages. Read on if you’d like to learn more."
  },
  {
    "objectID": "posts/entrepreneurship-as-a-risk-management-strategy.html#the-plan",
    "href": "posts/entrepreneurship-as-a-risk-management-strategy.html#the-plan",
    "title": "Entrepreneurship as a Risk Management Strategy",
    "section": "The Plan",
    "text": "The Plan\nI had considered starting my own business for a long time but didn’t feel prepared. Also, I didn’t have enough savings and worried that not having a job would risk my family’s wellbeing.\nIn 2019, I devised a strategy to get into a position where I could devote all my time to building a business. First, I’d maximize my savings rate and learn as much as possible about businesses until I had at least one year worth of savings. Then, I’d take time away from work to build a business.\nI thought a good starting point could be freelancing for companies outside of Spain. That would allow me to learn the ins and outs of running a one-man business and save money at a higher rate compared to full-time employment or freelancing with local companies.\nLast year, I landed my first freelancing contract at the European Commission, and, a few months later, at Deliveroo. I’ll finish my contract with Deliveroo next month and have saved enough money to take time away from work.\nIn addition to freelancing, I also spent a lot of time learning about businesses. I read, listened to podcasts, and consumed any useful material I could find. More importantly, I created a few small products, launched them, and talked to users.\nI haven’t decided for how long I will be away from work. Most likely, I’ll start with 6 months and will correct course depending on my burn rate, inflation, and whether I feel I’m progressing toward my goal.\nRegarding business ideas, I’ll concentrate on projects that could help me decouple the amount of time I work from how much money I make. That is, I’ll either build products or develop productized services.\nI have a list of +45 ideas that I’ve started keeping since I devised this plan. Most suck. But these are the ones I’ve shortlisted to tackle in the short-term:\n\nFast Flood: I built a small game called Fast Flood. It got good traction, and I even got a low five-figure offer for it. I have a few improvements that I’d like to try, and I hope, later on, to sell it for more.\n2X your salary using LinkedIn course: In the last 6 years, I’ve increased my income by 30% on average every year. I have an okayish educational background but have found good-paying jobs at international organizations, a top-tier consulting firm, and a top-tech European company. I believe I can help other Data scientists learn how to use LinkedIn to make more money and find better jobs.\nData Visualization with Python course: I teach a course about Data Science at Nuclio Digital School. I’ve received pretty positive feedback, so I believe I could build an online course and sell it through Gumroad or Udemy.\nCareer coaching for Data scientists: Instead of doing a course, I would personally help Data scientists get a higher-paying jobs or help them get into freelancing. I’d charge them a variable fee depending on the salary I help them achieve. There’s already at least one startup doing that.\nGrammarly + Quillbot in Spanish: Grammarly is a $10bn company. There might be 364 million Spanish speakers users on the internet. I believe there’s a market for a tool that helps you write better in that language.\n\nI’ll work on these ideas in the order listed and  dedicate up to a month to each of them. If they don’t get any traction, I’ll move on to the next one. It’s also possible that, along the way, I replace some of them with more promising ideas.\nAlso, I recently received some leads for projects that I believe could become productized services. So I’m going to make some time for that in parallel, while working in the other ideas."
  },
  {
    "objectID": "posts/entrepreneurship-as-a-risk-management-strategy.html#whats-next",
    "href": "posts/entrepreneurship-as-a-risk-management-strategy.html#whats-next",
    "title": "Entrepreneurship as a Risk Management Strategy",
    "section": "What’s Next?",
    "text": "What’s Next?\nThis week I’m starting with Fast Flood. I already have a list of improvements I want to make, so I will focus on that for now.\nYou can follow my updates on Twitter and LinkedIn.\nThanks to Luis for his feedback on this piece."
  },
  {
    "objectID": "posts/fastapi-nginx-gunicorn.html",
    "href": "posts/fastapi-nginx-gunicorn.html",
    "title": "How to Securely Deploy a FastAPI app with NGINX and Gunicorn",
    "section": "",
    "text": "Deploying a FastAPI web app to a Virtual Private Server (VPS) is tricky. If you’re not familiar with technologies such as NGINX, Gunicorn, and Uvicorn, it can easily overwhelm you. I wrote this tutorial so you won’t have to spend as much time on your first deployment as I did.\nFastAPI is one of the most popular Python libraries for developing APIs, thanks to its performance and ease of use. If you’re using machine learning models in your web app, it’s likely the go-to tool for you.\nNGINX , Gunicorn, and Uvicorn are battle-tested technologies that are frequently used as a reverse proxy and ASGI server when deploying Python web apps. If you’re familiar with Django or Flask, you’ve probably heard about some of them before.\nIn this tutorial, I’ll show you how to combine these tools to deploy a FastAPI web app. You will:\nLet’s get to it!"
  },
  {
    "objectID": "posts/fastapi-nginx-gunicorn.html#prerequisites",
    "href": "posts/fastapi-nginx-gunicorn.html#prerequisites",
    "title": "How to Securely Deploy a FastAPI app with NGINX and Gunicorn",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou should have access to a Debian-based VPS. I will use Ubuntu 20.04.\nYou should be familiar with basic shell commands, such as sudo, mkdir, or cd.\nYou should know how to exit vim 😜"
  },
  {
    "objectID": "posts/fastapi-nginx-gunicorn.html#tech-stack",
    "href": "posts/fastapi-nginx-gunicorn.html#tech-stack",
    "title": "How to Securely Deploy a FastAPI app with NGINX and Gunicorn",
    "section": "Tech Stack",
    "text": "Tech Stack\nBefore we go any further, I’ll give you a quick rundown of the technologies you’ll be using:\n\nFastAPI is one of the most popular Python frameworks for building APIs.\nIt’s built on top of Starlette and Pydantic and uses standard Python type hints. It’s loved by developers because of it is performant, easy to learn, and provides a great developer experience.\nGunicorn is a popular web server used to deploy Python web apps. Typically, it’s used as a WSGI server, but it’s possible to combine it with Uvicorn to work as an ASGI server.\nUvicorn is an ASGI web server implementation for Python. It’s the recommended web server for Starlette and FastAPI.\nNGINX is an open-source tool with many uses. It started out as a web server but can now be used as a reverse proxy server, a load balancer, and more.\nNGINX is often used as a reverse proxy in front of the app’s web server when working with Python web frameworks.\n\nNow, let’s get to the interesting part!"
  },
  {
    "objectID": "posts/fastapi-nginx-gunicorn.html#optional-step-1-secure-your-server",
    "href": "posts/fastapi-nginx-gunicorn.html#optional-step-1-secure-your-server",
    "title": "How to Securely Deploy a FastAPI app with NGINX and Gunicorn",
    "section": "(Optional) Step 1: Secure Your Server",
    "text": "(Optional) Step 1: Secure Your Server\nThis step isn’t required, but it’s still a good idea to at least skim it. Even more so if you’re not sure what you’re doing. This will make your application more secure.\n\nEnable Automatic Updates\nFirst, you should make sure your server has the latest software:\nsudo apt update && sudo apt upgrade -y\nThese are common commands you’ll see when working with Debian-based servers:\n\nsudo apt update updates the package list index on the user’s system.\nsudo apt upgrade upgrades the installed packages to their latest versions. You provide the -y flag to proceed with the installation without requiring confirmation.\n\nNext, you should set up automatic security updates, so that you don’t have to do them manually. For that, you’ll need to install and enable unnattended-upgrades:\nsudo apt install unattended-upgrades\nOnce the installation is finished, edit /etc/apt/apt.conf.d/20auto-upgrades to include the following lines:\nAPT::Periodic::Update-Package-Lists \"1\";\nAPT::Periodic::Unattended-Upgrade \"1\";\nAPT::Periodic::AutocleanInterval \"7\";\nThese lines configure unattended-upgrades so that it runs automatically. Here’s what they do:\n\nAPT::Periodic::Update-Package-Lists \"1\" means that the list of packages will be automatically updated every day.\nAPT::Periodic::Unattended-Upgrade \"1\" means that the system will be updated to the latest version of the packages without the user having to intervene.\n\"APT::Periodic::AutocleanInterval \"7\" means that the auto-clean operation, which gets rid of old and unnecessary package files, will run once a week.\n\nLastly, edit /etc/apt/apt.conf.d/50unattended-upgrades to make sure the system automatically reboots when kernel updates require it:\nUnattended-Upgrade::Automatic-Reboot \"true\"; # change to true\nYou can also configure your system to send emails when there are issues with the upgrades. If you want to do that, take a look at this article.\nWhew! You’ve now ensured that your system is up to date and will remain so. Next, you’ll create a user to make sure you don’t give your app more permissions than it needs to run.\n\n\nCreate a Non-root User\nIf your server ever gets hacked, having a non-root user reduces the damage the malicious actor can do. That, among other reasons, justifies the creation of a non-root user.\nsudo adduser fastapi-user # replace fastapi-user with your preferred name\nsudo gpasswd -a fastapi-user sudo # add to sudoers\nsu - fastapi-user # login as fastapi-user\nThese commands will create a user name fastapi-user, add it to the sudo group (which contains all users with root privileges), and then log in as that user.\nThen, you will set up your server so that you connect to it using an SSH key instead of a password. It’s safer and faster, so you have nothing to lose.\nIf you don’t already have an SSH key, open a new terminal on your local machine and run the following command. Otherwise, skip this step, and move directly to copy your public SSH key.\nssh-keygen -t ed25519 -C \"username@email.com\"\n‌This command will create and store an SSH key in your local machine. You employ two parameters:\n\n-t ed25519 to specify which algorithm to use to generate the key. You went with ED25519, which is a very safe and efficient algorithm.\n-C username@email.com to append your email as a comment at the end of the key. Make sure to replace username@email.com it with your actual email.\n\nThen, copy your public SSH key by using this command and copying the output:\ncat ~/.ssh/id_ed25519.pub\nGo back to the remote server’s terminal and type in the following commands:\nmkdir ~/.ssh/\nchmod 700 -R ~/.ssh/\nsudo vim ~/.ssh/authorized_keys\nThese commands will:\n\nCreate a .ssh directory\nSet the necessary permissions (the owner of .ssh/ has full read, write, and execute permissions, but other users and groups shouldn’t).\nOpen authorized_keys with an editor\n\nPaste your public SSH key into authorized_keys. Save the changes and close the editor. Make sure the changes worked by closing the terminal and logging back into your machine using the following command:\nssh fastapi-user@your-server-ip\nOnce you’ve tested that it works, you should disable the root login and use password authentication for SSH connections. To do this, you’ll have to update the following values in /etc/ssh/sshd_config using vim (or any other editor) using sudo privileges:\nPermitRootLogin no # change to no\n...\nPasswordAuthentication no # change to no\nThese modifications will prohibit users from logging in as root and also disable the option of authenticating using a password rather than an SSH key.\n\n\nOther Security Measures\nMost cloud providers offer firewall services, but if yours doesn’t, you should configure one and only allow incoming traffic to the necessary ports: 80, 443, and 22.\nAlso, you can install fail2ban to prevent brute-force authentication attacks. To learn more about the best practices to secure a Linux server, check out this guide from Linode."
  },
  {
    "objectID": "posts/fastapi-nginx-gunicorn.html#step-2-install-software-tools",
    "href": "posts/fastapi-nginx-gunicorn.html#step-2-install-software-tools",
    "title": "How to Securely Deploy a FastAPI app with NGINX and Gunicorn",
    "section": "Step 2: Install Software Tools",
    "text": "Step 2: Install Software Tools\nYou will require a few software tools. Begin by running the following commands to install Python:\nsudo add-apt-repository ppa:deadsnakes/ppa\nsudo apt update\nsudo apt install python3.11 python3.11-venv -y\nThen, install Supervisor and NGINX:\nsudo apt install supervisor nginx -y\nSupervisor is a process control system for Unix-like operating systems, including Linux. It’s intended to monitor and manage the processes of programs, ensuring that they are always running and restarting them if they crash or shut down.\nNGINX,as mentioned before, is a popular multifaceted software, that’s often used as a reverse proxy when deploying web applications.\nEnable and start Supervisor:\nsudo systemctl enable supervisor\nsudo systemctl start supervisor\nenable will make sure Supervisor starts on boot, and start will start Supervisor right away."
  },
  {
    "objectID": "posts/fastapi-nginx-gunicorn.html#step-3-set-up-your-fastapi-app",
    "href": "posts/fastapi-nginx-gunicorn.html#step-3-set-up-your-fastapi-app",
    "title": "How to Securely Deploy a FastAPI app with NGINX and Gunicorn",
    "section": "Step 3: Set Up Your FastAPI App",
    "text": "Step 3: Set Up Your FastAPI App\nStart by cloning the sample app into /home/fastapi-user:\ngit clone https://github.com/dylanjcastillo/fastapi-nginx-gunicorn\nThis will work with public repositories. If you want to deploy an app from a private GitHub repository, you should set up a GitHub deploy key and clone the repository using it.\nNext, create a virtual environment and activate it in the project directory:\ncd /home/fastapi-user/fastapi-nginx-gunicorn\npython3.11 -m venv .venv\nsource .venv/bin/activate\nThese commands will change your current location to the project directory, create a virtual environment in it, and activate it. From now on, you should see a (.venv) prefix in your command line.\nNow, use pip to install the libraries specified in requirements.txt:\npip install -r requirements.txt\nThis will install the packages in requirements.txt: fastapi, gunicorn, and uvicorn, in your current virtual environment.\nVerify that everything went well by running the application:\nuvicorn main:app\nYou shouldn’t get any errors when you run this command. You can also verify that it’s working by opening a new terminal window, connecting to the server, and making a request with curl:\ncurl http://localhost:8000\nYou should get the following response:\n{\"message\":\"It's working!\"}\nYou’re halfway there. You got your FastAPI app running, next you’ll configure Gunicorn to serve as a WSGI server."
  },
  {
    "objectID": "posts/fastapi-nginx-gunicorn.html#step-4-configure-gunicorn",
    "href": "posts/fastapi-nginx-gunicorn.html#step-4-configure-gunicorn",
    "title": "How to Securely Deploy a FastAPI app with NGINX and Gunicorn",
    "section": "Step 4: Configure Gunicorn",
    "text": "Step 4: Configure Gunicorn\nThere are two parts to configuring Gunicorn. First, specifying the configuration requirements of gunicorn. Second, setting up a Supervisor program to run it.\n\nSet Up Gunicorn\nYou’ll first create a file to define the parameters you’ll use when running Gunicorn. For that, create a file called gunicorn_start in the project directory:\nvim gunicorn_start\nThen, add the following content to it:\n#!/bin/bash\n\nNAME=fastapi-app\nDIR=/home/fastapi-user/fastapi-nginx-gunicorn\nUSER=fastapi-user\nGROUP=fastapi-user\nWORKERS=3\nWORKER_CLASS=uvicorn.workers.UvicornWorker\nVENV=$DIR/.venv/bin/activate\nBIND=unix:$DIR/run/gunicorn.sock\nLOG_LEVEL=error\n\ncd $DIR\nsource $VENV\n\nexec gunicorn main:app \\\n  --name $NAME \\\n  --workers $WORKERS \\\n  --worker-class $WORKER_CLASS \\\n  --user=$USER \\\n  --group=$GROUP \\\n  --bind=$BIND \\\n  --log-level=$LOG_LEVEL \\\n  --log-file=-\nHere’s what you’re defining:\n\nLine 1 indicates that the script is to be run by the bash shell.\nLines 3 to 11 specify the configuration options that you’ll pass to Gunicorn. Most parameters are self-explanatory, except for WORKERS, WORKER_CLASS, and BIND:\n\nWORKERS: defines the number of workers that Gunicorn will use, it’s usually recommended to use the number of CPU cores + 1.\nWORKER_CLASS: type of worker used. In this case, you specify Uvicorn workers, which allows you to use it as an ASGI server.\nBIND: Specifies the server socket that Gunicorn binds to.\n\nLines 13 and 14 change the location to the project directory and activate the virtual environment.\nLines 16 to 24 run Gunicorn with the specified parameters.\n\nSave and close the fine. Then, make it executable by running the following:\nchmod u+x gunicorn_start\nFinally, make a run folder in your project directory for the Unix socket file you defined in the BIND parameter:\nmkdir run\nWhen you have multiple servers communicating on the same machine, using a Unix socket file is better.\n\n\nConfigure Supervisor\nFirst, create a directory called logs in the project directory to store your application’s error logs:\nmkdir logs\nNext, create a Supervisor’sconfiguration file by running the following command:\nsudo vim /etc/supervisor/conf.d/fastapi-app.conf\nThere copy and paste the following:\n[program:fastapi-app]\ncommand=/home/fastapi-user/fastapi-nginx-gunicorn/gunicorn_start\nuser=fastapi-user\nautostart=true\nautorestart=true\nredirect_stderr=true\nstdout_logfile=/home/fastapi-user/fastapi-nginx-gunicorn/logs/gunicorn-error.log\nThis configuration file runs the file you created earlier, gunicorn_start, using the fastapi-user. Supervisor will start the application anytime the server starts, and will also restart it if it fails.\nThis configuration file executes the gunicorn_start file you created earlier using fastapi-user as the user. Supervisor will launch the application whenever the server boots up and will restart it if the application fails. The errors are logged into gunicorn-error.log in logs in the project directory.\nReread Supervisor’s configuration file and restart the service by running these commands:\nsudo supervisorctl reread\nsudo supervisorctl update\nFinally, you can check the status of the program by running this command:\nsudo supervisorctl status fastapi-app\nIf everything went well, the fastapi-app service status should be set to RUNNING.\nYou can also test it by opening a new terminal window, connecting to the server, and issuing a GET request using curl:\ncurl --unix-socket /home/fastapi-user/fastapi-nginx-gunicorn/run/gunicorn.sock localhost\nYou should see the following output:\n{ \"message\": \"It's working!\" }\nFinally, if you make changes to the code, you can restart the service to apply to changes by running this command:\nsudo supervisorctl restart fastapi-app\nWay to go! You’ve got an ASGI server running using Gunicorn and Uvicorn. Next, you’ll set up a reverse proxy server using NGINX."
  },
  {
    "objectID": "posts/fastapi-nginx-gunicorn.html#step-5-configure-nginx",
    "href": "posts/fastapi-nginx-gunicorn.html#step-5-configure-nginx",
    "title": "How to Securely Deploy a FastAPI app with NGINX and Gunicorn",
    "section": "Step 5: Configure NGINX",
    "text": "Step 5: Configure NGINX\nCreate a new NGINX configuration file for your project:\nsudo vim /etc/nginx/sites-available/fastapi-app\nOpen the NGINX configuration file and paste the following content:\nupstream app_server {\n    server unix:/home/fastapi-user/fastapi-nginx-gunicorn/run/gunicorn.sock fail_timeout=0;\n}\n\nserver {\n    listen 80;\n\n    # add here the ip address of your server\n    # or a domain pointing to that ip (like example.com or www.example.com)\n    server_name XXXX;\n\n    keepalive_timeout 5;\n    client_max_body_size 4G;\n\n    access_log /home/fastapi-user/fastapi-nginx-gunicorn/logs/nginx-access.log;\n    error_log /home/fastapi-user/fastapi-nginx-gunicorn/logs/nginx-error.log;\n\n    location / {\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header Host $http_host;\n        proxy_redirect off;\n\n        if (!-f $request_filename) {\n            proxy_pass http://app_server;\n            break;\n        }\n    }\n}\nThis is the NGINX configuration file. Here’s how it works:\n\nLines 1 to 3 define a cluster of servers called app_server that NGINX will proxy requests to. The requests are redirected to the Unix socket file located at /home/fastapi-user/fastapi-nginx-gunicorn/run/gunicorn.sock. Setting fail_timeout=0 tells NGINX not to consider the server as failed even if it does not respond.\nLines 1 to 5 define the configuration for the virtual server that NGINX will use to serve requests. In this case, it listens on port 80. Replace XXXX by the IP or the site’s name.\nLines 12 and 13 specify keepalive_timeout to set the maximum amount of time that a client can keep a persistent connection open, and client_max_body_size to set a limit to size of the client request body that NGINX will allow.\nLines 15 and 16 specify the locations where NGINX will write its access and error logs.\nLines 18 to 27 defines how NGINX will handle requests to the root directory /. You provide some specifications to handle headers, and set a directive to proxy the requests to the app_server you defined earlier.\n\nEnable the configuration of your site by creating a symbolic link from the file in sites-available into sites-enabled by running this command:\nsudo ln -s /etc/nginx/sites-available/fastapi-app /etc/nginx/sites-enabled/\nTest that the configuration file is OK and restart NGINX:\nsudo nginx -t\nsudo systemctl restart nginx\nIf everything went well, now you should be able to make a GET request successfully to the IP of your server from your browser or using curl. Once again, you should see the following output:\n{ \"message\": \"It's working!\" }\nYou should have your FastAPI app running by now, as well as Gunicorn+Uvicorn as an ASGI server and NGINX in front of them as a reverse proxy.\n\nPermissions Error\nIf you get a permission error telling you that NGINX cannot access the unix socket, you can add the www-data user (which typically is the user running the NGINX processes) to the fastapi-user group. You can use the following command:\nsudo usermod -aG fastapi-user www-data\nGood job! If you haven’t bought a domain name for your API, you can stop reading here. If you have one, proceed to the next step to obtain an SSL certificate and enable HTTPS."
  },
  {
    "objectID": "posts/fastapi-nginx-gunicorn.html#optional-step-6-obtain-a-free-ssl-certificate-using-certbot",
    "href": "posts/fastapi-nginx-gunicorn.html#optional-step-6-obtain-a-free-ssl-certificate-using-certbot",
    "title": "How to Securely Deploy a FastAPI app with NGINX and Gunicorn",
    "section": "(Optional) Step 6: Obtain a Free SSL Certificate Using Certbot",
    "text": "(Optional) Step 6: Obtain a Free SSL Certificate Using Certbot\nThis only applies if you have a domain for which you want to obtain an SSL certificate.\nIf you’re using Ubuntu, you can skip this step. Otherwise, you first need to install snapd:\nsudo apt install snapd\nNext, make sure you have the latest version available:\nsudo snap install core; sudo snap refresh core\nInstall certbot and make sure the cerbot command is executable:\nsudo snap install --classic certbot\nsudo ln -s /snap/bin/certbot /usr/bin/certbot\nNext, generate a certificate for your domain interactively by running the following command:\nsudo certbot --nginx\nFinally, Certbot will automatically handle the renewal of your certificate. To test that it works run the following:\nsudo certbot renew --dry-run\nIf it worked as expected, you should see a Congratulations, all simulated renewals succeeded... message.\nIf everything went well, you should be able to make a successful get request using HTTPS."
  },
  {
    "objectID": "posts/fastapi-nginx-gunicorn.html#conclusion",
    "href": "posts/fastapi-nginx-gunicorn.html#conclusion",
    "title": "How to Securely Deploy a FastAPI app with NGINX and Gunicorn",
    "section": "Conclusion",
    "text": "Conclusion\nThat’s all there is to it! This tutorial showed you how to use NGINX, Gunicorn, and Uvicorn to deploy a FastAPI application. FastAPI is one of the most popular Python web frameworks. It’s become the go-to option for deploying machine learning-powered web apps, so becoming acquainted with it is a wise career move.\nIn this article you’ve learned:\n\nWhy and when should you use FastAPI, NGINX, Gunicorn, and Uvicorn.\nHow to set up Gunicorn+Uvicorn as an ASGI server.\nHow to use Supervisor to run Gunicorn.\nHow to configure NGINX and generate a free SSL certificate using certbot.\n\nIf you have any questions or feedback, let me know in the comments!\nAll the code for this tutorial is available on GitHub."
  },
  {
    "objectID": "posts/clustering-documents-with-openai-langchain-hdbscan.html",
    "href": "posts/clustering-documents-with-openai-langchain-hdbscan.html",
    "title": "Clustering Documents with OpenAI embeddings, HDBSCAN and UMAP",
    "section": "",
    "text": "In the past, the most common way to cluster documents was by building vectors with traditional Machine Learning methods such as bag-of-words or smaller pre-trained NLP models, like BERT, and then creating groups out of them. But LLMs have changed that.\nWhile older methods are still relevant, if I had to cluster text data today, I’d start using the OpenAI or Cohere (embeddings and generation) APIs. It’s faster, easier, and gives you additional goodies such as coming up with fitting titles for each cluster.\nI haven’t seen many tutorials on this topic, so I wrote one. In this tutorial, I’ll show you how to cluster news articles using OpenAI embeddings, and HDBSCAN.\nLet’s get to it!"
  },
  {
    "objectID": "posts/clustering-documents-with-openai-langchain-hdbscan.html#prerequisites",
    "href": "posts/clustering-documents-with-openai-langchain-hdbscan.html#prerequisites",
    "title": "Clustering Documents with OpenAI embeddings, HDBSCAN and UMAP",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo make the most of this tutorial, you should be familiar with the following concepts:\n\nHow to cluster text data using traditional ML methods.\nWhat are OpenAI Embeddings\nHow HDBSCAN works\n\nIn addition, you’ll need an OpenAI account."
  },
  {
    "objectID": "posts/clustering-documents-with-openai-langchain-hdbscan.html#set-up-your-local-environment",
    "href": "posts/clustering-documents-with-openai-langchain-hdbscan.html#set-up-your-local-environment",
    "title": "Clustering Documents with OpenAI embeddings, HDBSCAN and UMAP",
    "section": "Set Up Your Local Environment",
    "text": "Set Up Your Local Environment\n\nCreate a virtual environment using venv:\n\npython3.10 -m venv .venv\n\nCreate a requirements.txt file that contains the following packages:\n\nhdbscan\nopenai\npandas\nnumpy\npython-dotenv\ntiktoken\nnotebook\nplotly\numap-learn\n\nActivate the virtual environment and install the packages:\n\nsource .venv/bin/activate\npip3 install -r requirements.txt\n\nCreate a file called .env, and add the your OpenAI key:\n\nOPENAI_API_KEY=&lt;your key&gt;\n\nCreate an empty notebook file. For the rest of this tutorial, you’ll work on it."
  },
  {
    "objectID": "posts/clustering-documents-with-openai-langchain-hdbscan.html#clustering-documents",
    "href": "posts/clustering-documents-with-openai-langchain-hdbscan.html#clustering-documents",
    "title": "Clustering Documents with OpenAI embeddings, HDBSCAN and UMAP",
    "section": "Clustering Documents",
    "text": "Clustering Documents\nYou should think of the clustering process in three steps:\n\nGenerate numerical vector representations of documents using OpenAI’s embedding capabilities.\nApply a clustering algorithm on the vectors to group the documents.\nGenerate a title for each cluster summarizing the articles contained in it.\n\nThat’s it! Now, you’ll see how that looks in practice.\n\nImport the Required Packages\nStart by importing the required Python libraries. Copy the following code in your notebook:\nimport os\n\nimport hdbscan\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport requests\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom umap import UMAP\n\nload_dotenv()\nThis code imports the libraries you’ll use throughout the tutorial. Here’s the purpose of each one:\n\nos helps you read the environment variables.\nhdbscan gives you a wrapper of HDBSCAN, the clustering algorithm you’ll use to group the documents.\nopenai to use OpenAI LLMs.\numap loads UMAP for dimensionality reduction and visualizing clusters.\ndotenv load the environment variables you define in .env.\n\nNext, you’ll get a sample of news articles to cluster.\n\n\nDownload the data and generate embeddings\nDownload, read these articles, and generate documents you’ll use to create the embeddings:\ndf = pd.read_csv(\"news_data_dedup.csv\")\ndocs = [\n    f\"{title}\\n{description}\"\n    for title, description in zip(df.title, df.description)\n]\nThen, initialize the OpenAI client and generate the embeddings:\nclient = OpenAI()\nresponse = client.embeddings.create(input=docs, model=\"text-embedding-3-small\")\nembeddings = [np.array(x.embedding) for x in response.data]\n\n\nCluster documents\nOnce you have the embeddings, you can cluster them using hdbscan:\nhdb = hdbscan.HDBSCAN(min_samples=3, min_cluster_size=3).fit(embeddings)\nThis code will generate clusters using the embeddings generated, and then create a DataFrame with the results. Itfits the hdbscan algorithm. In this case, I set min_samples and min_cluster_size to 3, but depending on your data this may change. Check HDBSCAN’s documentation to learn more about these parameters.\nNext, you’ll create topic titles for each cluster based on their contents.\n\n\nVisualize the clusters\nAfter you’ve generated the clusters, you can visualize them using UMAP:\numap = UMAP(n_components=2, random_state=42, n_neighbors=80, min_dist=0.1)\n\ndf_umap = (\n    pd.DataFrame(umap.fit_transform(np.array(embeddings)), columns=['x', 'y'])\n    .assign(cluster=lambda df: hdb.labels_.astype(str))\n    .query('cluster != \"-1\"')\n    .sort_values(by='cluster')\n)\n\nfig = px.scatter(df_umap, x='x', y='y', color='cluster')\nfig.show()\nYou should get something similar to this graph:\n\nThis will give you a sense of how good are the clusters generated.\n\n\nCreate a Topic Title per Cluster\nFor each cluster, you’ll generate a topic title summarizing the articles in that cluster. Copy the following code to your notebook:\ndf[\"cluster_name\"] = \"Uncategorized\"\n\ndef generate_topic_titles():\n    system_message = \"You're an expert journalist. You're helping me write short but compelling topic titles for groups of news articles.\"\n    user_template = \"Using the following articles, write a 4 to 5 word title that summarizes them.\\n\\nARTICLES:\\n\\n{}\\n\\nTOPIC TITLE:\"\n\n    for c in df.cluster.unique():\n        sample_articles = df.query(f\"cluster == '{c}'\").to_dict(orient=\"records\")\n        articles_str = \"\\n\\n\".join(\n            [\n                f\"[{i}] {article['title']}\\n{article['description'][:200]}{'...' if len(article['description']) &gt; 200 else ''}\"\n                for i, article in enumerate(\n                    sample_articles, start=1\n                )\n            ]\n        )\n        messages = [\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": user_template.format(articles_str)},\n        ]\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\", messages=messages, temperature=0.7, seed=42\n        )\n\n        topic_title = response.choices[0].message.content\n        df.loc[df.cluster == c, \"cluster_name\"] = topic_title\nThis code takes all the articles per cluster and uses gpt-3.5-turbo to generate a relevant topic title from them. Itgoes through each cluster, takes the articles in it, and makes a prompt using that to generate a topic title for that cluster.\nFinally, you can check the resulting clusters and topic titles, as follows:\nc = 6\nwith pd.option_context(\"display.max_colwidth\", None):\n    print(df.query(f\"cluster == '{c}'\").topic_title.values[0])\n    display(df.query(f\"cluster == '{c}'\").drop(columns=[\"topic_title\"]).head())\nIn my case, running this code produces the following articles and topic titles:\n\nAll articles seem to be related to the topic title. Yay!\n\n\nConclusion\nIn this short tutorial, you’ve learned how to cluster documents using OpenAI embeddings, HDBSCAN, and UMAP. I hope you find this useful. Let me know in the comments if you have any questions.\nCheck out the code on GitHub. You can also check this project I built with Iswar which shows this in practice."
  },
  {
    "objectID": "posts/semantic-search-elasticsearch-openai-langchain.html",
    "href": "posts/semantic-search-elasticsearch-openai-langchain.html",
    "title": "Semantic Search with Elasticsearch, OpenAI, and LangChain",
    "section": "",
    "text": "LangChain is the new cool kid on the block. It’s a library designed to help you interact with Large Language Models (LLMs). Up until recently, I was building most things from scratch when working with LLMs, so I decided to give LangChain a try.\nAfter a few projects using it, I’m truly impressed. It simplifies many of the routine tasks associated with working with LLMs, such as extracting text from documents or indexing them in a vector database. If you’re working with LLMs today, LangChain can save you hours of work.\nHowever, one drawback is that its documentation, despite being extensive, can be scattered and difficult for newcomers to comprehend. Moreover, most of the content online focuses on the latest generation of vector databases. Since many organizations still use older, but battle-tested technologies such as Elasticsearch and I decided to write a tutorial using it.\nI combined LangChain and Elasticsearch in one of the most common LLM applications: semantic search. In this tutorial, I’ll walk you through building a semantic search service using Elasticsearch, OpenAI, LangChain, and FastAPI. You’ll create an application that lets users ask questions about Marcus Aurelius’ Meditations and provides them with concise answers by extracting the most relevant content from the book.\nLet’s dive in!"
  },
  {
    "objectID": "posts/semantic-search-elasticsearch-openai-langchain.html#prerequisites",
    "href": "posts/semantic-search-elasticsearch-openai-langchain.html#prerequisites",
    "title": "Semantic Search with Elasticsearch, OpenAI, and LangChain",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou should be familiar with these topics to make the most out of this tutorial:\n\nWhat semantic search is.\nHow to use Elasticsearch in Python.\nWhat text embeddings are.\n\nIn addition, you must install Docker and create an account at OpenAI."
  },
  {
    "objectID": "posts/semantic-search-elasticsearch-openai-langchain.html#designing-a-semantic-search-service",
    "href": "posts/semantic-search-elasticsearch-openai-langchain.html#designing-a-semantic-search-service",
    "title": "Semantic Search with Elasticsearch, OpenAI, and LangChain",
    "section": "Designing a Semantic Search Service",
    "text": "Designing a Semantic Search Service\nYou’re going to build a service with three components:\n\nIndexer: This creates the index, generates the embeddings and the metadata (source and title of the book, in this case), and adds them to the vector database.\nVector database: This is a database that you use to store and retrieve the embeddings you generate.\nSearch app: This is a backend service that uses the user’s search term, generates an embedding from it, and then looks for the most similar embeddings in the vector database.\n\nHere’s a diagram of this architecture:\n\nIf you’ve read my previous tutorial on semantic search, you might have noticed that the vectorizer isn’t part of this architecture. I’m purposely skipping it because langchain takes care of that part for you, so you can have the indexer and vectorizer in the same place.\nNext, you’ll set up your local environment."
  },
  {
    "objectID": "posts/semantic-search-elasticsearch-openai-langchain.html#set-up-your-local-environment",
    "href": "posts/semantic-search-elasticsearch-openai-langchain.html#set-up-your-local-environment",
    "title": "Semantic Search with Elasticsearch, OpenAI, and LangChain",
    "section": "Set Up Your Local Environment",
    "text": "Set Up Your Local Environment\nFollow these steps to set up your local environment:\n\nInstall Python 3.10.\nInstall Poetry. It’s optional but highly recommended.\nClone the project’s repository:\n\ngit clone https://github.com/dylanjcastillo/semantic-search-elasticsearch-openai-langchain\n\nFrom the root folder of the project, install the dependencies:\n\nUsing Poetry: Create the virtual environment in the same directory as the project and install the dependencies:\n\npoetry config virtualenvs.in-project true\npoetry install\n\nUsing venv and pip: Create a virtual environment and install the dependencies listed in requirements.txt:\n\npython3.10 -m venv .venv && source .venv/bin/activate\npip install -r requirements.txt\nOpen src/.env-example, add your secret OpenAI key, and save the file as .env.\n\nBy now, you’ll have a virtual environment set up with the required libraries and a local copy of the repository. Your project structure should look like this:\nsemantic-search-elasticsearch-openai-langchain\n├── LICENSE\n├── README.md\n├── poetry.lock\n├── pyproject.toml\n├── requirements.txt\n├── run_elasticsearch.sh\n├── src\n│   ├── app.py\n│   ├── config.py\n│   ├── data\n│   │   └── Marcus_Aurelius_Antoninus...\n│   │       ├── index.html\n│   │       ├── metadata.opf\n│   │       └── style.css\n│   ├── indexer.py\n│   └── .env-example\n└── .venv/\nThese are the most relevant files and directories in the project:\n\npoetry.lock and pyproject.toml: These files contain the project’s specifications and dependencies and are used by Poetry to create a virtual environment.\nrequirements.txt: This file contains a list of Python packages required by the project.\nrun_elasticsearch_docker.sh: This file contains a bash script used to run an Elasticsearch cluster locally.\nsrc/app.py: This file contains the code of the search application.\nsrc/config.py: This file contains project configuration specifications such as OpenAI’s API key (read from a .env file), the paths to the data, and the name of the index.\nsrc/data/: This directory contains Meditations as originally downloaded from Wikisource. You’ll use that as the corpus of text for this tutorial.\nsrc/indexer.py: This file contains the code you use to create an index and insert the documents in OpenSearch.\n.env-example: This file is typically used for environment variables. In this case, you use it to pass OpenAI’s API key to your application.\n.venv/: This directory contains the project’s virtual environment.\n\nAll done! Let’s get going."
  },
  {
    "objectID": "posts/semantic-search-elasticsearch-openai-langchain.html#start-a-local-elasticsearch-cluster",
    "href": "posts/semantic-search-elasticsearch-openai-langchain.html#start-a-local-elasticsearch-cluster",
    "title": "Semantic Search with Elasticsearch, OpenAI, and LangChain",
    "section": "Start a Local Elasticsearch Cluster",
    "text": "Start a Local Elasticsearch Cluster\nBefore we get into the code, you should start a local Elasticsearch cluster. Open a new terminal, navigate to the project’s root folder, and run:\nsh run_elasticsearch_docker.sh\nIf everything went well, a lengthy text string will appear on the terminal. For the rest of the tutorial, keep this terminal window open in the background."
  },
  {
    "objectID": "posts/semantic-search-elasticsearch-openai-langchain.html#split-and-index-the-book",
    "href": "posts/semantic-search-elasticsearch-openai-langchain.html#split-and-index-the-book",
    "title": "Semantic Search with Elasticsearch, OpenAI, and LangChain",
    "section": "Split and Index the Book",
    "text": "Split and Index the Book\nIn this step, you’ll do two things:\n\nProcess the text from the book by splitting it into chunks of 1,000 tokens.\nIndex the text chunks you’ve generated (from now on called documents) in your Elasticsearch cluster.\n\nTake a look at src/indexer.py:\nfrom langchain.document_loaders import BSHTMLLoader\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import ElasticVectorSearch\n\nfrom config import Paths, openai_api_key\n\n\ndef main():\n    loader = BSHTMLLoader(str(Paths.book))\n    data = loader.load()\n\n    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n        chunk_size=1000, chunk_overlap=0\n    )\n    documents = text_splitter.split_documents(data)\n\n    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n    db = ElasticVectorSearch.from_documents(\n        documents,\n        embeddings,\n        elasticsearch_url=\"http://localhost:9200\",\n        index_name=\"elastic-index\",\n    )\n    print(db.client.info())\n\n\nif __name__ == \"__main__\":\n    main()\nThis code takes Meditations, splits it into text chunks of 1,000 tokens, and then indexes those chunks in your Elasticsearch cluster. Here’s a detailed breakdown:\n\nLines 1 to 4 import the required components from langchain:\n\nBSHTMLLoader: This loader uses BeautifulSoup4 to parse the documents.\nOpenAIEmbeddings: This component is a wrapper around OpenAI embeddings. It helps you generate embeddings for documents and queries.\nRecursiveCharacterTextSplitter: This utility function divides the input text by attempting various characters in an order designed to maintain semantically similar content in proximity. The characters used for splitting, in the following order, are: \"\\n\\n\", \"\\n\", \" \", \"\".\nElasticSearchVector: This is a wrapper around the Elasticsearch client, that simplifies interacting with your cluster.\n\nLine 6 imports the relevant configurations from config.py\nLines 11 and 12 extract the book’s text using BSHTMLLoader.\nLines 13 to 16 initialize the text splitter and split the text in chunks of no more than 1,000 tokens. In this case, you use tiktoken to count the tokens but you can also use different length functions, such as counting the number of characters instead of tokens or a different tokenizing function.\nLines 18 to 25 initialize the embeddings function, create a new index, and index the documents generated by the text splitter. In elasticsearch_url, you specify the port where your application is running locally, and in index_name you specify the name of the index you’ll use. Finally, you print the Elasticsearch client information.\n\nTo run this script, open a terminal, activate the virtual environment, and from src folder of your project, run the following command:\n# ../src/\npython indexer.py\nIf everything went well, you should get an output that looks similar to this (but not properly formatted):\n{\n  \"name\": \"0e1113eb2915\",\n  \"cluster_name\": \"docker-cluster\",\n  \"cluster_uuid\": \"og6mFMqwQtaJiv_3E_q2YQ\",\n  \"version\": {\n    \"number\": \"8.7.0\",\n    \"build_flavor\": \"default\",\n    \"build_type\": \"docker\",\n    \"build_hash\": \"09520b59b6bc1057340b55750186466ea715e30e\",\n    \"build_date\": \"2023-03-27T16:31:09.816451435Z\",\n    \"build_snapshot\": false,\n    \"lucene_version\": \"9.5.0\",\n    \"minimum_wire_compatibility_version\": \"7.17.0\",\n    \"minimum_index_compatibility_version\": \"7.0.0\"\n  },\n  \"tagline\": \"You Know, for Search\"\n}\nNext, let’s create a simple FastAPI app, to interact with your cluster."
  },
  {
    "objectID": "posts/semantic-search-elasticsearch-openai-langchain.html#create-a-search-application",
    "href": "posts/semantic-search-elasticsearch-openai-langchain.html#create-a-search-application",
    "title": "Semantic Search with Elasticsearch, OpenAI, and LangChain",
    "section": "Create a Search Application",
    "text": "Create a Search Application\nIn this step, you’ll create a simple application to interact with Meditations. You’ll connect to the Elasticsearch cluster, initialize the Retrieval Questioning/Answering Chain, and create an /ask endpoint that lets the user interact with the app.\nTake a look at the code of src/app.py:\nfrom fastapi import FastAPI\nfrom langchain.chains import RetrievalQA\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import ElasticVectorSearch\n\nfrom config import openai_api_key\n\nembedding = OpenAIEmbeddings(openai_api_key=openai_api_key)\n\ndb = ElasticVectorSearch(\n    elasticsearch_url=\"http://localhost:9200\",\n    index_name=\"elastic-index\",\n    embedding=embedding,\n)\nqa = RetrievalQA.from_chain_type(\n    llm=ChatOpenAI(temperature=0),\n    chain_type=\"stuff\",\n    retriever=db.as_retriever(),\n)\n\napp = FastAPI()\n\n\n@app.get(\"/\")\ndef index():\n    return {\n        \"message\": \"Make a post request to /ask to ask questions about Meditations by Marcus Aurelius\"\n    }\n\n\n@app.post(\"/ask\")\ndef ask(query: str):\n    response = qa.run(query)\n    return {\n        \"response\": response,\n    }\nThis code lets the user ask questions about Marcus Aurelius’ Meditations and provides answers back to the users. Let me show you how it works:\n\nLines 1 to 5 import the required libraries:\n\nFastAPI: This class initializes the app.\nRetrievalQA: This is a chain that allows you to ask questions about documents in a vector database. It finds the most relevant documents based on your question and generates an answer from them.\nChatOpenAI: This is a wrapper around OpenAI’s chat models.\nOpenAIEmbeddings and ElasticVectorSearch: These are the same wrappers discussed in the previous section.\n\nLine 7 imports the OpenAI secret key.\nLines 9 to 15 initialize the Elasticsearch cluster using OpenAI embeddings.\nLines 16 to 20 initialize the RetrievalQA chain with the following parameters:\n\nllm: Specifies the LLM used to run prompts defined in the chain.\nchain_type: Defines how documents are retrieved and processed from the vector database. By specifying stuff, documents will be retrieved and passed to the chain to answer the question as-is. Alternatively, you can use map_reduce or map_rerank for additional processing before answering the question, but these methods use more API calls. For more information, consult the langchain documentation.\nretriever: Specifies the vector database used by the chain to retrieve documents.\n\nLines 22 to 36 initialize the FastAPI app and define two endpoints. The / endpoint provides users with information on how to use the application. The /ask endpoint takes a user’s question (query parameter) and returns an answer using the previously initialized chain.\n\nFinally, you can run the app from the terminal (using your virtual environment):\nuvicorn app:app --reload\nThen, visit http://127.0.0.1:8000/docs, and test /ask by making a question about the book:\n\nIf everything went well, you should get something like this:\n\nThat’s it! You now have your own semantic search service up and running, based on Elasticsearch, OpenAI, Langchain, and FastAPI."
  },
  {
    "objectID": "posts/semantic-search-elasticsearch-openai-langchain.html#going-to-production",
    "href": "posts/semantic-search-elasticsearch-openai-langchain.html#going-to-production",
    "title": "Semantic Search with Elasticsearch, OpenAI, and LangChain",
    "section": "Going to Production",
    "text": "Going to Production\nMoving to production can vary greatly for different software products. However, at a minimum, you should:\n\nBuild appropriate data ingestion and processing pipelines. When adding new data, you generally do not want to rebuild the index, as you did in this tutorial.\nUse a production-ready Elasticsearch-managed instance or a Docker image optimized for production. The instance used in this tutorial was intended for testing and development purposes only.\nChoose a deployment strategy that suits your organization’s needs. For example, I often use NGINX with Gunicorn and Uvicorn workers for small projects."
  },
  {
    "objectID": "posts/semantic-search-elasticsearch-openai-langchain.html#conclusion",
    "href": "posts/semantic-search-elasticsearch-openai-langchain.html#conclusion",
    "title": "Semantic Search with Elasticsearch, OpenAI, and LangChain",
    "section": "Conclusion",
    "text": "Conclusion\nWay to go! In this tutorial, you’ve learned how to build a semantic search engine using Elasticsearch, OpenAI, and Langchain.\nIn particular, you’ve learned:\n\nHow to structure a semantic search service.\nHow to use LangChain to split and index documents.\nHow to use Elasticsearch as a vector database with LangChain.\nHow to use the Retrieval Questioning/Answering Chain to answer questions with a vector database.\nWhat you should consider when productizing this type of application.\n\nHope you found this tutorial useful. Let me know if you have any questions!\nAll the code for this tutorial is available on GitHub."
  },
  {
    "objectID": "posts/text-classification-using-python-and-scikit-learn.html",
    "href": "posts/text-classification-using-python-and-scikit-learn.html",
    "title": "Text Classification Using Python and Scikit-learn",
    "section": "",
    "text": "Text classification is the task of automatically assigning labels to pieces of text, such as articles, blog posts, or reviews. Many businesses use text classification algorithms to save time and money by reducing the amount of manual labor needed to organize and analyze their text data.\nThese algorithms are extremely powerful tools when used correctly. Text classification models keep your email free of spam, assist authors in detecting plagiarism, and help your grammar checker understand the various parts of speech.\nIf you want to build a text classifier, you have many options to choose from. You can use traditional methods such as bag of words, advanced methods like Word2Vec, or cutting-edge approaches like BERT or GPT-3.\nBut if your goal is to get something up and running quickly and at no cost, you should build your text classification model with Python and Scikit-learn. I’ll show you how in this tutorial.\nSo let’s get started!"
  },
  {
    "objectID": "posts/text-classification-using-python-and-scikit-learn.html#prerequisites",
    "href": "posts/text-classification-using-python-and-scikit-learn.html#prerequisites",
    "title": "Text Classification Using Python and Scikit-learn",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore we start, you need to install some libraries. The best way to do that is to create a new virtual environment and install the packages there.\nIf you’re using venv, run these commands:\npython3 -m venv .textcl\nsource .textcl/bin/activate\npython3 -m pip install pandas==1.4.3 notebook==6.3.0 numpy==1.23.2 scikit-learn==1.1.2\nIf you’re using conda, this is how you do it:\nconda create --name textcl\nconda activate textcl\nconda install pandas==1.4.3 notebook==6.3.0 numpy==1.23.2 scikit-learn==1.1.2\nThat’s it! These commands will create a virtual environment, activate it, and install the required packages.\nFinally, start a Jupyter Notebook session by executing jupyter notebook on the same terminal where you ran the previous commands and create a new notebook."
  },
  {
    "objectID": "posts/text-classification-using-python-and-scikit-learn.html#import-the-required-libraries",
    "href": "posts/text-classification-using-python-and-scikit-learn.html#import-the-required-libraries",
    "title": "Text Classification Using Python and Scikit-learn",
    "section": "Import the Required Libraries",
    "text": "Import the Required Libraries\nThe first step, as always, is to import the necessary libraries. Create a new cell in your notebook, paste the following code in it, and run the cell:\nimport joblib\nimport re\nimport string\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, classification_report\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nThis code will import the required libraries. Here’s why you need them:\n\nYou import joblib to save your model artifacts.\nYou import re and string to process the text.\nYou import numpy and pandas to read and transform the data.\nYou import multiple components of scikit-learn to extract features from the text, calculate the evaluation metrics, split the data into a training and a test set, and use the Multinomial Naive Bayes algorithm.\n\nNext, you’ll read and process the data."
  },
  {
    "objectID": "posts/text-classification-using-python-and-scikit-learn.html#read-the-data",
    "href": "posts/text-classification-using-python-and-scikit-learn.html#read-the-data",
    "title": "Text Classification Using Python and Scikit-learn",
    "section": "Read the Data",
    "text": "Read the Data\nStart by reading the data. You’ll use a dataset included in scikit-learn called 20 newsgroups. This dataset consists of roughly 20,000 newsgroup documents, split into 20 categories. For the sake of simplicity, you’ll only use five of those categories in this example.\nCreate a new cell and paste this code to read the data:\ncategories = [\n    \"alt.atheism\",\n    \"misc.forsale\",\n    \"sci.space\",\n    \"soc.religion.christian\",\n    \"talk.politics.guns\",\n]\n\nnews_group_data = fetch_20newsgroups(\n    subset=\"all\", remove=(\"headers\", \"footers\", \"quotes\"), categories=categories\n)\n\ndf = pd.DataFrame(\n    dict(\n        text=news_group_data[\"data\"],\n        target=news_group_data[\"target\"]\n    )\n)\ndf[\"target\"] = df.target.map(lambda x: categories[x])\nThis code reads the 20 newsgroups dataset. Here’s how it works:\n\nLines 1 to 7: Define a list of categories, which are the different newsgroup categories used in the analysis: alt.atheism, misc.forsale, sci.space, soc.religion.christian, and talk.politics.guns.\nLines 9 to 11: Use fetch_20newsgroupsto get the data from the 20 newsgroups dataset. This function removes the headers, footers, and quotes from the data and only gets data from the categories specified in the categories list.\nLines 13 and 19: Create a data frame from the data fetched. The data frame has two columns, one for the text of the newsgroup post and one for the category (target) of the newsgroup. You change the target column to display the actual category name instead of a number.\n\nThat’s it. Now, you’ll do a bit of cleaning of the data."
  },
  {
    "objectID": "posts/text-classification-using-python-and-scikit-learn.html#prepare-the-data",
    "href": "posts/text-classification-using-python-and-scikit-learn.html#prepare-the-data",
    "title": "Text Classification Using Python and Scikit-learn",
    "section": "Prepare the Data",
    "text": "Prepare the Data\nBefore you build your text classification model, you need to prepare the data. You’ll do it in three steps: clean the text column, create the training and testing splits, and generating bag of words features from the documents.\n\nClean the Text Column\nUse this code to clean the text. It’ll remove the punctuation marks and multiple adjacent spaces:\ndef process_text(text):\n    text = str(text).lower()\n    text = re.sub(\n        f\"[{re.escape(string.punctuation)}]\", \" \", text\n    )\n    text = \" \".join(text.split())\n    return text\n\ndf[\"clean_text\"] = df.text.map(process_text)\nThis code lowercases the text and removes any punctuation marks or duplicated spaces, and stores the results in a new column called clean_text. For that, you use process_text, which takes a string as input, lowercases it, replaces all punctuation marks with spaces, and removes the duplicated spaces.\n\n\nSplit the Data Into Train and Test Sets\nNext, you’ll split the dataset into a training and a testing set:\ndf_train, df_test = train_test_split(df, test_size=0.20, stratify=df.target)\ntrain_test_split is used to split a dataset into training and testing sets. You provide the data frame you wish to split to the function and specify the following parameters:\n\ntest_size=0.20: this defines the size of the test set to 20% of the total.\nstratify=df.target: ensures that the training and testing sets are split in a stratified manner using target. This is important because it prevents bias.\n\nNext, you’ll use these datasets to train and evaluate your model.\n\n\nCreate Bag of Words Features\nMachine Learning models cannot handle text features directly. To train a model, you first need to turn your text into numerical features. One popular approach to do that is called bag of words, and that’s what you’ll use in this example.\nIn the bag of words approach, each document is represented as a row in a matrix, with each word or token appearing in the document represented by a column.\nFor example, consider these two sentences:\n\nI like reading books\nI do not like cooking\n\nThe simplest bag of words representation for these two sentences will look like this:\n\n\n\nid_doc\nI\nlike\nreading\nbooks\ndo\nnot\ncooking\n\n\n\n\n1\n1\n1\n1\n1\n0\n0\n0\n\n\n2\n1\n1\n0\n0\n1\n1\n1\n\n\n\nOnce you have this numerical representation, you can pass this dataset to your machine learning model. This is what you’ll do with the documents in the 20 newsgroup dataset. Keep in mind that because the dataset has so many documents, you’ll end up with a matrix with many more columns than the example above.\nTo create a bag of words representation in scikit-learn , you must use CountVectorizer. You can use this code:\nvec = CountVectorizer(\n    ngram_range=(1, 3),\n    stop_words=\"english\",\n)\n\nX_train = vec.fit_transform(df_train.clean_text)\nX_test = vec.transform(df_test.clean_text)\n\ny_train = df_train.target\ny_test = df_test.target\nCountVectorizer turns text into numerical features. Here’s what’s happening in the code above:\n\nLines 1 to 4: You use CountVectorizer to build a bag of words representation of clean_text. You specify two parameters: ngram_range and stop_words. ngram_range is the range of n-grams that the function will use. An n-gram is a sequence of n words. (1, 3) means that the function will use sequences of 1, 2, and 3 words to generate the counts. stop_words is a list of words that the function will ignore. In this case, the list “english” means that the function will ignore the most common words in English.\nLines 6 and 7: You generate the matrices of token counts for your training and testing set and save them into X_train and X_test.\nLines 9 and 10: You save the response variable from the training and testing set into y_train and y_test.\n\nNext, you’ll train your text classification model."
  },
  {
    "objectID": "posts/text-classification-using-python-and-scikit-learn.html#train-and-evaluate-the-model",
    "href": "posts/text-classification-using-python-and-scikit-learn.html#train-and-evaluate-the-model",
    "title": "Text Classification Using Python and Scikit-learn",
    "section": "Train and Evaluate the Model",
    "text": "Train and Evaluate the Model\nFinally, you can train the model by running this code:\nnb = MultinomialNB()\nnb.fit(X_train, y_train)\n\npreds = nb.predict(X_test)\nprint(classification_report(y_test, preds))\nIn lines 1 and 2, you train a Multinomial Naive Bayes model. This simple probabilistic model is commonly used in cases with discrete features such as word counts.\nThen, in lines 4 and 5, you evaluate the model’s results by computing the precision, recall, and f1 scores.\nAfter you run the code, you’ll get an output that will look something like this:\nYou’ve trained the model and obtained the relevant evaluation metrics. You achieved an 0.83 f1 score, which is not bad!\nNext, you’ll learn how to save and load your model so that you can use it for inference."
  },
  {
    "objectID": "posts/text-classification-using-python-and-scikit-learn.html#saving-and-loading-the-model",
    "href": "posts/text-classification-using-python-and-scikit-learn.html#saving-and-loading-the-model",
    "title": "Text Classification Using Python and Scikit-learn",
    "section": "Saving and Loading the Model",
    "text": "Saving and Loading the Model\nIf you’d like to save the model for later, then you can use joblib. You’ll need to save all the artifacts required to run the model, which in this case would be the vectorizer vec and the model nb.\nYou can use the following code to save your model artifacts:\njoblib.dump(nb, \"nb.joblib\")\njoblib.dump(vec, \"vec.joblib\")\nIf you want to reuse your model later, simply read it and use it to classify new data samples as follows:\nnb_saved = joblib.load(\"nb.joblib\")\nvec_saved = joblib.load(\"vec.joblib\")\n\nsample_text = [\"Space, Stars, Planets and Astronomy!\"]\n\n# Process the text in the same way you did when you trained it!\nclean_sample_text = process_text(sample_text)\nsample_vec = vec_saved.transform(sample_text)\nnb_saved.predict(sample_vec)\nThe code above will read the previously saved artifacts into nb_saved and vec_saved. Then you can apply them to new samples of text you’d like to classify.\nThat’s all! You’ve learned how to use Python and Scikit-learn to train a text classification model. Let me offer some suggestions for what you could do next."
  },
  {
    "objectID": "posts/text-classification-using-python-and-scikit-learn.html#next-steps",
    "href": "posts/text-classification-using-python-and-scikit-learn.html#next-steps",
    "title": "Text Classification Using Python and Scikit-learn",
    "section": "Next Steps",
    "text": "Next Steps\nIf you want to take your modeling skills to the next level, here are some ideas to explore:\n\nUsing Cross-validation to ensure that your results generalize well.\nGet the best out of your model by tuning your model’s hyperparameters.\nUsing scikit-learn’s pipelines to generate fewer artifacts and simplify deployment.\n\nAlso, given the latest advances in Natural Language Processing (NLP), transformer-based approaches are becoming the go-to options for many problems that use text features. A good starting point is huggingface’s NLP course."
  },
  {
    "objectID": "posts/text-classification-using-python-and-scikit-learn.html#conclusion",
    "href": "posts/text-classification-using-python-and-scikit-learn.html#conclusion",
    "title": "Text Classification Using Python and Scikit-learn",
    "section": "Conclusion",
    "text": "Conclusion\nIn Machine Learning, text classification is the task of labeling pieces of text through automated methods. This tutorial showed you how to build your first text classification model using Python and Scikit-learn.\nYou’ve learned:\n\nHow to clean text data and create features for your model.\nHow to train a text classification model and generate evaluation metrics.\nHow to save and load your model for future use.\n\nIf you have any questions or feedback, let me know in the comments!\nAll the code for this tutorial is available on GitHub."
  },
  {
    "objectID": "posts/elasticseach-python.html",
    "href": "posts/elasticseach-python.html",
    "title": "How to Use Elasticsearch in Python",
    "section": "",
    "text": "Elasticsearch (ES) is a technology used by many companies, including GitHub, Uber, and Facebook. It’s not often taught in Data Science courses, but it’s something you’ll likely come across in your career.\nMany data scientists have trouble setting up a local environment or understanding how to interact with Elasticsearch in Python. Furthermore, there aren’t many up-to-date resources.\nThat’s why I decided to create this tutorial. It will teach you the basics, and you will be able to set up an Elasticsearch cluster on your machine for local development in no time. You’ll also learn how to create an index, store data in it, and use it to search your data.\nLet’s get started!"
  },
  {
    "objectID": "posts/elasticseach-python.html#whats-elasticsearch",
    "href": "posts/elasticseach-python.html#whats-elasticsearch",
    "title": "How to Use Elasticsearch in Python",
    "section": "What’s Elasticsearch?",
    "text": "What’s Elasticsearch?\nElasticsearch is a distributed, fast, and easy-to-scale search engine capable of handling textual, numerical, geospatial, structured, and unstructured data. It’s a popular search engine for apps, websites, and log analytics. It is also a key component of the Elastic Stack (also known as ELK), which includes Logstash and Kibana.\nTo understand the inner workings of Elasticsearch, think of it as two distinct processes. One is ingestion, which normalizes and enriches the raw data before indexing using an inverted index. The second is retrieval, which enables users to retrieve data by writing queries that run against the index.\nThat’s all you need to know for now. Next, you’ll prepare your local environment to run an ES cluster."
  },
  {
    "objectID": "posts/elasticseach-python.html#prerequisites",
    "href": "posts/elasticseach-python.html#prerequisites",
    "title": "How to Use Elasticsearch in Python",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou need to set up a few things before you get started. Make sure you have these covered, and I’ll see you in the next section:\n\nInstall docker.\nDownload the necessary data.\nCreate a virtual environment and install the required packages. If you like venv, you can run these commands:\n\npython3 -m venv .venv\nsource .venv/bin/activate\npython3 -m pip install pandas==1.4.3 notebook==6.3.0 elasticsearch==8.7.0\nAll good? Let’s continue."
  },
  {
    "objectID": "posts/elasticseach-python.html#create-a-local-elasticsearch-cluster",
    "href": "posts/elasticseach-python.html#create-a-local-elasticsearch-cluster",
    "title": "How to Use Elasticsearch in Python",
    "section": "Create a Local Elasticsearch Cluster",
    "text": "Create a Local Elasticsearch Cluster\nThe easiest way to run Elasticsearch locally is by using docker.\nOpen a terminal and run this code to start a single-node ES cluster you can use for local development:\ndocker run --rm -p 9200:9200 -p 9300:9300 -e \"xpack.security.enabled=false\" -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:8.7.0\nOnce you run this command, you’ll see a lot of text on your terminal. But don’t worry, that’s fine!\nThis command will start an Elasticsearch cluster in your machine. There are a few things to unpack here:\n\ndocker run: It’s the command you use to run an image inside -a container.\n--rm: This parameter lets Docker know to clean up the container and remove the file system when the container exits.\n-p 9200:9200 -p 9300:9300 : This tells Docker which ports to open on the container’s network interface.\n-e \"xpack.security.enabled=false\": This tells Docker to start with the security features disabled. This parameter should be set to true (or excluded) when running in production.\n-e \"discovery.type=single-node\": This tells Docker to create a cluster with a single node."
  },
  {
    "objectID": "posts/elasticseach-python.html#connect-to-your-cluster",
    "href": "posts/elasticseach-python.html#connect-to-your-cluster",
    "title": "How to Use Elasticsearch in Python",
    "section": "Connect to Your Cluster",
    "text": "Connect to Your Cluster\nCreate a new Jupyter Notebook, and run the following code, to connect to your newly created ES cluster.\nfrom elasticsearch import Elasticsearch\n\nes = Elasticsearch(\"http://localhost:9200\")\nes.info().body\nThis will connect to your local cluster. Make sure to use http instead of https. If you don’t, you’ll get an error as you don’t have a valid SSL/TLS certificate. Note that in production, you’ll want to use https.\nIf everything went well, you should see an output similar to mine:\n\nNow let’s get some data in your newly created index!"
  },
  {
    "objectID": "posts/elasticseach-python.html#read-the-dataset",
    "href": "posts/elasticseach-python.html#read-the-dataset",
    "title": "How to Use Elasticsearch in Python",
    "section": "Read the Dataset",
    "text": "Read the Dataset\nUse pandas to read the dataset and get a sample of 5,000 rows from it. You’ll use a sample because otherwise, it’ll take a long time to index the documents.\nimport pandas as pd\n\ndf = (\n    pd.read_csv(\"wiki_movie_plots_deduped.csv\")\n    .dropna()\n    .sample(5000, random_state=42)\n    .reset_index()\n)\nNext, you’ll create an index to store this data."
  },
  {
    "objectID": "posts/elasticseach-python.html#create-an-index",
    "href": "posts/elasticseach-python.html#create-an-index",
    "title": "How to Use Elasticsearch in Python",
    "section": "Create an Index",
    "text": "Create an Index\nAn index is a collection of documents that Elasticsearch stores and represents through a data structure called an inverted index. This data structure identifies the documents in which each unique word appears.\nElasticsearch creates this inverted index when you index documents. This is how it can perform quick full-text searches.\nAs you can imagine, you must first create an index before you can begin indexing documents. This is how you do it:\nmappings = {\n        \"properties\": {\n            \"title\": {\"type\": \"text\", \"analyzer\": \"english\"},\n            \"ethnicity\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n            \"director\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n            \"cast\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n            \"genre\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n            \"plot\": {\"type\": \"text\", \"analyzer\": \"english\"},\n            \"year\": {\"type\": \"integer\"},\n            \"wiki_page\": {\"type\": \"keyword\"}\n    }\n}\n\nes.indices.create(index=\"movies\", mappings=mappings)\nThis code will create a new index called movies using the cluster you set up earlier.\nLines 1 to 12 define a mapping, which tells the index how the documents should be stored. A mapping specifies the data types assigned to each field in the documents stored in your index.\nYou can use either a dynamic or explicit mapping. In a dynamic mapping, Elasticsearch detects which data type should be used for each field. In an explicit mapping, each data type is manually defined. The latter allows you greater freedom in defining each field, which is why you used one in the code above.\nNow you’ll start adding data to your index."
  },
  {
    "objectID": "posts/elasticseach-python.html#add-data-to-your-index",
    "href": "posts/elasticseach-python.html#add-data-to-your-index",
    "title": "How to Use Elasticsearch in Python",
    "section": "Add Data to Your Index",
    "text": "Add Data to Your Index\nYou can use es.index() or bulk() to add data to an index. es.index() adds one item at a time while bulk() lets you add multiple items at the same time.\nYou can use any of the two methods to add data to your index:\n\nUsing es.index()\nHere’s how you use es.index() to store your data:\nfor i, row in df.iterrows():\n    doc = {\n        \"title\": row[\"Title\"],\n        \"ethnicity\": row[\"Origin/Ethnicity\"],\n        \"director\": row[\"Director\"],\n        \"cast\": row[\"Cast\"],\n        \"genre\": row[\"Genre\"],\n        \"plot\": row[\"Plot\"],\n        \"year\": row[\"Release Year\"],\n        \"wiki_page\": row[\"Wiki Page\"]\n    }\n\n    es.index(index=\"movies\", id=i, document=doc)\nThis code iterates through the rows of the dataset you read earlier and adds to the index the relevant information from each row using es.index(). You use three parameters of that method:\n\nindex=\"movies\": this tells Elasticsearch which index to use to store the data. You can have multiple indexes in a cluster.\nid=i: this is the document’s identifier when you add it to the index. In this case, you set it to be the row number.\ndocument=doc: this specifies to Elasticsearch what information it should store.\n\n\n\nUsing bulk()\nHere’s how you use bulk() to store your data:\nfrom elasticsearch.helpers import bulk\n\nbulk_data = []\nfor i,row in df.iterrows():\n    bulk_data.append(\n        {\n            \"_index\": \"movies\",\n            \"_id\": i,\n            \"_source\": {\n                \"title\": row[\"Title\"],\n                \"ethnicity\": row[\"Origin/Ethnicity\"],\n                \"director\": row[\"Director\"],\n                \"cast\": row[\"Cast\"],\n                \"genre\": row[\"Genre\"],\n                \"plot\": row[\"Plot\"],\n                \"year\": row[\"Release Year\"],\n                \"wiki_page\": row[\"Wiki Page\"],\n            }\n        }\n    )\nbulk(es, bulk_data)\nbulk() requires the same information as .index(): the index’s name, the document’s ID, and the document itself. But instead of adding each item one by one, you must create a list of dictionaries with all the documents you want to add to the index. Then, you pass this information and the cluster object to bulk().\nAfter you add the data, you can make sure it worked by counting the number of items in the index:\nes.indices.refresh(index=\"movies\")\nes.cat.count(index=\"movies\", format=\"json\")\nYour output should look like this:"
  },
  {
    "objectID": "posts/elasticseach-python.html#search-your-data-using-elasticsearch",
    "href": "posts/elasticseach-python.html#search-your-data-using-elasticsearch",
    "title": "How to Use Elasticsearch in Python",
    "section": "Search Your Data Using Elasticsearch",
    "text": "Search Your Data Using Elasticsearch\nFinally, you’ll want to start running searches using your index. Elasticsearch has a powerful DSL that lets you build many types of queries.\nHere’s an example of a search that looks for movies starring Jack Nicholson but whose director isn’t Roman Polanski:\nresp = es.search(\n    index=\"movies\",\n    query={\n            \"bool\": {\n                \"must\": {\n                    \"match_phrase\": {\n                        \"cast\": \"jack nicholson\",\n                    }\n                },\n                \"filter\": {\"bool\": {\"must_not\": {\"match_phrase\": {\"director\": \"roman polanski\"}}}},\n            },\n        },\n)\nresp.body\nWhen you run this code, you should get a very long response that looks something like this:\nNow it’s time for you to try and build your own searches. A good starting point is the query DSL documentation."
  },
  {
    "objectID": "posts/elasticseach-python.html#delete-documents-from-the-index",
    "href": "posts/elasticseach-python.html#delete-documents-from-the-index",
    "title": "How to Use Elasticsearch in Python",
    "section": "Delete Documents From the Index",
    "text": "Delete Documents From the Index\nYou can use the following code to remove documents from the index:\nes.delete(index=\"movies\", id=\"2500\")\nThe code above will delete the document with ID 2500 from the index movies."
  },
  {
    "objectID": "posts/elasticseach-python.html#delete-an-index",
    "href": "posts/elasticseach-python.html#delete-an-index",
    "title": "How to Use Elasticsearch in Python",
    "section": "Delete an Index",
    "text": "Delete an Index\nFinally, if for whatever reason, you’d like to delete an index (and all of its documents), here’s how you do it:\nes.indices.delete(index='movies')"
  },
  {
    "objectID": "posts/elasticseach-python.html#conclusion",
    "href": "posts/elasticseach-python.html#conclusion",
    "title": "How to Use Elasticsearch in Python",
    "section": "Conclusion",
    "text": "Conclusion\nThis tutorial taught you the basics of Elasticsearch and how to use it. This will be useful in your career, as you will surely come across Elasticsearch at some point.\nIn this tutorial, you’ve learned:\n\nHow to set up an Elasticsearch cluster in your machine\nHow to create an index and store data in it\nHow to search your data using Elasticsearch\n\nIf you have any questions or feedback, let me know in the comments!\nAll the code for this tutorial is available on GitHub."
  },
  {
    "objectID": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html",
    "href": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html",
    "title": "Use Google Sheets, S3, and Python to Build a Website Quickly",
    "section": "",
    "text": "A few days ago, I was looking for courses that were free or discounted due to the COVID-19. I thought that others might be doing the same, so I decided to compile some resources and publish them online.\nI started compiling the courses using a Google Sheet and was planning on sharing it after I had enough resources. However, something was bothering me. Opening sheets on a mobile suck and most people use their phones for browsing the internet. I thought I could do better.\nThe thing is that I have little experience in web development. Also, I didn’t want to dedicate more than a few hour on developing and launching the site. So I decided to build something quickly.\nHere are the requirements I set for building the site quickly and the approach I took to meet them:\nProduct Requirements:\nApproach:\nAfter a couple of hours, I launched stayhomeandlearn.org.\nThe rest of this article is a tutorial on how to build a static site using Google Sheets, AWS, and Python.\nFor this tutorial, we will set up a script that reads data from Google Sheets, generates a static site using a predefined template, and deploys it to an S3 bucket. This article is meant for programmers with little knowledge of web development that want to get something running quickly.\nThere are five sections in the tutorial: Requirements, Review Code and Jinja Template, Using the Google Sheets API, and Build and Deploy Your Site."
  },
  {
    "objectID": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html#requirements",
    "href": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html#requirements",
    "title": "Use Google Sheets, S3, and Python to Build a Website Quickly",
    "section": "Requirements",
    "text": "Requirements\nThese you’ll need to set up or review on your own. I added some links for that purpose.\n\nPython &gt;= 3.7\nGoogle account\nGoogle Cloud Platform (GCP) account\nAmazon AWS account\nAWS CLI (You can use brew if you have a Mac)\nA profile configured in the AWS CLI\nA bit of HTML and CSS"
  },
  {
    "objectID": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html#code-and-jinja-template",
    "href": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html#code-and-jinja-template",
    "title": "Use Google Sheets, S3, and Python to Build a Website Quickly",
    "section": "Code and Jinja Template",
    "text": "Code and Jinja Template\nFirst, create a directory called my_pokemon_stats and open a terminal from there. Then, create a virtual environment and install the required packages as follows:\npython3 -m venv venv\nsource venv/bin/activate\npip3 install boto3 gspread jinja2 oauth2client\nCreate virtual environment and install required libraries\nNext, download and save these two files there: template.html and site_builder.py. These are the building blocks for generating the site.\ntemplate.html is the Jinja template we will use for building the site. It’s an HTML-like file where you can add logic that will be processed in Python and generate the definitive site. This file looks as follows:\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"UTF-8\" /&gt;\n    &lt;meta\n      name=\"viewport\"\n      content=\"width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0\"\n    /&gt;\n    &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\" /&gt;\n    &lt;link\n      href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\"\n      rel=\"stylesheet\"\n      integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\"\n      crossorigin=\"anonymous\"\n    /&gt;\n    &lt;title&gt;My Pokemon Stats&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;header id=\"header\"&gt;\n      &lt;div class=\"container text-center\"&gt;\n        &lt;h1 class=\"pt-5 pb-1 font-weight-bold\"&gt;My Pokemon Stats&lt;/h1&gt;\n        &lt;hr /&gt;\n        &lt;p class=\"pt-2\"&gt;\n          This is a site I use to store the stats of all my Pokemon.\n        &lt;/p&gt;\n      &lt;/div&gt;\n    &lt;/header&gt;\n\n    &lt;section id=\"pokemon_table\"&gt;\n      &lt;div class=\"container py-4\"&gt;\n        &lt;div class=\"table-responsive\"&gt;\n          &lt;table class=\"table table-hover\"&gt;\n            &lt;thead class=\"thead-dark\"&gt;\n              &lt;tr&gt;\n                &lt;th scope=\"col\"&gt;Name&lt;/th&gt;\n                &lt;th scope=\"col\"&gt;Type 1&lt;/th&gt;\n                &lt;th scope=\"col\"&gt;Type 2&lt;/th&gt;\n                &lt;th scope=\"col\"&gt;Total&lt;/th&gt;\n                &lt;th scope=\"col\"&gt;HP&lt;/th&gt;\n                &lt;th scope=\"col\"&gt;Attack&lt;/th&gt;\n                &lt;th scope=\"col\"&gt;Defense&lt;/th&gt;\n              &lt;/tr&gt;\n            &lt;/thead&gt;\n            &lt;tbody&gt;\n              {% for row in data %}\n              &lt;tr&gt;\n                &lt;td&gt;{{ row[\"Name\"] }}&lt;/td&gt;\n                &lt;td&gt;{{ row[\"Type 1\"] }}&lt;/td&gt;\n                &lt;td&gt;{{ row[\"Type 2\"] }}&lt;/td&gt;\n                &lt;td&gt;{{ row[\"Total\"] }}&lt;/td&gt;\n                &lt;td&gt;{{ row[\"HP\"] }}&lt;/td&gt;\n                &lt;td&gt;{{ row[\"Attack\"] }}&lt;/td&gt;\n                &lt;td&gt;{{ row[\"Defense\"] }}&lt;/td&gt;\n              &lt;/tr&gt;\n              {% endfor %}\n            &lt;/tbody&gt;\n          &lt;/table&gt;\n        &lt;/div&gt;\n      &lt;/div&gt;\n    &lt;/section&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\nLet’s break it down:\n\nYou can safely ignore most of what’s inside the &lt;head&gt; tag. It’s standard HTML5 code that you’ll see in most pages. However, there are just two interesting tags that we’ll take a closer a look at: &lt;link&gt; and &lt;title&gt;.\nIn this case, the &lt;link&gt; tag is used to import the Bootstrap component library. We  will use it to define simple styles for the different sections of the page and make it look good without much effort. &lt;title&gt; defines the title of the page (what you see in the browser’s tab) and it is useful for SEO and social media sharing.\nNext, there’s the &lt;header&gt; section inside the &lt;body&gt; tag. This is where we define the text that will appear in the page. It will look like the image below. I used standard styling from Bootstrap to center the text and added a bit of padding.\n\n\nMy Pokemon stats header\n\nFinally, we have the &lt;section id=\"pokemon_table\"&gt;. The &lt;div&gt; and &lt;table&gt; tags provide some basic styling for building a table. Next, we define the header of the table inside the &lt;thead&gt; tags. Inside the &lt;tbody&gt; tag is where Jinja does its magic\nThe {% for row in data %} is a loop that goes through each row of the Pokemon’s data. In each of the &lt;td&gt;{{ row[\"...\"] }}&lt;/td&gt; we get the information per row from field (e.g. Name, Type 1, Type 2). This generates something that will look as follows:\n\n\nTable with Pokemon stats\nNext, we have the site_builder.py file. This script downloads the Pokemon’s data from Google Sheets, processes the data and the template.html file, and then uploads the resulting file to an S3 bucket.\nimport csv\n\nimport boto3\nimport gspread\nimport jinja2\nfrom oauth2client.service_account import ServiceAccountCredentials\n\nAWS_PROFILE = \"INSERT-AWS-PROFILE-HERE\"\nBUCKET = \"INSERT-BUCKET-NAME-HERE\"\nWORKBOOK = \"INSERT-WORKBOOK-NAME-HERE\"\n\n\ndef download_data():\n    \"\"\"Download data using the Google Sheets API\"\"\"\n    scope = [\n        \"https://spreadsheets.google.com/feeds\",\n        \"https://www.googleapis.com/auth/drive\",\n    ]\n    credentials = ServiceAccountCredentials.from_json_keyfile_name(\n        \"credentials.json\", scope\n    )\n    client = gspread.authorize(credentials)\n\n    worksheet = client.open(WORKBOOK).get_worksheet(0)\n    sheet_values = worksheet.get_all_values()\n\n    print(f\"Downloading: {worksheet.title}\")\n    with open(\"my_pokemon_stats.csv\", \"w\") as f:\n        writer = csv.writer(f)\n        writer.writerows(sheet_values)\n\n\ndef generate_site():\n    \"\"\"Generate site in local directory\"\"\"\n    print(\"Process data and build site\")\n\n    template_loader = jinja2.FileSystemLoader(searchpath=\"./\")\n    template_env = jinja2.Environment(loader=template_loader)\n    template = template_env.get_template(\"template.html\")\n\n    with open(\"my_pokemon_stats.csv\") as csv_file:\n        csv_reader = csv.DictReader(csv_file)\n        data = [row for row in csv_reader]\n\n    output = template.render(data=data)\n\n    with open(\"index.html\", \"w\") as f:\n        f.write(output)\n\n\ndef deploy_site():\n    \"\"\"Deploy site S3 bucket\"\"\"\n    print(\"Upload data to S3\")\n    session = boto3.Session(profile_name=AWS_PROFILE)\n    s3 = session.resource(\"s3\")\n    s3.Bucket(BUCKET).upload_file(\n        Filename=\"index.html\", Key=\"index.html\", ExtraArgs={\"ContentType\": \"text/html\"}\n    )\n\n\nif __name__ == \"__main__\":\n    download_data()\n    generate_site()\n    deploy_site()\nThe code is structured in 3 functions: download_sheets, generate_site, and deploy_site. We will fill the details for accessing AWS and the Google Sheets API in the next sections."
  },
  {
    "objectID": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html#using-the-google-sheets-api",
    "href": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html#using-the-google-sheets-api",
    "title": "Use Google Sheets, S3, and Python to Build a Website Quickly",
    "section": "Using the Google Sheets API",
    "text": "Using the Google Sheets API\nFollow these steps to download the Pokemon’s data using the Google Sheets API:\n\nCreate a Workbook in Google Sheets (you can copy mine: My Pokemon Stats)\nGo to the Google APIs Console\nCreate a new project called MyPokemonStats.\nClick on Enable API and Services. Search for and enable the Google Sheets API.\nGo back to the Google APIs Console and click on Enable API and Services again. Now search for and enable the Google Drive API.\nClick on Create credentials.For the next 4 questions select:Google Drive API, Web Server (e.g. node.js, Tomcat), Application data, and No, I’m not using them.\nClick on What credentials do I need? Select a name for the service account (e.g. get-data) grant it a Project Role of Editor. Select the JSON option for Key type\nA dialog box will open. Save the JSON file, copy it to the my_pokemon_stats directory, and rename it to credentials.json.\nOpen the credentials.json file. Find a key called client_email, copy its value (e.g., get-data@iam….). Go back to your Workbook in Google Sheets, click the Share button in the top right, and paste the client email into the People field to give it edit rights. Hit Send.\nGo to the site_builder.py script, and set the WORKBOOK variable to whatever name you gave to your workbook on step 1."
  },
  {
    "objectID": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html#setting-up-an-s3-bucket-and-aws-related-configurations",
    "href": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html#setting-up-an-s3-bucket-and-aws-related-configurations",
    "title": "Use Google Sheets, S3, and Python to Build a Website Quickly",
    "section": "Setting up an S3 bucket and AWS-related configurations",
    "text": "Setting up an S3 bucket and AWS-related configurations\nNow, let’s create the S3 bucket and configure our code to access AWS programmatically:\n\nGo to the Amazon S3 Console\nCreate an S3 bucket\nOnce in the bucket, click on Properties and then on Static website hosting\nSelect the option Use this bucket to host a website\nUnder Index document and Error document put index.html\nSave the URL from Endpoint. You’ll use that URL for connecting to your site.\nGo to Permissions and click Edit\nClear Block all public access, choose Save, and confirm. When you change this anyone on the internet will have access to the contents of this bucket. That’s what you want when you are publishing a site, however don’t put anything private there!\nNow go to Bucket Policy, replace the bucket name in the policy below, paste it there, and click Save.\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"PublicReadGetObject\",\n      \"Effect\": \"Allow\",\n      \"Principal\": \"*\",\n      \"Action\": [\"s3:GetObject\"],\n      \"Resource\": [\"arn:aws:s3:::BUCKET-NAME-HERE/*\"]\n    }\n  ]\n}\n\nGo to the site_builder.py script. Set the variable AWS_PROFILE variable to the profile name you use for accessing AWS (in a UNIX sytem it should be one of the profiles in ~/.aws/credentials)."
  },
  {
    "objectID": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html#build-and-deploy-your-site",
    "href": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html#build-and-deploy-your-site",
    "title": "Use Google Sheets, S3, and Python to Build a Website Quickly",
    "section": "Build and Deploy Your Site",
    "text": "Build and Deploy Your Site\nFinally, you should be able to run python site_builder.py from the root folder of the project to generate the site. This will download the data from Google Sheets, process the template.html file using Jinja and upload the site to the S3 bucket.\nIf you want to check the site, go to the endpoint URL (step 6 from the previous section)."
  },
  {
    "objectID": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html#closing-words",
    "href": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html#closing-words",
    "title": "Use Google Sheets, S3, and Python to Build a Website Quickly",
    "section": "Closing Words",
    "text": "Closing Words\nThis method is by no means perfect but will help you ship something quickly. I used this strategy to build stayhomeandlearn.org and it worked quite well. From April 1st until April 16th, the site got over fifteen thousand visitors, which exceeded my most optimistic expectations.\n\nThe site is slowly walking towards its death now. However, this process taught me how important it is to focus on shipping instead of wasting time looking for the perfect tools. I built the site quickly, people liked it, and after the first day it already had more traffic than any of the side projects I’ve done so far. That thing about perfect being the enemy of good is real.\nIn my case, I had to add more functionality to the script for styling and deploying purposes. If you are interested, you can take a look at the code in my GitHub repository: https://github.com/dylanjcastillo/stayhomeandlearn.org\n[1] G. Bauges, Google Spreadsheets and Python (2017)\n[2] V. Drake, Hosting your static site with AWS S3, Route 53, and CloudFront (2017)\n[3] A. Barradas, Pokemon With Stats (2016)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dylan Castillo",
    "section": "",
    "text": "Hi there 👋, I’m Dylan, the founder of Iwana Labs.\nI’ve been working in AI for the past 8 years and I like working on open-source projects. I’ve delivered projects for large-scale corporations, government agencies, and startups.\nIf you’d like to get notified when I publish new posts, you can subscribe to my newsletter."
  },
  {
    "objectID": "index.html#posts",
    "href": "index.html#posts",
    "title": "Dylan Castillo",
    "section": "Posts",
    "text": "Posts\nThese are longer posts covering mostly technical topics and a few personal posts such as yearly reviews.\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nDec 8, 2024\n\n\nSay What You Mean… Sometimes\n\n\n14 min\n\n\n\n\nNov 9, 2024\n\n\nStructured outputs: don’t put the cart before the horse\n\n\n5 min\n\n\n\n\nSep 21, 2024\n\n\nDeploying a FastAPI app with Kamal, AWS ECR, and Github Actions\n\n\n13 min\n\n\n\n\nSep 15, 2024\n\n\nDeploying a Django app with Kamal, AWS ECR, and Github Actions\n\n\n15 min\n\n\n\n\nSep 8, 2024\n\n\nClassifying images with Gemini Flash 1.5\n\n\n7 min\n\n\n\n\nAug 11, 2024\n\n\nCreate a Kamal-ready VPS on Hetzner using Terraform\n\n\n3 min\n\n\n\n\nJan 5, 2024\n\n\n2023: Personal Snapshot\n\n\n9 min\n\n\n\n\nJun 9, 2023\n\n\nClustering Documents with OpenAI embeddings, HDBSCAN and UMAP\n\n\n6 min\n\n\n\n\nMay 12, 2023\n\n\nCreate a Code Interpreter Chatbot with Pyodide, LangChain, and OpenAI\n\n\n12 min\n\n\n\n\nApr 27, 2023\n\n\nSemantic Search with Elasticsearch, OpenAI, and LangChain\n\n\n10 min\n\n\n\n\nApr 11, 2023\n\n\nSemantic Search with OpenSearch, Cohere, and FastAPI\n\n\n12 min\n\n\n\n\nMar 3, 2023\n\n\nBuild an AI Search Engine Using FastAPI, Qdrant, and ChatGPT\n\n\n17 min\n\n\n\n\nFeb 17, 2023\n\n\nTips for Standing Out on LinkedIn\n\n\n10 min\n\n\n\n\nFeb 3, 2023\n\n\nHow to Securely Deploy a FastAPI app with NGINX and Gunicorn\n\n\n14 min\n\n\n\n\nJan 20, 2023\n\n\nMy Experience at Entrepreneur First\n\n\n11 min\n\n\n\n\nSep 21, 2022\n\n\nPyScript 101\n\n\n7 min\n\n\n\n\nAug 31, 2022\n\n\nHow to Use OpenSearch in Python\n\n\n10 min\n\n\n\n\nAug 23, 2022\n\n\nText Classification Using Python and Scikit-learn\n\n\n9 min\n\n\n\n\nAug 12, 2022\n\n\nHow to Use Elasticsearch in Python\n\n\n10 min\n\n\n\n\nJun 17, 2022\n\n\nEntrepreneurship as a Risk Management Strategy\n\n\n7 min\n\n\n\n\nJan 31, 2022\n\n\n2021: Personal Snapshot\n\n\n8 min\n\n\n\n\nOct 7, 2021\n\n\nHow to Use GitHub Deploy Keys\n\n\n5 min\n\n\n\n\nJul 3, 2021\n\n\nHow to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express\n\n\n100 min\n\n\n\n\nJan 18, 2021\n\n\nHow to Cluster Documents Using Word2Vec and K-means\n\n\n21 min\n\n\n\n\nDec 10, 2020\n\n\nClean and Tokenize Text With Python\n\n\n11 min\n\n\n\n\nMay 25, 2020\n\n\n4 Ways To Improve Your Graphs Using Plotly\n\n\n12 min\n\n\n\n\nApr 16, 2020\n\n\nUse Google Sheets, S3, and Python to Build a Website Quickly\n\n\n13 min\n\n\n\n\nMar 1, 2020\n\n\nFast & Asynchronous In Python: Accelerate Your Requests Using asyncio\n\n\n12 min\n\n\n\n\nDec 20, 2019\n\n\nMind-reading Algorithms: An Introduction to Recommender Systems\n\n\n15 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#today-i-learned-til",
    "href": "index.html#today-i-learned-til",
    "title": "Dylan Castillo",
    "section": "Today I Learned (TIL)",
    "text": "Today I Learned (TIL)\nI’m a big fan of Simon Willison’s “TIL” posts, so I copied his idea.\nThey’re a great way to force yourself to write more often by lowering the bar for what it’s worth sharing.\nThese are short, less polished posts about things I’ve learned that I think others might find useful.\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nNov 23, 2024\n\n\nTransform any image to WebP from the terminal\n\n\n2 min\n\n\n\n\nAug 12, 2024\n\n\nFixing missing ‘python’ error in macOS\n\n\n1 min\n\n\n\n\nJun 22, 2024\n\n\nA Dockerfile for a Django app using Poetry\n\n\n4 min\n\n\n\n\nJun 16, 2024\n\n\nMigrate a blog from Ghost to Quarto\n\n\n8 min\n\n\n\n\nJun 8, 2024\n\n\nInstalling Alacritty, Zellij, and Neovim in macOS\n\n\n6 min\n\n\n\n\nJan 28, 2024\n\n\nLive Components with Django and htmx\n\n\n10 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "extras/say-what-you-mean-sometimes/say-what-you-mean-llama-8b.html",
    "href": "extras/say-what-you-mean-sometimes/say-what-you-mean-llama-8b.html",
    "title": "GSM8K",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nimport json\nfrom enum import Enum\nfrom pathlib import Path\nfrom textwrap import dedent\nfrom typing import Callable, List\n\nimport numpy as np\nimport outlines\nimport pandas as pd\nimport torch\nfrom datasets import load_dataset\nfrom dotenv import load_dotenv\nfrom langsmith import traceable\nfrom outlines.fsm.json_schema import build_regex_from_schema\nfrom outlines.samplers import greedy\nfrom pydantic import BaseModel, Field, constr\nfrom scipy import stats\nfrom transformers import AutoTokenizer\n\nnp.random.seed(42)\n\nload_dotenv()\n\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nUSE_SAMPLE = False\n\nmodel = outlines.models.transformers(\n    MODEL_NAME,\n    device=\"cuda\",\n    model_kwargs={\"torch_dtype\": torch.bfloat16, \"trust_remote_code\": True},\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nclass PromptType(Enum):\n    NATURAL_LANGUAGE = \"nl\"\n    STRUCTURED_GENERATION = \"sg\"\n\n\nclass ClientConfig(BaseModel):\n    name: str\n    use_response_model: bool\n    col_name: str\n    score_col_name: str\n\n\nCONFIGS = [\n    ClientConfig(\n        name=PromptType.NATURAL_LANGUAGE.value,\n        use_response_model=False,\n        col_name=f\"response_{PromptType.NATURAL_LANGUAGE.value}\",\n        score_col_name=f\"score_{PromptType.NATURAL_LANGUAGE.value}\",\n    ),\n    ClientConfig(\n        name=PromptType.STRUCTURED_GENERATION.value,\n        use_response_model=True,\n        col_name=f\"response_{PromptType.STRUCTURED_GENERATION.value}\",\n        score_col_name=f\"score_{PromptType.STRUCTURED_GENERATION.value}\",\n    ),\n]\nclass LLMEvaluator:\n    def __init__(\n        self,\n        configs: List[ClientConfig],\n        create_prompt_fn: Callable,\n        parse_response_fn: Callable,\n        response_model: BaseModel,\n        max_tokens: int = 256,\n    ):\n        self.configs = configs\n        self.create_prompt_fn = create_prompt_fn\n        self.parse_response_fn = parse_response_fn\n        self.schema_regex = build_regex_from_schema(\n            json.dumps(response_model.model_json_schema())\n        )\n        self.nl_generate = outlines.generate.text(model, sampler=greedy())\n        self.sg_generate = outlines.generate.regex(\n            model, self.schema_regex, sampler=greedy()\n        )\n        self.max_tokens = max_tokens\n        self.tokenizer = tokenizer\n\n    @traceable(run_type=\"prompt\")\n    def create_prompt(\n        self,\n        question: str,\n        prompt_type: str,\n    ) -&gt; List[dict]:\n        return self.create_prompt_fn(\n            question=question,\n            prompt_type=prompt_type,\n            tokenizer=self.tokenizer,\n        )\n\n    @traceable(run_type=\"parser\")\n    def parse_response(\n        self,\n        response: str,\n        prompt_type: str,\n    ) -&gt; str | int:\n        try:\n            return self.parse_response_fn(response, prompt_type)\n        except Exception as e:\n            print(f\"Error parsing response: {e}\")\n            return None\n\n    @traceable(run_type=\"llm\")\n    def call_llm(\n        self,\n        config: ClientConfig,\n        question: str,\n    ) -&gt; str | None:\n        message = self.create_prompt(question=question, prompt_type=config.name)\n        if config.name == PromptType.STRUCTURED_GENERATION.value:\n            return self.sg_generate(message)\n        return self.nl_generate(message, max_tokens=self.max_tokens)\n\n    @traceable(run_type=\"chain\")\n    def process_question(\n        self,\n        question: str,\n        config: ClientConfig,\n    ) -&gt; str | int | None:\n        answer = self.call_llm(\n            config=config,\n            question=question,\n        )\n        return self.parse_response(answer, config.name)\n\n    @traceable(run_type=\"chain\")\n    def process_questions(\n        self,\n        run_name: str,\n        questions: List[dict],\n        config: ClientConfig,\n    ) -&gt; List[str | int | None]:\n        return [\n            self.process_question(\n                question=question[\"question\"],\n                config=config,\n            )\n            for question in questions\n        ]\n\n    def generate_outputs(self, questions: List[dict]) -&gt; pd.DataFrame:\n        df = pd.DataFrame(\n            {\n                \"id\": [i for i in range(len(questions))],\n                \"question\": [question[\"question\"] for question in questions],\n                \"answer\": [question[\"answer\"] for question in questions],\n            }\n        )\n        for config in self.configs:\n            responses = self.process_questions(\n                run_name=config.name,\n                questions=questions,\n                config=config,\n            )\n            df[config.col_name] = responses\n        return df\n\n    def evaluate_outputs(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        df_copy = df.copy()\n        for config in self.configs:\n            df_copy[config.score_col_name] = (\n                df_copy[\"answer\"] == df_copy[config.col_name]\n            ) * 1\n        return df_copy\n\n    def calculate_confidence_intervals(\n        self, df: pd.DataFrame, conf_level: float = 0.95\n    ) -&gt; None:\n        print(\n            f\"Calculating confidence intervals ({conf_level}) with {len(df)} observations:\"\n        )\n        for config in self.configs:\n            score_col = config.score_col_name\n            scores = df[score_col]\n\n            if len(scores) == 0:\n                print(f\"No scores available for {score_col}\")\n                continue\n\n            mean_score = scores.mean()\n            se_score = scores.std() / np.sqrt(len(scores))\n\n            z_score = stats.norm.ppf((1 + conf_level) / 2)\n            margin_error = z_score * se_score\n            ci = [\n                max(0.0, mean_score - margin_error),\n                min(1.0, mean_score + margin_error),\n            ]\n            print(\n                f\"{score_col} - Mean: {mean_score * 100:.2f}% CI: {ci[0] * 100:.2f}% - {ci[1] * 100:.2f}%\"\n            )\n        print()\n\n    def run_paired_t_test(self, df: pd.DataFrame) -&gt; None:\n        scores = {}\n\n        for config in self.configs:\n            score_col = config.score_col_name\n            scores[score_col] = df[score_col] * 1\n\n        for score_col_1, score_col_2 in [\n            (\"score_nl\", \"score_sg\"),\n        ]:\n            if score_col_1 in scores and score_col_2 in scores:\n                t_stat, p_value = stats.ttest_rel(\n                    scores[score_col_1], scores[score_col_2]\n                )\n                print(f\"{score_col_1} vs {score_col_2}\")\n                print(f\"t-statistic: {t_stat}, p-value: {p_value}\")\nclass Response(BaseModel):\n    reasoning: constr(max_length=1000)\n    answer: int = Field(pattern=r'[1-9][0-9]{0,9}')\n\n\ndef create_prompt_gsm8k(question: str, prompt_type: str, tokenizer: AutoTokenizer):\n    if prompt_type == PromptType.STRUCTURED_GENERATION.value:\n        system_prompt = dedent(\"\"\"\n        You are an expert in solving grade school math tasks. You will be presented with a grade-school math word problem and be asked to solve it.\n\n        You will always respond with a JSON object matching the following format:\n        \n        {\"reasoning\": &lt;str, reasoning about the answer&gt;, \"answer\": &lt;int, final answer&gt;}\n        \n        First, provide your step by step reasoning in the \"reasoning\" field. Then, in the \"answer\" field, provide an integer that corresponds to the correct answer to the question. Don't include any other text in the \"answer\" field.\n        \"\"\")\n    else:\n        system_prompt = dedent(\"\"\"\n        You are an expert in solving grade school math tasks. You will be presented with a grade-school math word problem and be asked to solve it.\n        \n        You will always respond in the following format:\n        \n        &lt;str, reasoning about the answer&gt;\n        ANSWER: &lt;int, final answer&gt;\n        \n        First, provide your step by step reasoning. Then, in ANSWER, provide an integer that corresponds to the correct answer to the question. Don't include any other text in ANSWER.\n        \"\"\")\n\n    examples = [\n        (\n            \"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\",\n            \"There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6.\",\n            6,\n        ),\n        (\n            \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\",\n            \"There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5.\",\n            5,\n        ),\n        (\n            \"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\",\n            \"Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39.\",\n            39,\n        ),\n    ]\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": system_prompt,\n        },\n    ]\n\n    for example_q, example_reason, example_ans in examples:\n        messages.append(\n            {\n                \"role\": \"user\",\n                \"content\": f\"Question: {example_q}\",\n            }\n        )\n        if prompt_type == PromptType.STRUCTURED_GENERATION.value:\n            response = f'{{\"reasoning\": \"{example_reason}\", \"answer\": {example_ans}}}'\n        else:\n            response = f\"{example_reason}\\nANSWER: {example_ans}\"\n        messages.append(\n            {\n                \"role\": \"assistant\",\n                \"content\": response,\n            }\n        )\n    messages.extend(\n        [\n            {\n                \"role\": \"user\",\n                \"content\": f\"Question: {question}\",\n            },\n        ]\n    )\n\n    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n\ndef parse_response_gsm8k(response: str, prompt_type: str) -&gt; int:\n    if prompt_type == PromptType.STRUCTURED_GENERATION.value:\n        return int(json.loads(response)[\"answer\"])\n    else:\n        cleaned_response = (\n            response.split(\"\\nANSWER:\")[1].replace(\",\", \"\").rstrip(\".\").strip()\n        )\n        return int(cleaned_response)\nevaluator = LLMEvaluator(\n    configs=CONFIGS,\n    create_prompt_fn=create_prompt_gsm8k,\n    parse_response_fn=parse_response_gsm8k,\n    response_model=Response,\n)\ndataset = load_dataset(\"gsm8k\", \"main\")\nevals = [\n    {\n        \"question\": d[\"question\"],\n        \"answer\": int(d[\"answer\"].split(\"#### \")[1].replace(\",\", \"\").strip()),\n    }\n    for d in dataset[\"test\"]\n]\n\nif USE_SAMPLE:\n    evals = evals[:5]\n\ndf = evaluator.generate_outputs(evals)\ndf_results = evaluator.evaluate_outputs(df)\n\ndf_results.to_csv(\"gsm8k_results.csv\", index=False)\n\n/usr/local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/usr/local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n\n\nError parsing response: list index out of range\nError parsing response: list index out of range\nError parsing response: list index out of range\nError parsing response: list index out of range\nError parsing response: invalid literal for int() with base 10: '33.3'\nError parsing response: list index out of range\nError parsing response: list index out of range\nError parsing response: list index out of range\nError parsing response: list index out of range\nError parsing response: list index out of range\nError parsing response: invalid literal for int() with base 10: '27 22'\nError parsing response: list index out of range\nError parsing response: invalid literal for int() with base 10: 'x - 48'\nError parsing response: list index out of range\nError parsing response: list index out of range\n\n\n/usr/local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/usr/local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nevaluator.calculate_confidence_intervals(df_results)\nevaluator.run_paired_t_test(df_results)\n\nCalculating confidence intervals (0.95) with 1319 observations:\nscore_nl - Mean: 79.98% CI: 77.82% - 82.14%\nscore_sg - Mean: 79.45% CI: 77.27% - 81.64%\n\nscore_nl vs score_sg\nt-statistic: 0.6023185325028948, p-value: 0.547065732878361"
  },
  {
    "objectID": "extras/say-what-you-mean-sometimes/say-what-you-mean-llama-8b.html#last-letter",
    "href": "extras/say-what-you-mean-sometimes/say-what-you-mean-llama-8b.html#last-letter",
    "title": "GSM8K",
    "section": "Last Letter",
    "text": "Last Letter\n\nclass Response(BaseModel):\n    reasoning: constr(max_length=300)\n    answer: str = Field(pattern=r'[A-Za-z]{4}')\n\n\ndef create_prompt_last_letter(question, prompt_type: str, tokenizer: AutoTokenizer):\n    if prompt_type == PromptType.STRUCTURED_GENERATION.value:\n        system_prompt = dedent(\"\"\"\n        You are an expert in solving word puzzles. Your specific task is going to be to take a list of 4 names, get the last letter of each and concatenate these letters into a word. \n          \n        You will always respond with JSON in the following format:\n\n        {\"reasoning\": &lt;str, reasoning about the answer&gt;, \"answer\": &lt;str, final answer&gt;}\n        \n        First, provide your step by step reasoning in the \"reasoning\" field. Then, in the 'answer' field, write the word formed by concatenating the last letters of all the names.\n        \"\"\")\n    else:\n        system_prompt = dedent(\"\"\"\n        You are an expert in solving word puzzles. Your specific task is going to be to take a list of 4 names, get the last letter of each and concatenate these letters into a word. \n        \n        You will always respond in the following format:\n        \n        &lt;str, reasoning about the answer&gt;\n        ANSWER: &lt;str, final answer&gt;\n\n        First, provide your step by step reasoning. Then, in ANSWER, write the word formed by concatenating the last letters of all the names.\n        \"\"\")\n\n    fewshot_examples = [\n        (\n            \"Ian Peter Bernard Stephen\",\n            \"The last letter of 'Ian' is 'N'. The last letter of 'Peter' is 'R'. The last letter of 'Bernard' is 'D'. The last letter of 'Stephen' is 'N'. Concatenating them is 'NRDN'.\",\n            \"NRDN\",\n        ),\n        (\n            \"Javier Dylan Christopher Joseph\",\n            \"The last letter of 'Javier' is 'R'. The last letter of 'Dylan' is 'N'. The last letter of 'Christopher' is 'R'. The last letter of 'Joseph' is 'H'. Concatenating them is 'RNRH'.\",\n            \"RNRH\",\n        ),\n        (\n            \"Anthony Elizabeth Carlos Jesus\",\n            \"The last letter of 'Anthony' is 'Y'. The last letter of 'Elizabeth' is 'H'. The last letter of 'Carlos' is 'S'. The last letter of 'Jesus' is 'S'. Concatenating them is 'YHSS'.\",\n            \"YHSS\",\n        ),\n    ]\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": system_prompt,\n        },\n    ]\n\n    for example_q, example_reason, example_ans in fewshot_examples:\n        messages.append(\n            {\n                \"role\": \"user\",\n                \"content\": f\"Question: Take the last letters of the words in '{example_q}' and concatenate them.\",\n            }\n        )\n        if prompt_type == PromptType.STRUCTURED_GENERATION.value:\n            response = f'{{\"reasoning\": \"{example_reason}\", \"answer\": \"{example_ans}\"}}'\n        else:\n            response = f\"{example_reason}\\nANSWER: {example_ans}\"\n        messages.append(\n            {\n                \"role\": \"assistant\",\n                \"content\": response,\n            }\n        )\n\n    messages.extend(\n        [\n            {\n                \"role\": \"user\",\n                \"content\": f\"Question: {question}\",\n            }\n        ]\n    )\n    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n\ndef parse_response_last_letter(response: str, prompt_type: str) -&gt; str | None:\n    if prompt_type == PromptType.STRUCTURED_GENERATION.value:\n        return json.loads(response)[\"answer\"].lower()\n    else:\n        return response.split(\"\\nANSWER:\")[1].rstrip(\".\").strip().lower()\n\n\nevaluator = LLMEvaluator(\n    configs=CONFIGS,\n    create_prompt_fn=create_prompt_last_letter,\n    parse_response_fn=parse_response_last_letter,\n    response_model=Response,\n    max_tokens=128,\n)\n\n\ndataset = load_dataset(\"ChilleD/LastLetterConcat\")\nevals = [\n    {\"question\": d[\"question\"], \"answer\": d[\"answer\"].lower()} for d in dataset[\"test\"]\n]\n\nif USE_SAMPLE:\n    evals = evals[:5]\n\ndf = evaluator.generate_outputs(evals)\ndf_results = evaluator.evaluate_outputs(df)\n\ndf_results.to_csv(\"last_letter_results.csv\", index=False)\n\n/usr/local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/usr/local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/usr/local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n\n\n\nevaluator.calculate_confidence_intervals(df_results)\nevaluator.run_paired_t_test(df_results)\n\nCalculating confidence intervals (0.95) with 150 observations:\nscore_nl - Mean: 74.00% CI: 66.96% - 81.04%\nscore_sg - Mean: 78.00% CI: 71.35% - 84.65%\n\nscore_nl vs score_sg\nt-statistic: -1.743793659390529, p-value: 0.08325752502816747"
  },
  {
    "objectID": "extras/say-what-you-mean-sometimes/say-what-you-mean-llama-8b.html#shuffled-objects",
    "href": "extras/say-what-you-mean-sometimes/say-what-you-mean-llama-8b.html#shuffled-objects",
    "title": "GSM8K",
    "section": "Shuffled objects",
    "text": "Shuffled objects\n\nclass Response(BaseModel):\n    reasoning: constr(max_length=1200)\n    answer: str = Field(pattern=r'[A-E]')\n\n\ndef create_prompt_shuffled_objects(question: str, prompt_type: str, tokenizer: AutoTokenizer):\n    if prompt_type == PromptType.STRUCTURED_GENERATION.value:\n        system_prompt = dedent(\"\"\"\n        You are an expert in performing common sense tasks involving the ordering of a sequence of events.\n        Each question will present you with a sequence of shuffling events involving 5 people (switching objects, partners, positions, etc.). Your task is to determine the correct answer from the options provided.\n          \n        You will always respond with JSON in the format described below:\n                   \n        {\"reasoning\": &lt;reasoning about the answer&gt;, \"answer\": &lt;final answer&gt;}\n\n        The \"reasoning\" field will contain your step by step reasoning about the sequence of events. Then, in the \"answer\" field, provide the single letter representing the correct choice you are presented with. \n        \"\"\")\n    else:\n        system_prompt = dedent(\"\"\"\n        You are an expert in performing common sense tasks involving the ordering of a sequence of events.\n        Each question will present you with a sequence of shuffling events involving 5 people (switching objects, partners, positions, etc.). Your task is to determine the correct answer from the options provided.\n        \n        You will always respond in the format described below:\n        \n        &lt;str, reasoning about the answer&gt;\n        ANSWER: &lt;str, final answer&gt;\n        \n        First, provide your step by step reasoning about the sequence of events. Then, in ANSWER, provide the single letter representing the correct choice you are presented with.\n        \"\"\")\n\n    fewshot_examples = [\n        (\n            \"Alice, Bob, Claire, Dave, and Eve are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Patrick, Bob is dancing with Sam, Claire is dancing with Jamie, Dave is dancing with Lola, and Eve is dancing with Melissa.\\nThroughout the song, the dancers often trade partners. First, Dave and Eve switch partners. Then, Dave and Alice switch partners. Then, Eve and Alice switch partners. Then, Claire and Bob switch partners. Finally, Dave and Alice switch partners. At the end of the dance, Alice is dancing with\\nOptions:\\n(A) Patrick\\n(B) Sam\\n(C) Jamie\\n(D) Lola\\n(E) Melissa\",\n            \"Dave and Eve switch partners, so Dave's partner is now Melissa and Eve's partner is now Patrick. Then Dave and Alice switch partners so Dave's partner is now Patrick and Alice's partner is now Melissa. Then Eve and Alice switch partners so Eve's partner is now Melissa and Alice's partner is now Lola. Then Claire and Bob switch patners so Claire's partner is now Sam, and Bob's partner is now Jamie. Finally, Dave and Alice switch partners so Dave's new partner is Lola, and Alice's new partner is Patrick. Alice is dance in with Patrick, choice A.\",\n            \"A\",\n        ),\n        (\n            \"Alice, Bob, Claire, Dave, and Eve are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Ophelia, Bob is dancing with Jamie, Claire is dancing with Melissa, Dave is dancing with Rodrigo, and Eve is dancing with Patrick.\\nThroughout the song, the dancers often trade partners. First, Claire and Bob switch partners. Then, Claire and Eve switch partners. Then, Claire and Bob switch partners. Then, Eve and Dave switch partners. Finally, Claire and Alice switch partners. At the end of the dance, Alice is dancing with\\nOptions:\\n(A) Ophelia\\n(B) Jamie\\n(C) Melissa\\n(D) Rodrigo\\n(E) Patrick\",\n            \"Claire and Bob switch partners, so Claire's partner is now Jamie and Bob's partner is now Melissa. Then, Claire and Eve switch partners, so Claire's partner becomes Patrick and Eve's partner becomes Jamie. Next, Claire and Bob switch partners again, making Claire's partner Melissa and Bob's partner Patrick. After that, Eve and Dave switch partners, resulting in Eve's partner being Rodrigo and Dave's partner being Jamie. Finally, Claire and Alice switch partners, so Claire's partner is now Ophelia and Alice's partner becomes Melissa. Alice is dancing with Melissa, which is choice C.\",\n            \"C\",\n        ),\n        (\n            \"Alice, Bob, Claire, Dave, and Eve are friends and avid readers who occasionally trade books. At the start of the semester, they each buy one new book: Alice gets Catch-22, Bob gets Hound of the Baskervilles, Claire gets Frankenstein, Dave gets The Pearl, and Eve gets The Fellowship of the Ring.\\nAs the semester proceeds, they start trading around the new books. First, Eve and Alice swap books. Then, Alice and Claire swap books. Then, Alice and Bob swap books. Then, Dave and Alice swap books. Finally, Dave and Claire swap books. At the end of the semester, Dave has\\nOptions:\\n(A) Catch-22\\n(B) Hound of the Baskervilles\\n(C) Frankenstein\\n(D) The Pearl\\n(E) The Fellowship of the Ring\",\n            \"First, Eve and Alice swap, so Alice gets The Fellowship of the Ring and Eve gets Catch-22. Next, Alice and Claire swap, giving Claire The Fellowship of the Ring and Alice Frankenstein. Then, Alice and Bob swap, resulting in Bob holding Frankenstein and Alice having Hound of the Baskervilles. Dave and Alice then swap, so Dave takes Hound of the Baskervilles and Alice receives The Pearl. Finally, Dave and Claire swap books, which means Dave takes The Fellowship of the Ring from Claire. Therefore, at the end of all the swaps, Dave possesses The Fellowship of the Ring, making option E the correct answer.\",\n            \"E\",\n        ),\n    ]\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": system_prompt,\n        },\n    ]\n    for example_q, example_reason, example_ans in fewshot_examples:\n        messages.append(\n            {\n                \"role\": \"user\",\n                \"content\": f\"Question: {example_q}\",\n            }\n        )\n        if prompt_type == PromptType.STRUCTURED_GENERATION.value:\n            response = f'{{\"reasoning\": \"{example_reason}\", \"answer\": \"{example_ans}\"}}'\n        else:\n            response = f\"{example_reason}\\nANSWER: {example_ans}\"\n        messages.append(\n            {\n                \"role\": \"assistant\",\n                \"content\": response,\n            }\n        )\n\n    messages.extend(\n        [\n            {\n                \"role\": \"user\",\n                \"content\": f\"Question: {question}\",\n            }\n        ]\n    )\n    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n\ndef parse_response_shuffled_objects(response: str, prompt_type: str) -&gt; str:\n    if prompt_type == PromptType.STRUCTURED_GENERATION.value:\n        return json.loads(response)[\"answer\"].upper()\n    else:\n        return response.split(\"\\nANSWER:\")[1].rstrip(\".\").strip().upper()\n\n\nevaluator = LLMEvaluator(\n    configs=CONFIGS,\n    create_prompt_fn=create_prompt_shuffled_objects,\n    parse_response_fn=parse_response_shuffled_objects,\n    response_model=Response,\n    max_tokens=300,\n)\n\n\ndataset = load_dataset(\n    \"openeval/BIG-Bench-Hard\", data_files=\"tracking_shuffled_objects_five_objects.json\"\n)\nevals = [\n    {\n        \"question\": d[\"input\"],\n        \"answer\": d[\"target\"].replace(\"(\", \"\").replace(\")\", \"\").strip().upper(),\n    }\n    for d in dataset[\"train\"][\"examples\"][0][4:]  # first 3 are few-shot examples\n]\n\nif USE_SAMPLE:\n    evals = evals[:5]\n\ndf = evaluator.generate_outputs(evals)\ndf_results = evaluator.evaluate_outputs(df)\n\ndf_results.to_csv(\"shuffled_objects_results.csv\", index=False)\n\n\nevaluator.calculate_confidence_intervals(df_results)\nevaluator.run_paired_t_test(df_results)\n\nCalculating confidence intervals (0.95) with 246 observations:\nscore_nl - Mean: 42.68% CI: 36.49% - 48.88%\nscore_sg - Mean: 43.90% CI: 37.69% - 50.12%\n\nscore_nl vs score_sg\nt-statistic: -0.5380372188223832, p-value: 0.591039777132225"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Dylan – I’m the founder of Iwana Labs. I’ve worked in the AI & ML space for almost a decade.\nI’ve delivered successful projects for organizations like Deliveroo, Boston Consulting Group (BCG), the European Commission, and the Olympics.\nIn my free time, I like working on random side projects (open source, personal, or otherwise). Some of them have been featured in The Economist, praised by the CMOs of Zapier and HubSpot, and reached hundreds of stars on GitHub.\nEvery once in a while, I write about topics I’m interested in. You’ll find a collection of my articles here, covering everything from data science and machine learning to financial independence.\nIf you’re interested in working with me or just want to chat, feel free to reach out to me at dylan@iwanalabs.com or connect with me on LinkedIn."
  },
  {
    "objectID": "til/django-poetry-dockerfile.html",
    "href": "til/django-poetry-dockerfile.html",
    "title": "A Dockerfile for a Django app using Poetry",
    "section": "",
    "text": "Pieter Levels makes over $100k/month with a single VPS using PHP and jQuery. And until very recently, his deployment process was simply uploading files via FTP.\nIf you focus on what your users want and know how to market your product, you can make a lot of money.\nWhich is why I decided to stay poor and spent an inordinate amount of time improving my deployment process.\nWho needs money when you can get the satisfaction of that beautiful green check mark after you’ve run your CI/CD pipeline?\nAnyways…\nThese days, I’m using kamal to deploy most of my projects.\nI used to hate Docker. But, like with Gollum, I’ve come to realize that it’s not that bad after all.\nI wanted to create a simple Dockerfile to run a Django app using Poetry, with a SQLite database, and hot reload. Additionally, I wanted to switch between the development and production versions of the container.\nSo here’s a simple Dockerfile that does just that."
  },
  {
    "objectID": "til/django-poetry-dockerfile.html#project-structure",
    "href": "til/django-poetry-dockerfile.html#project-structure",
    "title": "A Dockerfile for a Django app using Poetry",
    "section": "Project structure",
    "text": "Project structure\nThis is my project structure:\n.\n├── db/\n├── src/\n├── entrypoint.sh\n├── Dockerfile\n├── docker-compose.yml\n├── poetry.lock\n└── pyproject.toml\nThe src/ directory contains the Django project. The db/ directory contains the SQLite database. The entrypoint.sh file is the entrypoint for the Docker container.\nIf your project is not structured in a similar way, you might need to adapt the files below to your needs."
  },
  {
    "objectID": "til/django-poetry-dockerfile.html#dockerfile",
    "href": "til/django-poetry-dockerfile.html#dockerfile",
    "title": "A Dockerfile for a Django app using Poetry",
    "section": "Dockerfile",
    "text": "Dockerfile\nI created a Dockerfile that fulfilled this:\n\nA base stage with Python 3.10 and Poetry installed.\nA builder stage that installs the dependencies.\nA runner stage that copies the virtual environment from the builder stage.\nA development stage that runs the entrypoint as a root user.\nA production stage that runs the entrypoint as a non-root user.\n\nHere’s the full Dockerfile:\n\n\nDockerfile\n\nFROM python:3.10-slim AS base\n\nENV POETRY_HOME=/opt/poetry\nENV PATH=${POETRY_HOME}/bin:${PATH}\n\nRUN apt-get update \\\n    && apt-get install --no-install-recommends -y \\\n    curl \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\nRUN curl -sSL https://install.python-poetry.org | python3 - && poetry --version\n\nFROM base AS builder\n\nWORKDIR /app\nCOPY poetry.lock pyproject.toml ./\nRUN poetry config virtualenvs.in-project true\nRUN poetry install --only main --no-interaction\n\nFROM base AS runner\n\nWORKDIR /app\nCOPY --from=builder /app/.venv/ /app/.venv/\n\nCOPY . /app\nRUN mkdir -p /db\n\nEXPOSE 8000\n\nRUN chmod +x /app/src/entrypoint.sh\n\nFROM runner AS development\n\nWORKDIR /app/src\nENTRYPOINT [ \"/app/src/entrypoint.sh\" ]\n\nFROM runner AS production\n\n# Set user and group\nARG user=django\nARG group=django\nARG uid=1000\nARG gid=1000\nRUN groupadd -g ${gid} ${group}\nRUN useradd -u ${uid} -g ${group} -s /bin/sh -m ${user}\n\n# Switch to user\nRUN chown -R ${uid}:${gid} /app\nRUN chown -R ${uid}:${gid} /db\n\nUSER ${uid}:${gid}\n\nWORKDIR /app/src\nENTRYPOINT [ \"/app/src/entrypoint.sh\" ]"
  },
  {
    "objectID": "til/django-poetry-dockerfile.html#entrypoint",
    "href": "til/django-poetry-dockerfile.html#entrypoint",
    "title": "A Dockerfile for a Django app using Poetry",
    "section": "Entrypoint",
    "text": "Entrypoint\nFor entrypoint.sh I use this:\n#!/bin/sh\n\nif [ \"$ENVIRONMENT\" = \"production\" ]; then\n    echo \"Running in production mode\"\n    exec poetry run gunicorn -c gunicorn.conf.py\nelif [ \"$ENVIRONMENT\" = \"development\" ]; then\n    echo \"Running in development mode\"\n    exec poetry run python manage.py runserver 0.0.0.0:8000\nelse\n    echo \"ENVIRONMENT variable is not set\"\nfi\nIf ENVIRONMENT is set to production, the container will run the production server using gunicorn. If it is development, the container will run Django’s development server."
  },
  {
    "objectID": "til/django-poetry-dockerfile.html#docker-compose",
    "href": "til/django-poetry-dockerfile.html#docker-compose",
    "title": "A Dockerfile for a Django app using Poetry",
    "section": "Docker-compose",
    "text": "Docker-compose\nThen, I have a docker-compose that lets you run the development and production containers:\nservices:\n  app:\n    build:\n      context: .\n      target: ${ENVIRONMENT}\n    platform: linux/amd64\n    environment:\n      - DJANGO_SECRET_KEY=${DJANGO_SECRET_KEY}\n      - DJANGO_DEBUG=${DJANGO_DEBUG}\n      - DJANGO_SECURE_SSL_REDIRECT=${DJANGO_SECURE_SSL_REDIRECT}\n      - DJANGO_SECURE_HSTS_SECONDS=${DJANGO_SECURE_HSTS_SECONDS}\n      - DJANGO_SECURE_HSTS_INCLUDE_SUBDOMAINS=${DJANGO_SECURE_HSTS_INCLUDE_SUBDOMAINS}\n      - DJANGO_SECURE_HSTS_PRELOAD=${DJANGO_SECURE_HSTS_PRELOAD}\n      - DJANGO_SESSION_COOKIE_SECURE=${DJANGO_SESSION_COOKIE_SECURE}\n      - DJANGO_CSRF_COOKIE_SECURE=${DJANGO_CSRF_COOKIE_SECURE}\n      - CACHE_REDIS_URL=${CACHE_REDIS_URL}\n      - ENVIRONMENT=${ENVIRONMENT}\n    env_file:\n      - .env\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - \"./db/:/app/db/\"\n    develop:\n      watch:\n        - action: sync\n          path: ./src/\n          target: /app/src\n        - action: rebuild\n          path: pyproject.toml\nIn this docker-compose, I use ENVIRONMENT to switch between the development and production containers.\nI also use the Compose Watch to reload the container when I make changes to the code and to rebuild the container when I make changes to the pyproject.toml file."
  },
  {
    "objectID": "til/django-poetry-dockerfile.html#conclusion",
    "href": "til/django-poetry-dockerfile.html#conclusion",
    "title": "A Dockerfile for a Django app using Poetry",
    "section": "Conclusion",
    "text": "Conclusion\nThat’s it. I hope you find this useful.\nAnd remember, while Pieter is busy counting his cash, here you are counting the number of successful builds."
  },
  {
    "objectID": "til/fixing-python-not-found-error-in-macos.html",
    "href": "til/fixing-python-not-found-error-in-macos.html",
    "title": "Fixing missing ‘python’ error in macOS",
    "section": "",
    "text": "After the last macOS update, I started getting the following error when trying to run poetry install:\n[Errno 2] No such file or directory: 'python'\nI went through GitHub issues, StackOverflow questions, and blog posts, but none of the suggested solutions worked.\nFinally, I found the solution somewhat hidden in this blog post.\nSo, what’s the fix?\n🥁 🥁 🥁\nJust add the following line to your .zshrc file:\nexport PATH=\"$(brew --prefix python)/libexec/bin:$PATH\"\nThis gets the installation prefix for python installed via Homebrew (e.g. /opt/homebrew/opt/python@3.12), gets the libexec/bin directory, and adds it to the PATH.\nIn that libexec/bin, there’s a python executable that gets called when you run python in the terminal.\nThat’s all. Hope that helps!"
  },
  {
    "objectID": "til/transforming-images-to-webp.html",
    "href": "til/transforming-images-to-webp.html",
    "title": "Transform any image to WebP from the terminal",
    "section": "",
    "text": "I was annoyed by the file size of my photo in the About page, because it was slowing down the page load.\nIs it important? No.\nDon’t I have better things to do on a Saturday afternoon? Yes.\nBut it’s like going to bed with the closet door open—you know there’s nothing in there, but you just can’t shake the feeling that the devil (or Diosdado Cabello) might jump out and kill you in your sleep unless you get up and shut it.\nSo I got o1-mini to write a simple script for me, and thought others might find it useful.\nHere it is:\nfunction img2webp() {\n\n  # Check if the input file is provided or if help is requested\n  if [[ $# -lt 1 || \"$1\" == \"--help\" || \"$1\" == \"-h\" ]]; then\n    echo \"Usage: img2webp input_image [quality]\"\n    echo \"  input_image: Path to the input image file\"\n    echo \"  quality: Quality of the output WebP image (0-100, default is 80)\"\n    return 1\n  fi\n\n  local input=\"$1\"\n  local quality=\"${2:-80}\"  # Default quality is 80 if not specified\n  local output=\"${input%.*}.webp\"\n\n  # Convert the image to WebP using ffmpeg\n  ffmpeg -i \"$input\" -qscale:v \"$quality\" \"$output\"\n\n  # Check if the conversion was successful\n  if [[ $? -eq 0 ]]; then\n    echo \"Successfully converted '$input' to '$output' with quality $quality.\"\n  else\n    echo \"Failed to convert '$input' to WebP.\"\n    return 1\n  fi\n}\nIf you’re using MacOS, you first need to install ffmpeg using Homebrew:\nbrew install ffmpeg\nThen you can add it to your .zshrc and use it by running img2webp &lt;path_to_image&gt; [quality].\nJust as reference, keeping the same quality, I decreased my profile picture from 234KB to 36KB by just changing from PNG to WebP.\nHope you found this useful."
  },
  {
    "objectID": "til/live-components-with-django-and-htmx.html",
    "href": "til/live-components-with-django-and-htmx.html",
    "title": "Live Components with Django and htmx",
    "section": "",
    "text": "I discovered django-components late last year and I quickly realized it was the missing piece in my Django + htmx workflow. It made my developer experience so much better, that I even started contributing to it.\ndjango-components lets you build components that combine HTML, JS, and CSS in a single place. Plus, it now lets you use components as views. This feature allows you to keep all the logic for a part of your application in one place, giving you great locality of behavior.\nA click-to-load component would look something like this:\nYou can use this component in any view using {% component 'click_to_load' page_obj=page_obj %} or render it outside of a view by adding it to urls.py:\nShort and sweet, just like the best things in life."
  },
  {
    "objectID": "til/live-components-with-django-and-htmx.html#django-live-components",
    "href": "til/live-components-with-django-and-htmx.html#django-live-components",
    "title": "Live Components with Django and htmx",
    "section": "Django Live Components",
    "text": "Django Live Components\nI thought it’d be fun to use the library for something it wasn’t designed for: streaming component changes through server-sent events (SSE).\nIt took me a few hours and several reads of Víðir’s tutorial to figure it out, but it worked. It’s a bit hacky but all the pieces were there. I just had to find a way to put them together.\nThe code is available here.\nI had a simple idea: set up a Redis pub/sub channel for server notifications. When the client loads the page, it subscribes to this notification channel. Each time the server publishes a new notification, the system reads it from the channel. Then, it renders the HTML and sends it to the client using Server-Sent Events (SSE).\nFirst, you need a notification component, with a streaming view that updates the client whenever a new notification occurs, and a way to subscribe to new notifications sent from the server.\nHere’s what I came up with:\nimport asyncio\nimport json\nfrom typing import AsyncGenerator\n\nimport redis.asyncio as redis\nfrom django.http import StreamingHttpResponse\nfrom django.utils.decorators import classonlymethod\nfrom django_components import component\n\nr = redis.from_url(\"redis://localhost\")\n\n\ndef sse_message(event_id: int, event: str, data: str) -&gt; str:\n    data = data.replace(\"\\n\", \"\")\n    return f\"id: {event_id}\\n\" f\"event: {event}\\n\" f\"data: {data.strip()}\\n\\n\"\n\n\nclass NotificationComponent(component.Component):\n\n    @classonlymethod\n    def as_live_view(cls, **initkwargs):\n        view = super().as_view(**initkwargs)\n        view._is_coroutine = asyncio.coroutines._is_coroutine\n        return view\n\n    template = \"\"\"\n    &lt;div style=\"color: {{color}};\" role=\"alert\"&gt;\n        &lt;span style=\"font-weight: bold;\"&gt;{{ title }}&lt;/span&gt; {{ message }}\n    &lt;/div&gt;\n    \"\"\"\n\n    async def streaming_response(self, *args, **kwargs) -&gt; AsyncGenerator[str, None]:\n        async with r.pubsub() as pubsub:\n            await pubsub.subscribe(\"notifications_channel\")\n            try:\n                while True:\n                    message = await pubsub.get_message(\n                        ignore_subscribe_messages=True, timeout=1\n                    )\n                    if message is not None:\n                        notification_data = json.loads(message[\"data\"].decode())\n                        sse_message_rendered = sse_message(\n                            notification_data[\"id\"],\n                            \"notification\",\n                            self.render(\n                                {\n                                    \"title\": notification_data[\"title\"],\n                                    \"message\": notification_data[\"message\"],\n                                    \"color\": notification_data[\"color\"],\n                                }\n                            ),\n                        )\n                        yield sse_message_rendered\n                    await asyncio.sleep(0.1)\n            finally:\n                await r.aclose()\n\n    async def get(self, request, *args, **kwargs):\n        return StreamingHttpResponse(\n            streaming_content=self.streaming_response(),\n            content_type=\"text/event-stream\",\n        )\nAnd you should include this in your urls.py:\nfrom django.urls import path\nfrom components.notification import NotificationComponent\n\nurlpatterns = [\n    path(\n        \"notification/\",\n        NotificationComponent.as_live_view(),\n        name=\"stream_notification\",\n    ),\n]\nThen, you need a simple HTML template to show these notifications. I used the htmx SSE extension to handle the SSE connection on the client. This was my template:\n&lt;!-- src/templates/index.html --&gt;\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"UTF-8\" /&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" /&gt;\n    &lt;title&gt;Django Live Components&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;div\n      hx-ext=\"sse\"\n      sse-connect=\"{% url 'stream_notification' %}\"\n      sse-swap=\"notification\"\n    &gt;&lt;/div&gt;\n    &lt;script\n      src=\"https://unpkg.com/htmx.org@1.9.10\"\n      integrity=\"sha384-D1Kt99CQMDuVetoL1lrYwg5t+9QdHe7NLX/SoJYkXDFfX37iInKRy5xLSi8nO7UC\"\n      crossorigin=\"anonymous\"\n    &gt;&lt;/script&gt;\n    &lt;script src=\"https://unpkg.com/htmx.org/dist/ext/sse.js\"&gt;&lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\nFinally, you need a script to simulate these server notifications:\n# random_notifications.py\nimport redis\nimport json\nimport random\nimport time\n\nREDIS_HOST = \"localhost\"\nREDIS_PORT = 6379\nREDIS_CHANNEL = \"notifications_channel\"\n\nr = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)\n\n\ndef create_random_notification():\n    \"\"\"Create a random notification message\"\"\"\n    return {\n        \"id\": random.randint(1, 1000),\n        \"title\": \"Notification \" + str(random.randint(1, 100)),\n        \"message\": \"This is a random message \" + str(random.randint(1, 100)),\n        \"color\": random.choice([\"blue\", \"green\", \"red\", \"black\", \"gray\", \"purple\"]),\n        \"timestamp\": time.ctime(),\n    }\n\n\ndef publish_notification():\n    \"\"\"Publish a random notification to the Redis channel\"\"\"\n    notification = create_random_notification()\n    r.publish(REDIS_CHANNEL, json.dumps(notification))\n    print(f\"Published: {notification}\")\n\n\nif __name__ == \"__main__\":\n    try:\n        while True:\n            publish_notification()\n            time.sleep(3)\n    except KeyboardInterrupt:\n        print(\"Stopped notification publisher\")\nYou can run Redis on Docker to run this script. It’ll start adding notifications to the Redis channel, that you’ll see flash on the page.\nThis was fun. I ended up using a similar pattern in AItheneum."
  },
  {
    "objectID": "til/install-alacritty-and-zellij-in-macos.html",
    "href": "til/install-alacritty-and-zellij-in-macos.html",
    "title": "Installing Alacritty, Zellij, and Neovim in macOS",
    "section": "",
    "text": "Ever since I saw The Matrix, I’ve wanted to be a hacker.\nWhen I say hacker, I mean having a cool-looking terminal. The kind that makes people think I’m a stealing millions from banks when, in reality, I’m just struggling to exit vim.\nI use macOS. The closest I’ve been to being a hacker is using vim hotkeys in VSCode.\nIt’s not that I haven’t tried to look the part. I was a just one audio driver away from saying “I use Arch btw”. I did succeed with Ubuntu, but honestly, using Linux as my main OS always felt like too much work1. So that didn’t last long.\nBut, today, after reading about DHH’s Omakub, I though it was time to give my hacker dreams a second shot.\nInstalling Ubuntu felt like a bit too much work2, so I decided to settle on just upgrading my terminal.\nI decided to set up Alacritty, Zellij, and Neovim on my M3 MacBook Pro.\nSure, I have client projects to deliver. But how could I let go of one-in-a-lifetime opportunity to procrastinate and imagine I’m a genius hacker for an afternoon?"
  },
  {
    "objectID": "til/install-alacritty-and-zellij-in-macos.html#installing-alacritty",
    "href": "til/install-alacritty-and-zellij-in-macos.html#installing-alacritty",
    "title": "Installing Alacritty, Zellij, and Neovim in macOS",
    "section": "Installing Alacritty",
    "text": "Installing Alacritty\nAlacritty is a terminal emulator, similar to Iterm2 and others. It’s selling point is that it’s very fast due to GPU-acceleration. Plus, you also get 256 colors support by default.\nThe best way to install it is using Homebrew:\nbrew install --cask alacritty\nThen, you you can customize it to your liking by creating a ~/.config/alacritty/alacritty.toml file.\nI ended up modifying just a couple of things:\n\nIncrease padding.\nChange the font.\nChange the color scheme. I used One Dark from this gist.\n\n\n\nShow the code\n\n[window]\npadding.x = 16\npadding.y = 14\ndecorations = \"none\" # Removes the window decoration (title bar, etc.)\n\n[font]\nsize = 13\n\n# FiraCode Nerd Font\nnormal = { family = \"FiraCode Nerd Font\", style = \"Regular\" }\nbold = { family = \"FiraCode Nerd Font\", style = \"Bold\" }\nitalic = { family = \"FiraCode Nerd Font\", style = \"Italic\" }\n\n# One Dark theme\n[colors]\n[colors.primary]\nbackground = '0x1e2127'\nforeground = '0xabb2bf'\nbright_foreground = '0xe6efff'\n\n# Normal colors\n[colors.normal]\nblack = '0x1e2127'\nred = '0xe06c75'\ngreen = '0x98c379'\nyellow = '0xd19a66'\nblue = '0x61afef'\nmagenta = '0xc678dd'\ncyan = '0x56b6c2'\nwhite = '0x828791'\n\n# Bright colors\n[colors.bright]\nblack = '0x5c6370'\nred = '0xe06c75'\ngreen = '0x98c379'\nyellow = '0xd19a66'\nblue = '0x61afef'\nmagenta = '0xc678dd'\ncyan = '0x56b6c2'\nwhite = '0xe6efff'\n\n# Dim colors\n[colors.dim]\nblack = '0x1e2127'\nred = '0xe06c75'\ngreen = '0x98c379'\nyellow = '0xd19a66'\nblue = '0x61afef'\nmagenta = '0xc678dd'\ncyan = '0x56b6c2'\nwhite = '0x828791'\n\nI use FiraCode Nerd Font. Nerd Fonts are a collection of fonts that include glyphs such as icons that represent folders, file types, weird arrows, etc.\nYou can install them using Homebrew:\nbrew install font-&lt;name-of-the-font&gt;-nerd-font # For example, font-fira-code-nerd-font"
  },
  {
    "objectID": "til/install-alacritty-and-zellij-in-macos.html#installing-zellij",
    "href": "til/install-alacritty-and-zellij-in-macos.html#installing-zellij",
    "title": "Installing Alacritty, Zellij, and Neovim in macOS",
    "section": "Installing Zellij",
    "text": "Installing Zellij\nZellij is an improved version of tmux. Whenever I used tmux, I spent half my time figuring out the key bindings. Zellij shows the key bindings on screen (unless you disable it), which greatly improves the user experience.\nThis is what it looks like:\n\n\n\nZellij\n\n\nSame as before, the best way to install it is using Homebrew.\nbrew install zellij\nYou can customize it by creating a ~/.config/zellij/config.kdl file.\nI just copied the One Half Dark theme they provide.\ntheme \"one-half-dark\"\n\nthemes {\n    one-half-dark {\n        fg 169 177 214\n        bg 26 27 38\n        black 56 62 90\n        red 249 51 87\n        green 158 206 106\n        yellow 224 175 104\n        blue 122 162 247\n        magenta 187 154 247\n        cyan 42 195 222\n        white 192 202 245\n        orange 255 158 100\n    }\n}\nIs One Half Dark the same as One Dark? I don’t really know. I like to live dangerously.\n\nMaking Zellij play nice with Alacritty\nGetting Alacritty to work with Zellij took me a while to figure out. But luckily it’s as simple as adding the full path to the zellij binary in the ~/.config/alacritty/alacritty.toml file.\n[shell]\n     program = \"/opt/homebrew/bin/zellij\""
  },
  {
    "objectID": "til/install-alacritty-and-zellij-in-macos.html#installing-neovim-and-lazyvim",
    "href": "til/install-alacritty-and-zellij-in-macos.html#installing-neovim-and-lazyvim",
    "title": "Installing Alacritty, Zellij, and Neovim in macOS",
    "section": "Installing Neovim and LazyVim",
    "text": "Installing Neovim and LazyVim\nNeovim is a fork from Vim, that solves some of Vim’s issues3. I didn’t really know if it was better or worse than Vim, but given that DHH recommended it, I thought it was a good idea to give it a shot.\nLazyVim is a premade configuration for Neovim. The purists will probably hate it, but it’s a good start.\nI had my own .vimrc, and after trying LazyVim for 30 minutes or so, I realized my config sucked.\nFirst, install Neovim using Homebrew.\nbrew install neovim\nThen, install LazyVim.\ngit clone https://github.com/LazyVim/starter ~/.config/nvim\nSo far, I’ve only made a few changes:\n\nInstalled Copilot, CopilotChat, and mini-surround.\nInstalled One Dark color scheme.\n\nThe plugins I mentioned are available in :LazyExtras, so it’s very easy to install them. Run :LazyExtras, select the plugins, and then install them with II.\nTo install One Dark, you must create a new file in ~/.config/nvim/lua/plugins/ containing the following code:\nreturn {\n  { \"navarasu/onedark.nvim\" },\n\n  -- Configure LazyVim to load One Dark\n  {\n    \"LazyVim/LazyVim\",\n    opts = {\n      colorscheme = \"onedark\",\n    },\n  },\n}"
  },
  {
    "objectID": "til/install-alacritty-and-zellij-in-macos.html#other-useful-tools",
    "href": "til/install-alacritty-and-zellij-in-macos.html#other-useful-tools",
    "title": "Installing Alacritty, Zellij, and Neovim in macOS",
    "section": "Other useful tools",
    "text": "Other useful tools\nWhile exploring Omakub’s repository, I also learned about other useful tools that I’ve now included in my daily workflow:\n\nlazydocker\nlazydocker: A simple terminal UI to manage everything Docker. Much better than everything else I’ve used.\nDocker is great. But, until now, the experience of managing Docker containers sucked.\nTake a look at the main screen:\n\n\n\nlazydocker\n\n\n\n\nlazygit\nlazygit is a simple terminal UI for git.\nI’ve found it better than GitHub Desktop when doing complex operations.\nHere’s a screenshot of the main screen:\n\n\n\nlazygit\n\n\n\n\neza\neza is an improved version of ls.\nI also added a couple of aliases from Omakub:\nalias ls='eza -lh --group-directories-first --icons --hyperlink'\nalias lsa='ls -a'\nalias lt='eza --tree --level=2 --long --icons --git'\nalias lta='lt -a'\nIf you run lta you’ll get a nice view of the current directory.\n\n\n\neza"
  },
  {
    "objectID": "til/install-alacritty-and-zellij-in-macos.html#conclusion",
    "href": "til/install-alacritty-and-zellij-in-macos.html#conclusion",
    "title": "Installing Alacritty, Zellij, and Neovim in macOS",
    "section": "Conclusion",
    "text": "Conclusion\nThat’s all. I hope you learned something from this post or, at least, got a cool looking terminal.\nThere’s nothing else to say except that I’m never going to recover those 4 hours of my life.\nI should get back to work."
  },
  {
    "objectID": "til/install-alacritty-and-zellij-in-macos.html#footnotes",
    "href": "til/install-alacritty-and-zellij-in-macos.html#footnotes",
    "title": "Installing Alacritty, Zellij, and Neovim in macOS",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’m sure you’re itching to explain how I’m completely wrong about this, and how you haven’t had to fix anything in years. Please reach me at elon@x.com↩︎\nCan you even install Ubuntu on a Mac?↩︎\nIncluding its horrible website.↩︎"
  },
  {
    "objectID": "til/migrate-blog-from-ghost-to-quarto.html",
    "href": "til/migrate-blog-from-ghost-to-quarto.html",
    "title": "Migrate a blog from Ghost to Quarto",
    "section": "",
    "text": "When I started blogging five years ago, I read all reddit posts comparing blogging platforms and concluded that Ghost was the best choice because I needed a powerful tool for all those millions of visitors my blog would get.\nI saw myself as the García Márquez of technical writing.\nFast forward five years, and I’ve paid $2,000 for hosting a blog that barely gets 8k visits per month. Plus, I’m forced to write it in an interface that I hate.\nWith that kind of money, I could have funded a moderately extravagant hamster-only summer party.\nNot that I should, but I could.\nYes, I’m not proud of that decision1. So I’m migrating my blog from Ghost to Quarto.\nHere’s a short guide on how to migrate your blog from Ghost to Quarto."
  },
  {
    "objectID": "til/migrate-blog-from-ghost-to-quarto.html#migrate-blog-from-ghost-to-quarto",
    "href": "til/migrate-blog-from-ghost-to-quarto.html#migrate-blog-from-ghost-to-quarto",
    "title": "Migrate a blog from Ghost to Quarto",
    "section": "Migrate blog from Ghost to Quarto",
    "text": "Migrate blog from Ghost to Quarto\n\nSetting up your blog\nFirst, install Quarto and create a blog in an empty repository:\nquarto create project blog myblog\nThe resulting myblog folder will contain the barebones configuration for a Quarto blog and a posts folder with some example posts. You can remove those posts. Later on, you’ll add your own.\n\n\nExporting your Ghost’s blog content\nThen, you need to download a copy of your Ghost’s blog content.\nGo to &lt;YOUR_BLOG_URL&gt;/ghost/#/settings/migration and click on Export, then Export JSON.\n\n\n\nExporting my blog\n\n\nThis is pretty obvious, but remember to replace &lt;YOUR_BLOG_URL&gt; with your blog’s URL.\nYou’ll get a JSON file with all the posts and pages in your blog. You’ll need to process it to convert your posts to Quarto posts.\nThis small Python script did the heavy lifting for me:\n\n\nShow the code\n\nimport json\nimport os\nfrom datetime import datetime\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom markdownify import markdownify as md\n\n\nBLOG_URL = \"https://dylancastillo.co\" # Replace with your blog's URL\nBLOG_JSON_DUMP = \"./dylan-castillo.ghost.2024-05-28-10-39-09.json\" # Replace with the path to the JSON file you downloaded\nBLOG_AUTHOR_NAME = \"Dylan Castillo\" # Replace with your name\n\n\ndef download_images(markdown_content, post_slug):\n    soup = BeautifulSoup(markdown_content, \"html.parser\")\n    images = soup.find_all(\"img\")\n    if images:\n        os.makedirs(post_slug, exist_ok=True)\n        for img in images:\n            img_url_raw = img[\"src\"]\n            img_url = img_url_raw.replace(\"__GHOST_URL__\", BLOG_URL)\n            img_name = os.path.basename(img_url)\n            response = requests.get(img_url, stream=True)\n            if response.status_code == 200:\n                print(f\"Downloading image: {img_url} to {post_slug}/{img_name}\")\n                with open(os.path.join(post_slug, img_name), \"wb\") as f:\n                    f.write(response.content)\n                markdown_content = markdown_content.replace(\n                    img_url_raw, os.path.join(post_slug, img_name)\n                )\n            else:\n                print(f\"Failed to download image: {img_url}\")\n    return markdown_content\n\n\ndef process_posts(data):\n    posts = data[\"db\"][0][\"data\"][\"posts\"]\n    for post in posts:\n        print(\"Processing post:\", post[\"title\"])\n        title = post[\"title\"]\n        description = post[\"custom_excerpt\"]\n        author = BLOG_AUTHOR_NAME\n        date = (\n            datetime.strptime(post[\"published_at\"], \"%Y-%m-%dT%H:%M:%S.%fZ\").strftime(\n                \"%m/%d/%Y\"\n            )\n            if post[\"published_at\"]\n            else \"\"\n        )\n        date_modified = (\n            datetime.strptime(post[\"updated_at\"], \"%Y-%m-%dT%H:%M:%S.%fZ\").strftime(\n                \"%m/%d/%Y\"\n            )\n            if post[\"updated_at\"]\n            else \"\"\n        )\n\n        # Convert HTML content to Markdown\n        markdown_content = download_images(\n            post[\"html\"] if post[\"html\"] else \"\", post[\"slug\"]\n        )\n        markdown_content = md(markdown_content, code_language=\"python\")\n        markdown_content = markdown_content.replace(\"__GHOST_URL__\", BLOG_URL)\n        markdown_content = f\"\"\"---\\ntitle: \"{title}\"\\ndescription: \"{description}\"\\nauthor: \"{author}\"\\ndate: \"{date}\"\\ndate-modified: \"{date_modified}\"\\n---\\n\\n{markdown_content}\"\"\"\n\n        # Save the markdown content to a file\n        filename = f\"{post['slug']}.md\"\n        with open(filename, \"w\", encoding=\"utf-8\") as file:\n            file.write(markdown_content)\n\n\nif __name__ == \"__main__\":\n    with open(BLOG_JSON_DUMP) as file:\n        data = json.load(file)\n    process_posts(data)\n\nWhen you run the script, it will create a folder with all the posts in .md format and their images. Feel free to adapt it to your needs.\n\n\nCustomizing your blog\nThrough trial and error, I found some settings that helped me customize the look and feel of my blog.\nHere are some of the things I modified:\n\nAdded RSS, favicon, and customized the navbar:\n\n\n\n_quarto.yml\n\nwebsite:\n  title: # The title of your blog\n  site-url: # For the RSS feed that no one will read\n  favicon: # Add a favicon to the blog\n  navbar: # Customize the navbar if you want\n  page-footer: # Add a page footer like \"Copyright 2024, Saul Goodman\" to sound legit\n\n\nAdded custom CSS and JS and a custom theme:\n\n\n\n_quarto.yml\n\nformat:\n  html:\n    include-in-header:\n      - text: |\n          &lt;link href=\"&lt;YOUR_CUSTOM_FONT_URL&gt;\" rel=\"stylesheet\"&gt;\n          &lt;script src=\"&lt;YOUR_CUSTOM_JS_URL&gt;\" defer&gt;&lt;/script&gt;\n    page-layout: \"article\"\n    theme: # Pick a theme and customize it in `custom.scss`\n      - &lt;YOUR_THEME&gt;\n      - custom.scss # Add your custom CSS here\n    code-line-numbers: true # Add line numbers to code blocks\n\n\nFor each post, I used this front matter:\n\n\n\n&lt;POST_SLUG&gt;.md\n\n---\ntitle: \"&lt;POST_TITLE&gt;\"\naliases:\n  - /&lt;POST_SLUG&gt;/ # Add an alias to the previous post's URL\ndescription-meta: \"&lt;POST_DESCRIPTION&gt;\"\ndate: \"&lt;POST_DATE&gt;\"\ndate-modified: last-modified # Automatically set to the last modified date\ntoc: true\ntoc-depth: 3\nlightbox: true # For images\nfig-cap-location: margin # Captions for images\ncategories:\n  - &lt;CATEGORY&gt;\nauthor:\n  - name: &lt;AUTHOR_NAME&gt;\n    url: &lt;AUTHOR_URL&gt;\n    affiliation: &lt;AUTHOR_AFFILIATION&gt;\n    affiliation-url: &lt;AUTHOR_AFFILIATION_URL&gt;\ncitation: true\ncomments:\n  utterances: # For comments\n    repo: &lt;YOUR_GITHUB_USERNAME&gt;/&lt;YOUR_GITHUB_REPO&gt;\n    issue-term: pathname\n---\n\nSee my settings for an example and a recent post source for reference.\nFor the CSS, I copied darkly and created a custom custom.scss file to modify some Bootstrap styles. I just changed some colors and a couple of styles to make the blog look closer to my Ghost theme. It was super easy.\n\n\nDeployment using GitHub Pages + GitHub Actions\nQuarto offers multiple deployment options. I wanted one where I could push changes to a GitHub repository, and have the blog automatically deployed. I went with GitHub Pages combined with GitHub Actions.\nTo deploy the blog, I created a GitHub repository, added the blog’s content, updated .gitignore to ignore the /.quarto/ and /_site/ and updated _quarto.yml to only compute code locally (otherwise you’d need a Python kernel running on your GitHub Actions runner):\n\n\n_quarto.yml\n\nexecute:\n  freeze: auto\n\nThen I ran this command to automatically generate the workflow .github/workflows/publish.yml for me:\nquarto publish gh-pages\nSince then, every time I push changes to the main branch, GitHub Actions automatically renders the website and updates the gh-pages branch.\n\n\nUsing a custom domain\nThat seemed to work at first, but very quickly I noticed that whenever I pushed changes to the main branch, the site would no longer be served from my custom domain dylancastillo.co.\nWhen you render your website, Quarto recreates the CNAME file in the gh-pages branch, which seems to break the custom domain setup in GitHub Pages.\nI found a solution in this discussion and added a CNAME file to the root of the repository with my custom domain:\n\n\nCNAME\n\ndylancastillo.co\n\nThen, I added this to _quarto.yml:\n\n\n_quarto.yml\n\nproject:\n  type: website\n  resources: # New\n    - CNAME\n\nAnd that worked!"
  },
  {
    "objectID": "til/migrate-blog-from-ghost-to-quarto.html#conclusion",
    "href": "til/migrate-blog-from-ghost-to-quarto.html#conclusion",
    "title": "Migrate a blog from Ghost to Quarto",
    "section": "Conclusion",
    "text": "Conclusion\nThere you go, my friend.\nNow you can also break free from Ghost.\nSee you in the next post."
  },
  {
    "objectID": "til/migrate-blog-from-ghost-to-quarto.html#footnotes",
    "href": "til/migrate-blog-from-ghost-to-quarto.html#footnotes",
    "title": "Migrate a blog from Ghost to Quarto",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nChoosing Ghost. No regrets about the hypothetical hamster party.↩︎"
  },
  {
    "objectID": "extras/say-what-you-mean-sometimes/say-what-you-mean-gpt-4o-mini.html#last-letter",
    "href": "extras/say-what-you-mean-sometimes/say-what-you-mean-gpt-4o-mini.html#last-letter",
    "title": "Setup",
    "section": "Last Letter",
    "text": "Last Letter\n\nSetup\n\n\nZero-shot\n\n\nFew-shot"
  },
  {
    "objectID": "extras/say-what-you-mean-sometimes/say-what-you-mean-gpt-4o-mini.html#shuffled-objects",
    "href": "extras/say-what-you-mean-sometimes/say-what-you-mean-gpt-4o-mini.html#shuffled-objects",
    "title": "Setup",
    "section": "Shuffled Objects",
    "text": "Shuffled Objects\n\nSetup\n\n\nZero-shot\n\n\nFew-shot"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "These are some of my most popular side projects, from newest to oldest.\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nAItheneum\n\n\nA collection of classic books enhanced with AI to make them easier to read and understand. \n\n\n\n\nDjango HTMX Components\n\n\nAn open-source collection of components built with Django and HTMX that you can copy and paste into your projects. \n\n\n\n\nlystinct\n\n\nAI real estate listing description generator. It reduces the time it takes to write a listing from 1 hour to 2 minutes. \n\n\n\n\ndeepsheet\n\n\nAI tool that helps you analyze data and make graphs using ChatGPT. \n\n\n\n\nNamemancer\n\n\nAI tool that helps you come up with cool titles for your startup. It generates a list of 20 potential titles, accompanied by fitting slogans, and checks if the corresponding .com domains are available. Then, it scores and ranks the titles for you. \n\n\n\n\nAsk Seneca\n\n\nA GPT3-based stoic philosopher who gives you life advice based on Seneca’s writings. It also gives you the sources it used to answer your questions. \n\n\n\n\nShell Genie\n\n\nA command-line tool that lets you interact with the terminal in plain English. You ask the genie what you want to do and it will give you the command you need. It uses GPT-3 in the backend. \n\n\n\n\nPython Data Visualization Cookbook\n\n\nAn open-source interactive cookbook with +35 recipes for creating commonly used graphs with pandas, matplotlib, seaborn, and plotly.express. \n\n\n\n\nPandas Cheat Sheet\n\n\nAn open-source interactive cheat sheet with code snippets demonstrating basic pandas operations. \n\n\n\n\nFast Flood\n\n\nA daily logic puzzle based on Flood. I made it after the NYT bought Wordle for more than $1 million. Fast Flood got more than 100k visitors, and was featured in The Hustle, and led to a five-figure offer. I turned it down and didn’t make any money :D \n\n\n\n\nplanMIR\n\n\nA web app that helps physicians prepare for the Spanish medical residence entry exam. I made it to help my wife prepare for the exam while learning more about Django. \n\n\n\n\nAnchoring\n\n\nAnother cognitive science experiment. The instructions were lengthier and a bit confusing, so it got less traction than the the 2-4-6 task. \n\n\n\n\nThe 2-4-6 Task\n\n\nA fun cognitive science experiment designed by Peter Wason in the 1970s. It sparked some insightful discussions on reddit. Try it yourself before reading about it! \n\n\n\n\nPolituits\n\n\nA web app that tracked the sentiment toward Spanish politicians on Twitter in near-real time. I trained the models, built the frontend and backend, and deployed the app on an Ubuntu server. I learned a lot in the process! \n\n\n\n\nStay Home & Learn\n\n\nA collection of free educational and self-improvement resources made free due to the pandemic. I created it with Google Sheets and a little HTML and CSS. \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html",
    "href": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html",
    "title": "Create a Code Interpreter Chatbot with Pyodide, LangChain, and OpenAI",
    "section": "",
    "text": "OpenAI has been giving access to users to the Code Interpreter plugin and people are loving it. I wanted to replicate it outside of ChatGPT, so I created my own (simpler) version, specifically to analyze and visualize data.\nFollowing that, I figured more people might be interested in building user-facing chatbots capable of running code in the browser. So I put together this tutorial. It will teach you how to create a simple but powerful chatbot that uses Pyodide, LangChain, and OpenAI to generate and run code in the browser for a user.\nC’mon, let’s get to work!"
  },
  {
    "objectID": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#prerequisites",
    "href": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#prerequisites",
    "title": "Create a Code Interpreter Chatbot with Pyodide, LangChain, and OpenAI",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo make the most of the tutorial, you should review these topics before getting started:\n\nWhat Pyodide is.\nWhat LangChain is.\nThe basics of Python backend servers such as Litestar or FastAPI.\n\nIn addition, you must create an account at OpenAI."
  },
  {
    "objectID": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#building-a-code-interpreter-chatbot",
    "href": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#building-a-code-interpreter-chatbot",
    "title": "Create a Code Interpreter Chatbot with Pyodide, LangChain, and OpenAI",
    "section": "Building a Code Interpreter Chatbot",
    "text": "Building a Code Interpreter Chatbot\nSecurity and scalability are two major challenges in developing a user-facing chatbot that’s capable of executing code. Companies like Replit must manage extremely complex infrastructures in order to provide users with online IDEs.\nWe’re fortunate, however, because what we’re doing in this tutorial isn’t as complex as Replit. We can use a simple solution: Pyodide. Pyodide is a CPython port to WebAssembly/Emscripten that lets Python to run in the browser.\nThere are some restrictions to what you can do with Pyodide, such as the fact that not every package is compatible with it and that the maximum memory it can manage is 2GB. But it is more than adequate to process small to medium datasets, which is what you’ll do in this tutorial.\nThe chatbot you’ll build will work as follows:\n\nA user asks a question about a preloaded dataset.\nThat question, along with a predefined prompt, is sent to the OpenAI API.\nThe API responds with a code snippet that helps answer the question.\nThe code snippet is executed on the browser using Pyodide, and the result is displayed to the user.\n\nNext, you’ll set up your local environment."
  },
  {
    "objectID": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#set-up-your-local-environment",
    "href": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#set-up-your-local-environment",
    "title": "Create a Code Interpreter Chatbot with Pyodide, LangChain, and OpenAI",
    "section": "Set Up Your Local Environment",
    "text": "Set Up Your Local Environment\nBefore you begin, you must first set up a few things. Follow these steps:\n\nInstall Python 3.10.\nInstall Poetry. It’s optional but highly recommended.\nClone the project’s repository:\n\ngit clone https://github.com/dylanjcastillo/chatbot-code-interpreter\n\nFrom the root folder of the project, install the dependencies:\n\nUsing Poetry: Create the virtual environment in the same directory as the project and install the dependencies:\n\npoetry config virtualenvs.in-project true\npoetry install\n\nUsing venv and pip: Create a virtual environment and install the dependencies listed in requirements.txt:\n\npython3.10 -m venv .venv && source .venv/bin/activate\npip install -r requirements.txt\nOpen src/.env-example, add your OpenAI secret key in the corresponding variable, and save the file as .env.\n\nYou should now have a virtual environment set up with the necessary libraries and a local copy of the repository. Your project structure should look like this:\nchatbot-code-interpreter\n├── LICENSE\n├── README.md\n├── poetry.lock\n├── pyproject.toml\n├── requirements.txt\n├── src\n│   ├── app.py\n│   ├── config.py\n│   ├── prompts\n│   │   ├── system.prompt\n│   │   └── user.prompt\n│   ├── static\n│   │   └── all_stocks.csv\n│   ├── templates\n│   │   └── index.html\n│   ├── utils.py\n│   └── .env-example\n└── .venv/\nThese are the most relevant files and directories in the project:\n\npoetry.lock and pyproject.toml: These files contain the project’s specifications and dependencies. They’re used by Poetry to create a virtual environment.\nrequirements.txt: This file contains a list of Python packages required by the project.\nsrc/app.py: This file contains the code of the chatbot.\nsrc/config.py: This file contains project configuration details such as OpenAI’s API key (read from a .env file), and the path to the prompts used by the chatbot.\nsrc/prompts/: This directory contains the system and user prompts used by the chatbot. I’ve found that keeping the prompts in text files makes it easier to manage them instead of using strings.\nsrc/static and src/templates: These files contain the data used in the example and the HTML template used for the chatbot’s interface. You’ll use a dataset with prices and the volume of a group of stocks.\nsrc/utils.py: This file contains a function you use to read the prompts from src/prompts.\n.env-example: This file is a sample file that provides the required environment variables you should provide. In this case, you use it to pass OpenAI’s API key to your application and choose the name the chatbot should use behind the scenes (gpt-3.5.-turbo).\n.venv/: This directory contains the project’s virtual environment.\n\nAlright! On to the exciting stuff now."
  },
  {
    "objectID": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#run-code-on-the-browser-with-pyodide",
    "href": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#run-code-on-the-browser-with-pyodide",
    "title": "Create a Code Interpreter Chatbot with Pyodide, LangChain, and OpenAI",
    "section": "Run Code on the Browser with Pyodide",
    "text": "Run Code on the Browser with Pyodide\nIn this step, you’ll configure Pyodide to run Python in the browser. You’ll load pyodide.js, start it, import the required libraries, and define an event handler to process the questions asked by the user.\nTo make the analysis simpler, I’ll split the code of src/templates/index.html into two sections. In the first section, you’ll only look at the contents of &lt;head&gt; and &lt;body&gt;, and in the second part, you’ll go through the JavaScript code defined in &lt;script&gt;.\nHere’s part one:\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"UTF-8\" /&gt;\n    &lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" /&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" /&gt;\n    &lt;title&gt;Code Runner Chatbot&lt;/title&gt;\n    &lt;link\n      rel=\"stylesheet\"\n      href=\"https://cdn.jsdelivr.net/npm/@picocss/pico@1/css/pico.min.css\"\n    /&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/pyodide/v0.23.2/full/pyodide.js\"&gt;&lt;/script&gt;\n  &lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;main class=\"container\"&gt;\n      &lt;h1 style=\"text-align:center;\"&gt;Code Runner Chatbot&lt;/h1&gt;\n      &lt;textarea\n        name=\"query\"\n        id=\"query\"\n        cols=\"30\"\n        rows=\"10\"\n        placeholder=\"Ask a question about the dataset\"\n      &gt;&lt;/textarea&gt;\n      &lt;button id=\"ask-btn\"&gt;Ask&lt;/button&gt;\n      &lt;blockquote id=\"output\"&gt;&lt;/blockquote&gt;\n    &lt;/main&gt;\n    &lt;script&gt;\n      &lt;!-- SECOND PART OF CODE --&gt;\n    &lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\nThis section of code loads the necessary libraries and styles, as well as defines the chatbot’s user interface. Here’s how it works:\n\nLines 5 to 10 set a title for the page, load pyodide.js and pico.css (a minimal CSS framework), and define a couple of standard meta tags.\nLines 14 to 20 define a simple UI that lets users input a question, and submit them by clicking on a button, to get answers about the dataset.\n\nThen, let’s go through the JavaScript code defined in &lt;script&gt;. This code will load Pyodide, install the required libraries, make the data available for use in Pyodide, and set an event handler to process the user’s question.\nHere’s how it looks:\nconst queryTextArea = document.getElementById(\"query\");\nconst outputElement = document.getElementById(\"output\");\nconst askBtn = document.getElementById(\"ask-btn\");\n\nasync function setupPyodide() {\n  let pyodide = await loadPyodide();\n\n  await pyodide.loadPackage([\"pandas\", \"numpy\"]);\n\n  const response = await fetch(\"/static/all_stocks.csv\");\n  const fileContentArrayBuffer = await response.arrayBuffer();\n  const fileContent = new Uint8Array(fileContentArrayBuffer);\n  pyodide.FS.writeFile(\"all_stocks.csv\", fileContent);\n\n  return pyodide;\n}\n\nlet pyodideReadyPromise = setupPyodide();\n\naskBtn.addEventListener(\"click\", async (event) =&gt; {\n  let pyodide = await pyodideReadyPromise;\n\n  const query = queryTextArea.value;\n  const df_info = await pyodide.runPythonAsync(`\n    import pandas as pd\n    df = pd.read_csv('all_stocks.csv')\n    pd.set_option('display.max_columns', None)\n    df.head(3).T\n    `);\n\n  const data = new FormData();\n  data.append(\"query\", query);\n  data.append(\"df_info\", df_info);\n\n  try {\n    const response = await fetch(\"/ask\", {\n      method: \"POST\",\n      body: data,\n    });\n\n    if (response.ok) {\n      const result = await response.text();\n      const output = await pyodide.runPythonAsync(result);\n\n      outputElement.innerText = output;\n    } else {\n      console.error(\"Error:\", response.statusText);\n    }\n  } catch (error) {\n    console.error(\"Error:\", error);\n  }\n});\nThis code loads Pyodide, installs the necessary libraries, makes the data accessible in Pyodide, and finally sets an event handler to process the user’s question. It works as follows:\n\nLines 1 to 3 select the elements of the DOM that you’ll be interacting with.\nLines 5 to 18 define a helper function you use to load Pyodide and the data. You first load Pyodide, and install pandas and numpy. Then, you fetch the dataset and write it into Pyodide’s filesystem, to make it available to use the data in it.\nLines 20 to 52 define the event handler of a click on ask-btn. This means that whenever that button is clicked, this event will execute. The handler does a few things:\n\nLines 21 to 33 wait until Pyodide is fully loaded, extract the question that the user has asked, get the first rows of the dataset (which is how the model can know how to generate the right code), and put together the data to send in a POST request.\nLines 35 to 51 make a POST request to “/ask” with the parameters mentioned earlier. When the server responds, that text is run using Pyodide, and the result is then saved in output.\n\n\nThat’s it! Next, you’ll create the chatbot application."
  },
  {
    "objectID": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#create-the-chatbot",
    "href": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#create-the-chatbot",
    "title": "Create a Code Interpreter Chatbot with Pyodide, LangChain, and OpenAI",
    "section": "Create the Chatbot",
    "text": "Create the Chatbot\nNow, you’ll create a simple application that will allow users to make questions to the chatbot about the dataset.\nTake a look at the code in src/app.py:\nfrom dataclasses import dataclass\nfrom pathlib import Path\n\nfrom langchain import LLMChain\nfrom langchain.chat_models import ChatOpenAI\nfrom litestar import Litestar, get, post\nfrom litestar.contrib.jinja import JinjaTemplateEngine\nfrom litestar.enums import RequestEncodingType\nfrom litestar.params import Body\nfrom litestar.response_containers import Template\nfrom litestar.static_files.config import StaticFilesConfig\nfrom litestar.template.config import TemplateConfig\nfrom typing_extensions import Annotated\n\nfrom config import OpenAI\nfrom utils import get_prompt\n\nchain_create = LLMChain(\n    llm=ChatOpenAI(\n        temperature=0,\n        model_name=OpenAI.model_name,\n        openai_api_key=OpenAI.secret_key,\n    ),\n    prompt=get_prompt(),\n)\n\n\n@get(path=\"/\", name=\"index\")\ndef index() -&gt; Template:\n    return Template(name=\"index.html\")\n\n\n@dataclass\nclass Query:\n    query: str\n    df_info: str\n\n\n@post(path=\"/ask\", name=\"ask\", sync_to_thread=True)\ndef ask(\n    data: Annotated[Query, Body(media_type=RequestEncodingType.MULTI_PART)],\n) -&gt; str:\n    query = data.query\n    df_info = data.df_info\n\n    chain_result = chain_create.run(\n        {\n            \"df_info\": df_info,\n            \"query\": query,\n        }\n    )\n    result = chain_result.split(\"```python\")[-1][:-3].strip()\n\n    return result\n\n\napp = Litestar(\n    route_handlers=[index, ask],\n    static_files_config=[\n        StaticFilesConfig(directories=[\"static\"], path=\"/static\", name=\"static\"),\n    ],\n    template_config=TemplateConfig(\n        engine=JinjaTemplateEngine, directory=Path(\"templates\")\n    ),\n)\nThis code provides users with a straightforward interface, allowing them to interact with the chatbot and ask questions about the dataset. Here’s how it works:\n\nLines 1 to 16 import the required libraries. The application uses Litestar, which is a bit verbose; hence, you’ll notice many import statements. But there’s no real mystery to it, it’s pretty similar to FastAPI or Flask.\nLines 18 to 25 create an LLMChain to interact with the model using the prompt read from get_prompt, which is a function defined in utils.py reads the prompts defined in src/prompts. The chain takes the prompts, the user’s query, and the first three rows from the dataset, and asks the model for a completion. Make sure to read the prompt to see how they work.\nLines 28 to 30 define the index route, which returns the index.html when a user visits /.\nLines 33 to 54 define a dataclass used to validate the parameters of the request made to /ask and define /ask, which is an endpoint that helps users answer questions about the dataset by generating relevant code.\nLines 57 to 65 set up the Litestar app, incorporating the previously defined routes and the locations of the templates and static files.\n\nTo test the app, cd into src/ and run this code in a terminal within the virtual environment:\nlitestar run --reload\nIf everything goes well, you’ll see an output similar to this one:\n\nNext, open http://127.0.0.1:8000 on your browser. You should see the app’s UI.\nTry asking a question to the chatbot. For example, you can ask when were the highest and lowest closing prices of AAPL.\nYou should get the following result:\n\nThat’s all! You’ve built a chatbot capable of running code on your browser."
  },
  {
    "objectID": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#next-steps",
    "href": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#next-steps",
    "title": "Create a Code Interpreter Chatbot with Pyodide, LangChain, and OpenAI",
    "section": "Next steps",
    "text": "Next steps\nI won’t cover deployment in this article. This application is pretty standard, so simply choose a method that works well for you and your organization.\nI like using NGINX with Gunicorn and Uvicorn workers and wrote a tutorial about it. That tutorial uses FastAPI, but the same process would also work with Litestar."
  },
  {
    "objectID": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#conclusion",
    "href": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#conclusion",
    "title": "Create a Code Interpreter Chatbot with Pyodide, LangChain, and OpenAI",
    "section": "Conclusion",
    "text": "Conclusion\nWay to go! By now, you’ve built a chatbot that can run Python code on the browser and help you answer complex questions about a dataset.\nThis is what you’ve covered in this tutorial:\n\nHow to integrate Pyodide into a web app to run code in the browser.\nHow to use LangChain’s LLMChain to generate Python code.\nHow to build a simple application with Litestar.\n\nI hope this is useful. Let me know if you have questions.\nThe code for this tutorial is available on GitHub."
  },
  {
    "objectID": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html",
    "href": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html",
    "title": "Deploying a FastAPI app with Kamal, AWS ECR, and Github Actions",
    "section": "",
    "text": "These days I use Kamal to deploy my FastAPI (or Django) projects. Kamal is a simpler alternative to Kubernetes that you can use to deploy containerized apps to a VPS.\nOnce you get the hang of it, it’ll only take you a few minutes to set up a CI/CD pipeline that automatically deploys your app to production with each push to the main branch.\nIn this tutorial, I’ll walk you through the process of deploying a FastAPI app with Kamal, AWS ECR, and Github Actions."
  },
  {
    "objectID": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#prerequisites",
    "href": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#prerequisites",
    "title": "Deploying a FastAPI app with Kamal, AWS ECR, and Github Actions",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo make the most of this tutorial, you should:\n\nHave a FastAPI app ready to deploy.\nHave an AWS account and its CLI installed.\nBe comfortable with Docker.\nHave a basic understanding of Kamal. You’ll need to install version 1.9.0 for this tutorial.\nHave a basic understanding of Github Actions.\nHave a VPS with Ubuntu ready to host your app."
  },
  {
    "objectID": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#prepare-your-vps",
    "href": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#prepare-your-vps",
    "title": "Deploying a FastAPI app with Kamal, AWS ECR, and Github Actions",
    "section": "Prepare your VPS",
    "text": "Prepare your VPS\nYou’ll need to install docker, curl, git, and snapd on your VPS, and create a non-root user called kamal that can sudo. You should also set the UID and GID of the user to 1000.\nIf you’re using Hetzner, you can use my terraform script to prepare the VPS.\nOtherwise, you can run these commands on your VPS’s terminal:\n# Install docker, curl, and git, and snapd\napt-get update\napt-get install -y docker.io curl git snapd\n\n# Start and enable the docker service\nsystemctl start docker\nsystemctl enable docker\n\n# Create a non-root user called kamal\nuseradd -m -s /bin/bash -u 1000 kamal\nusermod -aG sudo kamal\necho \"kamal ALL=(ALL) NOPASSWD:ALL\" &gt;&gt; /etc/sudoers.d/kamal\n\n# Add SSH key to login as kamal user\nmkdir -p /home/kamal/.ssh\necho \"&lt;YOUR_PUBLIC_SSH_KEY&gt;\" &gt;&gt; /home/kamal/.ssh/authorized_keys # you need a public key to login as the kamal user\nchmod 700 /home/kamal/.ssh\nchmod 600 /home/kamal/.ssh/authorized_keys\nchown -R kamal:kamal /home/kamal/.ssh\n\n# Disable root login\nsed -i '/PermitRootLogin/d' /etc/ssh/sshd_config\necho \"PermitRootLogin no\" &gt;&gt; /etc/ssh/sshd_config\nsystemctl restart sshd\n\n# Add the kamal user to the docker group\nusermod -aG docker kamal\ndocker network create --driver bridge kamal_network\n\n# Create a folder for the Let's Encrypt ACME JSON\nmkdir -p /letsencrypt && touch /letsencrypt/acme.json && chmod 600 /letsencrypt/acme.json\nchown -R kamal:kamal /letsencrypt\n\nreboot\nTo run these commands, you need to login as root. This assumes that there isn’t already a non-root user with UID 1000. Otherwise, you’ll have to adjust the commands accordingly.\nAlso, if you don’t have a public SSH key for the “Add SSH key” step, you can generate one with the following command:\nssh-keygen -t ed25519 -C \"your-email@example.com\"\nThese commands will:\n\nInstall docker, curl, and git\nStart and enable the docker service\nCreate a non-root user called kamal\nDisable root login\nAdd the kamal user to the docker group (this allows the user to run docker without needing to use sudo)\nCreate a Docker bridge network for Traefik\nCreate a folder for the Let’s Encrypt ACME JSON file\nMake the Let’s Encrypt ACME JSON folder writable by the kamal user\nRestart the server\n\nFinally, configure the SSH key in your local .ssh/config file so you can login as the kamal user without using the root account.\nHost kamal\n  HostName &lt;YOUR_VPS_IP&gt;\n  User kamal\n  IdentityFile ~/.ssh/&lt;YOUR_PRIVATE_SSH_KEY&gt;"
  },
  {
    "objectID": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#create-a-dockerfile-for-your-fastapi-app",
    "href": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#create-a-dockerfile-for-your-fastapi-app",
    "title": "Deploying a FastAPI app with Kamal, AWS ECR, and Github Actions",
    "section": "Create a Dockerfile for your FastAPI app",
    "text": "Create a Dockerfile for your FastAPI app\nKamal works with containerized apps, so you’ll need to have a Dockerfile. I also recommend using an entrypoint.sh script to run the application, because that also allows you to run commands in the container.\n\nDockerfile\nHere’s the Dockerfile I’m using for my projects. You can use this as a template and adjust it to your needs.\n\n\nDockerfile\n\nFROM python:3.10-slim AS base\n\nENV POETRY_HOME=/opt/poetry\nENV POETRY_VERSION=1.8.3\nENV PATH=${POETRY_HOME}/bin:${PATH}\n\nRUN apt-get update \\\n    && apt-get install --no-install-recommends -y \\\n    curl \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\nRUN curl -sSL https://install.python-poetry.org | python3 - && poetry --version\n\nFROM base AS builder\n\nWORKDIR /app\n\nCOPY poetry.lock pyproject.toml ./\n\nRUN poetry config virtualenvs.in-project true && \\\n    poetry install --only main --no-interaction\n\nFROM base AS runner\n\nWORKDIR /app\nCOPY --from=builder /app/.venv/ /app/.venv/\n\nCOPY . /app\n\nRUN chmod +x /app/entrypoint.sh\n\nFROM runner AS production\n\nEXPOSE 8000\n\nARG user=kamal\nARG group=kamal\nARG uid=1000\nARG gid=1000\nRUN groupadd -g ${gid} ${group} && \\\n    useradd -u ${uid} -g ${group} -s /bin/sh -m ${user} && \\\n    chown -R ${uid}:${gid} /app\n\nUSER ${uid}:${gid}\n\nWORKDIR /app\nCMD [ \"/app/entrypoint.sh\" , \"app\"]\n\nThis multi-stage Dockerfile does the following:\n\nInstalls poetry and sets up the virtual environment\nCreates the user kamal with the UID and GID 1000 and runs the application with that user.\nExposes port 8000 and runs the application by executing the entrypoint.sh script. Kamal automatically detects that is the port the app runs on and will use that to set up the reverse proxy.\n\nFeel free to adjust this Dockerfile to your needs.\n\n\nentrypoint.sh script\nI use an entrypoint.sh script to run the application because that makes it easier to collect static files, run migrations when the container starts, and also running commands in the container.\nHere’s an example of a simple entrypoint.sh script:\n\n\nentrypoint.sh\n\n#!/bin/sh\n\nset -e\n\nif [ \"$1\" = \"app\" ]; then\n    echo \"Collecting static files\"\n    exec poetry run gunicorn -c gunicorn.conf.py\nelse\n    exec \"$@\"\nfi\n\nThis script starts the gunicorn server with uvicorn workers and some sensible defaults. It also allows you to pass other arguments to the script, which is useful if you want to run other commands in the container. You can add or remove commands to the script as needed."
  },
  {
    "objectID": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#configure-an-ecr-registry-in-aws",
    "href": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#configure-an-ecr-registry-in-aws",
    "title": "Deploying a FastAPI app with Kamal, AWS ECR, and Github Actions",
    "section": "Configure an ECR registry in AWS",
    "text": "Configure an ECR registry in AWS\nNext, you’ll need a place to push and pull your Docker images. I use AWS ECR, so that’s what I’ll show you how to do here. Kamal also supports other registries.\nLog in to the AWS Management Console and go to Amazon ECR. Click on Create repository and set a name for your repository.\nThen, create a new IAM user in your AWS account by going to Services &gt; IAM &gt; Users &gt; Add user.\nDuring the process you’ll have to assign a permissions to the user. You can create a new policy with the following content and attach it to the user:\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"ListImagesInRepository\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"ecr:ListImages\"],\n      \"Resource\": [\n        \"arn:aws:ecr:&lt;REGION&gt;:&lt;ACCOUNT_ID&gt;:repository/&lt;REPOSITORY_NAME&gt;\"\n      ]\n    },\n    {\n      \"Sid\": \"GetAuthorizationToken\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"ecr:GetAuthorizationToken\"],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"ManageRepositoryContents\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ecr:BatchCheckLayerAvailability\",\n        \"ecr:GetDownloadUrlForLayer\",\n        \"ecr:GetRepositoryPolicy\",\n        \"ecr:DescribeRepositories\",\n        \"ecr:ListImages\",\n        \"ecr:DescribeImages\",\n        \"ecr:BatchGetImage\",\n        \"ecr:InitiateLayerUpload\",\n        \"ecr:UploadLayerPart\",\n        \"ecr:CompleteLayerUpload\",\n        \"ecr:PutImage\"\n      ],\n      \"Resource\": [\n        \"arn:aws:ecr:&lt;REGION&gt;:&lt;ACCOUNT_ID&gt;:repository/&lt;REPOSITORY_NAME&gt;\"\n      ]\n    }\n  ]\n}\nThis policy enables users to list, access, and manage the ECR repository they have previously created, as well as obtain an authorization token necessary for pushing and pulling images. You must replace &lt;REGION&gt;, &lt;ACCOUNT_ID&gt;, and &lt;REPOSITORY_NAME&gt; with the specific details of your own repository.\nThen, select the user you created and navigate to Security credentials &gt; Access keys &gt; Create access key. Download the generated CSV file and store it in a secure location.\nThe GitHub Actions workflow will use these credentials for pushing and pulling images from the ECR registry."
  },
  {
    "objectID": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#set-up-kamal-in-your-project",
    "href": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#set-up-kamal-in-your-project",
    "title": "Deploying a FastAPI app with Kamal, AWS ECR, and Github Actions",
    "section": "Set up Kamal in your project",
    "text": "Set up Kamal in your project\nOpen your FastAPI project in your favorite code editor. Create a folder called deploy in the root directory. Then go into the folder and initialize Kamal:\nkamal init\nThis will create two folders (.kamal/ and config/) and an .env file. Inside config/, you’ll find a deploy.yml file. This is where you’ll provide the instructions for Kamal to build and deploy your app.\nYou can use the following deploy.yml file as a template for your FastAPI app:\n\n\ndeploy.yml\n\nservice: example\n\nimage: example\n\nssh:\n  user: kamal\n\nenv:\n  secret:\n    - FASTAPI_ENV\n\ntraefik:\n  options:\n    publish:\n      - \"443:443\"\n    volume:\n      - \"/letsencrypt/:/letsencrypt/\"\n    memory: 500m\n    network: private_network\n  args:\n    entryPoints.web.address: \":80\"\n    entryPoints.websecure.address: \":443\"\n    entryPoints.web.http.redirections.entryPoint.to: websecure\n    entryPoints.web.http.redirections.entryPoint.scheme: https\n    entryPoints.web.http.redirections.entrypoint.permanent: true\n    certificatesResolvers.letsencrypt.acme.email: &lt;YOUR_EMAIL&gt;\"\n    certificatesResolvers.letsencrypt.acme.storage: \"/letsencrypt/acme.json\"\n    certificatesResolvers.letsencrypt.acme.httpchallenge: true\n    certificatesResolvers.letsencrypt.acme.httpchallenge.entrypoint: web\n\nservers:\n  web:\n    hosts:\n      - 128.140.0.209\n    healthcheck:\n      port: 8000\n      interval: 5s\n    options:\n      network: private_network\n    labels:\n      traefik.http.routers.app.tls: true\n      traefik.http.routers.app.entrypoints: websecure\n      traefik.http.routers.app.rule: Host(`&lt;YOUR_DOMAIN&gt;`)\n      traefik.http.routers.app.tls.certresolver: letsencrypt\n\nregistry:\n  server: &lt;account_id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com\n  username: AWS\n  password:\n    - KAMAL_REGISTRY_PASSWORD\n\nbuilder:\n  dockerfile: \"../Dockerfile\"\n  context: \"../\"\n  multiarch: false\n  cache:\n    type: gha\n\nThis will set up your app and a reverse proxy using Traefik (with automatic SSL certificates using Let’s Encrypt). Remember to replace the placeholders with your own values. It will also do a healthcheck on /up on port 8000.\n\nTest the configuration locally\nTo test it locally, first, you must define the required environment variables in .env, such as keys for AI services, email providers, etc.\nYou’ll also need to get a temporary password to authenticate into the ECR registry. You can get this password by running the following command from your terminal:\naws ecr get-login-password --region &lt;YOUR_REGION&gt;\nYou should copy the output of this command and paste it in the KAMAL_REGISTRY_PASSWORD field in the .env file.\nThen, run the following command to deploy your application to your VPS:\nkamal env push\nkamal deploy\nThe first command will push the environment variables to the VPS. The second command will build the Docker image, push it to the ECR registry, and deploy it to your VPS.\nAfter a few minutes, your app should be live at https://&lt;YOUR_DOMAIN&gt;.\nIf you see any errors, you can:\n\nRun kamal app logs to see the logs of the app.\nOpen a terminal in the container by running kamal app exec -it bash.\n\nThis is how I usually debug the app."
  },
  {
    "objectID": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#automate-the-deployment-with-github-actions",
    "href": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#automate-the-deployment-with-github-actions",
    "title": "Deploying a FastAPI app with Kamal, AWS ECR, and Github Actions",
    "section": "Automate the deployment with Github Actions",
    "text": "Automate the deployment with Github Actions\nNow that you have a working deployment process in your local environment, you can set up your CI/CD pipeline using GitHub Actions.\nCreate a new file in the .github/workflows folder called deploy.yml and add the following code:\nname: Deploy FastAPI app to VPS\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\non:\n  push:\n    branches: [\"main\"]\n  workflow_dispatch:\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n\n      - uses: webfactory/ssh-agent@v0.7.0\n        with:\n          ssh-private-key: ${{ secrets.VPS_SSH_PRIVATE_KEY }}\n\n      - name: Set up Ruby and install kamal\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: 3.2.2\n      - run: gem install kamal -v 1.9.0\n\n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID_ECR }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY_ECR }}\n          aws-region: us-east-1\n          mask-aws-account-id: false # otherwise the mask will hide your account ID and cause errors in the deployment\n\n      - name: Login to AWS ECR\n        id: login-ecr\n        uses: aws-actions/amazon-ecr-login@v2\n\n      - name: Set up Docker Buildx for cache\n        uses: docker/setup-buildx-action@v3\n\n      - name: Expose GitHub Runtime for cache\n        uses: crazy-max/ghaction-github-runtime@v3\n\n      - name: Create .env file\n        run: |\n          cd &lt;YOUR_PROJECT_ROOT&gt;/deploy\n          touch .env\n          echo KAMAL_REGISTRY_PASSWORD=\"${{ steps.login-ecr.outputs.docker_password_&lt;YOUR_ACCOUNT_ID&gt;_dkr_ecr_&lt;YOUR_REGION&gt;_amazonaws_com }}\" &gt;&gt; .env\n          # if you have other secrets, add them here\n          cat .env\n\n      - name: Kamal Deploy\n        id: kamal-deploy\n        run: |\n          cd &lt;YOUR_PROJECT_ROOT&gt;/deploy\n          kamal lock release\n          kamal env push\n          kamal deploy\nThis workflow will:\n\nCheckout the code\nSet up the Ruby environment and install Kamal\nConfigure the AWS credentials\nLogin to the AWS ECR registry\nSet up Docker Buildx for cache\nExpose GitHub Runtime for cache\nCreate the .env file\nRun Kamal deploy\n\nIt will run everytime you make a push to the main branch or by manually triggering the workflow. It’ll cancel any in-progress runs to avoid conflicts.\nAlso, before you push your code to the repository, you’ll need to add the following secrets to the repository:\n\nVPS_SSH_PRIVATE_KEY: The private key to connect to your VPS\nAWS_ACCESS_KEY_ID_ECR: The access key ID for the AWS ECR registry\nAWS_SECRET_ACCESS_KEY_ECR: The secret access key for the AWS ECR registry\n\nFinally, to speed up the deployment, add these options to the builder section of the deploy.yml file:\nbuilder:\n  dockerfile: \"../Dockerfile\"\n  context: \"../\"\n  multiarch: false # new\n  cache: # new\n    type: gha # new\nThis will enable the Docker Buildx cache for the build process in Github Actions. You can set multiarch to false if your CI pipeline shares the same architecture as your VPS, which was the case for me."
  },
  {
    "objectID": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#conclusion",
    "href": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#conclusion",
    "title": "Deploying a FastAPI app with Kamal, AWS ECR, and Github Actions",
    "section": "Conclusion",
    "text": "Conclusion\nYou now have a fully automated deployment pipeline for your FastAPI app. A push to the main branch will trigger the workflow, that will build the Docker image, push it to the ECR registry, and deploy it to your VPS.\nBreak free from the tyranny of manual deployments and expensive cloud services. Sleep like a baby and let Kamal handle your deployments.\nIf you have any questions or feedback, please feel free to leave a comment below."
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "",
    "text": "I’m teaching a course about the essential tools of Data Science at Nuclio Digital School. Among other topics, I planned to go through the most popular data visualization libraries in Python: pandas, matplotlib, seaborn, and plotly.express.\nWhile preparing the class materials, I thought, is there any site that shows you how to make frequently used graphs with all these libraries?\nIt turns out there isn’t. Most of what I found just scratched the surface, focused on one or two graphs, or didn’t show you how to make graphs starting from a DataFrame.\nI thought this was a great opportunity to write something helpful. So I came up with this article and an interactive cookbook you can use to learn about pandas, matplotlib, seaborn, and plotly.express. You can use them as references when looking for ways to visualize your data.\nLet’s get to it!"
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-the-most-of-this-tutorial",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-the-most-of-this-tutorial",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "How to Make the Most of this Tutorial",
    "text": "How to Make the Most of this Tutorial\nThere’s only one mandatory section in this tutorial: Initial setting and reading the data. It’ll show you’ve how to set your local environment, install the required libraries, and read the data.\nIf you’re in a hurry, start with that section and then go to the type of graph you’d like to make. Otherwise, you can browse through all the sections.\nI didn’t want to add fluff, so I only added comments to the parts I thought were hard to understand. Most code snippets in the tutorial are short and use parameters with simple names like x, y, or color."
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#what-are-the-pros-and-cons-of-each-library",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#what-are-the-pros-and-cons-of-each-library",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "What Are the Pros and Cons of Each Library",
    "text": "What Are the Pros and Cons of Each Library\nIn this tutorial, I compared four libraries: pandas, matplotlib, seaborn, and plotly.express. These are mature and popular Python libraries that will cover most of your data visualization needs.\nIf you’d like to know which one will work better for you, here’s a brief description of each, with their strong and weak points:\n\npandas\nYou can use the [plot](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html) method of pandas to create graphs. It’s a wrapper of matplotlib.pyplot. It’s especially useful if you’re working with pandas Series or DataFrames.\n\nPros\n\nIt’s easy to use.\nIt supports DataFrames.\nIt’s popular, so there’s lots of information available.\n\n\n\nCons\n\nIf you want to customize graphs, you’ll need to be familiar with matplotlib.\n\n\n\n\nmatplotlib\nIt’s one of the oldest and most popular data visualization library in the Python ecosystem. It provides you with many options to generate and customize graphs, but this control comes at a cost. It’s harder to use than the alternatives.\nYou can make graphs in matplotlib using a state-based interface (like MATLAB) and an object-oriented one. While this is useful for developers with a MATLAB or R background, it’s often confusing for newcomers looking for help.\n\nPros\n\nIt gives you complete control to customize graphs.\nIf you come from a MATLAB or R background, then you’ll find the state-based interface easy to grasp.\nIt’s popular, so there’s lots of information available.\n\n\n\nCons\n\nIt’s harder to use than other popular alternatives.\nIts two interfaces can generate confusion when solving issues.\n\n\n\n\nseaborn\nIt’s a wrapper on top of matplotlib that makes it easier to create graphs. seaborn provides you with reasonable defaults for most charts, statistical utilities, and an easy way to use pandas DataFrames.\n\nPros\n\nIt provides good defaults and useful statistical tools for most graphs.\nIt uses DataFrames.\nIt’s popular, so there’s lots of information available.\n\n\n\nCons\n\nFor basic charts, it doesn’t provide lots of benefits compared to pandas.\nIt doesn’t include popular types of graphs like stacked areas, or pie/donut charts.\n\n\n\n\nplotly.express\nIt’s a high-level interface for building graphs. It uses plotly in the background and provides the user with an easy and consistent way to create charts. It’s newer than the rest but offers many types of charts and options to customize them.\n\nPros\n\nIt’s easy to use.\nIt uses DataFrames.\nIt generates interactive graphs by default.\n\n\n\nCons\n\nIt’s one of the many available interfaces within the Plotly ecosystem. Beginners can get confused when trying to solve issues.\nIt’s more likely to change its interface, compared to the other libraries.\n\nThere you go. You’ve gone through the upsides and downsides of each library. Now, remember what Uncle Ben said to Peter: with great power, comes great responsibility. The next time you need to make a graph, choose wisely."
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#local-set-up-and-data",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#local-set-up-and-data",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "Local Set Up and Data",
    "text": "Local Set Up and Data\nIn this section, you’ll set up your local environment to start working. You’ll create a virtual environment, install and import the required libraries, and inspect the data.\n\nSet Up a Virtual Environment\nIf you’re working on a Python project, then using a virtual environment will save you lots of headaches. So, you’ll start by creating one and installing the required libraries.\nIf you’re using venv, then run these commands:\npython3 -m venv .dataviz\nsource .dataviz/bin/activate\npython3 -m pip install pandas==1.2.4 numpy==1.2.0 matplotlib==3.4.2 plotly==4.14.3 seaborn==0.11.1 notebook==6.4.0\njupyter notebook\nIf you’re using conda, then this is how you do it:\nconda create --name .dataviz\nconda activate .dataviz\nconda install pandas==1.2.4 numpy==1.19.2 matplotlib==3.4.2 plotly==4.14.3 seaborn==0.11.1 notebook==6.4.0 -y\njupyter notebook\nThat’s it! These commands will:\n\nCreate a virtual environment called .dataviz\nActivate the virtual environment\nInstall the required packages with the specified versions\n\nYou don’t need to install the rest if you only want to use one of the data visualization libraries. For example, if you want to use plotly.express, you can remove matplotlib and seaborn from the command.\n\n\nStart Jupyter Notebook and Import Libraries\nOpen Jupyter Notebook. Create a new notebook by clicking on New &gt; Python3 notebook in the menu. By now, you should have an empty Jupyter notebook in front of you. Let’s get to the fun part!\nFirst, you’ll need to import the required libraries. Create a new cell in your notebook and paste the following code to import the required libraries:\n# All\nimport pandas as pd\nimport numpy as np\n\n# matplotlib\nimport matplotlib.ticker as mtick\nimport matplotlib.pyplot as plt\n\n# plotly\nimport plotly.io as pio\nimport plotly.express as px\n\n# seaborn\nimport seaborn as sns\n\n# Set templates\npio.templates.default = \"seaborn\"\nplt.style.use(\"seaborn\")\nOn lines 1 to 14, you’ll import the required libraries and set up the themes for matplotlib and plotly. Each library provides you with some useful functionality:\n\npandas helps you read the data\nmatplotlib.pyplot, plotly.express, and seaborn help you make the charts\nmatplotlib.ticker makes it easy to customize the tickers on your axes in your matplotlib graphs\nplotly.io allows you to define a specific theme for your plotly graphs\n\nOn lines 17 and 18, you define the themes for plotly.express and matplotlib. In this case, you set them to use the seaborn theme. This will make the graphs from all the libraries look similar.\n\n\nReview the Data\nThroughout this tutorial, you’ll use a dataset with stock market data for 29 companies compiled by ichardddddd. It has the following columns:\n\nDate: Date corresponding to the observed value\nOpen: Price (in USD) at market open at the specified date\nHigh: Highest price (in USD) reached during the corresponding date\nLow: Lowest price (in USD) reached during the corresponding date\nClose: Price (in USD) at market close at the specified date\nVolume: Number of shares traded\nName: Stock symbol of the company\n\nTake a look at the data by reviewing a sample of rows:\nurl = \"https://raw.githubusercontent.com/szrlee/Stock-Time-Series-Analysis/master/data/all_stocks_2006-01-01_to_2018-01-01.csv\"\ndf = pd.read_csv(url)\ndf.sample(5)\nThis code will read the data from the URL you specified and generate a sample of 5 rows from the data. Take a look at the resulting sample:\n\n\n\n\nDate\nOpen\nHigh\nLow\nClose\nVolume\nName\n\n\n\n\n53053\n2012-10-24\n88.45\n88.45\n87.09\n87.28\n6498524\nMCD\n\n\n9078\n2006-01-31\n69.00\n69.05\n68.31\n68.31\n4095000\nBA\n\n\n62012\n2012-06-05\n26.08\n26.44\n26.00\n26.38\n9183184\nNKE\n\n\n81843\n2007-03-27\n47.57\n47.80\n47.03\n47.49\n12950422\nWMT\n\n\n49556\n2010-12-03\n39.07\n39.67\n38.70\n39.61\n30070142\nJPM\n\n\n\nThis is a long dataset (in regards to the stock names). In some graphs, you’ll have to reshape it into a wide dataset.\nThat’s it! You’re ready for the next sections."
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-line-plot",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-line-plot",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "How to Make a Line Plot",
    "text": "How to Make a Line Plot\nA line plot shows how a variable changes using points connected by line segments. It consists of two axes, a horizontal one, where you represent continuous and equally-spaced levels of a variable, and a vertical axis, with numerical values of a given metric.\nIn this case, you’ll plot the closing price of four stocks over time.\nYou’ll start by preparing the data you’ll use in the graphs. Copy the following code in a new cell in your notebook:\nurl = \"https://raw.githubusercontent.com/szrlee/Stock-Time-Series-Analysis/master/data/all_stocks_2006-01-01_to_2018-01-01.csv\"\ndf = pd.read_csv(url)\n\ndf = df.loc[df.Name.isin([\"AAPL\", \"JPM\", \"GOOGL\", \"AMZN\"]), [\"Date\", \"Name\", \"Close\"]]\ndf[\"Date\"] = pd.to_datetime(df.Date)\ndf.rename(columns={\"Close\": \"Closing Price\"}, inplace=True)\nThis code will prepare the data you’ll use in the plots. It works as follows:\n\nLines 1-2: you read the data from an URL.\nLine 4: you filter the DataFrame object to include only the stocks that you want to plot.\nLine 5: you adjust the type of the Date column. Using datetime will make most plotting libraries set the tickers in a better way.\nLine 6: you rename the Close column.\n\nNext, you’ll make a line plot using this dataset.\n\nLine Plot Using pandas\nThis is the code to make a line plot using pandas:\ndf_wide = df.pivot(index=\"Date\", columns=\"Name\", values=\"Closing Price\")\ndf_wide.plot(\n    title=\"Stock prices (2006 - 2017)\", ylabel=\"Closing Price\", figsize=(12, 6), rot=0\n)\nThis code generates a line plot. There are two important details that you should take into account:\n\nLine 1: you use the pivot method to go from a long dataset to a wide one. To plot multiple series in pandas you need a wide dataset.\nLines 2-3: you create the plot. You set the size of the figure by using figsize and keep the x-axis ticks in a horizontal position by setting rot=0.\n\nHere’s the resulting graph:\n\n\n\nLine Plot Using matplotlib\nHere’s how you create a line plot with matplotlib:\nfig, ax = plt.subplots(figsize=(12, 6))\n\nfor l, g in df.groupby(\"Name\"):\n    ax.plot(g[\"Date\"], g[\"Closing Price\"], label=l)\n\nax.set_title(\"Stock prices (2006 - 2017)\")\nax.set_ylabel(\"Closing Price\")\nax.set_xlabel(\"Date\")\nax.legend(title=\"Name\")\nThis code creates a line plot. Here are some relevant highlights:\n\nLine 1: you create a figure and axes objects, and set the size of the plot. The figure is a container for the axes. You draw the plot in the axes.\nLines 3-4: you iterate over the groups in the DataFrame. Each group is a tuple of the name of the stock and a series with its closing prices. You plot the closing prices of each stock on a separate series.\nLines 6-9: you set the labels, title, and show the legend of the plot.\n\nThis is the resulting graph:\n\n\n\nLine Plot Using seaborn\nHere’s the code to create a line plot with seaborn:\nfig, ax = plt.subplots(figsize=(12, 6))\nsns.lineplot(data=df, x=\"Date\", y=\"Closing Price\", hue=\"Name\", ax=ax)\nax.set_title(\"Stock Prices (2006 - 2017)\")\nThis code creates a line plot using seaborn. Here’s what it does:\n\nLine 1: You start by creating a figure and axes objects and setting the size of the plot. T\nLines 2-3: you create the graph and set its title.\n\nHere’s the resulting graph:\n\n\n\nLine Plot Using plotly.express\nThis is how you use plotly.express to create a line plot:\nfig = px.line(\n    df, x=\"Date\", y=\"Closing Price\", color=\"Name\", title=\"Stock Prices (2006 - 2017)\"\n)\nfig.show()\nHere’s the resulting graph:\n\n20082010201220142016020040060080010001200NameAAPLJPMGOOGLAMZNStock Prices (2006 - 2017)DateClosing Price"
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-grouped-bar-chart",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-grouped-bar-chart",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "How to Make a Grouped Bar Chart",
    "text": "How to Make a Grouped Bar Chart\nA grouped bar chart is like a regular bar chart, but plots values for two categories instead of one. You can use grouped bars when you want to compare how a second category changes within each level of the first.\nIn this case, you’ll plot the maximum opening and closing price per year for Apple’s stock (AAPL) between 2014 and 2017.\nYou’ll start by preparing the data for the graphs. Copy the following code in a new cell in your notebook:\nurl = \"https://raw.githubusercontent.com/szrlee/Stock-Time-Series-Analysis/master/data/all_stocks_2006-01-01_to_2018-01-01.csv\"\ndf = pd.read_csv(url)\n\ndf = df.loc[df.Name == \"AAPL\", [\"Date\", \"Open\", \"Close\"]]\ndf[\"Year\"] = pd.to_datetime(df.Date).dt.year\ndf = df.query(\"Year &gt;= 2014\").groupby(\"Year\").max().reset_index(drop=False)\nThis code will prepare the data you’ll use in the plots. It works as follows:\n\nLines 1-2: you read the data from an URL.\nLine 4: you keep the information of AAPL and the columns Date, Open, and Close.\nLine 5: youcreate a new column with the year of each data point.\nLine 6: you remove the observations from before 2014, and find the max value per year of each column in the DataFrame.\n\nNext, you’ll see how to make a grouped bars plot using this dataset.\n\nGrouped Bar Chart Using  pandas\nHere’s the code to make a grouped bar plot with pandas:\ndf.plot.bar(\n    x=\"Year\",\n    y=[\"Open\", \"Close\"],\n    rot=0,\n    figsize=(12, 6),\n    ylabel=\"Price in USD\",\n    title=\"Max Opening and Closing Prices per Year for AAPL\",\n)\nThis is how you make a grouped bar plot. There’s one detail worth mentioning: in the plot method, you set the size of the figure using figsize and keep the x-axis ticks in a horizontal position by setting rot=0.\nHere’s the resulting graph:\n\n\n\nGrouped Bar Chart Using matplotlib\nHere’s the code to make a grouped bar plot using matplotlib:\nfig, ax = plt.subplots(figsize=(12, 6))\n\nx = np.arange(len(df.Year))\nwidth = 0.25\n\nax.bar(x - width / 2, df.Open, width, label=\"Open\")\nax.bar(x + width / 2, df.Close, width, label=\"Close\")\n\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Price in USD\")\nax.set_title(\"Max Opening and Closing Prices per Year for AAPL\")\n\nax.set_xticks(x)\nax.set_xticklabels(df.Year)\n\nax.legend()\nThis code will create a grouped bar plot using matplotlib. Here’s how it works:\n\nLine 1: you create a figure and axes objects, and set the size of the plot. The figure is a container for the axes. You draw the plot in the axes.\nLines 3-4: you create x to set the position of the ticks of the x-axis. In addition, you set width to 0.25, to define the width of the bars.\nLines 6-7: you create the bars at each tick in the x axis, taking in consideration the width of the bars.\nLines 9-11: you set the labels and title of the plot.\nLines 13-14: you set the locations and labels of the x-axis ticks.\nLine 16: you create a legend for the chart.\n\nHere’s is the resulting graph:\n\n\n\nGrouped Bar Chart Using seaborn\nHere’s the code to make a grouped bar plot using seaborn:\ndf_long = df.melt(\n    id_vars=\"Year\",\n    value_vars=[\"Open\", \"Close\"],\n    var_name=\"Category\",\n    value_name=\"Price\",\n)\n\nfig, ax = plt.subplots(figsize=(12, 6))\nsns.barplot(data=df_long, x=\"Year\", y=\"Price\", hue=\"Category\", ax=ax)\n\nax.set_title(\"Max Opening and Closing Prices per Year for AAPL\")\nax.legend(title=None)\nThis is how you make a grouped bars plot using seaborn. There are two details worth mentioning:\n\nLines 1-6: you apply the melt method to transform the original dataset into a long one (in regards to the closing and opening prices). seaborn doesn’t work well with wide datasets.\nLine 7: you start by creating a figure and axes objects and setting the size of the plot. You’ll pass the axes to the ax parameter of barplot.\n\nThis is the resulting graph:\n\n\n\nGrouped Bar Chart Using plotly.express\nHere’s the code to make a grouped bar plot using plotly.express:\nfig = px.bar(\n    df,\n    x=\"Year\",\n    y=[\"Open\", \"Close\"],\n    title=\"Max Opening and Closing Prices per Year for AAPL\",\n    barmode=\"group\",\n    labels={\"value\": \"Price in USD\"},\n)\nfig.show()\nThis is how you make a grouped bars plot using plotly.express. There are a few things worth highlighting:\n\nLine 4: to plot the opening and closing prices, you specify both in the y parameter of px.bar. plotly.express works well with wide datasets, so you don’t need to reshape the DataFrame.\nLine 6: you set barmode=group in px.bar so that bars don’t get stacked on top of each other.\n\nThis is the resulting graph:\n\n2014201520162017050100150variableOpenCloseMax Opening and Closing Prices per Year for AAPLYearPrice in USD"
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-stacked-bar-chart",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-stacked-bar-chart",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "How to Make a Stacked Bar Chart",
    "text": "How to Make a Stacked Bar Chart\nA stacked bar chart is like a normal bar chart, except a normal bar chart shows the total of all the bars, and a stacked bar chart shows the total of all the bars, plus how each part of the bar is made up.\nIn this case, you’ll plot the total volume traded per year for a sample of stocks: AAPL, JPM, GOOGL, AMZN,and IBM.\nFirst, you’ll prepare the data for the graphs. Copy this code in a cell in your notebook:\nurl = \"https://raw.githubusercontent.com/szrlee/Stock-Time-Series-Analysis/master/data/all_stocks_2006-01-01_to_2018-01-01.csv\"\ndf = pd.read_csv(url)\n\nstocks_filter = [\"AAPL\", \"JPM\", \"GOOGL\", \"AMZN\", \"IBM\"]\ndf = df[df.Name.isin(stocks_filter)]\ndf[\"Date\"] = pd.to_datetime(df.Date)\ndf[\"Year\"] = pd.to_datetime(df.Date).dt.year\ndf[\"Volume\"] = df[\"Volume\"] / 1e9\n\ndf = (\n    df[[\"Year\", \"Volume\", \"Name\"]]\n    .query(\"Year &gt;= 2012\")\n    .groupby([\"Year\", \"Name\"])\n    .sum()\n    .reset_index(drop=False)\n)\nThis code will help you prepare the data for the plots. It works as follows:\n\nLines 1-2: you read the data from an URL.\nLines 4-5: you keep the rows for the AAPL, JPM, GOOGL, AMZN, IBM.\nLine 6: youcreate a new column with the year of each data point.\nLine 9: you divide the total volume by one billion to make it more tractable.\nLine 6: you sum the volume per stock and year to get the total traded per year for each stock symbol.\n\nNext, you’ll see how to make a stacked bar plot using this dataset.\n\nStacked Bar Chart Using pandas\nHere’s the code to make a **stacked bar** plot using pandas:\ndf_wide = df.pivot(index=\"Year\", columns=\"Name\", values=\"Volume\")\ndf_wide.plot.bar(\n    rot=0,\n    figsize=(12, 6),\n    ylabel=\"Volume (billions of shares)\",\n    title=\"Trading volume per year for selected shares\",\n    stacked=True,\n)\nThere are a few details worth mentioning:\n\nLine 1: you use the pivot method to go from a long dataset to a wide one. To plot multiple series in pandas you need a wide dataset.\nLines 3-4: you set the size of the figure by using figsize and keep the x-axis ticks horizontally by setting rot=0.\nLine 7: you set stacked=True, so that bars get stacked instead of grouped together.\n\nThis is the resulting graph:\n\n\n\nStacked Bar Chart Using matplotlib\nHere’s the code to make a **stacked bar** plot using matplotlib:\nfig, ax = plt.subplots(figsize=(12, 6))\n\nbottom = np.zeros(df.Year.nunique())\nfor i, g in df.groupby(\"Name\"):\n    ax.bar(g[\"Year\"], g[\"Volume\"], bottom=bottom, label=i, width=0.5)\n    bottom += g[\"Volume\"].values\n\nax.set_title(\"Trading volume per year for selected shares\")\nax.set_ylabel(\"Volume (billions of shares)\")\nax.set_xlabel(\"Year\")\n\nax.legend()\nThis code will create a stacked bar plot using matplotlib. Here’s how it works:\n\nLine 1: you create a figure and axes objects, and set the size of the plot. The figure is a container for the axes. You draw the plot in the axes.\nLines 3: you initialize an array filled with zeroes of the same size of the number of ticks in the x-axis.\nLines 4-6: you iterate over groups in the DataFrame. Each group is a tuple of the name of the stock and a series with its volume per date. You add a bar plot of the Volume to the axes. At each iteration, bottom accumulates the total volume. You use it to stack the bars on top of each other.\nLines 8-12: you set the labels, title, and create a legend for the plot.\n\nThis is the resulting graph:\n\n\n\nStacked Bar Chart Using seaborn\nHere’s how you make a **stacked bar** plot using seaborn:\nfig, ax = plt.subplots(figsize=(12, 6))\n\nax = sns.histplot(\n    data=df,\n    x=\"Year\",\n    hue=\"Name\",\n    weights=\"Volume\",\n    multiple=\"stack\",\n    shrink=0.5,\n    discrete=True,\n    hue_order=df.groupby(\"Name\").Volume.sum().sort_values().index,\n)\n\nax.set_title(\"Trading volume per year for selected shares\")\nax.set_ylabel(\"Volume (billions of shares)\")\n\nlegend = ax.get_legend()\nlegend.set_bbox_to_anchor((1, 1))\nThis code will make a stacked bars plot in seaborn. There are some details worth mentioning:\n\nLine 1: you create a figure and axes objects, and set the size of the plot. The figure is a container for the axes. You draw the plot in the axes.\nLine 7: you set the size of the slices in the stacked bars by using the weights argument. In this case, you set the size of slices to the total volume of each stock.\nLine 8-10: you allow stacking the bars by setting multiple=\"stack\". In addition, you reduce the width of the bars usingshrink=0.5, and center the bars in the ticks of the x-axis using discrete=True.\nLine 11: you set the order for the stacking of the bars. In this case, you draw the bars from biggest to smallest, starting from the bottom.\n\nThis is the resulting graph:\n\n\n\nStacked Bar Using plotly.express\nHere’s how you make a stacked bars plot using plotly.express:\nfig = px.bar(\n    df,\n    x=\"Year\",\n    y=\"Volume\",\n    color=\"Name\",\n    title=\"Trading volume per year for selected shares\",\n    barmode=\"stack\",\n    labels={\"Volume\": \"Volume (billions of shares)\"},\n)\nfig.show()\nAs you can see, making a stacked bars plot using plotly.express is straightforward. Just remember to set barmode=\"stack\".\nThis is the resulting graph:\n\n201220132014201520162017010203040NameAAPLAMZNGOOGLIBMJPMTrading volume per year for selected sharesYearVolume (billions of shares)"
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-stacked-area-chart",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-stacked-area-chart",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "How to Make a Stacked Area Chart",
    "text": "How to Make a Stacked Area Chart\nThe stacked area chart is a non-discrete version of a stacked bar chart. It’s useful when you want to visualize changes in the total value of a variable and its composition, in the same graph. Though, it’s often used to visualize only changes of composition over time.\nIn this case, you’ll plot the changes in the composition of the daily volume traded for a sample of stocks: AAPL, JPM, GOOGL, AMZN,and IBM.\nStart by preparing the data for the graphs using this code:\nurl = \"https://raw.githubusercontent.com/szrlee/Stock-Time-Series-Analysis/master/data/all_stocks_2006-01-01_to_2018-01-01.csv\"\ndf = pd.read_csv(url)\n\nstocks = [\"AAPL\", \"AMZN\", \"GOOGL\", \"IBM\", \"JPM\"]\ndf = df.loc[df.Name.isin(stocks), [\"Date\", \"Name\", \"Volume\"]]\ndf[\"Date\"] = pd.to_datetime(df.Date)\ndf = df[df.Date.dt.year &gt;= 2017]\ndf[\"Volume Perc\"] = df[\"Volume\"] / df.groupby(\"Date\")[\"Volume\"].transform(\"sum\")\nThis code will help you prepare the data for the plots. It works as follows:\n\nLines 1-2: you read the data from an URL.\nLines 4-5: you keep the rows for the AAPL, JPM, GOOGL, AMZN, IBM.\nLine 6-7: youkeep the data for 2017 onwards.\nLine 6: you calculate the percentage of the total volume traded corresponding to each stock symbol.\n\nNext, you’ll see how to make a stacked area plot using this dataset.\n\nStacked Area Chart Using pandas\nHere’s how you make a stacked area plot using pandas:\ndf_wide = df.pivot(index=\"Date\", columns=\"Name\", values=\"Volume Perc\")\n\nax = df_wide.plot.area(\n    rot=0,\n    figsize=(12, 6),\n    title=\"Distribution of daily trading volume - 2017\",\n    stacked=True,\n)\nax.legend(bbox_to_anchor=(1, 1), loc=\"upper left\")\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\nThis code will make a stacked area plot using pandas. There are a few details worth mentioning:\n\nLine 1: you use the pivot method to go from a long dataset to a wide one. To plot multiple series in pandas you need a wide dataset.\nLines 4-5: you set the size of the figure by using figsize and keep the x-axis ticks horizontally by setting rot=0.\nLine 7: you set stacked=True to stack the areas.\nLines 9-10: you move the legend to the upper left corner, and set format the y-axis tick labels to use percentages.\n\nThis is the resulting graph:\n\n\n\nStacked Area Chart Using matplotlib\nHere’s how you make a stacked areas plot using matplotlib:\ndf_wide = df.pivot(index=\"Date\", columns=\"Name\", values=\"Volume Perc\")\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\nax.stackplot(df_wide.index, [df_wide[col].values for col in stocks], labels=stocks)\nax.legend(bbox_to_anchor=(1, 1), loc=\"upper left\")\n\nax.set_title(\"Distribution of daily trading volume - 2017\")\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\n\nLine 1: you use the pivot method to go from a long dataset to a wide one. For this type of chart in matplotlib, is better to use a wide dataset.\nLine 3: you create a figure and axes objects, and set the size of the plot. The figure is a container for the axes. You draw the plot in the axes.\nLine 5: you create the plot by passing it the values for the x-axis, a list of lists for the areas, and the labels for each series.\nLines 4-6: you iterate over groups in the DataFrame. Each group is a tuple of the name of the stock and a series with its volume per date. At each iteration, you add a bar plot of the volume to the axes and sum the volume values to bottom. You use bottom to set the distance between the bars and the x-axis.\nLines 8-12: you set the labels, title, and create a legend for the plot.\n\nThis is the resulting graph:\n\n\n\nStacked Area Chart Using plotly.express\nHere’s how you make a stacked area plot using plotly.express:\nfig = px.area(\n    df,\n    x=\"Date\",\n    y=\"Volume Perc\",\n    color=\"Name\",\n    title=\"Distribution of daily trading volume - 2017\",\n)\nfig.update_layout(yaxis_tickformat=\"%\")\nfig.show()\nThis is the resulting graph:\n\nMar 2017May 2017Jul 2017Sep 2017Nov 20170%20%40%60%80%100%NameAAPLIBMJPMGOOGLAMZNDistribution of daily trading volume - 2017DateVolume Perc"
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-pie-or-donut-chart",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-pie-or-donut-chart",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "How to Make a Pie or Donut Chart",
    "text": "How to Make a Pie or Donut Chart\nThe pie or donut chart shows the composition of a variable into categories by using radial slices. For example, you could use it to show what percentage of your day you dedicate to sleep, work, and leisure.\nIn this case, you’ll plot the distribution of the total volume traded for a sample of stocks: AAPL, JPM, GOOGL, AMZN,and IBM.\nStart by preparing the data for the graphs using this code:\nurl = \"https://raw.githubusercontent.com/szrlee/Stock-Time-Series-Analysis/master/data/all_stocks_2006-01-01_to_2018-01-01.csv\"\ndf = pd.read_csv(url)\n\nstocks_filter = [\"AAPL\", \"JPM\", \"GOOGL\", \"AMZN\", \"IBM\"]\ndf = df.loc[df.Name.isin(stocks_filter), [\"Name\", \"Volume\"]]\ndf = df.groupby(\"Name\").sum().reset_index()\nThis code will help you prepare the data for the plots. It works as follows:\n\nLines 1-2: you read the data from an URL.\nLines 4-5: you keep the rows for the AAPL, JPM, GOOGL, AMZN, IBM.\nLine 6: you sum the total volume per stock for the whole dataset.\n\nNext, you’ll see how to make a pie or donut plot using this dataset.\n\nPie or Donut Chart Using pandas\nHere’s the code to make a donut chart using pandas:\ndf.set_index(\"Name\").plot.pie(\n    y=\"Volume\",\n    wedgeprops=dict(width=0.5),\n    figsize=(8, 8),\n    autopct=\"%1.0f%%\",\n    pctdistance=0.75,\n    title=\"Distribution of trading volume for selected stocks (2006 - 2017)\",\n)\nYou can use this code to create a pie or donut chart using pandas. Here’s how it works:\n\nLine 1: you set Name as the DataFrame index. This is needed if you want to make a pie or donut chart with pandas. Then, you call plot.pie.\nLine 2: you use Volume to calculate the size of the radial slices.\nLine 3-7: you create the “hole” in the pie, set the figure size, define the format and location of the labels, and set the title of the chart.\n\nThis is the resulting graph:\n\n\n\nPie or Donut Chart Using matplotlib\nHere’s how you make a donut chart using matplotlib:\nfig, ax = plt.subplots(figsize=(8, 8))\n\nax.pie(\n    df.Volume,\n    labels=df.Name,\n    wedgeprops=dict(width=0.5),\n    autopct=\"%1.0f%%\",\n    pctdistance=0.75,\n)\nax.set_title(\"Distribution of trading volume for selected stocks (2006 - 2017)\")\nax.legend()\nThis is code will create a donut chart with matplotlib. Here’s how it works:\n\nLine 1: you create a figure and axes objects, and set the size of the plot. The figure is a container for the axes. You draw the plot in the axes.\nLine 4-5: you use Volume to calculate the size of the radial slices and use Name for the labels.\nLine 6-11: you define the size of the “hole” in the pie, define the format and location of the labels, set the title, and create the legend of the chart.\n\nThis is the resulting graph:\n\n\n\nPie or Donut Chart Using plotly.express\nHere’s how you make a donut chart using plotly.express:\nfig = px.pie(\n    data_frame=df,\n    values=\"Volume\",\n    names=\"Name\",\n    hole=0.5,\n    color=\"Name\",\n    title=\"Distribution of trading volume for selected stocks (2006 - 2017)\",\n)\nfig.show()\nThis code will result in the following graph:\n\n75.1%16.2%3.4%3.31%2.03%AAPLJPMAMZNIBMGOOGLDistribution of trading volume for selected stocks (2006 - 2017)"
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-histogram",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-histogram",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "How to Make a Histogram",
    "text": "How to Make a Histogram\nA histogram shows the distribution of a numerical variable using bars. Each bar’s height indicates the frequency of a certain range of that numerical variable. You can use a histogram to evaluate attributes such as shape, skew, and outliers of a variable.\nIn this case, you’ll make a histogram with the distribution of closing prices of GOOGL and AMZN. Note that plotting a histogram with a single group is trivial, so I chose to create one for multiple groups.\nStart by preparing the data for the graphs using this code:\nurl = \"https://raw.githubusercontent.com/szrlee/Stock-Time-Series-Analysis/master/data/all_stocks_2006-01-01_to_2018-01-01.csv\"\ndf = pd.read_csv(url)\n\nstocks_filter = [\"GOOGL\", \"AMZN\"]\ndf = df.loc[df.Name.isin(stocks_filter), [\"Name\", \"Close\"]]\nThis code will help you prepare the data for the plots. Here’s how it works:\n\nLines 1-2: you read the data from an URL.\nLines 4-5: you keep the rows for the GOOGL and AMZN, and the columns you’ll use in the plot.\n\nNext, you’ll see how to make a histogram using this dataset.\n\nHistogram Using pandas and matplotlib\nHere’s how you make a histogram with multiple groups using matplotlib:\nfig, ax = plt.subplots(figsize=(12, 6))\n\nfor i, (l, g) in enumerate(df.groupby(\"Name\")):\n    if i == 0:\n        _, bins, _ = ax.hist(g.Close, alpha=0.75, label=l, bins=30)\n    else:\n        ax.hist(g.Close, alpha=0.75, label=l, bins=bins)\n\nax.legend()\nax.set_title(\"Distribution of Closing Prices - GOOGL vs. AMZN\")\nax.set_xlabel(\"Closing Price\")\nYou use this code to create a histogram with multiple groups. Here’s what it does:\n\nLine 1: you create a figure and axes objects, and set the size of the plot. The figure is a container for the axes. You draw the plot in the axes.\nLine 3-7: you iterate over the groups in the DataFrame. Each group is a tuple of the name of the stock and a series with its closing prices. In addition, you use enumerate to identify the index of each group. You use the first group (index 0), to calculate how many bins you’ll use in the histogram.\nLine 9-11: you create the legend of the chart, set the title, and set label of the x-axis.\n\nThis is the resulting graph:\n\n\n\nHistogram Using seaborn\nHere’s how you make a histogram using seaborn:\nfig, ax = plt.subplots(figsize=(12, 6))\nsns.histplot(data=df, x=\"Close\", hue=\"Name\", ax=ax)\nax.set_title(\"Distribution of Closing Prices - GOOGL vs. AMZN\")\nax.set_xlabel(\"Closing Price\")\nThere’s one detail worth mentioning: in the first line, you create a figure and axes objects (the latter you pass to the histplot method) and set the size of the plot. The figure is a container for the axes. You use the axes to draw the plot.\nThis is the resulting graph:\n\n\n\nHistogram Using plotly.express\nHere’s how you make a histogram using plotly.express:\nfig = px.histogram(\n    df,\n    x=\"Close\",\n    color=\"Name\",\n    labels={\"Close\": \"Closing Price\"},\n    title=\"Distribution of Closing Prices - GOOGL vs. AMZN\",\n    barmode=\"overlay\",\n)\nfig.show()\nThis is the resulting graph: 20040060080010001200050100150200250300NameGOOGLAMZNDistribution of Closing Prices - GOOGL vs. AMZNClosing Pricecount"
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-scatter-plot",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-scatter-plot",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "How to Make a Scatter Plot",
    "text": "How to Make a Scatter Plot\nA scatter plot consists of dots graphed in a space defined by a horizontal and a vertical axis. You can use it to understand the relationship between two variables. For example, the relationship between height and weight for a group of individuals.\nFor this part of the tutorial, you’ll make a scatter plot with the daily returns of GOOGL and AMAZN.\nIn the code below, you prepare the data to create the graphs:\nurl = \"https://raw.githubusercontent.com/szrlee/Stock-Time-Series-Analysis/master/data/all_stocks_2006-01-01_to_2018-01-01.csv\"\ndf = pd.read_csv(url)\n\nstocks_filter = [\"GOOGL\", \"AMZN\"]\ndf = df.loc[\n    (df.Name.isin(stocks_filter)) & (pd.to_datetime(df.Date).dt.year &gt;= 2017),\n    [\"Date\", \"Name\", \"Open\", \"Close\"],\n]\ndf[\"Return\"] = (df[\"Close\"] - df[\"Open\"]) / df[\"Open\"]\ndf_wide = df.pivot(index=\"Date\", columns=\"Name\", values=\"Return\")\nIn this code snippet, you read and transformed the data. It works as follows:\n\nLines 1-2: you read the data from an URL.\nLines 4-8: you remove the rows and columns you don’t need from the DataFrame.\nLine 9: You calculate the intraday return per day for each stock.\nLine 10: You transform the dataset from long to wide. The resulting dataset will have two columns with the intraday returns for AMZN and GOOGL.\n\n\nScatter Plot Using pandas\nIn the code below, you’ll see how to make a scatter plot with pandas:\nax = df_wide.plot.scatter(\n    x=\"GOOGL\", y=\"AMZN\", title=\"Daily returns - GOOGL vs. AMZN\", figsize=(8, 8)\n)\n\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\nax.xaxis.set_major_formatter(mtick.PercentFormatter(1))\nThis is the resulting graph:\n\n\n\nScatter Plot Using matplotlib\nHere’s how you make a scatter plot using matplotlib:\nimport matplotlib.ticker as mtick\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\nax.scatter(x=df_wide[\"GOOGL\"], y=df_wide[\"AMZN\"])\n\nax.set_xlabel(\"GOOGL\")\nax.set_ylabel(\"AMZN\")\nax.set_title(\"Daily returns - GOOGL vs. AMZN\")\n\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\nax.xaxis.set_major_formatter(mtick.PercentFormatter(1))\nThis is the resulting graph:\n\n\n\nScatter Plot Using seaborn\nHere’s how you make a scatter plot using seaborn:\nfig, ax = plt.subplots(figsize=(8, 8))\n\nsns.scatterplot(data=df_wide, x=\"GOOGL\", y=\"AMZN\", ax=ax)\n\nax.set_title(\"Daily returns - GOOGL vs AMZN\")\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\nax.xaxis.set_major_formatter(mtick.PercentFormatter(1))\nThis is the resulting graph:\n\n\n\nScatter Plot Using plotly.express\nHere’s how you make a scatter plot using plotly.express:\nfig = px.scatter(df_wide, x=\"GOOGL\", y=\"AMZN\", title=\"Daily returns - GOOGL vs. AMZN\")\nfig.update_layout(yaxis_tickformat=\"%\", xaxis_tickformat=\"%\")\nfig.show()\nThis is the resulting graph:\n\n−3%−2%−1%0%1%2%−3%−2%−1%0%1%2%3%4%Daily returns - GOOGL vs. AMZNGOOGLAMZN"
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-box-plot",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-box-plot",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "How to Make a Box Plot",
    "text": "How to Make a Box Plot\nA box plot shows you a statistical summary of a dataset through a graphical representation of quartiles. It shows the following information of the variable studied:\n\nMinimum\nMaximum\nMedian\nQ1 (first quartile)\nQ3 (third quartile)\nOutliers\n\nIn this case, you’ll create a boxplot of the intraday of 2016 for a sample of stocks: AAPL, GOOGL, IBM, andJPM.\nStart by preparing the data for the graphs using this code:\nurl = \"https://raw.githubusercontent.com/szrlee/Stock-Time-Series-Analysis/master/data/all_stocks_2006-01-01_to_2018-01-01.csv\"\ndf = pd.read_csv(url)\n\nstocks = [\"AMZN\", \"GOOGL\", \"IBM\", \"JPM\"]\ndf = df.loc[\n    (df.Name.isin(stocks)) & (pd.to_datetime(df.Date).dt.year == 2016),\n    [\"Date\", \"Name\", \"Close\", \"Open\"],\n]\ndf[\"Return\"] = (df[\"Close\"] - df[\"Open\"]) / df[\"Open\"]\ndf[\"Date\"] = pd.to_datetime(df.Date)\nThis code will help you prepare the data for the plots. It works as follows:\n\nLines 1-2: you read the data from an URL.\nLines 4-8: you keep the information for the stocks that interest you and remove data that’s not from 2016. In addition, you drop the columns you don’t need for the plots.\nLine 9: you calculate the intraday return of each stock.\nLine 10: you set the correct data type to Date.\n\nNext, you’ll see how to make a box plot using this dataset.\n\nBox Plot Using pandas\nHere’s how you make a box plot using pandas:\ndf_wide = df.pivot(index=\"Date\", columns=\"Name\", values=\"Return\")\nax = df_wide.boxplot(column=[\"AMZN\", \"GOOGL\", \"IBM\", \"JPM\"])\n\nax.set_ylabel(\"Daily returns\")\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\nThere are a few details worth mentioning:\n\nLine 1: you use the pivot method to go from a long dataset to a wide one. To plot multiple series in pandas you need a wide dataset.\nLine 2: you create the plot. You specify which columns of the dataset should be used for the boxplot.\nLines 4-5: you set the label of the y-axis, and change the format of the ticks of the y-axis to show percentages.\n\nThis is the resulting graph:\n\n\n\nBox Plot Using matplotlib\nHere’s how you make a box plot using matplotlib:\ndf_wide = df.pivot(index=\"Date\", columns=\"Name\", values=\"Return\")\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\nstocks = [\"AMZN\", \"GOOGL\", \"IBM\", \"JPM\"]\nax.boxplot([df_wide[col] for col in stocks], vert=True, autorange=True, labels=stocks)\n\nax.set_ylabel(\"Daily returns\")\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\n\nLine 1: you use the pivot method to transform the dataset from long to wide.\nLine 3: you create a figure and axes objects, and set the size of the plot. The figure is a container for the axes. You draw the plot in the axes.\nLine 6: you create the plot by passing a list of lists with the values of the Daily returns of each stock.\nLines 8-9: you set the label of the y-axis, and change the format of the ticks of the y-axis to show percentages.\n\nThis is the resulting graph:\n\n\n\nBox Plot Using seaborn\nHere’s how you make a box plot using seaborn:\nax = sns.boxplot(x=\"Name\", y=\"Return\", data=df, order=stocks)\n\nax.set_ylabel(\"Daily returns\")\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\nThere’s one detail worth highlighting: on lines 3 and 4, you set the label of the y-axis, and change the format of the ticks of the y-axis to show percentages.\nThis is the resulting graph:\n\n\n\nBox Plot Using plotly.express\nHere’s how you make a box plot using plotly.express:\nfig = px.box(df, x=\"Name\", y=\"Return\", category_orders={\"Name\": stocks})\nfig.show()\nThis is the resulting graph:\n\nAMZNGOOGLIBMJPM−0.06−0.04−0.0200.020.04NameReturn"
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#conclusion",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#conclusion",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tutorial, you’ve learned how to make some of the most popular types of charts with four data visualization libraries in Python: pandas, matplotlib, seaborn, and plotly.express.\nYou understood the strengths and weaknesses of each data visualization library, and learned how to make the following type of graphs:\n\nLine plots\nGrouped and stacked bar charts\nArea charts\nPie/donut charts\nHistograms\nBox plots\nScatter plots\n\nI hope you’ve found this tutorial helpful. If you have any questions or feedback, please let me know in the comments!"
  },
  {
    "objectID": "posts/2023-personal-snapshot.html",
    "href": "posts/2023-personal-snapshot.html",
    "title": "2023: Personal Snapshot",
    "section": "",
    "text": "This is my annual review. It serves two purposes: a deep analysis of the past year and a record of my thoughts at the time of writing.\nI hope it’s fun to read or, at the very least, provides some interesting insights.\nIf it’s me rereading this, welcome back. This is Dylan from 2023."
  },
  {
    "objectID": "posts/2023-personal-snapshot.html#what-went-well",
    "href": "posts/2023-personal-snapshot.html#what-went-well",
    "title": "2023: Personal Snapshot",
    "section": "What went well?",
    "text": "What went well?\nAccording to my GitHub account, I committed code 276 out of 365 days in 2023. I likely coded even more since there were days when I didn’t commit my code. Also, sometimes I used a different account for projects due to company policy.\n\n\n\nSquint to see the secret message\n\n\nThe more I code the more I learn. So, as long as I stay honest and avoid committing useless code, coding serves as a good proxy of how much I’ve learned about technical topics throughout the year. This year, I felt I learned a lot, so I’m happy.\nDespite reaching an all-time high compared to previous years, I think I could have achieved more. Sometimes, especially between projects or during periods when I felt burned out, I procrastinated quite a bit. Next year, I want to improve this.\nI shipped five AI apps (not including client work). I developed four on my own and created one with a friend. Two of them made it to the front page of Hacker News and got featured in The Economist. Another one won brownie points in an AI Shark Tank.\n\n\n\nAI apps I built in 2023\n\n\nI also wrote 9 blog posts. I was very motivated at first and worked with a good friend, Jing, as an accountability partner. We ended up prioritizing other projects and stopped the accountability challenge, but it was fun and effective while it lasted. I’m very grateful to Jing for joining me in this challenge.\nI focused on writing tutorials about AI topics, but I got tired after a while. Writing high-quality tutorials demands a lot of effort. Plus, AI evolves so fast that my tutorials often become outdated within months. That sucked!\n\n\n\nMy stats looking like 💩\n\n\nWhen I stopped writing regularly, my site’s traffic started to drop. I also believe ChatGPT was a major factor. My basic-level tutorials, which ChatGPT can easily replace, have stopped growing.\nI’m making a change this year in my content creation strategy. I’ll focus on creating videos for technical topics and write about evergreen subjects on my blog. I’ve realized that making videos for technical content is a time-saver compared to writing tutorials. Showing users directly through videos is simpler than writing detailed descriptions or taking screenshots and explaining them.\nMy original plan for my blog was to share my thoughts. I ended up writing technical tutorials because it was a more effective way to drive traffic, but in all honesty, I didn’t enjoy it very much. So I’m going back to my original plan.\nI posted more or less consistently on LinkedIn throughout the year. I got roughly 420k views on my posts. A third of those views came from a single post, and I gained ~2.3k followers.\n\n\n\nThe outcome of spamming LinkedIn\n\n\nI met lots of great people and had tons of catch-ups this year. I am grateful for all the people I met this year. Special thanks go to Max, Emanuel, Sebastián, Edu, and Rhys for the collaborations we did. Not all things went as planned, but we had fun.\nPosting random things online helps me chat, befriend, learn from, and even do business with people I wouldn’t have met otherwise. It feels great!\nI did more sales this year. I took part in two 6-figure proposals that didn’t pan out, and landed two 5-figure contracts, each setting a new record for my hourly rate. I also sold quite a few small projects. Selling is fun but I’m not great at it. This is one of the focuses for next year.\nFinancially, things went well. Despite being more focused on learning than on making money this year, I made ~2.5x my annual burn rate (I’m frugal!). All my income was made through freelancing, and 68% of my income came from a single project. This situation is better than the past two years because I’m a bit less dependent on one client. But I’m still not where I want to be. To lower my risk, I want to spread my income more evenly across different clients.\nHealthwise, this year went well. I completed 169 strength training sessions, averaging 3 gym visits per week. I’m pretty happy with that. I faced some minor injuries but dealt with them effectively.\nAlso, I completed 4,711 minutes of Z2 training (roughly 90 minutes per week). For Z2, I experimented with running, cycling, and stair-climbing. Running is my favorite, but stair-climbing lets me multitask. So, I mostly split my time between these two activities.\nThis year, I gave stand-up comedy a shot, all thanks to my wife. I’ve wanted to try it for a long time but kept coming up with excuses. For our fifth anniversary, she surprised me by signing me up for a course. It’s my favorite gift since getting a Game Boy Advance twenty years ago.\n\n\n\nMe, holding a fart on stage\n\n\nFor those who know me in real life, you’ll know that I don’t enjoy speaking in public. So this was a real challenge for me. I did a presentation with family and friends and went to four open mics.\nIT WAS GREAT! Even though I felt terrified before stepping onto the stage, I ended up having a lot of fun. Most of my jokes made people laugh, which felt comforting.\nI do need to work on my stage presence. I often stood in awkward places, struggled to make eye contact with the audience during punchlines, and frequently said “Ehhh… Uhhh…”\nI’ve opened an IG account for my comedy stuff. I haven’t posted anything yet, but I’m planning to start in the next few weeks. For now, I will post in Spanish. Follow me there if you want to stay updated!\nFinally, this year has been good for my relationships with my wife, family, and friends. My wife and I continue to work as a team, each of us progressing in our respective areas and supporting one another. And I made time to enjoy with family and friends.\nIn my last snapshot, I didn’t include any photos. When I revised it a few days ago, I realized I missed having them. So, this time, I’ve included a few.\n\n\nClick to see photos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis year, a family highlight was my in-laws’ visit to Madrid for a few months. We found out that our family loves karaoke, especially my dad and my father-in-law. It was tough to get them off the microphone!\nMore importantly, my loved ones remain healthy and happy, and so am I."
  },
  {
    "objectID": "posts/2023-personal-snapshot.html#what-didnt-go-well",
    "href": "posts/2023-personal-snapshot.html#what-didnt-go-well",
    "title": "2023: Personal Snapshot",
    "section": "What didn’t go well?",
    "text": "What didn’t go well?\nI didn’t write a 2022 personal snapshot. That wasn’t a great way to start 2023.\nAfter Entrepreneur First, I felt burned out and lacked the motivation to write a detailed review of the past year. I kept putting it off until it was way too late. And nobody wants to read your annual review in August!\nI failed to get funding twice this year. First from Entrepreneur First, and second, from Speedinvest. Pitching, preparing the materials, and all the discussions involved were a cool experience, but the outcome was frustrating both times.\nI’m not sure it would have worked out. In both cases, I wasn’t too excited about the idea, but I thought it was worth a try. My main concern with raising venture capital money too early often leads to poor financial outcomes for founders. That’s a no-no for me.\nThough, bootstrapping isn’t easy either. My most successful AI product made 60€. Just enough for a decent dinner for two in Madrid.\n\n\n\nMy “best” AI product\n\n\nThe AI apps I built this year landed me several freelancing projects. But they made little money by themselves. To be fair, I only launched two products you could pay for. I didn’t intend the others to make money, at least not initially.\nThis year, I finally grasped something you might find obvious: to have products that generate revenue, you must be intentional about it. You must create opportunities for people to pay.\nI used to believe that if I built something cool enough, people would discover it and somehow find a way to pay for it. I was wrong. I realized that I must actively set up a payment system for my products or deliberately plan how to monetize the attention they receive. Without that, after the initial burst of attention fades, you might end up with nothing. Many of my projects this year suffered this fate.\nI’ve always dreamed of creating a product, so I ignored the market’s pull towards a consulting company. Instead of doubling down on the demand, I often said no, aiming to focus on my product ideas. After spending much of this year like a person with a hammer in search of a nail, I’ve decided to reverse my approach.\n\n\n\nMe running away from the demand\n\n\nNow, I’ll start with the demand and then figure out how to offer services to meet it. My focus will be on establishing a consulting practice. From there, I might develop a product. But then again, maybe I don’t need to. I’m content with the idea of getting rich through a services-only company 😉\nAlthough I’m satisfied with my physical fitness, I’ve noticed that I often don’t rest enough. This happens either because I don’t sleep enough or because I overtrain. I’ve started feeling some symptoms of this, so improving my rest is a goal for next year."
  },
  {
    "objectID": "posts/2023-personal-snapshot.html#what-are-the-plans-for-next-year",
    "href": "posts/2023-personal-snapshot.html#what-are-the-plans-for-next-year",
    "title": "2023: Personal Snapshot",
    "section": "What are the plans for next year?",
    "text": "What are the plans for next year?\nFirst, focus on building a successful consulting practice. I’ll be doing a lot more sales this year and plan to share updates frequently. I have some exciting news about this that I’ll be sharing soon.\nNext, establishing a sustainable content engine. I believe shifting to video for technical content might yield a higher ROI than text. So, this year, I plan to explore this approach and reserve the blog for more personal articles.\nKeep doing stand-up comedy. I haven’t found such an enjoyable hobby in a long time. I want to do more of it. It also brings extra benefits, like improving my sales skills.\nFinally, make time for rest. I often struggle with guilt over taking breaks, leading to very few days off throughout the year. But this isn’t good for my mental and physical well-being. I aim to create more space for rest, and I’m sure my wife and family will appreciate that too 😁"
  },
  {
    "objectID": "posts/how-to-use-github-deploy-keys.html",
    "href": "posts/how-to-use-github-deploy-keys.html",
    "title": "How to Use GitHub Deploy Keys",
    "section": "",
    "text": "Deployment is more of an art than science. Ask ten developers, and you’ll end up with ten different ways to deploy your app. However, there are a few topics on which everyone agrees. One of those is using deploy keys.\nA GitHub deploy key is an SSH key that gives read –and optionally write– access to a single repository on GitHub. It makes it easy to pull your app’s code to a server automatically.\nWith a deploy key, you just connect to your server, do a git fetch, and you’re done! Forget about connecting with an SFTP and uploading your files manually.\nThis tutorial will show you how to use GitHub deploy keys in your next project. Let’s get started!"
  },
  {
    "objectID": "posts/how-to-use-github-deploy-keys.html#prerequisites",
    "href": "posts/how-to-use-github-deploy-keys.html#prerequisites",
    "title": "How to Use GitHub Deploy Keys",
    "section": "Prerequisites",
    "text": "Prerequisites\nIf you’re planning on using deploy keys, I suppose you have something to deploy, right? 🤔\nBefore you continue with the next sections, make sure that:\n\nYou have a repository with your app’s code on GitHub.\nYou have access to a Linux server with git installed.\n\nIf so, continue to the next section."
  },
  {
    "objectID": "posts/how-to-use-github-deploy-keys.html#create-an-ssh-key-on-your-server",
    "href": "posts/how-to-use-github-deploy-keys.html#create-an-ssh-key-on-your-server",
    "title": "How to Use GitHub Deploy Keys",
    "section": "Create an SSH Key on Your Server",
    "text": "Create an SSH Key on Your Server\nA GitHub deploy key is, ultimately, an SSH key. So you’ll start by creating an SSH key on your server.\nFirst, connect to your server and open a terminal. Once you’re in, create an SSH key by executing the following command:\nssh-keygen -t ed25519 -C \"USERNAME@EMAIL.com\"\n‌This command will create and store an SSH key. You use two parameters:\n\n-t ed25519 to specify which algorithm to use to generate the key. You chose  ED25519, a very secure and efficient algorithm.\n-C USERNAME@EMAIL.com to append your email as a comment at the end of the key. Make sure to replace USERNAME@EMAIL.com with your actual email.\n\nYou can learn more about these parameters here. But don’t worry, you don’t need to know much more to use GitHub deploy keys.\nAfter running the command, you’ll get a couple of questions asking you to specify a path and set a passphrase for the key. You can save the key at the default path and skip setting a passphrase. So just hit Enter when prompted."
  },
  {
    "objectID": "posts/how-to-use-github-deploy-keys.html#add-the-key-to-ssh-config",
    "href": "posts/how-to-use-github-deploy-keys.html#add-the-key-to-ssh-config",
    "title": "How to Use GitHub Deploy Keys",
    "section": "Add the Key to SSH Config",
    "text": "Add the Key to SSH Config\nNext, you’ll create an SSH config file. This file allows you to define the instructions required to connect to remote servers using SSH. It’s an easy way to manage multiple servers’ SSH keys or to keep deploy keys of different repositories in the same server.\nYou can create and edit the SSH config file by running these commands:\ntouch ~/.ssh/config\nchmod 600 ~/.ssh/config\nvim ~/.ssh/config # Or: nano ~/.ssh/config\nThese commands will create an SSH config file, set the correct permissions, and open it using vim. You can also use nano to open the file if you’re unfamiliar with vim’s shortcuts.\nAn SSH config file consists of sections specifying the instructions required for each server you’ll connect to. So, in this case, you’ll provide the instructions needed to connect to your repository. For that, copy this text to the file:\nHost github-YOUR-APP\n    HostName github.com\n    AddKeysToAgent yes\n    PreferredAuthentications publickey\n    IdentityFile ~/.ssh/id_ed25519\n‌In the code snippet above, you provide the settings required to connect to GitHub using the SSH key you just created. You specify the following parameters:\n\nHost: the name you’ll use in the terminal when referring to this server. Choose a name that’s easy to remember.\nHostName: the real hostname that you’ll connect to. In this case, github.com.\nAddKeysToAgent: specifies if it should add the private key to the ssh-agent.\nPreferredAuthentications: order in which the client tries authentication methods. In this case, you only use your publickey.\nIdentityFile: specifies a file from which the key is read. You need to specify the name of the private key you generated earlier. If you used the default name, it should be ~/.ssh/id_ed25519.\n\nThat’s all you need to do on the server. Next, you’ll use your SSH key to create a deploy key on GitHub."
  },
  {
    "objectID": "posts/how-to-use-github-deploy-keys.html#create-a-deploy-key-on-github",
    "href": "posts/how-to-use-github-deploy-keys.html#create-a-deploy-key-on-github",
    "title": "How to Use GitHub Deploy Keys",
    "section": "Create a Deploy Key on GitHub",
    "text": "Create a Deploy Key on GitHub\nFirst, copy the public key from the SSH key you created earlier. The easiest way of doing that is to run:\ncat ~/.ssh/id_ed25519.pub\nThen select and copy the resulting text from the command line. It should look something like this:\nssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAILs35pzG5jZakTEHDWeRErgkAmabhQj2yj/onxlIQgli USERNAME@EMAIL.com\n‌Next, open your preferred browser and go to the repository with your app’s code on GitHub. Click on Settings, select Deploy keys, and then click on Add deploy key.\n\nAdd deploy key on GitHub\nCopy the key in the Key textbox and set a title to the key. You can leave Allow write access unchecked and click on Add key. Allow write access allows you to make changes to the repository using the deploy key. For security reasons, you don’t want to do that in most cases.\n\nNext, you can go back to your server’s terminal and clone your repository. You can do that by running this command:\ngit clone git@github-YOUR-APP:dylanjcastillo/random.git\nThis will clone your app’s repository to your server. Please remember to replace github-YOUR-APP by the Host you specified in the SSH config file. Otherwise, it won’t work.\nThat’s all!"
  },
  {
    "objectID": "posts/how-to-use-github-deploy-keys.html#conclusion",
    "href": "posts/how-to-use-github-deploy-keys.html#conclusion",
    "title": "How to Use GitHub Deploy Keys",
    "section": "Conclusion",
    "text": "Conclusion\nThis tutorial showed you how to create and use GitHub deploy keys. Using a deploy key will save you time and provide you with a safer way to deploy your app.\nIn this tutorial, you’ve learned:\n\nHow to create an SSH key on your server.\nWhat an SSH config file is and how to use it.\nHow to add a deploy key to your GitHub repository.\n\nIf you have any questions or feedback, let me know in the comments!"
  },
  {
    "objectID": "posts/fast-and-async-in-python-accelerate-your-requests-using-asyncio.html",
    "href": "posts/fast-and-async-in-python-accelerate-your-requests-using-asyncio.html",
    "title": "Fast & Asynchronous In Python: Accelerate Your Requests Using asyncio",
    "section": "",
    "text": "A few weeks ago, I was working on a Python script to extract books’ metadata for a content-based recommender. After a couple of hours, I realized that I needed to make thousands of requests to the Google Books API to get the data. So I thought there had to be a way of speeding up the process.\nAs I enjoy learning, especially when it’s also a chance of procrastinating on my goals, I decided to build a project using asyncio. Afterward, feeling guilty for the time wasted, I decided to write this tutorial with what I learned in the process.\nThis article will show you how to use asyncio and aiohttp to do asynchronous requests to an API. It’s mostly focused on the code, apart from the short introduction below, so if you are looking for a more in-depth introduction to asyncio, check the recommendations in the references1."
  },
  {
    "objectID": "posts/fast-and-async-in-python-accelerate-your-requests-using-asyncio.html#asyncio-in-30-seconds-or-less",
    "href": "posts/fast-and-async-in-python-accelerate-your-requests-using-asyncio.html#asyncio-in-30-seconds-or-less",
    "title": "Fast & Asynchronous In Python: Accelerate Your Requests Using asyncio",
    "section": "asyncio in 30 Seconds or Less",
    "text": "asyncio in 30 Seconds or Less\nasyncio is a Python library that allows you to execute some tasks in a seemingly concurrent2 manner. It is commonly used in web-servers and database connections. It is also useful for speeding up IO-bound tasks, like services that require making many requests or do lots of waiting for external APIs3.\nThe essence of asyncio is that it allows the program to continue executing other instructions while waiting for specific processes to finish (e.g., a request to an API). In this tutorial, you will see how to use asyncio for accelerating a program that makes multiple requests to an API."
  },
  {
    "objectID": "posts/fast-and-async-in-python-accelerate-your-requests-using-asyncio.html#sequential-vs.-asynchronous",
    "href": "posts/fast-and-async-in-python-accelerate-your-requests-using-asyncio.html#sequential-vs.-asynchronous",
    "title": "Fast & Asynchronous In Python: Accelerate Your Requests Using asyncio",
    "section": "Sequential vs. Asynchronous",
    "text": "Sequential vs. Asynchronous\nSo let’s get down to business. To get the most out of this tutorial, try running the code yourself. These code snippets have been tested with Python 3.8.3. You can try running them with a more recent version, but they might require some small changes.\nBefore running the code, you need to install the required libraries: requests, and aiohttp. Then, you can run the code snippets below in a Jupyter Notebook. If you’d like to run the snippets using a Python script, you’ll need to do some small changes to get it working.\nYou’ll build a sequential and an asynchronous version of a small program and compare their results and structure. Both programs will do the same:\n\nRead a list of ISBNs (international identifier of books)\nRequest the books’ metadata to the Google Books API\nParse the results from the requests\nPrint the results to the screen.\n\nThe algorithm would look something like the diagram below.\n\nDiagram of algorithm\nThere’s two possible approaches for building this program. First, Option A, which executes the requests sequentially. Or, Option B, which uses asyncio to run requests asynchronously.\n\nOption A: Sequential Algorithm\nA sequential version of that algorithm could look as follows:\nimport os\nimport requests\nfrom requests.exceptions import HTTPError\n\n\nGOOGLE_BOOKS_URL = \"https://www.googleapis.com/books/v1/volumes?q=isbn:\"\nLIST_ISBN = [\n    '9780002005883',\n    '9780002238304',\n    '9780002261982',\n    '9780006163831',\n    '9780006178736',\n    '9780006280897',\n    '9780006280934',\n    '9780006353287',\n    '9780006380832',\n    '9780006470229',\n]\n\n\ndef extract_fields_from_response(item):\n    \"\"\"Extract fields from API's response\"\"\"\n    volume_info = item.get(\"volumeInfo\", {})\n    title = volume_info.get(\"title\", None)\n    subtitle = volume_info.get(\"subtitle\", None)\n    description = volume_info.get(\"description\", None)\n    published_date = volume_info.get(\"publishedDate\", None)\n    return (\n        title,\n        subtitle,\n        description,\n        published_date,\n    )\n\n\ndef get_book_details_seq(isbn, session):\n    \"\"\"Get book details using Google Books API (sequentially)\"\"\"\n    url = GOOGLE_BOOKS_URL + isbn\n    response = None\n    try:\n        response = session.get(url)\n        response.raise_for_status()\n        print(f\"Response status ({url}): {response.status_code}\")\n    except HTTPError as http_err:\n        print(f\"HTTP error occurred: {http_err}\")\n    except Exception as err:\n        print(f\"An error ocurred: {err}\")\n    response_json = response.json()\n    items = response_json.get(\"items\", [{}])[0]\n    return items\n\n\nwith requests.Session() as session:\n    for isbn in LIST_ISBN:\n        try:\n            response = get_book_details_seq(isbn, session)\n            parsed_response = extract_fields_from_response(response)\n            print(f\"Response: {json.dumps(parsed_response, indent=2)}\")\n        except Exception as err:\n            print(f\"Exception occured: {err}\")\n            pass\nSequential version of algorithm\nNow, let’s breakdown the code to understand what’s going on.\nAs usual, you start by importing the required libraries. Then, you define two variables:\n\nGOOGLE_BOOKS_URL for specifying the URL of the Google’s API we’ll use for the requests. This is how a request to the Google Books API looks like: https://www.googleapis.com/books/v1/volumes?q=isbn:9780002005883\nLIST_ISBN, which is a sample list of ISBNs for testing the program.\n\nimport os\nimport requests\nfrom requests.exceptions import HTTPError\n\nGOOGLE_BOOKS_URL = \"https://www.googleapis.com/books/v1/volumes?q=isbn:\"\nLIST_ISBN = [\n    '9780002005883',\n    '9780002238304',\n    '9780002261982',\n    '9780006163831',\n    '9780006178736',\n    '9780006280897',\n    '9780006280934',\n    '9780006353287',\n    '9780006380832',\n    '9780006470229',\n]\nNext, you define the extract_fields_from_responsefunction. This function takes as input the response from the API and extracts the fields we’re interested in.\ndef extract_fields_from_response(response):\n    \"\"\"Extract fields from API's response\"\"\"\n    item = response.get(\"items\", [{}])[0]\n    volume_info = item.get(\"volumeInfo\", {})\n    title = volume_info.get(\"title\", None)\n    subtitle = volume_info.get(\"subtitle\", None)\n    description = volume_info.get(\"description\", None)\n    published_date = volume_info.get(\"publishedDate\", None)\n    return (\n        title,\n        subtitle,\n        description,\n        published_date,\n    )\nFunction for parsing response from the Google Books API\nThe parsing process in extract_fields_from_response is based on the response’s structure from the Google Books API, which looks as follows:\n{\n \"kind\": \"books#volumes\",\n \"totalItems\": 1,\n \"items\": [\n  {\n   \"kind\": \"books#volume\",\n   \"id\": \"3Mx4QgAACAAJ\",\n   \"etag\": \"FWJF/JY16xg\",\n   \"selfLink\": \"https://www.googleapis.com/books/v1/volumes/3Mx4QgAACAAJ\",\n   \"volumeInfo\": {\n    \"title\": \"Mapping the Big Picture\",\n    \"subtitle\": \"Integrating Curriculum and Assessment, K-12\",\n    ...\nSample response from the Google Books API\nFinally, take a look at the most relevant parts of the program: how you make requests to the Google Books API.\ndef get_book_details_seq(isbn, session):\n    \"\"\"Get book details using Google Books API (sequentially)\"\"\"\n    url = GOOGLE_BOOKS_URL + isbn\n    response = None\n    try:\n        response = session.get(url)\n        response.raise_for_status()\n        print(f\"Response status ({url}): {response.status_code}\")\n    except HTTPError as http_err:\n        print(f\"HTTP error occurred: {http_err}\")\n    except Exception as err:\n        print(f\"An error ocurred: {err}\")\n    response_json = response.json()\n    items = response_json.get(\"items\", [{}])[0]\n    return items\n\n\nwith requests.Session() as session:\n    for isbn in LIST_ISBN:\n        try:\n            response = get_book_details_seq(isbn, session)\n            parsed_response = extract_fields_from_response(response)\n            print(f\"Response: {json.dumps(parsed_response, indent=2)}\")\n        except Exception as err:\n            print(f\"Exception occured: {err}\")\n            pass\nHow requests are executed to the Google Books API\nThere are two major pieces here:\n\nget_book_details_seq, which is the function that executes the requests. It takes as input an ISBN and a session object4 and returns the response from the API as a JSON structure. It also handles possible errors, like providing a wrong URL or going over your daily quota of requests.\nThe code block under with requests.Session() as session, is where the full pipeline is orchestrated. It iterates through the list of ISBNs, gets the books’ details, parses them, and finally prints the details to the screen.\n\nFor me, executing this process varies from 4 to 6 seconds. If you only need to do this a couple of times, you will not find much benefit from using asyncio. However, if instead of 10 requests, you need to do 10,000, having some concurrency in your program pays out. In the next section, you’ll see how to make this algorithm faster using asyncio.\n\n\nOption B: Asynchronous Algorithm\nAn asynchronous version of the same algorithm may look something as follows:\nimport aiohttp\nimport asyncio\nimport os\n\nfrom aiohttp import ClientSession\n\n\nGOOGLE_BOOKS_URL = \"https://www.googleapis.com/books/v1/volumes?q=isbn:\"\nLIST_ISBN = [\n    '9780002005883',\n    '9780002238304',\n    '9780002261982',\n    '9780006163831',\n    '9780006178736',\n    '9780006280897',\n    '9780006280934',\n    '9780006353287',\n    '9780006380832',\n    '9780006470229',\n]\n\n\ndef extract_fields_from_response(response):\n    \"\"\"Extract fields from API's response\"\"\"\n    item = response.get(\"items\", [{}])[0]\n    volume_info = item.get(\"volumeInfo\", {})\n    title = volume_info.get(\"title\", None)\n    subtitle = volume_info.get(\"subtitle\", None)\n    description = volume_info.get(\"description\", None)\n    published_date = volume_info.get(\"publishedDate\", None)\n    return (\n        title,\n        subtitle,\n        description,\n        published_date,\n    )\n\n\nasync def get_book_details_async(isbn, session):\n    \"\"\"Get book details using Google Books API (asynchronously)\"\"\"\n    url = GOOGLE_BOOKS_URL + isbn\n    try:\n        response = await session.request(method='GET', url=url)\n        response.raise_for_status()\n        print(f\"Response status ({url}): {response.status}\")\n    except HTTPError as http_err:\n        print(f\"HTTP error occurred: {http_err}\")\n    except Exception as err:\n        print(f\"An error ocurred: {err}\")\n    response_json = await response.json()\n    return response_json\n\n\nasync def run_program(isbn, session):\n    \"\"\"Wrapper for running program in an asynchronous manner\"\"\"\n    try:\n        response = await get_book_details_async(isbn, session)\n        parsed_response = extract_fields_from_response(response)\n        print(f\"Response: {json.dumps(parsed_response, indent=2)}\")\n    except Exception as err:\n        print(f\"Exception occured: {err}\")\n        pass\n\nasync with ClientSession() as session:\n    await asyncio.gather(*[run_program(isbn, session) for isbn in LIST_ISBN])\nAsynchronous version of the program using asyncio and aiohttp\nFirst, check the get_book_details_async function. An async keyword prepends it. This keyword tells Python that your function is a coroutine. Then, in the function’s body, there are two await keywords. These tell that coroutine to suspend execution and give back control to the event loop, while the operation the couroutine is awaiting finishes.\nA coroutine is a type of generator function in Python that, instead of producing values, consumes values5. The interesting thing about it is that its execution pauses while waiting for new data being sent to it. In our case, this allows the execution of other parts of the program to continue in a seemingly concurrent manner.\nIn this case, the execution of get_book_details_async is suspended while the request is being performed: await session.request(method='GET', url=url). It is suspended again, while the request response is being parsed into a JSON structure: await response.json().\nNext, we have the run_program coroutine. This one is simply a wrapper around the pipeline of getting a response from the API, parsing it, and printing the results in the screen. It awaits the execution of the get_book_details_async coroutine.\nFinally, we have the code block under async with ClientSession() as session:. Using the asyncio.gather syntax, we tell the program to schedule all the tasks based on the list of coroutines we provided. This is what allows us to execute tasks concurrently.\nFor me, running this process takes around 800-1000 milliseconds.\n\n\nResults\nComparing both versions, we see that the asynchronous one is around 4 to 7.5 times faster than the sequential version. If we increase the number of requests, you’ll likely get an even higher speedup. Besides, the version using asyncio is almost as simple as the sequential version. That makes asyncio an excellent option for the kind of task we reviewed in the tutorial."
  },
  {
    "objectID": "posts/fast-and-async-in-python-accelerate-your-requests-using-asyncio.html#additional-recommendations",
    "href": "posts/fast-and-async-in-python-accelerate-your-requests-using-asyncio.html#additional-recommendations",
    "title": "Fast & Asynchronous In Python: Accelerate Your Requests Using asyncio",
    "section": "Additional recommendations",
    "text": "Additional recommendations\nHere are some tips I gathered while working with asyncio:\n\nasyncio keeps changing all the time, so be wary of old Stack Overflow answers. Many of them are not up to date with the current best practices\nExternal APIs will not allow you to run unlimited concurrent requests. To overcome that, take a look at asyncio’sSemaphore. It will enable you to limit the concurrency of your application.\nNot all programs can be speedup with asyncio. Research the type of issue you are facing before doing any substantial modification of code. Other alternatives might work for you (e.g., threading, multiprocessing)\nI made a complete version of the program we went through in this tutorial for getting the metadata of almost 7 thousand books. Here’s a link to it: Google Books Crawler."
  },
  {
    "objectID": "posts/fast-and-async-in-python-accelerate-your-requests-using-asyncio.html#notes-and-references",
    "href": "posts/fast-and-async-in-python-accelerate-your-requests-using-asyncio.html#notes-and-references",
    "title": "Fast & Asynchronous In Python: Accelerate Your Requests Using asyncio",
    "section": "Notes and References",
    "text": "Notes and References\n[1] Real Python has a two of amazing articles introducing asyncio: Async IO in Python and Speed Up Your Python Program With Concurrency\n[2] It is not strictly concurrent execution. But in practical terms, it looks like it is.\n[3] S. Buczyński, What Is the use case of coroutines and asyncio in Python 3.6? (2017)\n[4] The session object is a functionality from the requests library that allows you to persist certain parameters across sessions. This usually results in requests with lower latency. Read more here.\n[5] D. Beasly, A Curious Course on Couroutines and Concurrency (2009)\n[6] Cover image by Marc-Olivier Jodoin"
  },
  {
    "objectID": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html",
    "href": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html",
    "title": "Build an AI Search Engine Using FastAPI, Qdrant, and ChatGPT",
    "section": "",
    "text": "With all the buzz surrounding Bing AI and Bard, I was keen on building a (tiny) AI search engine myself. After a few days of tinkering, I released Ask Seneca. It’s a small app that allows you to consult a GPT-based Seneca who answers your questions and cites his sources.\nWhen a user asks a question, Ask Seneca searches for Seneca’s most relevant writings to answer that question and then summarizes those writings into a coherent answer. I built it using FastAPI, Qdrant, Sentence Transformers, and GPT-3. I recently updated it to use the ChatGPT API.\nDespite the setbacks that Bing AI and Bard are facing, the potential for this technology is vast - you could build tools for quick and efficient searches through legal documents, internal knowledge bases, product manuals, and more.\nIn this tutorial, I’ll show you how to build your own AI search engine. You’ll create an app that lets users ask questions to a GPT-based Marcus Aurelius, and provides them with concise answers and references to his Meditations.\nLet’s get to it!"
  },
  {
    "objectID": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#prerequisites",
    "href": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#prerequisites",
    "title": "Build an AI Search Engine Using FastAPI, Qdrant, and ChatGPT",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo make the most out of this tutorial, you should know:\n\nWhat semantic search is.\nWhat vector databases are.\nWhat FastAPI is and how to use it.\n\nYou don’t have to be an expert in any of these areas, but familiarity with them will help you understand the sections that follow."
  },
  {
    "objectID": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#designing-a-tiny-search-engine-with-chatgpt",
    "href": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#designing-a-tiny-search-engine-with-chatgpt",
    "title": "Build an AI Search Engine Using FastAPI, Qdrant, and ChatGPT",
    "section": "Designing a (Tiny) Search Engine with ChatGPT",
    "text": "Designing a (Tiny) Search Engine with ChatGPT\nBefore you get started, you should understand the overall approach you’ll take to build your AI search engine. There are three parts to it:\n\nExtraction: This part consists of extracting the data that you want users to be able to search. In this case, that means parsing Meditations. I won’t go into detail about this because it is very project-specific. The parsed data is available in the repository.\nIndexing: This entails indexing the extracted data so that it can be accessed later when running searches. In this case, you’ll use a semantic search approach, which means you’ll search the data based on its meaning rather than keywords. That is, if you search for “How can I be happy?” you should get passages from Meditations that discuss happiness or feeling good, not just those that contain the exact words from the query.\nSearch: This consists of a backend service that processes the user’s query, vectorizes it, finds vectors in the index that are the most similar to it, and then calls OpenAI’s API to generate a summarized answer for the user.\n\nHere’s a visual representation of how the parts of the application you’ll build in this tutorial fit together:\n\nThat’s all. Let’s continue!"
  },
  {
    "objectID": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#set-up-your-local-environment",
    "href": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#set-up-your-local-environment",
    "title": "Build an AI Search Engine Using FastAPI, Qdrant, and ChatGPT",
    "section": "Set Up Your Local Environment",
    "text": "Set Up Your Local Environment\nTake the following steps to prepare your local environment:\n\nInstall Python 3.10.\nInstall Poetry. It’s not mandatory but I highly recommend it.\nClone the repository with the sample app:\n\ngit clone https://github.com/dylanjcastillo/ai-search-fastapi-qdrant-chatgpt\n\nGo to the root folder of the project and install the dependencies with:\n\nPoetry: Create the virtual environment in the same directory as the project and install the dependencies:\n\npoetry config virtualenvs.in-project true\npoetry install\n\nvenv and pip: Create a virtual environment and install the dependencies listed in requirements.txt:\n\npython3.10 -m venv .venv && source .venv/bin/activate\npip install -r requirements.txt\n\n\n\n\n\n\n\nNote\n\n\n\nBecause PyTorch does not yet support Python 3.11 in MacOS and Windows, this tutorial will not work if you are running Python 3.11 on those operating systems.\n\n\nIf everything went well, you should have a virtual environment with all of the necessary libraries and a project structure that looks like this:\nai-search-fastapi-qdrant-gpt3\n│\n├── README.md\n├── config.py\n├── data\n│   ├── processed\n│   │   └── Marcus_Aurelius_Antoninus...\n│   │       └── Marcus_Aurelius_Antoninus...json\n│   └── unzipped\n│       └── Marcus_Aurelius_Antoninus...\n│           ├── index.html\n│           ├── metadata.opf\n│           └── style.css\n├── main.py\n├── notebooks\n│   ├── extract_text.ipynb\n│   └── vectorize_text.ipynb\n├── poetry.lock\n├── pyproject.toml\n├── requirements.txt\n├── .env-example\n└── .venv/\nThis is your project’s structure. Let me explain the purpose of the most important files and directories:\n\nconfig.py: This file contains project configuration specifications such as Qdrant’s host, port, and API key (read from a .env file)\ndata/: This directory contains the project’s data. It contains Meditations as originally downloaded from Wikisource as well as the processed file that you will use in the project.\nmain.py: This file contains the code of the FastAPI application.\nnotebooks/: This directory contains Jupyter notebooks for extracting, vectorizing, and indexing the data. extract_text.ipynb contains code to parse the HTML file and vectorize_text.ipynb contains code to vectorize and index the data.\npoetry.lock and pyproject.toml: These files contain information about the project’s dependencies and are used by Poetry to replicate the environment.\nrequirements.txt: This file contains a list of Python packages required by the project and their respective versions.\n.env-example: This file is an example of the environment variables you must provide.\n.venv/: This directory contains the project’s virtual environment.\n\nThat’s it! You’re now ready to get started."
  },
  {
    "objectID": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#configure-qdrant-and-openai",
    "href": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#configure-qdrant-and-openai",
    "title": "Build an AI Search Engine Using FastAPI, Qdrant, and ChatGPT",
    "section": "Configure Qdrant and OpenAI",
    "text": "Configure Qdrant and OpenAI\nStart by renaming .env-example to .env. Don’t worry about filling in the values in .env. After you’ve created a cluster and the API keys for Qdrant and OpenAI, you’ll fill in the blanks.\n\nQdrant\nCreate an account at Qdrant, if you don’t already have one. Then, on your account page go to Clusters &gt; Create, and create a cluster of 1GB of RAM, 0.5 vCPU, and 20GB Disk. Qdrant has a generous free tier, and it’s free to run a cluster with those specifications.\n\nNext, paste the host and API key you obtained when you created your cluster into .env:\nQDRANT_PORT=6333\nQDRANT_HOST=&lt;your_qdrant_host&gt;\nQDRANT_API_KEY=&lt;your_qdrant_api_key&gt;\nIf you didn’t copy the key, you can still create a new one in Access.\nFinally, you can test that everything went well by running the first three cells in vectorize_data.ipynb.\n\n\nOpenAI\nIf you don’t have an OpenAI account, create one. After that, go to Manage account &gt; API keys &gt;  + Create new secret key.\n\nThen, paste the generated key in .env:\nQDRANT_PORT=6333\nQDRANT_HOST=&lt;your_qdrant_host&gt;\nQDRANT_API_KEY=&lt;your_qdrant_api_key&gt;\nOPENAI_API_KEY=&lt;your_openai_api_key&gt; # new\nThat’s it! Let’s continue."
  },
  {
    "objectID": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#extract-data",
    "href": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#extract-data",
    "title": "Build an AI Search Engine Using FastAPI, Qdrant, and ChatGPT",
    "section": "Extract Data",
    "text": "Extract Data\nThe data extraction pipeline will vary greatly between projects, so I won’t go into too much detail here.\nHere are some useful guidelines to keep in mind when doing so:\n\nGarbage in, garbage out: The quality of your data will heavily influence your search results, so take your time with this step.\nSplitting documents: When you do semantic search, you need to divide documents into smaller chunks so that you can compare the similarity of each chunk to the user’s query. There is no right or wrong way to do this. In this case, I took a straightforward approach: divide the text into paragraphs, and if the paragraph is above a certain number of characters, divide it into multiple sentences.\nProduction: For real-world scenarios, you should think about how frequently you’ll be extracting and ingesting data, adapting your pipeline for different data sources (e.g., scraping, APIs), and building pipeline monitors, among other things. In this example, because data extraction is a one-time event, I’m using jupyter notebooks, which isn’t always a good idea.\n\nHere’s a sneak peek at the data from this tutorial:\n{\n    \"book_title\": \"Meditations by Marcus Aurelius\",\n    \"url\": \"https://en.wikisource.org/wiki/Marcus_Aurelius_Antoninus_-_His_Meditations_concerning_himselfe\",\n    \"data\": [\n        {\n            \"title\": \"THE FIRST BOOK\",\n            \"url\": \"https://en.wikisource.org/wiki/Marcus_Aurelius_Antoninus_-_His_Meditations_concerning_himselfe#THE_FIRST_BOOK\",\n            \"sentences\": [\n                \"I. Of my grandfather Verus I have learned to be gentle and meek...\",\n                \"II. Of him that brought me up, not to be fondly addicted...\",\n                \"III. Of Diognetus, not to busy myself about vain things...\",\n                \"IV. To Rusticus I am beholding, that I first entered into the...\",\n    ...\n        }\n    ]\n }\nThe tutorial’s data includes general metadata such as the book title and source URL, as well as information from each chapter with the sentences you’ll index.\nIf you want to take a look at how I extracted the data used in this tutorial, check out the [extract_data.ipynb](https://github.com/dylanjcastillo/ai-search-fastapi-qdrant-gpt3/blob/main/notebooks/extract_text.ipynb)."
  },
  {
    "objectID": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#vectorize-and-index-data",
    "href": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#vectorize-and-index-data",
    "title": "Build an AI Search Engine Using FastAPI, Qdrant, and ChatGPT",
    "section": "Vectorize and Index Data",
    "text": "Vectorize and Index Data\nOnce you’ve extracted the data, you’ll want to index it in your vector database.\nThe process consists of two steps:\n\nGenerate vectors for each sentence you extracted earlier.\nInsert those vectors in a collection(the set of vectors you can search in the vector database).\n\nYou can find the code for this section in notebooks/vectorize_data.ipynb.\nAs usual, you start by importing the required libraries:\nimport json\n\nimport numpy as np\nimport pandas as pd\nimport torch\n\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http import models\nfrom sentence_transformers import SentenceTransformer\n\nfrom tqdm.notebook import tqdm\n\nfrom config import QDRANT_HOST, QDRANT_PORT, QDRANT_API_KEY, DATA, COLLECTION_NAME\nThis code imports all the libraries and configuration variables you need to vectorize and index the data. Here are some things worth mentioning:\n\nqdrant_client and qdrant_client.http let you interact with Qdrant’s client, so that you can insert and retrieve data from the collection.\nsentence_transformers let you generate the vectors from text, using pretrained models.\n\nNext, you read the data as follows:\nBOOK_FILENAME = \"Marcus_Aurelius_Antoninus_-_His_Meditations_concerning_himselfe\"\n\nwith open(f\"{DATA}/processed/{BOOK_FILENAME}/{BOOK_FILENAME}.json\", \"r\") as file:\n    meditations_json = json.load(file)\n\nrows = []\nfor chapter in tqdm(meditations_json[\"data\"]):\n    for sentence in chapter[\"sentences\"]:\n        rows.append(\n            (\n                chapter[\"title\"],\n                chapter[\"url\"],\n                sentence,\n            )\n        )\n\ndf = pd.DataFrame(\n    data=rows, columns=[\"title\", \"url\", \"sentence\"]\n)\n\ndf = df[df[\"sentence\"].str.split().str.len() &gt; 15]\nThis code reads the previously processed data and removes short sentences. It works as follows:\n\nLines 1 to 4 read the JSON file you generated for Meditations, and save it as meditations_json.\nLines 6 to 15 go through all the chapters of the book stored in the data key from meditations_json and for each chapter, it extracts the relevant data (title of chapter, URL of chapter, and sentence), and adds it to rows.\nLines 17 to 21 create a DataFrame with the data from rows and removes the sentences with less than 15 words.\n\nNext, you create a collection in your vector database:\n# Create collection\nclient = QdrantClient(\n    host=QDRANT_HOST, port=QDRANT_PORT, api_key=QDRANT_API_KEY\n)\nclient.recreate_collection(\n    collection_name=COLLECTION_NAME,\n    vectors_config=models.VectorParams(\n        size=384,\n        distance=models.Distance.COSINE\n    ),\n)\nThis code connects to your Qdrant’s cluster and creates a collection based on the name and settings you provide. In this case, you set the size to 384 based on the needs of the model you’ll use for vectorizing the sentences. You also set distance to use Cosine distance, which will define how the similarity between vectors is computed.\nThe next step is to generate the vectors (embeddings) from the text. You’ll use a pretrained model from Sentence Transformers to generate them instead of OpenAI-based embeddings. The latter are more expensive and not necessarily better.\nTo accomplish this, you load the pretrained model, generate the embeddings from the DataFrame sentences, and insert them into the collection you created:\nmodel = SentenceTransformer(\n    \"msmarco-MiniLM-L-6-v3\",\n    device=\"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\",\n)\n\nvectors = []\nbatch_size = 512\nbatch = []\n\nfor doc in tqdm(df[\"sentence\"].to_list()):\n    batch.append(doc)\n\n    if len(batch) &gt;= batch_size:\n        vectors.append(model.encode(batch))\n        batch = []\n\nif len(batch) &gt; 0:\n    vectors.append(model.encode(batch))\n    batch = []\n\nvectors = np.concatenate(vectors)\n\nbook_name = meditations_json[\"book_title\"]\n\nclient.upsert(\n    collection_name=COLLECTION_NAME,\n    points=models.Batch(\n        ids=[i for i in range(df.shape[0])],\n        payloads=[\n            {\n                \"text\": row[\"sentence\"],\n                \"title\": row[\"title\"] + f\", {book_name}\",\n                \"url\": row[\"url\"],\n            }\n            for _, row in df.iterrows()\n        ],\n        vectors=[v.tolist() for v in vectors],\n    ),\n)\nThis code loads the model, generates vectors from the sentences in the DataFrame, and inserts them into the collection you created. Here’s how it works:\n\nLines 1 to 8 load the msmarco-MiniLM-L-6-v3 sentence transformer model, and set the correct device in case you have a GPU available.\nLines 10 to 23 generate an array of vectors using the model you loaded. Each vector is a numerical representation of the sentences from your DataFrame.\nLines 29 to 43 insert the vectors and the additional data (actual sentence, book and chapter title, and URL) into the collection in your vector database."
  },
  {
    "objectID": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#create-a-server-with-fastapi",
    "href": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#create-a-server-with-fastapi",
    "title": "Build an AI Search Engine Using FastAPI, Qdrant, and ChatGPT",
    "section": "Create a Server with FastAPI",
    "text": "Create a Server with FastAPI\nNext, you will create the FastAPI application that will let the user interact with your vector database and ChatGPT. The code for this section is in main.py.\nYou start by importing the required dependencies, setting up your Qdrant client, and loading the model:\nimport openai\nfrom fastapi import FastAPI\nfrom qdrant_client import QdrantClient\nfrom sentence_transformers import SentenceTransformer\n\nfrom config import (\n    COLLECTION_NAME,\n    OPENAI_API_KEY,\n    QDRANT_API_KEY,\n    QDRANT_HOST,\n    QDRANT_PORT,\n)\n\nopenai.api_key = OPENAI_API_KEY\n\nqdrant_client = QdrantClient(\n    host=QDRANT_HOST,\n    port=QDRANT_PORT,\n    api_key=QDRANT_API_KEY,\n)\n\nretrieval_model = SentenceTransformer(\"msmarco-MiniLM-L-6-v3\")\n\napp = FastAPI()\nThis code imports the libraries and configuration settings, initializes the Qdrant client, and loads the model to memory (the same one you used for vectorizing the sentences). You load your model globally so that you don’t slow down requests by loading it each time someone asks a question.\nNext, you define a function to help you create the prompt that you’ll use to get ChatGPT to generate a coherent answer based on the most relevant passages from Meditations:\ndef build_prompt(question: str, references: list) -&gt; tuple[str, str]:\n    prompt = f\"\"\"\n    You're Marcus Aurelius, emperor of Rome. You're giving advice to a friend who has asked you the following question: '{question}'\n\n    You've selected the most relevant passages from your writings to use as source for your answer. Cite them in your answer.\n\n    References:\n    \"\"\".strip()\n\n    references_text = \"\"\n\n    for i, reference in enumerate(references, start=1):\n        text = reference.payload[\"text\"].strip()\n        references_text += f\"\\n[{i}]: {text}\"\n\n    prompt += (\n        references_text\n        + \"\\nHow to cite a reference: This is a citation [1]. This one too [3]. And this is sentence with many citations [2][3].\\nAnswer:\"\n    )\n    return prompt, references_text\nThis code will combine a prompt to make ChatGPT “simulate” Marcus Aurelius answering a user-supplied question with a list of references previously obtained from your vector database. Then it will return the generated prompt, and a list of the references to add to the answer sent to the user.\nThen you create two endpoints as follows:\n@app.get(\"/\")\ndef read_root():\n    return {\n        \"message\": \"Make a post request to /ask to ask a question about Meditations by Marcus Aurelius\"\n    }\n\n\n@app.post(\"/ask\")\ndef ask(question: str):\n    similar_docs = qdrant_client.search(\n        collection_name=COLLECTION_NAME,\n        query_vector=retrieval_model.encode(question),\n        limit=3,\n        append_payload=True,\n    )\n\n    prompt, references = build_prompt(question, similar_docs)\n\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n        max_tokens=250,\n        temperature=0.2,\n    )\n\n    return {\n        \"response\": response[\"choices\"][0][\"text\"],\n        \"references\": references,\n    }\nThese are the two endpoints that you’ll use in your app. Here’s what each line does:\n\nLines 1 to 5 set up an endpoint that accepts GET requests on “/”. It returns a JSON response with a message key telling the user to use the “/ask” endpoint.\nLines 8 to 17 define an endpoint that takes accepts POST requests on “/ask”, with a single parameter, questionof string type. Once the user submits a request, you vectorize the question using the model you loaded previously, then you get the 3 most similar documents from your vector database.\nLines 19 to 32 combine the documents you got from the vector database with your prompt and make a request to the ChatGPT API. You set max_tokens=250 to keep answers short and set temperature=0.2, to prevent the model from getting “too creative” with its responses. Finally, you extract the answer from the ChatGPT API response and return it to the user, along with the references.\n\nIf you want to test it locally, type the following command into a terminal (inside the project’s virtual environment):\nuvicorn main:app --reload\nIn your browser, navigate to localhost:8000/docs to test your /ask endpoint:\n\nA successful response will look as follows:\n\nThat’s it! You have a working version of your AI search engine. Next, I’ll mention some ideas about deployment."
  },
  {
    "objectID": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#deploy-your-app",
    "href": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#deploy-your-app",
    "title": "Build an AI Search Engine Using FastAPI, Qdrant, and ChatGPT",
    "section": "Deploy Your App",
    "text": "Deploy Your App\nThere are many different ways to deploy an app, so you choose whatever approach you prefer. In my case, I like to use a VPS with NGINX acting as a reverse proxy and using Gunicorn as a process manager with Uvicorn workers. If you’d like to follow that approach, check a tutorial I wrote about it.\nIf you choose that route, keep the following points in mind:\n\nYou should use [--preload](https://dylancastillo.co/fastapi-nginx-gunicorn/) if you want to share the same model across all the processes and use less RAM memory.\nThere are memory leak issues when serving some types of models. A workaround that has worked for me is setting --max-requests and --max-requests-jitter to low numbers."
  },
  {
    "objectID": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#conclusion",
    "href": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#conclusion",
    "title": "Build an AI Search Engine Using FastAPI, Qdrant, and ChatGPT",
    "section": "Conclusion",
    "text": "Conclusion\nWay to go! In this tutorial, you’ve built a (tiny) AI search engine. You’ve learned:\n\nHow to structure the project.\nHow to set up a vector database using Qdrant.\nHow to vectorize your data using Sentence Transformers.\nHow to use ChatGPT to combine references into a coherent answer.\n\nHope you found this tutorial useful. Let me know if you have any questions!\nAll the code for this tutorial is available on GitHub."
  },
  {
    "objectID": "posts/nlp-snippets-clean-and-tokenize-text-with-python.html",
    "href": "posts/nlp-snippets-clean-and-tokenize-text-with-python.html",
    "title": "Clean and Tokenize Text With Python",
    "section": "",
    "text": "Every time I start a new project, I promise to save the most useful code snippets for the future, but I never do.\nThe old ways are too compelling. I end up copying code from old projects, looking for the same questions in Stack Overflow, or reviewing the same Kaggle notebooks for the hundredth time. At this point, I don’t know how many times I’ve googled for a variant of “remove extra spaces in a string using Python.”\nSo, finally, I’ve decided to compile snippets and small recipes for frequent tasks. I’m starting with Natural Language Processing (NLP) because I’ve been involved in several projects in that area in the last few years.\nFor now, I’m planning on compiling code snippets and recipes for the following tasks:\nThis article contains 20 code snippets you can use to clean and tokenize text using Python. I’ll continue adding new ones whenever I find something useful. They’re based on a mix of  Stack Overflow answers, books, and my experience.\nIn the next section, you can see an example of how to use the code snippets. Then, you can check the snippets on your own and take the ones you need."
  },
  {
    "objectID": "posts/nlp-snippets-clean-and-tokenize-text-with-python.html#how-to-use",
    "href": "posts/nlp-snippets-clean-and-tokenize-text-with-python.html#how-to-use",
    "title": "Clean and Tokenize Text With Python",
    "section": "How to use",
    "text": "How to use\nI’d recommend you combine the snippets you need into a function. Then, you can use that function for pre-processing or tokenizing text. If you’re using pandas, you can apply that function to a specific column using the .map method of pandas’ Series.\nTake a look at the example below:\nimport re\nimport pandas as pd\n\nfrom string import punctuation\n\ndf = pd.DataFrame({\n    \"text_col\": [\n        \"This TEXT needs \\t\\t\\tsome cleaning!!!...\",\n        \"This text too!!...       \",\n        \"Yes, you got it right!\\n This one too\\n\"\n    ]\n})\n\ndef preprocess_text(text):\n    text = text.lower()  # Lowercase text\n    text = re.sub(f\"[{re.escape(punctuation)}]\", \"\", text)  # Remove punctuation\n    text = \" \".join(text.split())  # Remove extra spaces, tabs, and new lines\n    return text\n\ndf[\"text_col\"].map(preprocess_text)"
  },
  {
    "objectID": "posts/nlp-snippets-clean-and-tokenize-text-with-python.html#code-snippets",
    "href": "posts/nlp-snippets-clean-and-tokenize-text-with-python.html#code-snippets",
    "title": "Clean and Tokenize Text With Python",
    "section": "Code snippets",
    "text": "Code snippets\nBefore testing the snippets, copy the following function at the top of your Python script or Jupyter notebook.\ndef print_text(sample, clean):\n    print(f\"Before: {sample}\")\n    print(f\"After: {clean}\")\n\nCleaning text\nThese are functions you can use to clean text using Python. Most of them just use Python’s standard libraries like re or string.\n\nLowercase text\nIt’s fairly common to lowercase text for NLP tasks. Luckily, Python strings include a .lower() method that makes that easy for you. Here’s how you use it:\nsample_text = \"THIS TEXT WILL BE LOWERCASED. THIS WON'T: ßßß\"\nclean_text = sample_text.lower()\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: THIS TEXT WILL BE LOWERCASED. THIS WON'T: ßßß\n# After: this text will be lowercased. this won't: ßßß\n\n\nRemove cases (useful for caseless matching)\nCase folding is a common approach for matching strings (especially in languages other than English). Python strings provide you with .casefold() for that. Here’s how to use it:\nsample_text = \"THIS TEXT WILL BE LOWERCASED. THIS too: ßßß\"\nclean_text = sample_text.casefold()\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: THIS TEXT WILL BE LOWERCASED. THIS too: ßßß\n# After: this text will be lowercased. this too: ssssss\n\n\nRemove hyperlinks\nIf you’re web scrapping, you’ll often deal with hyperlinks. It’s possible that you’d like to remove those to analyze the text. The easiest way to do that is by using regular expressions.\nPython’s re library is handy for those cases. This is how you’d remove hyperlinks using it:\nimport re\n\nsample_text = \"Some URLs: https://example.com http://example.io http://exam-ple.com More text\"\nclean_text = re.sub(r\"https?://\\S+\", \"\", sample_text)\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: Some URLs: https://example.com http://example.io http://exam-ple.com More text\n# After: Some URLs:    More text\n\n\nRemove &lt;a&gt; tags but keep their content\nSimilar to the previous case, if you’re doing web scrapping, you might often find dealing with tags. In some cases, such as &lt;a&gt;, you may want to remove the tag and its attributes but not its contents (e.g., the text it contains).\nYou can use Python’s re for that. Here’s how you’d remove the &lt;a&gt; tag and its attributes while keeping its content:\nimport re\n\nsample_text = \"Here's &lt;a href='https://example.com'&gt; a tag&lt;/a&gt;\"\nclean_text = re.sub(r\"&lt;a[^&gt;]*&gt;(.*?)&lt;/a&gt;\", r\"\\1\", sample_text)\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: Here's &lt;a href='https://example.com'&gt; a tag&lt;/a&gt;\n# After: Here's  a tag\nYou can also use this snippet as a starting point to remove other types of tags.\n\n\nRemove all HTML tags but keep their contents\nIf you’re dealing with web pages and want to remove all the tags in a document, you can use a generalized version of the previous snippet. Here’s how you remove all the HTML tags using Python’s re library:\nimport re\n\nsample_text = \"\"\"\n&lt;body&gt;\n&lt;div&gt; This is a sample text with &lt;b&gt;lots of tags&lt;/b&gt; &lt;/div&gt;\n&lt;br/&gt;\n&lt;/body&gt;\n\"\"\"\nclean_text = re.sub(r\"&lt;.*?&gt;\", \" \", sample_text)\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before:\n# &lt;body&gt;\n# &lt;div&gt; This is a sample text with &lt;b&gt;lots of tags&lt;/b&gt; &lt;/div&gt;\n# &lt;br/&gt;\n# &lt;/body&gt;\n\n# After:\n\n#  This is a sample text with lots of tags\n\n\nRemove extra spaces, tabs, and line breaks\nYou might think that the best approach to remove extra spaces, tabs, and line breaks would depend on regular expressions. But it doesn’t.\nThe best approach consists of using a clever combination two string methods: .split() and .join(). First, you apply the .split() method to the string you want to clean. It will split the string by any whitespace and output a list. Then, you apply the .join() method on a string with a single whitespace (” “), using as input the list you generated. This will put back together the string you split but use a single whitespace as separator.\nYes, I know it sounds a bit confusing. But, in reality, it’s fairly simple. Here’s how it looks in code:\nsample_text = \"     \\t\\tA      text\\t\\t\\t\\n\\n sample       \"\nclean_text = \" \".join(sample_text.split())\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before:           A      text\n\n#  sample\n# After: A text sample\n\n\nRemove punctuation\nMany NLP applications won’t work very well if you include punctuation. So it’s common to remove them. The easiest approach consists in using the string and re standard libraries are as follows:\nimport re\nfrom string import punctuation\n\nsample_text = \"A lot of !!!! .... ,,,, ;;;;;;;?????\"\nclean_text = re.sub(f\"[{re.escape(punctuation)}]\", \"\", sample_text)\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: A lot of !!!! .... ,,,, ;;;;;;;?????\n# After: A lot of\n\n\nRemove numbers\nIn some cases, you might want to remove numbers from text, when you don’t feel they’re very informative. You can use a regular expression for that:\nimport re\n\nsample_text = \"Remove these numbers: 1919191 2229292 11.233 22/22/22. But don't remove this one H2O\"\nclean_text = re.sub(r\"\\b[0-9]+\\b\\s*\", \"\", sample_text)\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: Remove these numbers: 1919191 2229292 11.233 22/22/22. But don't remove this one H2O\n# After: Remove these numbers: .//. But don't remove this one H2O\n\n\nRemove digits\nThere are cases where you might want to remove digits instead of any number. For instance, when you want to remove numbers but not dates. Using a regular expression gets a bit trickier.\nIn those cases, you can use the .isdigit() method of strings:\nsample_text = \"I want to keep this one: 10/10/20 but not this one 222333\"\nclean_text = \" \".join([w for w in sample_text.split() if not w.isdigit()]) # Side effect: removes extra spaces\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: I want to keep this one: 10/10/20 but not this one 222333\n# After: I want to keep this one: 10/10/20 but not this one\n\n\nRemove non-alphabetic characters\nSometimes, you’d like to remove non-alphabetic characters like numbers or punctuation. The .isalpha() method of Python strings will come in handy in those cases.\nHere’s how you use it:\nsample_text = \"Sample text with numbers 123455 and words !!!\"\nclean_text = \" \".join([w for w in sample_text.split() if w.isalpha()]) # Side effect: removes extra spaces\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: Sample text with numbers 123455 and words !!!\n# After: Sample text with numbers and words\n\n\nRemove all special characters and punctuation\nIn cases where you want to remove all characters except letters and numbers, you can use a regular expression.\nHere’s a quick way to do it:\nimport re\n\nsample_text = \"Sample text 123 !!!! Haha.... !!!! ##$$$%%%%\"\nclean_text = re.sub(r\"[^A-Za-z0-9\\s]+\", \"\", sample_text)\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: Sample text 123 !!!! Haha.... !!!! ##$$$%%%%\n# After: Sample text 123  Haha\n\n\nRemove stopwords from a list\nThere’s the case where you’d like to exclude words using a predefined list. A quick way to do it is by using list comprehensions. Here’s one way to do it:\nstopwords = [\"is\", \"a\"]\nsample_text = \"this is a sample text\"\ntokens = sample_text.split()\nclean_tokens = [t for t in tokens if not t in stopwords]\nclean_text = \" \".join(clean_tokens)\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: this is a sample text\n# After: this sample text\n\n\nRemove short tokens\nIn some cases, you may want to remove tokens with few characters. In this case, using list comprehensions will make it easy:\nsample_text = \"this is a sample text. I'll remove the a\"\ntokens = sample_text.split()\nclean_tokens = [t for t in tokens if len(t) &gt; 1]\nclean_text = \" \".join(clean_tokens)\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: this is a sample text. I'll remove the a\n# After: this is sample text. I'll remove the\n\n\nTransform emojis into characters\nIf you’re processing social media data, there might be cases where you’d like to extract the meaning of emojis instead of simply removing them. An easy way to do that is by using the emoji library.\nHere’s how you do it:\nfrom emoji import demojize\n\nsample_text = \"I love 🥑\"\nclean_text = demojize(sample_text)\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: I love 🥑\n# After: I love :avocado:\n\n\nRemove repeated characters\nIn some cases, you may want to remove repeated characters, so instead of “helloooo” you use “hello”. Here’s how you can do it (I got this code snippet from The Kaggle Book):\nimport re\n\nsample_text = \"hellooooo\"\nclean_text = re.sub(r'(.)\\1{3,}',r'\\1', sample_text)\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: hellooooo\n# After: hello\n\n\n\nNLTK\nBefore using NLTK’s snippets, you need to install NLTK. You can do that as follows: pip install nltk.\n\nTokenize text using NLTK\nfrom nltk.tokenize import word_tokenize\n\nsample_text = \"this is a text ready to tokenize\"\ntokens = word_tokenize(sample_text)\nprint_text(sample_text, tokens)\n\n# ----- Expected output -----\n# Before: this is a text ready to tokenize\n# After: ['this', 'is', 'a', 'text', 'ready', 'to', 'tokenize']\n\n\nTokenize tweets using NLTK\nfrom nltk.tokenize import TweetTokenizer\n\ntweet_tokenizer = TweetTokenizer()\nsample_text = \"This is a tweet @jack #NLP\"\ntokens = tweet_tokenizer.tokenize(sample_text)\nprint_text(sample_text, tokens)\n\n# ----- Expected output -----\n# Before: This is a tweet @jack #NLP\n# After: ['This', 'is', 'a', 'tweet', '@jack', '#NLP']\n\n\nSplit text into sentences using NLTK\nfrom nltk.tokenize import sent_tokenize\n\nsample_text = \"This is a sentence. This is another one!\\nAnd this is the last one.\"\nsentences = sent_tokenize(sample_text)\nprint_text(sample_text, sentences)\n\n# ----- Expected output -----\n# Before: This is a sentence. This is another one!\n# And this is the last one.\n# After: ['This is a sentence.', 'This is another one!', 'And this is the last one.']\n\n\n\nRemove stopwords using NLTK\nimport nltk\n\nfrom nltk.corpus import stopwords\n\nnltk.download(\"stopwords\")\n\nstopwords_ = set(stopwords.words(\"english\"))\n\nsample_text = \"this is a sample text\"\ntokens = sample_text.split()\nclean_tokens = [t for t in tokens if not t in stopwords_]\nclean_text = \" \".join(clean_tokens)\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: this is a sample text\n# After: sample text\n\n\nspaCy\nBefore using spaCy’s snippets, you need to install the library as follows: pip install spacy. You also need to download a language model. For English, here’s how you do it: python -m spacy download en_core_web_sm.\n\nTokenize text using spaCy\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nsample_text = \"this is a text ready to tokenize\"\ndoc = nlp(sample_text)\ntokens = [token.text for token in doc]\nprint_text(sample_text, tokens)\n\n# ----- Expected output -----\n# Before: this is a text ready to tokenize\n# After: ['this', 'is', 'a', 'text', 'ready', 'to', 'tokenize']\n\n\nSplit text into sentences using spaCy\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nsample_text = \"This is a sentence. This is another one!\\nAnd this is the last one.\"\ndoc = nlp(sample_text)\nsentences = [sentence.text for sentence in doc.sents]\nprint_text(sample_text, sentences)\n\n# ----- Expected output -----\n# Before: This is a sentence. This is another one!\n# And this is the last one.\n# After: ['This is a sentence.', 'This is another one!\\n', 'And this is the last one.']\n\n\n\nKeras\nBefore using Keras’ snippets, you need to install the library as follows: pip install tensorflow && pip install keras.\n\nTokenize text using Keras\nfrom keras.preprocessing.text import text_to_word_sequence\n\nsample_text = 'This is a text you want to tokenize using KERAS!!'\ntokens = text_to_word_sequence(sample_text)\nprint_text(sample_text, tokens)\n\n# ----- Expected output -----\n# Before: This is a text you want to tokenize using KERAS!!\n# After: ['this', 'is', 'a', 'text', 'you', 'want', 'to', 'tokenize', 'using', 'keras']"
  },
  {
    "objectID": "posts/tips-for-standing-out-on-linkedin.html",
    "href": "posts/tips-for-standing-out-on-linkedin.html",
    "title": "Tips for Standing Out on LinkedIn",
    "section": "",
    "text": "A few months ago I wrote a post that blew up on r/datascience about how to make your LinkedIn profile stand out when searching for a job.\nSince then, the economic situation has worsened. We’re now on the verge of an economic downturn, more than 100,000 people have been laid off from tech companies this year, and investors are warning founders of tough times ahead, so I’ve decided to expand on that original post.\nThis isn’t a post about how to get thousands of followers, nor how to become the most popular LinkedIn influencer. It’s about how to use LinkedIn to get more and better quality job opportunities.\nIt comes from reading many papers about LinkedIn’s search and ranking algorithms, learning about LinkedIn Recruiter, and making lots of mistakes while working on my own profile.\nLet’s get to it!"
  },
  {
    "objectID": "posts/tips-for-standing-out-on-linkedin.html#finding-a-great-job",
    "href": "posts/tips-for-standing-out-on-linkedin.html#finding-a-great-job",
    "title": "Tips for Standing Out on LinkedIn",
    "section": "Finding a (Great) Job",
    "text": "Finding a (Great) Job\nThere are two ways to find a good job: obsess over a few companies or play the numbers game. On the plus side, the former will increase your chances of landing your dream job, while the latter will likely lead to a better financial outcome. On the other hand, the former is riskier because it places all of your eggs in one or a few baskets, whereas the latter may not lead to the job you were hoping for.\nIn this article, I’ll focus on the second strategy as that’s the one I’ve focused on in my career. I never had a dream job in mind because I’ve always wanted to start my own business and the financial aspect was also important to me, so I prioritized learning relevant skills as well as a good salary. Despite not having a “dream job,” I was able to work at cool places like the Boston Consulting Group, Deliveroo, and the Olympics. I’ve also successfully transitioned into full-time freelancing in a very competitive industry.\nHowever, this does not imply that the approach I took is the best one. Everyone has different preferences and should choose a strategy based on what they want to optimize for."
  },
  {
    "objectID": "posts/tips-for-standing-out-on-linkedin.html#playing-the-numbers-game",
    "href": "posts/tips-for-standing-out-on-linkedin.html#playing-the-numbers-game",
    "title": "Tips for Standing Out on LinkedIn",
    "section": "Playing the Numbers Game",
    "text": "Playing the Numbers Game\nThe numbers game in job hunting consists of increasing the number of relevant job opportunities that you can access. It is not simply applying to as many job openings as possible.\nYour goal is to obtain as many relevant opportunities as possible by actively seeking them or by making your profile appealing to hiring managers and recruiters. Those who apply for jobs mindlessly are not following this strategy correctly. They’re just wasting their time.\nYou should think of your job search as a three-part funnel:\n\nLeads (job opportunities)\nInterviews\nOffers\n\nI will cover the first part of the funnel, Leads, and a will provide you with some tips for the second, Interviews. For the first part, the advice comes from my own experience, and the research I’ve done about LinkedIn’s algorithms. For the second part, the advice is mostly based on my own experience.\nYou’ll need to focus on two things to increase the number of relevant job opportunities or “qualified leads”: inbound and outbound leads.\n\nInbound Leads\nInbound leads are those that come to you without your intervention. Typically, recruiters and hiring managers connect with you on LinkedIn or send you InMail messages.\nRecruiters use LinkedIn Recruiter to find candidates. They can search for terms such as “Data Scientist” and define filters like  “has worked at Google” when looking for candidates.\nAfter a query is defined, LinkedIn Recruiter uses what they call a talent search algorithm. It works in two stages:\n\nSearch: It searches the network and defines a set of a few thousand candidates who meet the recruiter’s search criteria.\nRank: It provides the recruiter with a list of candidates ranked by how well they fit the search term and how likely they are to respond.\n\nThat’s all. If you want to get more job opportunities, you must figure out how to increase your chances of appearing in step 1 and ranking higher during step 2.\nFortunately, LinkedIn has released tons of research on its talent search algorithm. It’s not difficult to imagine what will help you stand out from the crowd. Here’s what I’ve found more impactful:\n\nUse relevant keywords in your profile. You won’t show up in the results if your profile doesn’t include search terms that recruiters use to find candidates. Examine the keywords used in the job descriptions for the positions you’re interested in, and make sure you have them in your profile.\nReply to recruiters. People often don’t reply to recruiters when they’re not interested in the job opportunity. But the algorithm prioritizes those who are likely to respond over those who are not. Even if it’s just to say no, respond to recruiters!\nEngage with the brands you’re interested in on LinkedIn. Recruiters can narrow down their search to candidates who have interacted with the brand or have connections who work for that company. If you’re particularly interested in a company, follow their profile, interact with their content, and add connections who work there.\nExpand your network. LinkedIn Recruiter Lite, a cheaper version of LinkedIn Recruiter, only lets users reach out to candidates up to their third-degree network. This means that the fewer connections you have, the less likely it is a recruiter can contact you.\nIncrease your influence. If you create engaging content, have a large number of visitors to your profile, or receive endorsements and recommendations, you will rank higher. As a general rule, try to write useful content on a regular basis and solicit recommendations from relevant contacts. LinkedIn’s Social Selling Index is a good proxy for how well you’re positioning yourself.\nGet a good photo. This is based on my personal experience. But I believe people are more likely to contact you if your photo looks somewhat professional.\n\nNone of these concepts are revolutionary, but most people overlook them when creating their profiles. LinkedIn’s goal is to match recruiters with the best possible candidates. So your job is to figure out what recruiters are searching for and how to best match that.\nFurthermore, even if recruiters or hiring managers do not pay for LinkedIn Recruiter and instead use the standard search service, the suggestions above will still help you improve your profile.\n\n\nOutbound Leads\nOutbound opportunities are the ones you apply to. Usually, that means applying for jobs on LinkedIn under the Jobs tab.\nThis takes time and has a very low ROI if not done correctly. I’ve discovered that the following increases its effectiveness:\n\nSet up alerts for roles you’re interested in.\nDon’t apply for jobs posted more than a week ago.\nPrioritize jobs for which you can contact the poster or someone in a relevant position within the company. In those cases, send them a personalized message expressing your interest. The best way to accomplish this is to connect with them and add a note to the connection request.\nChange your CV to fit the position you’re applying for. Read the job’s requirements and try to highlight the parts of your work history that match them.\n\nFinally, many opportunities are built through real-world connections, so reach out to people outside of LinkedIn, join relevant communities, and attend meetups."
  },
  {
    "objectID": "posts/tips-for-standing-out-on-linkedin.html#interviewing-101",
    "href": "posts/tips-for-standing-out-on-linkedin.html#interviewing-101",
    "title": "Tips for Standing Out on LinkedIn",
    "section": "Interviewing 101",
    "text": "Interviewing 101\nIt is usually simple to figure out how to prepare for an interview. The difficult part is carrying out the plan.\nI’ve only worked as a Data Scientist and ML Engineer, so I can’t offer advice for interviews outside of those roles. Here are some examples of things that help you get better results for interviews for those roles.\n\nGeneral Advice\nYou should do the following for any kind of interview:\n\nResearch the company and the role. You should know what the company does, its competitors, and recent news. You should also think about what the role you’re applying for entails. Also, use Glassdoor, Reddit, or ask other people to find out what questions they typically ask during interviews.\nReview the projects you’ve worked on. Make sure that you know the ins and outs of each one and have an elevator pitch for each of them. You’d be surprised how many people are rejected because they don’t fully understand key details in the projects they’ve worked on or can’t clearly explain what they did.\nBe assertive. One of the worst places to sell yourself short is during an interview. While it’s obvious that lying during interviews is bad (and you will be caught sooner or later), not expressing confidence is equally bad.\nConsider how many people are willing to lie to get a job; if you sell yourself short, you give dishonest people a better chance of winning.\nSee the interview as a conversation of equals. If you’ve mastered the technical bits, the only big obstacle in an interview is the mindset. They’re seeing if you’re a good fit, but you’re also seeing if they are a good fit. Don’t treat it like an exam.\nDon’t take rejections personally. Some interviews will go well, others won’t. Sometimes you’re to blame, and other times it’s due to circumstances beyond your control. When you fail an interview, consider why it didn’t go well, use the feedback to improve your next interview and move on.\n\nIn a nutshell, know yourself, know what you’re interviewing for, and be assertive.\n\n\nBehavioral interview\nThese questions are opportunities to sell yourself, so make sure you have good responses. It simply takes practice.\nHere’s what you should do:\n\nMake a list of 10-15 commonly asked questions (you can start here)\nRecord yourself answering those questions. For questions like “Give me an example when you did…” or “Tell me about a situation when you…” use the STAR framework.\nSee the recordings, give yourself honest feedback, and repeat the process.\nTry practicing with a friend or colleague, and ask them for feedback.\n\nLet me say it again: practice these questions! You should make sure you get these “easy” points during an interview.\n\n\nTechnical interview\nTechnical interviews aren’t usually a great measure of your skills, so don’t base your identity on how you do in them. They are like tests, and all you have to do to pass them is study. And if you don’t pass, it’s not the end of the world; you’ll have more chances elsewhere.\nHere’s what you should do to prepare at the very least:\n\nResearch how the company and team conduct technical interviews.\nExamine the job description to determine the most important topics to research.\nPractice questions in the programming languages used in the team on HackerRank or similar.\nUse Anki or another Spaced Repetition System (SRS) to practice key topics you might need to cover. Ideally, make your own cards using your favorite book on the topic.\n\n\n\nTake-home Challenge\nThese are hard to practice because every company does them their own way. If you’re a Data Scientist or ML Engineer, you can go to Kaggle, find a dataset that looks interesting to you, come up with some interesting questions, and then answer them. Or building a small ML model for a specific purpose.\nBonus points: Make sure your projects look nice, write a README explaining what you did, and put all that on GitHub. Also, write short posts on LinkedIn and share your projects."
  },
  {
    "objectID": "posts/tips-for-standing-out-on-linkedin.html#conclusion",
    "href": "posts/tips-for-standing-out-on-linkedin.html#conclusion",
    "title": "Tips for Standing Out on LinkedIn",
    "section": "Conclusion",
    "text": "Conclusion\nThat’s it! By now, you should have a good sense of how LinkedIn works and how you can use it to get more opportunities your way.\nIf you’re interested in learning more about the technical aspects of how the LinkedIn search works, they’ve published a lot of useful material you can check:\n\nPersonalized Expertise Search at LinkedIn\nTowards Deep and Representation Learning for Talent Search at LinkedIn\nTalent Search and Recommendation Systems at LinkedIn: Practical Challenges and Lessons Learned\nDeep Natural Language Processing For LinkedIn Search\nFrom Query-By-Keyword to Query-By-Example: LinkedIn Talent Search Approach\nDeText: A Deep Text Ranking Framework with BERT\n\nI hope you find this useful. If you have any questions reach out!"
  },
  {
    "objectID": "posts/2021-personal-snapshot.html",
    "href": "posts/2021-personal-snapshot.html",
    "title": "2021: Personal Snapshot",
    "section": "",
    "text": "When I see pictures from my childhood, I always try to remember who I was at the time the photos were shot. Like, what was going through my mind? What motivated me? What scared me?\nRemembering that is harder than it sounds. Pictures often help you recall sounds, smells, and emotions. But they don’t work well if you’re trying to recollect the inner workings of your mind.\nThere may not be a perfect way to remember how you felt at a certain period in your life. But I’ve found that reading things I’ve written in the past serves as a good substitute. After all, it’s easier to find out what was on your mind from reading your journal than it is to do so from looking at a random photo of you and your cousins.\nThis blog is pretty recent, so I don’t have much to refer to here. But when I was a teenager, I hung out in various online forums and wrote short stories and poems on text files. So if I want to remember Dylan from high school, I reread my posts, stories, and poems from that time.\nMany of those online forums have since died, and I’ve lost most of those text files due to faulty backups. That got me thinking that I had to find a better way to store those snapshots of myself.\nSo, for the foreseeable future, I’ve decided to write a Personal Snapshot of the year in my blog at the end of each year. It’s a yearly review that keeps a record of who I was when I wrote it. I’m hoping to get a few laughs out of this in a decade.\nIf it’s me rereading this, welcome back. This is Dylan from 2021."
  },
  {
    "objectID": "posts/2021-personal-snapshot.html#what-went-well",
    "href": "posts/2021-personal-snapshot.html#what-went-well",
    "title": "2021: Personal Snapshot",
    "section": "What Went Well?",
    "text": "What Went Well?\nDespite all the chaos due to COVID-19, I had a great year and achieved multiple goals I had set for myself.\nFirst, I’m a freelancer now. In mid-2019, I began to consider this option. My first actions included adding random people on LinkedIn and informing them that I intended to work as a freelancer. That was as useless as it sounds!\nThis year, I took a more systematic approach. I focused on contacting recruiters and managers hiring contractors and was more persistent throughout the process. I even went after contracts that seemed out of reach, asked past colleagues for referrals, and asked to reconsider when I got “no” for an answer.\nThis change resulted in two freelancing contracts (European Commission and Deliveroo) and a teaching gig at Nuclio. It took a long time to get the first contract, but now I’m always getting new opportunities. Needless to say, I’m delighted.\nTo be honest, my day-to-day as a freelancer hasn’t changed much compared to when I was a full-time employee. But that’s expected, considering that I’m still selling my time rather than a product. I hope to change that soon.\nSecond, I’m earning more and saving more. I hadn’t begun freelancing sooner because I didn’t want to compete for 10$/h jobs on Upwork or Fiverr. I would only leave my full-time job if I could secure a freelancing contract that allowed me to save money at a similar or higher rate than I did as an employee.\nThis year, I negotiated good contracts, which increased my income and, as a result, my savings rate. All in all, I had three sources of revenue this year (in order of importance): Freelancing, cashing out a bit of crypto, and the teaching gig.\nThird, a great relationship became a great team. This year my wife made a breakthrough in her career, which she had been fighting for over the last four years. It was tough, but we learned how to work together as a team in the process.\nI became her part-time coach, cheerleader, and teammate throughout this time. I attempted to provide as much help as possible, as she studied 12 to 14 hours each day for about a year. We spent hours debating techniques, going over practice tests, and experimenting with slight tweaks to her prep system. It was well worth the effort!\nIt was one of those times when someone else accomplishes something, but you’re so involved in the process that you feel like you’ve achieved it as well.\nThat got me thinking that good relationships are made of independent individuals who love and trust each other. But, just as important, is that they’ve decided to row in the same direction and work as a team to achieve their goals. That’s how I see us.\nFourth, I took good care of my mental health. I’ve found that practicing Stoic precepts (such as daily journaling, negative visualization, and reminding myself that I cannot control most events in my life), and exercising regularly have made me happier. I still have bad days, but very few compared to previous years.\nFifth, it’s been a good year for my family. My parents and sister are in good health, stay active, and have access to quality healthcare. Each family faces unique hurdles when they emigrate from Venezuela. Still, I’m pleased with how it’s going for us thus far."
  },
  {
    "objectID": "posts/2021-personal-snapshot.html#what-should-i-change",
    "href": "posts/2021-personal-snapshot.html#what-should-i-change",
    "title": "2021: Personal Snapshot",
    "section": "What Should I Change?",
    "text": "What Should I Change?\nFor the last couple of years, life has been comfortable enough that I haven’t needed to push my boundaries. That’s no good.\nFirst, I need to change my approach if I want to achieve financial freedom. I don’t want to spend the next forty years of my life working for someone else five days per week while enjoying freedom on weeknights and weekends. So I need to find a way to disentangle my income from my time, sooner rather than later.\nAlthough freelancing was the first step in that direction, I am still selling my time. So next, I’d like to start selling a product, such as an ebook or a course. To do so, I need an audience willing to pay for it.\nI studied people who’ve grown their online audiences quickly. With no exceptions, they did it by sharing high-quality content on social media on a regular basis. So I need to get off my butt and post regularly on social media.\nSecond, I must push myself out of my comfort zone. “What doesn’t kill me makes me stronger,” as Nietzsche famously said, should be complemented by the also valid “What keeps me cozy makes me weaker.”\nI feel that I’ve been too cozy the past few years, and I wasn’t taking enough risks. If this keeps up, I’ll be a boring old man with few stories to tell my grandchildren.\nThe problem is that society wants you to follow its standard script when you reach adulthood: find a job, get married, have kids, get a mortgage, buy a car, have a couple of midlife crises, possibly get divorced, and then retire at 65 to tend to your garden.\nThat’s not the life I want to live. Ask yourself, if that was a book, would you read it?\nMost people wouldn’t put a few hours to read such a book. They won’t mind, though, spending their entire lives living that by that script.\nOne thing that a happy life and a good book have in common is that their foundation is a good story. Staying in your lane and following society’s script is a certain way to write a story that no one wants to read, including yourself.\nYou’re the hero of your own story. Heroes don’t spend their lives binge-watching Netflix or scrolling through social media. They face challenges and grow as a result of that.\nThat’s my goal for next year: grow by stepping out of my comfort zone.\nWhat exactly will I be doing? You’ll need to follow my blog to learn more 😉"
  },
  {
    "objectID": "posts/2021-personal-snapshot.html#what-do-i-believe-in",
    "href": "posts/2021-personal-snapshot.html#what-do-i-believe-in",
    "title": "2021: Personal Snapshot",
    "section": "What do I Believe In?",
    "text": "What do I Believe In?\nFinally, I thought it’d be interesting to keep track of my beliefs, to see how they change over time. Some of these are mental frameworks, others are random thoughts.\nHere’s a non-exhaustive list of them:\n\nStoicism is the OG of mind hacks. Cognitive Behavioral Therapy, the leading technique of psychotherapy, is deeply influenced by Stoicism. My mental health has greatly improved as a result of following Stoic teachings. Others could benefit as well, so here are two good places to start: Enchiridion or A Guide to the Good Life.\nComfort kills. At a micro-level, make your life harder, by doing things such as taking cold showers, fasting, and exercising every day. At a macro-level, do what scares you and excites you at the same time. When it comes to determining what will help you grow, trust your instincts.\nDon’t discuss nutrition and vaccines. Add them to Religion, and Politics. Everyone has a very personal opinion on these topics. There’s no point in discussing them with friends.\nThe 80/20 of Data Science. Data Scientists like to build models, but most of the value comes from transforming raw data into useful insights for the business. That means setting up pipelines, generating reports, and building dashboards.\nThe 80/20 of staying healthy. Avoid ultra-processed foods, sugar, and vegetable oils. Eat foods that have been around for a long time. Exercise regularly, doing Strength Training and some sort of cardio."
  },
  {
    "objectID": "posts/2021-personal-snapshot.html#looking-ahead",
    "href": "posts/2021-personal-snapshot.html#looking-ahead",
    "title": "2021: Personal Snapshot",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nI have high hopes for 2022. I’m eager to work on decoupling my income from my time and putting more time into my personal growth.\nI hope to see you around!\nThanks to María, Caryn, Christine, Georgia, and Mack for their feedback on this piece."
  },
  {
    "objectID": "posts/say-what-you-mean-sometimes.html",
    "href": "posts/say-what-you-mean-sometimes.html",
    "title": "Say What You Mean… Sometimes",
    "section": "",
    "text": "When I read Let Me Speak Freely? by Tam et al. I thought they raised an interesting question: does constraining LLM outputs to structured formats impact the quality of their responses?\nIn both the original study and their recent update, Tam et al. concluded that is the case. They found that “structured generation constraints significantly impact LLM performance across various tasks”.\nBut the study had major flaws. The .txt team wrote a very compelling rebuttal to the paper. For Llama-3-8B-Instruct, they demonstrate that Tam, et al. results were mostly due to poor prompting, unfair comparisons and the improper use of an “AI parser” rather than the use of structured outputs.\nI liked the rebuttal but it still left me wondering how well their results generalize. They focused on a single model1, which represents a small fraction of the LLMs powering applications in production today. Open-weight models offer more flexibility on how to structure your output, such as letting users specify regex expressions to constrain the output. Proprietary models lack this. Right now, JSON is the only structured output format guaranteed to work across most popular providers.\nGiven this constraint, would the .txt team’s results still hold?\nPlus, both the original study and the rebuttal focused on tasks that might not be a good proxy for the full range of tasks people use LLMs for. Would the rebuttal results be different in settings outside of simple reasoning tasks?\nSo I decided to:\nThis article presents the results of the first two steps. All the code is available on Github."
  },
  {
    "objectID": "posts/say-what-you-mean-sometimes.html#results",
    "href": "posts/say-what-you-mean-sometimes.html#results",
    "title": "Say What You Mean… Sometimes",
    "section": "Results",
    "text": "Results\nIf you’re short on time, here are the main results:\n\nTam et al.’s conclusions about structured outputs might still hold, even if they did not properly test for it. There are cases where structured outputs perform worse than unstructured outputs.\n.txt’s rebuttal is correct, and shows that structured outputs are as good or better than unstructured outputs for LLaMA3-8B-Instruct. But the same approach does not hold for GPT-4o-mini (and possibly other models).\n\nIn the figure below you can see the results for GPT-4o-mini, using .txt’s fixes to the prompts and additional improvements I implemented.\n\n\n                                                \n\n\nFor GSM8K and Last Letter, structured and unstructured methods scored similarly. But for Shuffled Objects, unstructured outputs clearly surpassed a structured format.\nThe rest of the article will explain the approach I took to get these results."
  },
  {
    "objectID": "posts/say-what-you-mean-sometimes.html#study-design",
    "href": "posts/say-what-you-mean-sometimes.html#study-design",
    "title": "Say What You Mean… Sometimes",
    "section": "Study design",
    "text": "Study design\nTam et al. evaluated structured and unstructured outputs across three reasoning tasks and six classification tasks. They used exact match to evaluate reasoning tasks and accuracy to evaluate classification tasks. They ran the experiments using the following models:\n\nProprietary models: gpt-3.5-turbo-0125, claude-3-haiku-20240307, gemini-1.5-flash, and gpt-4o-mini-2024-07-18.\nOpen-weight models: LLaMA3-8B-Instruct, and Gemma-2-9B-Instruct.\n\n.txt used a similar setup, but only focused on the reasoning tasks and evaluating LLaMA3-8B-Instruct. They did not include classification tasks because Tam et al. observed that structured outputs resulted in better performance in these tasks, so there was no need to test for it.\nI also believe that structured outputs are better for classification tasks. So, I excluded them from my analysis as well.\nThe reasoning tasks were:\n\nGSM8K: A dataset from of grade school math word problems.\nLast Letter: A dataset of simple word puzzles that require concatening the last letters of a list of names.\nShuffled Objects: A dataset that requires reasoning about the state of a system after a sequence of shuffling operations.\n\nThis article will focus on replicating the results from .txt’s rebuttal on these tasks and evaluating the same tasks using a proprietary model."
  },
  {
    "objectID": "posts/say-what-you-mean-sometimes.html#replicating-.txts-rebuttal",
    "href": "posts/say-what-you-mean-sometimes.html#replicating-.txts-rebuttal",
    "title": "Say What You Mean… Sometimes",
    "section": "Replicating .txt’s rebuttal",
    "text": "Replicating .txt’s rebuttal\n.txt made it very easy to reproduce their results by sharing their code on Github. I just set up a machine at Modal and ran the code.\nWhile going through the code, I noticed some small issues with the prompts. So I decided to tweak them a bit.\nBelow are .txt’s original results compared to mine, after the prompt adjustments:\n\n\n\nTask\n.txt\nMe, 3-shot\n\n\nUnstructured\nStructured\nUnstructured\nStructured\n\n\nGSM8K\n77.18\n77.79\n79.98\n79.45\n\n\nLast Letter\n73.33\n77.33\n74.00\n78.00\n\n\nShuffled Objects\n40.72\n44.35\n42.68\n43.90\n\n\n\nExcept for Structured in the Shuffled Objects task, I was able to improve all the metrics. In GSM8K’s case, even reversing .txt’s result, with Unstructured outperforming Structured by a small margin.\nBut I don’t think this matters much.\nTheir conclusion still holds: structured outputs are either as good as or better than unstructured outputs, in the tasks considered.\nI’ll explain the prompt changes I made below, so that you can judge for yourself if they make sense.\n\nFormatting few-shot examples\nIn the GSM8K and Last Letter tasks, the few-shot prompt for both unstructured and structured used examples formatted as JSON objects and asked the LLM to produce the output in the same format, from which the answer was extracted.\nThat felt unfair. Even though you’re not formally constraining the LLM to produce a JSON object, you’re still asking it to format its response in somewhat unnatural way.\nI adjusted the prompts to be as similar as possible for both unstructured and structured outputs while still trying to get the most out of each approach.\nFor example, in GSM8K, the unstructured prompt is:\n\nYou are an expert in solving grade school math tasks. You will be presented with a grade-school math word problem and be asked to solve it. You will always respond in the following format:\n&lt;str, reasoning about the answer&gt;\nANSWER: &lt;int, final answer&gt;\nFirst, provide your step by step reasoning. Then, in ANSWER, provide an integer that corresponds to the correct answer to the question. Don’t include any other text in ANSWER.\n\nAnd the structured prompt is:\n\nYou are an expert in solving grade school math tasks. You will be presented with a grade-school math word problem and be asked to solve it. You will always respond in the following format:\n{“reasoning”: &lt;str, reasoning about the answer&gt;, “answer”: &lt;int, final answer&gt;}\nFirst, provide your step by step reasoning in the “reasoning” field. Then, in the “answer” field, provide an integer that corresponds to the correct answer to the question. Don’t include any other text in the “answer” field.\n\nFinally, for all the tasks, I used a 3-shot prompt.\n\n\nClarifying the task\nI also tried to make the prompts clearer. The description of the task in the original Last Letter prompt was:\n\nYou are an expert in solving simple word puzzles using reasoning steps. Your specific task is going to be to take a list of 4 names and reason about the last letter of each ., then you will concatenate those letters into a word.\n\nI changed it to:\n\nYou are an expert in solving word puzzles. Your specific task is going to be to take a list of 4 names, get the last letter of each and concatenate these letters into a word.\n\nThe original prompt was reasonable, but I thought the new version was clearer. Through trial and error, I’ve learned that when working with LLMs, it’s best to be as clear and direct as possible."
  },
  {
    "objectID": "posts/say-what-you-mean-sometimes.html#evaluating-gpt-4o-mini",
    "href": "posts/say-what-you-mean-sometimes.html#evaluating-gpt-4o-mini",
    "title": "Say What You Mean… Sometimes",
    "section": "Evaluating GPT-4o-mini",
    "text": "Evaluating GPT-4o-mini\nUsing the same setup as before, I ran the same tasks with gpt-4o-mini-2024-07-18.\nBelow are the results, including the original results from Tam et al. for comparison:\n\n\n\n\n\n\n\n\n\n\n\nTask\nMethod\nNL\nFRI\nJSON-Mode\nJSON-Schema\n\n\n\n\nGSM8K\nTam et al.\n94.57\n87.17\n86.95\n91.71\n\n\n\nMe (0-shot)\n94.31\n92.12\n93.33\n93.48\n\n\n\nMe (3-shot)\n93.86\n92.72\n93.25\n92.95\n\n\nLast Letter\nTam et al.\n83.11\n84.73\n76.00\n86.07\n\n\n\nMe (0-shot)\n87.33\n88.00\n90.00\n87.33\n\n\n\nMe (3-shot)\n92.00\n94.67\n90.00\n93.33\n\n\nShuffled Obj.\nTam et al.\n82.85\n81.46\n76.43\n81.77\n\n\n\nMe (0-shot)\n95.12\n79.67\n81.71\n89.84\n\n\n\nMe (3-shot)\n92.68\n69.51\n62.60\n65.85\n\n\n\nNL stands for “Natural Language”, which would correspond to the Unstructured method in the previous table.\nFRI stands for “Format Restricting Instructions”, which is a JSON generated through the OpenAI’s function calling. JSON-Mode is a JSON generated through the OpenAI’s JSON mode. JSON-Schema is a JSON generated using constrained decoding.\nJSON-Schema is the closest equivalent to Structured as referenced in the previous table. But, in real-life applications, you don’t really care about how the output was generated. You just want to get the output in the format you want. So, for the sake of comparison, I will consider the three other methods equivalent to Structured as well.\n\nAdjusting for proprietary models\nIn this case, I allowed for 3 retries in the case of parsing errors. I allowed for this because function calling had high error rates in the zero-shot prompting scenario.\nThese retries primarily affected FRI results. This might make the comparisons in Last Letter biased in favor of structured outputs (FRI was the best method in this case). But since JSON-Schema also outperformed NL in this case, this adjustment does not alter the overall conclusions. The other methods maintained error rates of &lt;0.5% in GSM8K and 0% in Last Letter and Shuffled Objects.\nI used slightly different parsing functions for Unstructured and Structured outputs. The Unstructured parser was more lenient, removing commas and periods at the end of responses. But I believe this remains a fair comparison given that in the Structured cases you provide a JSON schema which is more informative.\n\n\nAnalyzing the results\nSimilar to what the .txt team found, after adjusting the prompts, the performance of structured outputs increases substantially compared to Tam et al.\nExcept for NL in GSM8k and FRI in Last Letter, I was able to improve all the metrics for both unstructured and structured outputs using a 0-shot prompt. For 3-shot prompts, I improved GSM8k and Last Letter across all methods, and NL in Shuffled Objects.\nFor GSM8k and Last Letter, the results were very similar between unstructured and structured outputs. There was a slight edge for unstructured outputs in GSM8k and for structured outputs in Last Letter. In these cases, it’s not clear that one approach definitively outperforms the other.\nOn the other hand, Shuffled Objects shows a clear advantage for unstructured outputs over structured outputs. This was unexpected, and even after tweaking the prompts, I couldn’t fully close the gap.\nDespite the issues in Tam et al.’s study, their conclusion appears to hold. In this particular scenario, using a fairly popular model with reasonable prompts, there is a significant difference in performance between structured and unstructured outputs.\n\n\n\n\n\n\nNote\n\n\n\nIn GSM8k and Last Letter, few-shot prompting generally decreased performance. This is in line with other analyses."
  },
  {
    "objectID": "posts/say-what-you-mean-sometimes.html#conclusion",
    "href": "posts/say-what-you-mean-sometimes.html#conclusion",
    "title": "Say What You Mean… Sometimes",
    "section": "Conclusion",
    "text": "Conclusion\nYou’re here because you want to know whether to use structured or unstructured outputs. As a developer, I’m glad to say the answer is: it depends.\nI love using structured outputs in my daily work, because it makes it much easier to work with the output of LLMs. I always encourage clients who aren’t using them yet to give them a try.\nThat said, until there’s strong evidence favoring one approach over the other, the best course of action is to test things for yourself. Run your own evals and make a decision based on data.\nI expect that in most cases, structured outputs will have similar performance to unstructured outputs. But, if you blindly assume that structured outputs are always equal to or better than unstructured ones, you might be missing out on easy performance gains.\nTake the example of Shuffled Objects with GPT-4o-mini. You could potentially reduce the gap between the two methods by continuing improving the prompts or by switching to a more powerful model. But the effort and/or costs might outweigh the benefits compared to simply using unstructured outputs.\nI don’t think unstructured outputs are inherently better or worse than structured outputs across the board. The right choice depends on your task, the model, and your prompt engineering skills. Test for yourself to determine if a difference exists, and if it does, decide which option works best for you."
  },
  {
    "objectID": "posts/say-what-you-mean-sometimes.html#footnotes",
    "href": "posts/say-what-you-mean-sometimes.html#footnotes",
    "title": "Say What You Mean… Sometimes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlthough, they’ve also shared results of other open-weight models using a different setup.↩︎"
  },
  {
    "objectID": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html",
    "href": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html",
    "title": "Deploying a Django app with Kamal, AWS ECR, and Github Actions",
    "section": "",
    "text": "Every other night, my wife wakes me up to tell me I’m muttering unintelligible phrases in my sleep: “restart nginx,” “the SSL certificate failed to validate,” or “how do I exit vim?”\nI still suffer from PTSD from the days of manually deploying web apps. But since switching to Kamal, I’ve been sleeping like a baby1.\nKamal is sort of a lightweight version of Kubernetes that you can use to deploy containerized apps to a VPS. It has a bit of a learning curve, but once you get the hang of it, it’ll take you less than 5 minutes to get an app in production with a CI/CD pipeline.\nA single push to main, and that green GitHub Actions checkmark confirms that your 2-pixel padding change is live for the world to admire.\nIn this tutorial, I’ll walk you through the process of deploying a Django app with Kamal, AWS ECR, and Github Actions."
  },
  {
    "objectID": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#prerequisites",
    "href": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#prerequisites",
    "title": "Deploying a Django app with Kamal, AWS ECR, and Github Actions",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo make the most of this tutorial, you should:\n\nHave an AWS account and its CLI installed.\nBe comfortable with Docker.\nHave a basic understanding of Kamal. You’ll need to install version 1.9.0 for this tutorial.\nHave a basic understanding of Github Actions.\nHave a VPS with Ubuntu ready to host your app.\n\nIdeally, you should also have a Django project ready to deploy. But if you don’t have one, you can use this sample Django project for the tutorial."
  },
  {
    "objectID": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#prepare-the-vps-for-kamal",
    "href": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#prepare-the-vps-for-kamal",
    "title": "Deploying a Django app with Kamal, AWS ECR, and Github Actions",
    "section": "Prepare the VPS for Kamal",
    "text": "Prepare the VPS for Kamal\nAt a minimum, you’ll need to install docker, curl, git, and snapd on your VPS, and create a non-root user called kamal that can sudo. That user should have a 1000 UID and GID.\nI have a terraform script that will take care of this for you if you’re using Hetzner.\nBut if you’d like to do it manually, you can run these commands on your VPS’s terminal:\n# Install docker, curl, and git, and snapd\napt-get update\napt-get install -y docker.io curl git snapd\n\n# Start and enable the docker service\nsystemctl start docker\nsystemctl enable docker\n\n# Create a non-root user called kamal\nuseradd -m -s /bin/bash -u 1000 kamal\nusermod -aG sudo kamal\necho \"kamal ALL=(ALL) NOPASSWD:ALL\" &gt;&gt; /etc/sudoers.d/kamal\n\n# SSH key to login as kamal user\nmkdir -p /home/kamal/.ssh\necho \"&lt;YOUR_PUBLIC_SSH_KEY&gt;\" &gt;&gt; /home/kamal/.ssh/authorized_keys\nchmod 700 /home/kamal/.ssh\nchmod 600 /home/kamal/.ssh/authorized_keys\nchown -R kamal:kamal /home/kamal/.ssh\n\n# Disable root login\nsed -i '/PermitRootLogin/d' /etc/ssh/sshd_config\necho \"PermitRootLogin no\" &gt;&gt; /etc/ssh/sshd_config\nsystemctl restart sshd\n\n# Add the kamal user to the docker group\nusermod -aG docker kamal\ndocker network create --driver bridge kamal_network\n\n# Create a folder for the Let's Encrypt ACME JSON\nmkdir -p /letsencrypt && touch /letsencrypt/acme.json && chmod 600 /letsencrypt/acme.json\nchown -R kamal:kamal /letsencrypt\n\n# Create a folder for the SQLite database (skip this if you're using a different database)\nmkdir -p /db\nchown -R 1000:1000 /db\n\n# Create a folder for the redis data (skip this if you're not using redis)\nmkdir -p /data\nchown -R 1000:1000 /data\n\nreboot\nThis assumes that you’re using a root user to connect to your server and that there isn’t a non-root user with UID 1000 already. Otherwise, adjust the commands accordingly.\nAlso, if you don’t have a public SSH key for the “Add SSH key” step, you can generate one with the following command:\nssh-keygen -t ed25519 -C \"your-email@example.com\"\nThese commands will:\n\nInstall docker, curl, git, and snapd\nStart and enable the docker service\nCreate a non-root user called kamal\nRemove the root login\nAdd the kamal user to the docker group\nCreate a bridge network for Traefik, SQLite, and redis\nCreate a folder for the Let’s Encrypt ACME JSON\nMake the Let’s Encrypt ACME JSON folder writable by the kamal user\nCreate a folder for the SQLite database and redis data\nMake the SQLite database and redis data folders writable by the kamal user\nRestart the server\n\nIf you’re not using SQLite or redis, you can skip the database and redis data folder steps.\nFinally, configure the SSH key in your local .ssh/config file so you can login as the kamal user without using the root account.\nHost kamal\n  HostName &lt;YOUR_VPS_IP&gt;\n  User kamal\n  IdentityFile ~/.ssh/&lt;YOUR_PRIVATE_SSH_KEY&gt;"
  },
  {
    "objectID": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#create-a-dockerfile-for-your-app",
    "href": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#create-a-dockerfile-for-your-app",
    "title": "Deploying a Django app with Kamal, AWS ECR, and Github Actions",
    "section": "Create a Dockerfile for your app",
    "text": "Create a Dockerfile for your app\nKamal is meant to deploy containerized apps, so you’ll need to have a Dockerfile for your app. I also recommend using an entrypoint.sh script to run the application.\n\nDockerfile\nHere’s the Dockerfile I’m using for my projects. You can use this as a template and adjust it to your needs.\n\n\nDockerfile\n\nFROM python:3.10-slim AS base\n\nENV POETRY_HOME=/opt/poetry\nENV POETRY_VERSION=1.8.3\nENV PATH=${POETRY_HOME}/bin:${PATH}\n\nRUN apt-get update \\\n    && apt-get install --no-install-recommends -y \\\n    curl \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\nRUN curl -sSL https://install.python-poetry.org | python3 - && poetry --version\n\nFROM base AS builder\n\nWORKDIR /app\n\nCOPY poetry.lock pyproject.toml ./\n\nRUN poetry config virtualenvs.in-project true && \\\n    poetry install --only main --no-interaction\n\nFROM base AS runner\n\nWORKDIR /app\nCOPY --from=builder /app/.venv/ /app/.venv/\n\nCOPY . /app\nRUN mkdir -p /data /db\n\nRUN chmod +x /app/src/entrypoint.sh\n\nFROM runner AS production\n\nEXPOSE 8000\n\nARG user=django\nARG group=django\nARG uid=1000\nARG gid=1000\nRUN groupadd -g ${gid} ${group} && \\\n    useradd -u ${uid} -g ${group} -s /bin/sh -m ${user} && \\\n    chown -R ${uid}:${gid} /app /data /db\n\nUSER ${uid}:${gid}\n\nWORKDIR /app/src\nCMD [ \"/app/src/entrypoint.sh\" , \"app\"]\n\nThis is a multi-stage Dockerfile that:\n\nInstalls poetry and sets up the virtual environment\nCreates the user django with the UID and GID 1000 and runs the application with that user. It’s important that this user has the same UID and GID as the owner of the folders outside the container. Otherwise, you’ll have issues with file permissions and the app won’t persist data.\nExposes port 8000 and runs the application by executing the entrypoint.sh script. By exposing the port, Kamal will automatically detect that is the port the app runs on and will use that to set up the reverse proxy.\n\nFeel free to adjust this Dockerfile to your needs. If you are not planning on using redis or a SQLite database in your same VPS, you can remove those parts from the Dockerfile.\n\n\nentrypoint.sh script\nI use an entrypoint.sh script to run the application because that makes it easier to collect static files, run migrations when the container starts, and also running commands in the container.\nHere’s an example of a simple entrypoint.sh script:\n\n\nentrypoint.sh\n\n#!/bin/sh\n\nset -e\n\nif [ \"$1\" = \"app\" ]; then\n    echo \"Collecting static files\"\n    poetry run python manage.py collectstatic --clear --noinput\n\n    echo \"Running migrations\"\n    poetry run python manage.py migrate\n\n    echo \"Running in production mode\"\n    exec poetry run gunicorn -c gunicorn.conf.py\nelse\n    exec \"$@\"\nfi\n\nThis script just collects static files, runs migrations, and starts the Gunicorn server with the configuration in the gunicorn.conf.py file. You can add or remove commands to the script as needed."
  },
  {
    "objectID": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#configure-an-ecr-registry-in-aws",
    "href": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#configure-an-ecr-registry-in-aws",
    "title": "Deploying a Django app with Kamal, AWS ECR, and Github Actions",
    "section": "Configure an ECR registry in AWS",
    "text": "Configure an ECR registry in AWS\nNext, you’ll need a place to push and pull your Docker images. I like using AWS, so that’s what I’ll show you how to do. If you prefer other services, take a look at the instructions for other registries in the Kamal documentation.\nLog in to the AWS Management Console and go to Amazon ECR. Click on Create repository and set a name for your repository.\nThen, create a new IAM user in your AWS account by going to Services &gt; IAM &gt; Users &gt; Add user.\nInstead of using a predefined policy, create a new one with the following JSON and attach it to the user:\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"ListImagesInRepository\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"ecr:ListImages\"],\n      \"Resource\": [\n        \"arn:aws:ecr:&lt;REGION&gt;:&lt;ACCOUNT_ID&gt;:repository/&lt;REPOSITORY_NAME&gt;\"\n      ]\n    },\n    {\n      \"Sid\": \"GetAuthorizationToken\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"ecr:GetAuthorizationToken\"],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"ManageRepositoryContents\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ecr:BatchCheckLayerAvailability\",\n        \"ecr:GetDownloadUrlForLayer\",\n        \"ecr:GetRepositoryPolicy\",\n        \"ecr:DescribeRepositories\",\n        \"ecr:ListImages\",\n        \"ecr:DescribeImages\",\n        \"ecr:BatchGetImage\",\n        \"ecr:InitiateLayerUpload\",\n        \"ecr:UploadLayerPart\",\n        \"ecr:CompleteLayerUpload\",\n        \"ecr:PutImage\"\n      ],\n      \"Resource\": [\n        \"arn:aws:ecr:&lt;REGION&gt;:&lt;ACCOUNT_ID&gt;:repository/&lt;REPOSITORY_NAME&gt;\"\n      ]\n    }\n  ]\n}\nThis policy allows the user to list, get, and manage the ECR repository you created earlier and get the authorization token to push and pull the image. You will need to replace the &lt;REGION&gt;, &lt;ACCOUNT_ID&gt;, and &lt;REPOSITORY_NAME&gt; with the values for your repository.\nNext, select the user you created and go to Security credentials &gt; Access keys &gt; Create access key. Download the CSV file and keep it in a secure location.\nYou will use those credentials in your Github Actions pipeline to push and pull the image from the ECR registry."
  },
  {
    "objectID": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#set-up-kamal-in-your-project",
    "href": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#set-up-kamal-in-your-project",
    "title": "Deploying a Django app with Kamal, AWS ECR, and Github Actions",
    "section": "Set up Kamal in your project",
    "text": "Set up Kamal in your project\nOpen your Django project in your favorite code editor. Create a folder called deploy in the root directory. Then go into the folder and initialize Kamal:\nkamal init\nThis will create two folders (.kamal/ and config/) and an .env file. Inside config/, you’ll find a deploy.yml file. This is where you’ll provide the instructions for Kamal to build and deploy your app.\nYou can use the following deploy.yml file as a template for your Django app:\n\n\ndeploy.yml\n\n\nservice: &lt;YOUR_SERVICE_NAME&gt;\n\nimage: &lt;YOUR_IMAGE_NAME&gt;\n\nssh:\n  user: kamal\n\nenv:\n  secret:\n    - DJANGO_SECRET_KEY\n\ntraefik:\n  options:\n    publish:\n      - \"443:443\"\n    volume:\n      - \"/letsencrypt/:/letsencrypt/\"\n    memory: 500m\n    network: kamal_network\n  args:\n    entryPoints.web.address: \":80\"\n    entryPoints.websecure.address: \":443\"\n    entryPoints.web.http.redirections.entryPoint.to: websecure\n    entryPoints.web.http.redirections.entryPoint.scheme: https\n    entryPoints.web.http.redirections.entrypoint.permanent: true\n    certificatesResolvers.letsencrypt.acme.email: \"&lt;YOUR_EMAIL&gt;\"\n    certificatesResolvers.letsencrypt.acme.storage: \"/letsencrypt/acme.json\"\n    certificatesResolvers.letsencrypt.acme.httpchallenge: true\n    certificatesResolvers.letsencrypt.acme.httpchallenge.entrypoint: web\n\nservers:\n  web:\n    hosts:\n      - &lt;YOUR_VPS_IP&gt;\n    healthcheck:\n      port: 8000\n      interval: 5s\n    options:\n      network: kamal_network\n    labels:\n      traefik.http.routers.app.tls: true\n      traefik.http.routers.app.entrypoints: websecure\n      traefik.http.routers.app.rule: Host(`&lt;YOUR_DOMAIN&gt;`)\n      traefik.http.routers.app.tls.certresolver: letsencrypt\n\naccessories:\n  redis:\n    image: redis:7.0\n    roles:\n      - web\n    cmd: --maxmemory 200m --maxmemory-policy allkeys-lru\n    volumes:\n      - /var/redis/data:/data/redis\n    options:\n      memory: 250m\n      network: kamal_network\n\nvolumes:\n  - \"/db/:/app/db/\"\n\nregistry:\n  server: &lt;YOUR_AWS_ECR_URL&gt; # e.g. 123456789012.dkr.ecr.us-east-1.amazonaws.com\n  username: AWS\n  password:\n    - KAMAL_REGISTRY_PASSWORD\n\nbuilder:\n  dockerfile: \"../Dockerfile\"\n  context: \"../\"\n\nThis will set up your app and a reverse proxy using Traefik (with automatic SSL certificates using Let’s Encrypt), a Redis database, and a volume to persist the SQLite database. It will also do a healthcheck on /up on port 8000.\nRemember to replace the placeholders with your own values.\n\nTest the configuration locally\nTo test it locally, first, you’ll have to define the required environment variables in the .env file, such as the Django secret key, OpenAI API key, and any other secrets you need.\nYou’ll also need to get a temporary password for the ECR registry. You can get this password by running the following command:\naws ecr get-login-password --region &lt;YOUR_REGION&gt;\nYou should copy the output of this command and paste it in the KAMAL_REGISTRY_PASSWORD field in the .env file.\nThen, run the following command to deploy your application to your VPS:\nkamal env push\nkamal deploy\nThe first command will push the environment variables to the VPS. The second command will build the Docker image, push it to the ECR registry, and deploy it to your VPS.\nAfter a few minutes, your app should be live at https://&lt;YOUR_DOMAIN&gt;.\nIf you see any errors, there are two things you can do:\n\nRun kamal app logs to see the logs of the app.\nOpen a terminal in the container by running kamal app exec -it bash.\n\nThis is how I usually debug the app."
  },
  {
    "objectID": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#automate-the-deployment-with-github-actions",
    "href": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#automate-the-deployment-with-github-actions",
    "title": "Deploying a Django app with Kamal, AWS ECR, and Github Actions",
    "section": "Automate the deployment with Github Actions",
    "text": "Automate the deployment with Github Actions\nNow that you have a working deployment process in your local environment, you can automate the deployment with Github Actions.\nCreate a new file in the .github/workflows folder called deploy.yml and add the following code:\nname: Deploy webapp to VPS\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\non:\n  push:\n    branches: [\"main\"]\n  workflow_dispatch:\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n\n      - uses: webfactory/ssh-agent@v0.7.0\n        with:\n          ssh-private-key: ${{ secrets.VPS_SSH_PRIVATE_KEY }}\n\n      - name: Set up Ruby and install kamal\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: 3.2.2\n      - run: gem install kamal -v 1.9.0\n\n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID_ECR }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY_ECR }}\n          aws-region: us-east-1\n          mask-aws-account-id: false # otherwise the mask will hide your account ID and cause errors in the deployment\n\n      - name: Login to AWS ECR\n        id: login-ecr\n        uses: aws-actions/amazon-ecr-login@v2\n\n      - name: Set up Docker Buildx for cache\n        uses: docker/setup-buildx-action@v3\n\n      - name: Expose GitHub Runtime for cache\n        uses: crazy-max/ghaction-github-runtime@v3\n\n      - name: Create .env file\n        run: |\n          cd &lt;YOUR_PROJECT_ROOT&gt;/deploy\n          touch .env\n          echo KAMAL_REGISTRY_PASSWORD=\"${{ steps.login-ecr.outputs.docker_password_&lt;YOUR_ACCOUNT_ID&gt;_dkr_ecr_&lt;YOUR_REGION&gt;_amazonaws_com }}\" &gt;&gt; .env\n          echo DJANGO_SECRET_KEY=\"${{ secrets.DJANGO_SECRET_KEY }}\" &gt;&gt; .env\n          # if you have other secrets, add them here\n          cat .env\n\n      - name: Kamal Deploy\n        id: kamal-deploy\n        run: |\n          cd &lt;YOUR_PROJECT_ROOT&gt;/deploy\n          kamal lock release\n          kamal env push\n          kamal deploy\nThis workflow will:\n\nCheckout the code\nSet up the Ruby environment and install Kamal\nConfigure the AWS credentials\nLogin to the AWS ECR registry\nSet up Docker Buildx for cache\nExpose GitHub Runtime for cache\nCreate the .env file\nRun Kamal deploy\n\nIt will run everytime you make a push to the main branch or by manually triggering the workflow. It’ll cancel any in-progress runs to avoid conflicts.\nAlso, before you push your code to the repository, you’ll need to add the following secrets to the repository:\n\nVPS_SSH_PRIVATE_KEY: The private key to connect to your VPS\nAWS_ACCESS_KEY_ID_ECR: The access key ID for the AWS ECR registry\nAWS_SECRET_ACCESS_KEY_ECR: The secret access key for the AWS ECR registry\nDJANGO_SECRET_KEY: The Django secret key\n\nFinally, to speed up the deployment, add these options to the builder section of the deploy.yml file:\nbuilder:\n  dockerfile: \"../Dockerfile\"\n  context: \"../\"\n  multiarch: false # new\n  cache: # new\n    type: gha # new\nThis will enable the Docker Buildx cache for the build process in Github Actions. You can set multiarch to false if your CI pipeline shares the same architecture as your VPS, which was the case for me."
  },
  {
    "objectID": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#conclusion",
    "href": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#conclusion",
    "title": "Deploying a Django app with Kamal, AWS ECR, and Github Actions",
    "section": "Conclusion",
    "text": "Conclusion\nYou now have a fully automated deployment pipeline for your Django app. A push to the main branch will trigger the workflow, which will build the Docker image, push it to the ECR registry, and deploy it to your VPS.\nBreak free from the tyranny of manual deployments and expensive cloud services. Sleep like a baby and let Kamal handle your deployments.\nIf you have any questions or feedback, please feel free to leave a comment below."
  },
  {
    "objectID": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#footnotes",
    "href": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#footnotes",
    "title": "Deploying a Django app with Kamal, AWS ECR, and Github Actions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ncrying and sh*tting my diapers?↩︎"
  },
  {
    "objectID": "posts/my-entrepreneur-first-experience.html",
    "href": "posts/my-entrepreneur-first-experience.html",
    "title": "My Experience at Entrepreneur First",
    "section": "",
    "text": "On September 2, 2022, I received an invitation to join the 10th cohort of Entrepreneur First (EF) in Berlin. Less than a month later, I packed up and moved there, eager to find a cofounder to begin what I hoped would become a billion-dollar company.\nOver the past three months, I’ve spent many hours brainstorming with other founders, talking to +100 potential customers, and practicing our pitch for investors. Last week, my cofounder and I pitched our idea to EF’s Investment Committee (IC). Unfortunately, they informed us today that they’ve decided not to invest in our team.\nI’m writing this to reflect on my experience during these past three months. I’ll provide some background on what EF is and how it works, and then go into what I liked and didn’t like about the program. If you’re considering participating in Entrepreneur First, I hope this article will help you make an informed decision."
  },
  {
    "objectID": "posts/my-entrepreneur-first-experience.html#whats-entrepreneur-first",
    "href": "posts/my-entrepreneur-first-experience.html#whats-entrepreneur-first",
    "title": "My Experience at Entrepreneur First",
    "section": "What’s Entrepreneur First?",
    "text": "What’s Entrepreneur First?\nEF is a global organization that helps talented individuals start companies. They provide funding, mentorship, and resources to individuals with the potential to become successful tech entrepreneurs. EF focuses on assisting individuals with technical and business backgrounds in forming teams and building startups from the ground up. It has programs in Singapore, India, the United Kingdom, Canada, Germany, and France.\nThe program connects you with a group of roughly 50 individuals who have the ambition, urgency, and skills required to build a startup. You receive a stipend of 2,000€ per month for the first three months, but you have to work on the program full-time in return. This will allow you to solely focus on finding your cofounder and developing your idea. In our cohort, most participants had recently quit their jobs or finished their studies (usually PhDs) before joining.\nIf your team does well in the first three months, you may be eligible for an initial investment of £80,000 (as of January 2023)."
  },
  {
    "objectID": "posts/my-entrepreneur-first-experience.html#hows-the-application-process",
    "href": "posts/my-entrepreneur-first-experience.html#hows-the-application-process",
    "title": "My Experience at Entrepreneur First",
    "section": "How’s the Application Process?",
    "text": "How’s the Application Process?\nThe application process is pretty simple. One of EF’s recruiters contacted me on LinkedIn and asked if I wanted to join the program. Then he asked me to fill out an application. Most of the questions on the form were about why I wanted to join the program and why I would be a good fit for it. Then, I interviewed with the Berlin program’s general manager and the recruiter who had contacted me.\nI could have done a better job in the interview because I ended up rambling through some of the questions. But I was still able to pique people’s interest when I described some of the strategies I used to get more than 100,000 users to play Fast Flood when I launched it. Overall, it was a positive experience, and I got in.\nAfter talking with other participants, I found that the process was similar for everyone. The only difference appears to be the number of interviews you might do, which ranged from one to three. If you know someone from a previous cohort, you should definitely ask for a referral, as it will probably increase your chances of getting in."
  },
  {
    "objectID": "posts/my-entrepreneur-first-experience.html#whats-the-structure-of-the-program",
    "href": "posts/my-entrepreneur-first-experience.html#whats-the-structure-of-the-program",
    "title": "My Experience at Entrepreneur First",
    "section": "What’s the Structure of the Program?",
    "text": "What’s the Structure of the Program?\nThe program is split into two parts. Form, which lasts three months and culminates with you pitching your idea to IC, and Launch, which lasts six months and is only available if you are selected for funding after IC. Since I just finished Form and cannot access Launch, I will only talk about that part of the program in this article.\nForm is meant to help you find a cofounder, come up with ideas, and get customer feedback on them. During the first eight weeks of this phase, most of your time should be spent looking for the right cofounder. If, after this period, you’re not on a team, you have to leave the program, but you’ll still get your full stipend.\nDuring this time, you mostly do one of two things: participate in a sort of speed-dating event for entrepreneurs or test your idea with customers. Once the Form phase is over, you’ll only be working on your company.\nThe speed-dating contest consists of you talking to potential cofounders about business ideas (aka ideation sessions) to see if there’s anything you both want to work on. If you feel there’s a “match” with another cofounder, then you can team up and work on your idea. If you later realize you weren’t a good fit, you can split up and go back to the pool of sole founders to find another cofounder.\nWhen working in a team, most of the time is spent on three main tasks. The first task is to reach out to potential customers through your personal network, LinkedIn, and other platforms to understand their pain points and determine if your idea addresses them. The second task is to process feedback from customer interviews to update your hypotheses about the business. The third task is to make sure that the team is aiming for a big market, that the company has some way to defend itself, and that any other important factors are taken into account so that the company can be successful in the future.\nDuring the first few weeks of Form, you could also choose to attend several workshops about entrepreneurship and new businesses. We also had a few regular activities:\n\nWeekly check-ins with your venture partner and venture developer: You get feedback on your idea and provide progress updates. In my case, the venture partner was a previous EF founder who sold his company to Twitter, and the venture developer was an expert in product development. Both provided very useful feedback throughout the program.\nWeekly check-in with your Form representative: You can ask questions about the program and talk about how you feel as the program progresses.\nBi-weekly Friday pitches: You present your idea to the rest of the teams and receive feedback.\nWeekly social drinks: You drink beer and talk with other people :)\n\nAs you get closer to IC, you also get invited to other meetings to provide you with more feedback. For IC, you’re asked to provide a pitch video, a slide deck, and a product demo. For the funding decision, they will also look at what your venture partner and developer say about you, as well as what the local EF staff says.\nIn our cohort, many participants tried working with multiple teams before finding the right fit. I tried three teams. In total, 49 teams were formed (h/t Simon Farshid for tracking this), and 13 teams presented their ideas to IC. Roughly half of the participants were not in teams by the end of the Form phase."
  },
  {
    "objectID": "posts/my-entrepreneur-first-experience.html#what-i-liked-about-entrepreneur-first",
    "href": "posts/my-entrepreneur-first-experience.html#what-i-liked-about-entrepreneur-first",
    "title": "My Experience at Entrepreneur First",
    "section": "What I liked about Entrepreneur First",
    "text": "What I liked about Entrepreneur First\nBy now, you should have a good idea of what to expect during the program. Now, let me tell you what I liked about EF:\n\nThe people: The best thing about the program is the connections you make. You join a group of highly skilled people who are eager to start their own businesses. As my career progressed, I learned that not many people wanted that, so it was great to meet so many like-minded people. I’ve also made a few good friends during the program!\nPressure to talk to customers: There’s a lot of emphasis on talking to customers throughout the program, and I really appreciated that. This forced me to step outside of my comfort zone because I didn’t feel as comfortable reaching out to so many people before.\nRevealed preferences: I wasn’t expecting many people to drop out of the program before it finished, but it happened. Many people realize they don’t want to start a business after all. Even though there are ups and downs, I’ve never doubted my desire to start a business, so I was happy to find another data point that I’m on the right track. Overall, I believe this is a low-risk way for people to figure out if they really want to start a business.\nEF’s network: EF alumni have interesting positions in top companies and startups. They’re also very open to chatting. It’s great to know that you’re a short message away from talking to people with relevant positions in successful companies.\n\nOverall, I see many positive sides to the EF experience."
  },
  {
    "objectID": "posts/my-entrepreneur-first-experience.html#what-i-didnt-like-about-entrepreneur-first",
    "href": "posts/my-entrepreneur-first-experience.html#what-i-didnt-like-about-entrepreneur-first",
    "title": "My Experience at Entrepreneur First",
    "section": "What I didn’t like about Entrepreneur First",
    "text": "What I didn’t like about Entrepreneur First\nWhile my experience with EF was mostly positive, there were a few areas that I felt could have been improved. I noticed a few things that seemed to make it more difficult to focus on building a company, even though they were intended to help.\nThese are some of the things I didn’t like:\n\nToo much overhead: By the end of Form, I felt like we had too many unnecessary meetings. I often wondered if it would make sense to combine a few of those meetings to reduce the time it takes. For example, we had two separate meetings with our venture partner and developer, to essentially discuss the same topic. Why not combine those two meetings into one?\nThe Entrepreneur Game: I had the impression that EF wanted us to believe they had everything figured out and that there was a clear path to building a unicorn that we simply had to follow. It was kind of a game, and all we had to do to win was complete all the levels (e.g., find a counterintuitive belief on which to base your company, do 20 customer interviews per week, etc.). Why would most businesses fail if there was such a simple formula for success? The devil is in the details, and I think that founders who do well do so because they keep on grinding for a long time on a good problem space rather than because they find a foolproof recipe or framework. All frameworks for building startups are limited because the real world is opaque and the small, difficult-to-understand details are what matter most.\nCheck-ins should not be pitches. You get “graded” after the check-ins, which creates an incentive to turn those check-ins into pitching sessions. People have fewer incentives to ask difficult questions, as that may cast them in a negative light. Also, this wasn’t communicated clearly from the beginning.\n\nDespite these issues, I still believe that the EF staff works hard and has the challenging task of serving an audience with a high bar. They are trying to help entrepreneurs build successful companies."
  },
  {
    "objectID": "posts/my-entrepreneur-first-experience.html#whats-next-for-me",
    "href": "posts/my-entrepreneur-first-experience.html#whats-next-for-me",
    "title": "My Experience at Entrepreneur First",
    "section": "What’s next for me?",
    "text": "What’s next for me?\nEven though I didn’t reach the goal I set for myself when I joined EF, I’ve learned a lot and met some great people in the last three months. I will move forward with building a company but want to make sure that I use the lessons learned during the program. I’ll take a few days to reflect on the best path forward.\nThe past three months have been hectic, but I’ve realized that I often fell into the trap of simply “playing the game” of being an entrepreneur rather than focusing on creating something people truly wanted. I don’t plan on making that mistake again."
  },
  {
    "objectID": "posts/my-entrepreneur-first-experience.html#conclusion",
    "href": "posts/my-entrepreneur-first-experience.html#conclusion",
    "title": "My Experience at Entrepreneur First",
    "section": "Conclusion",
    "text": "Conclusion\nIf I knew what I know now, would I have done the program? Definitely.\nEven though there were things I didn’t like about it, I learned a lot during the past three months. I liked my time with EF, and I would recommend it to anyone who wants to start a venture-backed business.\nThere are always ways to make things better, but I know that the EF team is working very hard to create the best program possible. I have nothing but gratitude for the EF team and everyone I met through the program. This experience made me better."
  },
  {
    "objectID": "posts/my-entrepreneur-first-experience.html#addendum-how-to-prepare-for-the-interview-at-ef",
    "href": "posts/my-entrepreneur-first-experience.html#addendum-how-to-prepare-for-the-interview-at-ef",
    "title": "My Experience at Entrepreneur First",
    "section": "Addendum: How to Prepare for the Interview at EF?",
    "text": "Addendum: How to Prepare for the Interview at EF?\nA few people have asked me on LinkedIn about what’s the best way to prepare for the interview at EF. Here’s my advice:\n\nPrepare a 1-min pitch that explains who you are and why you want to start a company with EF.\nThink about your previous experience and projects, figure out what are the most interesting parts of them, and practice breaking them down to others in a compelling way (focusing on their business impact).\nHave good answers to the following questions:\n\nWhy do you want to start a company?\nWhy are you the right person to start a company with EF?\nHow is X going to change in the future? What’d be the impact of those changes? (replace X with your field of expertise)\nWhat would you do if EF doesn’t work out?\n\n\nHope this is useful!"
  },
  {
    "objectID": "posts/llm-pydantic-order-matters.html",
    "href": "posts/llm-pydantic-order-matters.html",
    "title": "Structured outputs: don’t put the cart before the horse",
    "section": "",
    "text": "Not long ago, you couldn’t reliably ask an LLM to provide you with a response using a specific format. Building tools that used LLM outputs was painful.\nThen, through function calling and structured outputs, we could instruct LLMs to respond in specific formats1. So, extracting information from LLM outputs stopped being a problem.\nBut then I started noticing that structured outputs also had their own set of problems. Most importantly, the apparent rigidity of a Pydantic model can make you forget that underneath, you’re still dealing with an LLM. Setting up a response model for your API calls is not the same as setting up a response model for your LLM outputs.\nFor example, take the following question from the LiveBench dataset:\nLet’s say I write a simple system prompt and two Pydantic models to format the responses:\nDo you think that there will be a difference in performance between ResponseFormatA and ResponseFormatB? If so, which one do you think will perform better?\nNot sure? Well, you’re in luck! Let’s run some experiments to find out."
  },
  {
    "objectID": "posts/llm-pydantic-order-matters.html#set-up-the-environment",
    "href": "posts/llm-pydantic-order-matters.html#set-up-the-environment",
    "title": "Structured outputs: don’t put the cart before the horse",
    "section": "Set up the environment",
    "text": "Set up the environment\nFirst, start by importing the necessary libraries:\n\nimport asyncio\nimport json\nfrom asyncio import Semaphore\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom langsmith import traceable\nfrom langsmith.wrappers import wrap_openai\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\nfrom scipy import stats\n\nnp.random.seed(42)\n\nload_dotenv()\n\nclient = wrap_openai(AsyncOpenAI())\n\nThis will set up all the necessary infrastructure to run the experiments. I like using LangSmith to track runs.\nTo run the experiment, you need some data. I ended up using a subset of the reasoning questions from LiveBench. You can download it and save it in the data directory.\nThen, you can read it into a pandas DataFrame:\n\ndata_dir = Path().absolute().parent / \"data\" / \"live_bench\"\nreasoning_dir = data_dir / \"reasoning\"\nlive_bench_json = reasoning_dir / \"question.jsonl\"\n\ndf = (\n    pd.read_json(live_bench_json, lines=True)\n    .query(\"livebench_release_date == '2024-07-26'\")\n    .assign(\n        turns_str=lambda x: x.turns.str[0], \n        expects_integer=lambda x: x.turns.str[0].str.contains(\"integer\", case=False)\n    )\n    .reset_index()\n    .rename(columns={\"index\": \"data_point_id\"})\n)\n\nNext, define the system prompt and the Pydantic models you’ll use to format the responses:\n\nsystem_prompt_template = (\n    \"You're a helpful assistant. You will help me answer a question.\"\n    \"\\nYou will use this JSON schema for your response:\"\n    \"\\n{response_format}\"\n)\n\nclass ResponseFormatA(BaseModel):\n    reasoning: str\n    answer: str \n\nclass ResponseFormatB(BaseModel):\n    answer: str \n    reasoning: str\n\nIn the system prompt you send to the LLM, you’ll replace {response_format} with the JSON schema of the response format you want to use.\nThen, let’s define a few helper functions to run the experiment:\n\ndef validate_response(response_json, response_format):\n    response_dict = json.loads(response_json)\n    expected_keys = list(response_format.model_json_schema()[\"properties\"].keys())\n    actual_keys = list(response_dict.keys())\n    if actual_keys != expected_keys:\n        raise ValueError(f\"Response keys {actual_keys} do not match expected keys {expected_keys}\")\n    return response_format.model_validate_json(response_json)\n\n@traceable\nasync def process_row(\n    row: pd.Series, \n    response_format: ResponseFormatA | ResponseFormatB, \n    semaphore: Semaphore\n) -&gt; ResponseFormatA | ResponseFormatB:\n    system_prompt = system_prompt_template.format(\n        response_format=response_format.model_json_schema()\n    )\n    async with semaphore:\n        for _ in range(3):\n            try:\n                response = await client.chat.completions.create(\n                    model=\"gpt-4o\", \n                    messages=[\n                        {\"role\": \"system\", \"content\": system_prompt},\n                        {\"role\": \"user\", \"content\": f\"Question:\\n{row.turns_str}\"}\n                    ],\n                    response_format={\"type\": \"json_object\"}\n                )\n                response_json = response.choices[0].message.content\n                return validate_response(response_json, response_format)\n            except Exception:\n                pass\n        raise Exception(\"Failed to generate a valid response\")\n\n@traceable\nasync def main(df, response_format, concurrency: int = 30):\n    semaphore = Semaphore(concurrency)\n    tasks = [process_row(row, response_format, semaphore) for _, row in df.iterrows()]\n    responses = await asyncio.gather(*tasks)\n\n    return responses\n\ndef extract_answer(answer):\n    return str(answer).replace(\"**\", \"\").strip()\n\nIn this code, validate_response is used to check if the response is valid (i.e. it matches the JSON schema in the same order). If it is, it returns the response. Otherwise, it raises an exception.\nextract_answer is used to remove ** from the answer if it exists in the response. Some of the questions in the LiveBench dataset included instructions to put the answer in bold, which is why we need to remove it.\nprocess_row is used to process a single row of the DataFrame. It sends the system prompt to the LLM and validates the response. It includes a simple retry mechanism in case the validation fails. Each run is tracked in LangSmith.\nFinally, main is used to run the experiment. It runs the process_row function concurrently for each row in the DataFrame."
  },
  {
    "objectID": "posts/llm-pydantic-order-matters.html#running-the-experiment",
    "href": "posts/llm-pydantic-order-matters.html#running-the-experiment",
    "title": "Structured outputs: don’t put the cart before the horse",
    "section": "Running the experiment",
    "text": "Running the experiment\nNow, you can run the experiment using the two response formats:\n\nn_runs = 3\ndf_runs = []\n\nfor run in range(n_runs):\n    print(f\"Run {run + 1}/{n_runs}\")\n    df_copy = df.copy()\n    \n    responses_A = asyncio.run(main(df_copy, ResponseFormatA))\n    df_copy[\"raw_answer_A\"] = [r.answer for r in responses_A]\n    df_copy[\"response_A\"] = df_copy[\"raw_answer_A\"].apply(extract_answer)\n    df_copy[\"is_correct_A\"] = (df_copy[\"response_A\"] == df_copy[\"ground_truth\"]).astype(int)\n    \n    responses_B = asyncio.run(main(df_copy, ResponseFormatB))\n    df_copy[\"raw_answer_B\"] = [r.answer for r in responses_B]\n    df_copy[\"response_B\"] = df_copy[\"raw_answer_B\"].apply(extract_answer)\n    df_copy[\"is_correct_B\"] = (df_copy[\"response_B\"] == df_copy[\"ground_truth\"]).astype(int)\n    \n    df_copy[\"run\"] = run\n    df_run = df_copy[[\"data_point_id\", \"ground_truth\", \"is_correct_A\", \"is_correct_B\", \"run\"]]\n    \n    df_runs.append(df_run)\n\nWe run the experiment multiple times with the same inputs to account for the randomness in the LLM’s responses. Ideally, we should run it more than three times, but I’m poor. So, we’ll just do it 3 times.\n\ndf_all_runs = pd.concat(df_runs, ignore_index=True)\n\nn_bootstraps = 10000\nbootstrap_accuracies_A = []\nbootstrap_accuracies_B = []\n\ndata_point_ids = df_all_runs['data_point_id'].unique()\nn_data_points = len(data_point_ids)\n\ngrouped_A = df_all_runs.groupby('data_point_id')['is_correct_A']\ngrouped_B = df_all_runs.groupby('data_point_id')['is_correct_B']\n\ndf_correct_counts_A = grouped_A.sum()\ndf_total_counts_A = grouped_A.count()\ndf_correct_counts_B = grouped_B.sum()\ndf_total_counts_B = grouped_B.count()\n\nfor _ in range(n_bootstraps):\n    sampled_ids = np.random.choice(data_point_ids, size=n_data_points, replace=True)\n    sampled_counts = pd.Series(sampled_ids).value_counts()\n    counts_index = sampled_counts.index\n    \n    total_correct_counts_A = (df_correct_counts_A.loc[counts_index] * sampled_counts).sum()\n    total_observations_A = (df_total_counts_A.loc[counts_index] * sampled_counts).sum()\n    mean_accuracy_A = total_correct_counts_A / total_observations_A\n    bootstrap_accuracies_A.append(mean_accuracy_A)\n    \n    total_correct_counts_B = (df_correct_counts_B.loc[counts_index] * sampled_counts).sum()\n    total_observations_B = (df_total_counts_B.loc[counts_index] * sampled_counts).sum()\n    mean_accuracy_B = total_correct_counts_B / total_observations_B\n    bootstrap_accuracies_B.append(mean_accuracy_B)\n\nci_A = np.percentile(bootstrap_accuracies_A, [2.5, 97.5])\nci_B = np.percentile(bootstrap_accuracies_B, [2.5, 97.5])\n\nmean_accuracy_A = df_all_runs['is_correct_A'].mean()\nmean_accuracy_B = df_all_runs['is_correct_B'].mean()\n\nprint(\n    f\"Response format A - Mean: {mean_accuracy_A * 100:.2f}% CI: {ci_A[0] * 100:.2f}% - {ci_A[1] * 100:.2f}%\"\n)\nprint(\n    f\"Response format B - Mean: {mean_accuracy_B * 100:.2f}% CI: {ci_B[0] * 100:.2f}% - {ci_B[1] * 100:.2f}%\"\n)\n\nThen, you can build bootstrap confidence intervals for the accuracies of the two response formats. Given that I’m asking the LLM the same question multiple times, I went with an approach called cluster bootstrapping, which accounts for the fact that the data points are not independent.\nIt should take a few seconds to run. Once it’s done, you should see output like the following:\n\n\n\nResponse Format\nAccuracy (95% CI)\n\n\n\n\nA\n46.67% (35.33% – 58.00%)\n\n\nB\n33.33% (22.67% – 44.67%)\n\n\n\nThese results suggest that the order of the fields in the JSON schema does matter.\nBut if you’re still unsure, you can perform a t-test to see if the two response formats are statistically different:\n\naccuracies_A = df_all_runs.pivot(index='data_point_id', columns='run', values='is_correct_A')\naccuracies_B = df_all_runs.pivot(index='data_point_id', columns='run', values='is_correct_B')\n\nmean_accuracies_A = accuracies_A.mean(axis=1)\nmean_accuracies_B = accuracies_B.mean(axis=1)\n\nt_stat, p_value = stats.ttest_rel(mean_accuracies_A, mean_accuracies_B, alternative='greater')\n\nprint(f\"t-statistic: {t_stat}, p-value: {p_value}\")\n\nI got a p-value &lt;0.01, meaning I can reject the null hypothesis that the two response formats are the same."
  },
  {
    "objectID": "posts/llm-pydantic-order-matters.html#conclusion",
    "href": "posts/llm-pydantic-order-matters.html#conclusion",
    "title": "Structured outputs: don’t put the cart before the horse",
    "section": "Conclusion",
    "text": "Conclusion\nBased on the results of the experiment, we can safely say that ResponseFormatA is better than ResponseFormatB.\nBut why?\nIn this case, it’s simple.\nThese response formats are meant to help the LLM reason step by step to arrive at the answer. This is known as chain of thought reasoning. However, for it to work, we need the LLM to first provide us with the reasoning of how it arrived at the answer and then the answer.\nIn ResponseFormatA, we defined our Pydantic model with the reasoning first and the answer second. This means that the LLM will give us the reasoning first, and then provide the answer. Which is exactly what we want.\nResponseFormatB works in the opposite way. This means that the LLM will give us the answer first, and then provide the reasoning. So our chain of thought reasoning becomes a zero-shot prompt. In this case, the reasoning is a byproduct of the answer.\nSo, to summarize, when using structured outputs, don’t put the cart before the horse.\nThat’s all! Let me know if you have any questions in the comments."
  },
  {
    "objectID": "posts/llm-pydantic-order-matters.html#footnotes",
    "href": "posts/llm-pydantic-order-matters.html#footnotes",
    "title": "Structured outputs: don’t put the cart before the horse",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’m referring to OpenAI models here. Open weight models allowed this using grammars.↩︎"
  }
]