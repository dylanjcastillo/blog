{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3b314c8",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Will they let you say what you mean? Proprietary LLMs and structured outputs\"\n",
    "date: \"11/30/2024\"\n",
    "date-modified: last-modified\n",
    "description-meta: \"Reviewing the performance of proprietary LLMs with structured outputs\"\n",
    "toc: true\n",
    "toc-depth: 3\n",
    "lightbox: true\n",
    "fig-cap-location: margin\n",
    "categories:\n",
    "  - llm\n",
    "  - openai\n",
    "  - pydantic\n",
    "  - python\n",
    "author:\n",
    "  - name: Dylan Castillo\n",
    "    url: https://dylancastillo.co\n",
    "    affiliation: Iwana Labs\n",
    "    affiliation-url: https://iwanalabs.com\n",
    "citation: true\n",
    "comments:\n",
    "  utterances:\n",
    "    repo: dylanjcastillo/blog_comments\n",
    "    theme: dark-blue\n",
    "    issue-term: pathname\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18dc8ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65549cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33194d54-582b-4b81-a195-8f1269a438a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T20:37:59.673511Z",
     "iopub.status.busy": "2024-11-17T20:37:59.673328Z",
     "iopub.status.idle": "2024-11-17T20:38:07.222035Z",
     "shell.execute_reply": "2024-11-17T20:38:07.221730Z",
     "shell.execute_reply.started": "2024-11-17T20:37:59.673491Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dcast/Documents/GitHub/blog/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "\n",
    "import asyncio\n",
    "import re\n",
    "from asyncio import Semaphore\n",
    "from enum import Enum\n",
    "from textwrap import dedent\n",
    "from typing import List\n",
    "\n",
    "import instructor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from openai import AsyncOpenAI\n",
    "from openai.types.chat import ChatCompletion\n",
    "from pydantic import BaseModel\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "evals = list(dataset[\"test\"])\n",
    "MODEL_NAME = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07712cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = wrap_openai(AsyncOpenAI())\n",
    "tool_calls_client = instructor.from_openai(client, mode=instructor.Mode.TOOLS)\n",
    "json_mode_client = instructor.from_openai(client, mode=instructor.Mode.JSON)\n",
    "strict_tool_calls_client = instructor.from_openai(\n",
    "    client, mode=instructor.Mode.TOOLS_STRICT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39c919d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response(BaseModel):\n",
    "    reasoning: str\n",
    "    answer: int\n",
    "\n",
    "\n",
    "class PromptType(Enum):\n",
    "    WITHOUT_STRUCTURED_OUTPUT = \"without_so\"\n",
    "    WITH_TOOL_CALLS = \"with_so_tool_calls\"\n",
    "    WITH_JSON_MODE = \"with_so_json_mode\"\n",
    "    WITH_STRICT_TOOL_CALLS = \"with_so_strict_tool_calls\"\n",
    "\n",
    "\n",
    "class ClientConfig(BaseModel):\n",
    "    name: str\n",
    "    use_json_format: bool\n",
    "    col_name: str\n",
    "    score_col_name: str\n",
    "\n",
    "\n",
    "CLIENT_MAPPING = {\n",
    "    PromptType.WITHOUT_STRUCTURED_OUTPUT.value: client,\n",
    "    PromptType.WITH_TOOL_CALLS.value: tool_calls_client,\n",
    "    PromptType.WITH_JSON_MODE.value: json_mode_client,\n",
    "    PromptType.WITH_STRICT_TOOL_CALLS.value: strict_tool_calls_client,\n",
    "}\n",
    "\n",
    "CONFIGS = [\n",
    "    ClientConfig(\n",
    "        name=PromptType.WITHOUT_STRUCTURED_OUTPUT.value,\n",
    "        use_json_format=False,\n",
    "        col_name=f\"response_{PromptType.WITHOUT_STRUCTURED_OUTPUT.value}\",\n",
    "        score_col_name=f\"score_{PromptType.WITHOUT_STRUCTURED_OUTPUT.value}\",\n",
    "    ),\n",
    "    ClientConfig(\n",
    "        name=PromptType.WITH_TOOL_CALLS.value,\n",
    "        use_json_format=True,\n",
    "        col_name=f\"response_{PromptType.WITH_TOOL_CALLS.value}\",\n",
    "        score_col_name=f\"score_{PromptType.WITH_TOOL_CALLS.value}\",\n",
    "    ),\n",
    "    ClientConfig(\n",
    "        name=PromptType.WITH_JSON_MODE.value,\n",
    "        use_json_format=True,\n",
    "        col_name=f\"response_{PromptType.WITH_JSON_MODE.value}\",\n",
    "        score_col_name=f\"score_{PromptType.WITH_JSON_MODE.value}\",\n",
    "    ),\n",
    "    ClientConfig(\n",
    "        name=PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "        use_json_format=True,\n",
    "        col_name=f\"response_{PromptType.WITH_STRICT_TOOL_CALLS.value}\",\n",
    "        score_col_name=f\"score_{PromptType.WITH_STRICT_TOOL_CALLS.value}\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "531655e1-b4a3-4d83-8d22-be34d7667b64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T20:38:34.799082Z",
     "iopub.status.busy": "2024-11-17T20:38:34.798903Z",
     "iopub.status.idle": "2024-11-17T20:38:34.800913Z",
     "shell.execute_reply": "2024-11-17T20:38:34.800585Z",
     "shell.execute_reply.started": "2024-11-17T20:38:34.799072Z"
    }
   },
   "outputs": [],
   "source": [
    "example_question = [\n",
    "    \"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\",\n",
    "    \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\",\n",
    "    \"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\",\n",
    "]\n",
    "\n",
    "example_explanation = [\n",
    "    \"There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6.\",\n",
    "    \"There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5.\",\n",
    "    \"Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39.\",\n",
    "]\n",
    "\n",
    "example_answer = [6, 5, 39]\n",
    "\n",
    "\n",
    "@traceable(run_type=\"prompt\")\n",
    "def create_prompt(question, use_json_format=True):\n",
    "    json_system_prompt = dedent(\"\"\"\n",
    "        You are an expert in solving grade school math tasks. You will be presented with a grade-school math word problem and be asked to solve it.\n",
    "        Before answering you should reason about the problem (using the \"reasoning\" field in the JSON response described below).\n",
    "          \n",
    "        You will always respond with JSON in the format described below:\n",
    "        \n",
    "        {\"reasoning\": <str, reasoning about the answer>, \"answer\": <int, final answer>}\n",
    "\n",
    "        In the \"reasoning\" field, you should explain your reasoning about the sequence of events. In the \"answer\" field, you should provide an integer that corresponds to the answer to the question. Don't include any other text in the \"answer\" field.\n",
    "        \"\"\")\n",
    "\n",
    "    explanation_system_prompt = dedent(\"\"\"\n",
    "        You are an expert in solving grade school math tasks. You will be presented with a grade-school math word problem and be asked to solve it.\n",
    "        Before answering, you should explain your reasoning step by step. Then, at the end, you should provide your final answer.\n",
    "        \n",
    "        You will always respond in the following format:\n",
    "        \n",
    "        <str, reasoning about the answer>\n",
    "        ANSWER: <int, final answer>\n",
    "        \n",
    "        In ANSWER, you should provide an integer that corresponds to the answer to the question. Don't include any other text in the ANSWER.\n",
    "        \"\"\")\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": json_system_prompt\n",
    "            if use_json_format\n",
    "            else explanation_system_prompt,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for i in range(len(example_question)):\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Question: {example_question[i]}\"})\n",
    "\n",
    "        if use_json_format:\n",
    "            response = f'{{\"reasoning\": \"{example_explanation[i]}\", \"answer\": {example_answer[i]}}}'\n",
    "        else:\n",
    "            response = f\"{example_explanation[i]}\\nANSWER: {example_answer[i]}\"\n",
    "\n",
    "        messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"Question: {question}\"})\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eb319a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(run_type=\"parser\")\n",
    "def parse_response(response: ChatCompletion | Response, prompt_type: str) -> int | None:\n",
    "    if isinstance(response, Response):\n",
    "        return int(response.answer)\n",
    "    elif (\n",
    "        isinstance(response, ChatCompletion)\n",
    "        and prompt_type == PromptType.WITHOUT_STRUCTURED_OUTPUT.value\n",
    "    ):\n",
    "        response_text = (\n",
    "            response.choices[0].message.content.split(\"\\nANSWER:\")[1].strip()\n",
    "        )\n",
    "        if response_text.isnumeric() and response_text.isdigit():\n",
    "            return int(response_text)\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid response type: {type(response)}\")\n",
    "\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "async def call_model(\n",
    "    client_name: str,\n",
    "    use_json_format: bool,\n",
    "    question: str,\n",
    ") -> Response:\n",
    "    params = {\n",
    "        \"messages\": create_prompt(question=question, use_json_format=use_json_format),\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"timeout\": 120,\n",
    "    }\n",
    "\n",
    "    if client_name in (\n",
    "        PromptType.WITH_TOOL_CALLS.value,\n",
    "        PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "        PromptType.WITH_JSON_MODE.value,\n",
    "    ):\n",
    "        params.update(\n",
    "            {\n",
    "                \"response_model\": Response,\n",
    "            }\n",
    "        )\n",
    "    response = await CLIENT_MAPPING[client_name].chat.completions.create(**params)\n",
    "    return parse_response(response, client_name)\n",
    "\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "async def process_question(\n",
    "    question: str,\n",
    "    client_name: str,\n",
    "    use_json_format: bool,\n",
    "    semaphore: Semaphore,\n",
    "    max_attempts: int = 3,\n",
    ") -> str:\n",
    "    async with semaphore:\n",
    "        for _ in range(max_attempts):\n",
    "            try:\n",
    "                answer = await call_model(\n",
    "                    client_name=client_name,\n",
    "                    use_json_format=use_json_format,\n",
    "                    question=question,\n",
    "                )\n",
    "                return answer\n",
    "            except Exception as e:\n",
    "                print(f\"{client_name}: Error processing question {question}: {e}\")\n",
    "                continue\n",
    "        raise Exception(\n",
    "            f\"{client_name}: Failed to process question {question}, after 3 attempts\"\n",
    "        )\n",
    "\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "async def process_questions(\n",
    "    questions: List[dict],\n",
    "    client_name: str,\n",
    "    use_json_format: bool,\n",
    "    concurrency: int = 100,\n",
    ") -> List[str]:\n",
    "    semaphore = Semaphore(concurrency)\n",
    "    tasks = [\n",
    "        process_question(\n",
    "            question=question[\"question\"],\n",
    "            client_name=client_name,\n",
    "            use_json_format=use_json_format,\n",
    "            semaphore=semaphore,\n",
    "        )\n",
    "        for question in questions\n",
    "    ]\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    return results\n",
    "\n",
    "\n",
    "def extract_gt_answer(question: str) -> int:\n",
    "    raw_int = question.split(\"#### \")[1]\n",
    "    raw_int = re.sub(\",\", \"\", raw_int)\n",
    "    return int(raw_int)\n",
    "\n",
    "\n",
    "def generate_outputs(questions: List[dict]):\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": [i for i in range(len(questions))],\n",
    "            \"question\": [question[\"question\"] for question in questions],\n",
    "            \"answer\": [extract_gt_answer(question[\"answer\"]) for question in questions],\n",
    "        }\n",
    "    )\n",
    "    for config in CONFIGS:\n",
    "        responses = asyncio.run(\n",
    "            process_questions(\n",
    "                questions=questions,\n",
    "                client_name=config.name,\n",
    "                use_json_format=config.use_json_format,\n",
    "            )\n",
    "        )\n",
    "        df[config.col_name] = responses\n",
    "    return df\n",
    "\n",
    "\n",
    "def evaluate_outputs(df):\n",
    "    df_copy = df.copy()\n",
    "    for config in CONFIGS:\n",
    "        df_copy[config.score_col_name] = (\n",
    "            df_copy[\"answer\"] == df_copy[config.col_name]\n",
    "        ) * 1\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc4f6fb",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fcd6a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "def calculate_confidence_intervals(df, conf_level: float = 0.95):\n",
    "    for config in CONFIGS:\n",
    "        score_col = config.score_col_name\n",
    "        scores = df[score_col]\n",
    "\n",
    "        if len(scores) == 0:\n",
    "            print(f\"No scores available for {score_col}\")\n",
    "            continue\n",
    "\n",
    "        mean_score = scores.mean()\n",
    "        se_score = scores.std() / np.sqrt(len(scores))\n",
    "\n",
    "        z_score = stats.norm.ppf((1 + conf_level) / 2)\n",
    "        margin_error = z_score * se_score\n",
    "        ci = [mean_score - margin_error, mean_score + margin_error]\n",
    "\n",
    "        print(\n",
    "            f\"{score_col} - Mean: {mean_score * 100:.2f}% CI: {ci[0] * 100:.2f}% - {ci[1] * 100:.2f}%\"\n",
    "        )\n",
    "    print()\n",
    "\n",
    "\n",
    "def run_paired_t_test(df):\n",
    "    scores = {}\n",
    "\n",
    "    for config in CONFIGS:\n",
    "        score_col = config.score_col_name\n",
    "        scores[score_col] = df[score_col] * 1\n",
    "\n",
    "    for score_col_1, score_col_2 in [\n",
    "        (\"score_without_so\", \"score_with_so_tool_calls\"),\n",
    "        (\"score_without_so\", \"score_with_so_json_mode\"),\n",
    "        (\"score_without_so\", \"score_with_so_strict_tool_calls\"),\n",
    "    ]:\n",
    "        if score_col_1 in scores and score_col_2 in scores:\n",
    "            t_stat, p_value = stats.ttest_rel(scores[score_col_1], scores[score_col_2])\n",
    "            print(f\"{score_col_1} vs {score_col_2}\")\n",
    "            print(f\"t-statistic: {t_stat}, p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8896e2c6",
   "metadata": {},
   "source": [
    "### GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8bb4c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_outputs(evals)\n",
    "df = evaluate_outputs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8a0c084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_without_so - Mean: 93.25% CI: 91.90% - 94.61%\n",
      "score_with_so_tool_calls - Mean: 92.42% CI: 90.99% - 93.85%\n",
      "score_with_so_json_mode - Mean: 91.74% CI: 90.25% - 93.22%\n",
      "score_with_so_strict_tool_calls - Mean: 92.87% CI: 91.48% - 94.26%\n",
      "\n",
      "score_without_so vs score_with_so_tool_calls\n",
      "t-statistic: 1.3058087263195448, p-value: 0.19184547875805577\n",
      "score_without_so vs score_with_so_json_mode\n",
      "t-statistic: 2.42986097186779, p-value: 0.015237431807874796\n",
      "score_without_so vs score_with_so_strict_tool_calls\n",
      "t-statistic: 0.6297966935389578, p-value: 0.5289367690095945\n"
     ]
    }
   ],
   "source": [
    "calculate_confidence_intervals(df)\n",
    "run_paired_t_test(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f6a1b9",
   "metadata": {},
   "source": [
    "### OpenAI Results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
