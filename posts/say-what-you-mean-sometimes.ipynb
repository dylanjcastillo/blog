{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3b314c8",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Say What You Mean... Sometimes\"\n",
    "date: \"12/06/2024\"\n",
    "date-modified: last-modified\n",
    "description-meta: \"A look at the impact of structured outputs on the performance of LLMs.\"\n",
    "toc: true\n",
    "toc-depth: 3\n",
    "lightbox: true\n",
    "fig-cap-location: margin\n",
    "categories:\n",
    "  - llm\n",
    "  - openai\n",
    "  - pydantic\n",
    "  - python\n",
    "author:\n",
    "  - name: Dylan Castillo\n",
    "    url: https://dylancastillo.co\n",
    "    affiliation: Iwana Labs\n",
    "    affiliation-url: https://iwanalabs.com\n",
    "citation: true\n",
    "comments:\n",
    "  utterances:\n",
    "    repo: dylanjcastillo/blog_comments\n",
    "    theme: dark-blue\n",
    "    issue-term: pathname\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f58607",
   "metadata": {},
   "source": [
    "When I read [Let Me Speak Freely?](https://arxiv.org/abs/2408.02442) by Tam et al. I thought they raised an interesting question: does constraining LLM outputs to structured formats impact the quality of their responses?\n",
    "\n",
    "In both the original study and their recent update, Tam et al. concluded that structured generation constraints do indeed affect LLM performance. They found that \"structured generation constraints significantly impact LLM performance across various tasks\".\n",
    "\n",
    "But the study had major flaws. The [.txt](https://dottxt.co/) team wrote a very compelling [rebuttal](https://dottxt.co/blog/let-me-speak-freely) to the paper. They demonstrate that Tam, et al. results were mostly due to poor prompting, unfair comparisons and the improper use of an \"AI parser\" rather than the use of structured outputs (at least for Llama 3 8B). \n",
    "\n",
    "They were right but it still left me wondering how well their results generalize. They only tested Llama 3 8B^[In this setup, but they've also shared results of other open-weight models in a different setup (https://blog.dottxt.co/performance-gsm8k.html)], which represents a small fraction of the LLMs powering applications in production today. Open-weight models offer a lot of flexibility on how to *structure* your output, such as using specific [regex expressions](https://dottxt-ai.github.io/outlines/latest/reference/generation/regex/) that the output must match instead of a specific JSON schema.\n",
    "\n",
    "Proprietary models lack this flexibility. Right now, JSON is the only structured output format guaranteed to work across most providers. Given this constraint, would the .txt team’s results still hold?\n",
    "\n",
    "Plus, both studies focused on tasks that might not be a good proxy for the full range of tasks people use LLMs for. For example, would the rebuttal results be different in settings outside of simple reasoning tasks?\n",
    "\n",
    "So I decided to:\n",
    "\n",
    "1. Replicate the results from .txt's rebuttal using Llama 3 8B.\n",
    "2. Replicate the same tasks using a proprietary model (GPT-4o-mini).\n",
    "3. Test results on a broader set of tasks such as [LiveBench](https://livebench.ai/).\n",
    "\n",
    "This article presents the results of the first two steps.\n",
    "\n",
    "## The results\n",
    "\n",
    "## The original tasks \n",
    "\n",
    "The original reasoning tasks from Tam et al. were:\n",
    "\n",
    "1. [GSM8K](https://huggingface.co/datasets/openai/gsm8k): A dataset from of grade school math word problems.\n",
    "2. [Last Letter](https://huggingface.co/datasets/ChilleD/LastLetterConcat): A dataset of simple word puzzles that require concatening the last letters of a list of names.\n",
    "3. [Shuffle Object](https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/tracking_shuffled_objects): A dataset that requires reasoning about the state of a system after a sequence of shuffling operations.\n",
    "\n",
    "Tam et al. also included four classification tasks in their study, where they observed that structured outputs resulted in better performance. The .txt team's rebuttal excluded these classification tasks. I also believe that structured outputs are likely better for classification tasks. So, I excluded them from my analysis as well.\n",
    "\n",
    "## Replicating .txt's rebuttal\n",
    "\n",
    ".txt made it very easy to reproduce their results by sharing their [code on Github](https://github.com/dottxt-ai/demos/tree/main/say-what-you-mean). I just set up a machine at [Modal](https://modal.com/) and ran the code and reproduced their results.\n",
    "\n",
    "But, while going through the code, I noticed some small issues with the prompts. So I decided to tweak them a bit.\n",
    "\n",
    "Below are .txt's original results compared to mine, after the prompt adjustments:\n",
    "\n",
    "| Task              | Unstructured (.txt) | Structured (.txt) | Unstructured (Me, 3-shot) | Structured (Me, 3-shot) |\n",
    "|-------------------|---------------------|-------------------|-------------------|-----------------|\n",
    "| **GSM8K**         | 77.18                 | 77.79               | 79.98            | 79.45          |\n",
    "| **Last Letter**   | 73.33                 | 77.33               | 74.00            | 78.00          |\n",
    "| **Shuffle Object**| 40.72                 | 44.35               | 42.68            | 43.90          |\n",
    "\n",
    "Except for **Structured** in the **Shuffle Object** task, I was able to improve all the metrics. In **GSM8K's** case, even reversing .txt's result, with **Unstructured** outperforming **Structured** by a small margin.\n",
    "\n",
    "But I don't think this matters much.\n",
    "\n",
    "Their conclusion still holds: structured outputs are either as good as or better than unstructured outputs, in the tasks considered.\n",
    "\n",
    "I'll explain the prompt changes I made below, so that you can judge for yourself if they make sense.\n",
    "\n",
    "#### Formatting few-shot examples\n",
    "\n",
    "In the **GSM8K** and **Last Letter** tasks, the few-shot prompt for both unstructured and structured used examples formatted as JSON objects and asked the LLM to produce the output in the same format, from which the answer was extracted.\n",
    "\n",
    "That felt unfair. Even though you're not formally constraining the LLM to produce a JSON object, you're still asking it to format its response in somewhat unnatural way.\n",
    "\n",
    "I adjusted the prompts to be as similar as possible while still trying to get the most out of each approach.\n",
    "\n",
    "For example, in **GSM8K**, the unstructured prompt is:\n",
    "\n",
    "> You are an expert in solving grade school math tasks. You will be presented with a grade-school math word problem and be asked to solve it.\n",
    "> You will always respond in the following format:\n",
    ">\n",
    "> <str, reasoning about the answer>\n",
    ">\n",
    "> ANSWER: <int, final answer>\n",
    ">\n",
    "> First, provide your step by step reasoning. Then, in ANSWER, provide an integer that corresponds to the correct answer to the question. Don't include any other text in ANSWER.\n",
    "\n",
    "And the structured prompt is:\n",
    "\n",
    "> You are an expert in solving grade school math tasks. You will be presented with a grade-school math word problem and be asked to solve it.\n",
    "> You will always respond in the following format:\n",
    ">\n",
    "> {\"reasoning\": <str, reasoning about the answer>, \"answer\": <int, final answer>}\n",
    ">\n",
    "> First, provide your step by step reasoning about the answer. Then, in the \"answer\" field, provide an integer that corresponds to the correct answer to the question.\n",
    "\n",
    "For all the tasks, I used a 3-shot prompt.\n",
    "\n",
    "#### Clarifying the task\n",
    "\n",
    "I also clarified the prompts. The description of the task in the original **Last Letter** prompt was:\n",
    "\n",
    "> You are an expert in solving simple word puzzles using reasoning steps. Your specific task is going to be to take a list of 4 names and reason about the last letter of each ., then you will concatenate those letters into a word. \n",
    "\n",
    "I changed it to:\n",
    "\n",
    "> You are an expert in solving word puzzles. Your specific task is going to be to take a list of 4 names, get the last letter of each and concatenate these letters into a word. \n",
    "\n",
    "The original prompt was reasonable, but I thought the new version was clearer. Through trial and error, I've learned that when working with LLMs, it's best to be as clear and direct as possible.\n",
    "\n",
    "## Evaluating GPT-4o-mini\n",
    "\n",
    "Using the same setup as before, I ran the same tasks with `gpt-4o-mini-2024-07-18`.\n",
    "\n",
    "Below are the results, including the original results from Tam et al. for comparison:\n",
    "\n",
    "| **Task**        | **Method**   | **NL** | **FRI** | **JSON-Mode** | **JSON-Schema** |\n",
    "|-----------------|--------------|--------|---------|---------------|------------------|\n",
    "| **GSM8K**       | **Tam et al.** | 94.57  | 87.17   | 86.95         | 91.71            |\n",
    "|                 | **Me (0-shot)** | 92.80  | 91.43   | 93.56         | 93.63            |\n",
    "|                 | **Me (3-shot)** | 93.10  | 93.18   | 93.63         | 93.18            |\n",
    "| **Last Letter** | **Tam et al.** | 83.11  | 84.73   | 76.00         | 86.07            |\n",
    "|                 | **Me (0-shot)** | 83.33  | 84.00   | 90.67         | 87.33            |\n",
    "|                 | **Me (3-shot)** | 90.00  | 91.33   | 90.67         | 88.67            |\n",
    "| **Shuffle Obj** | **Tam et al.** | 82.85  | 81.46   | 76.43         | 81.77            |\n",
    "|                 | **Me (0-shot)** | 94.31  | 82.11   | 81.71         | 87.40            |\n",
    "|                 | **Me (3-shot)** | 87.40  | 70.73   | 63.41         | 67.48            |\n",
    "\n",
    "*NL* stands for \"Natural Language\". This would correspond to the *Unstructured* method in the previous table.\n",
    "\n",
    "*FRI* stands for \"Format Restricting Instructions\", which is a JSON generated through the OpenAI's [function calling](https://platform.openai.com/docs/guides/function-calling). *JSON-Mode* is a JSON generated through the OpenAI's [JSON mode](https://platform.openai.com/docs/guides/structured-outputs#json-mode). *JSON-Schema* is a JSON generated using [constrained decoding](https://openai.com/index/introducing-structured-outputs-in-the-api/). \n",
    "\n",
    "*JSON-Schema* would be the most comparable to **Structured** in the previous table but, for real-life applications, you don't really care about how the output is formatted. You just want to get the output in the format you want. So, for the sake of comparison, I will consider the three other methods as structured outputs as well.\n",
    "\n",
    "#### Analyzing the results\n",
    "\n",
    "Similar to what the .txt team found, you can see that after adjusting the prompts, the performance of structured outputs increases substantially compared to Tam et al.\n",
    "\n",
    "Except for *NL* in **GSM8k** and *FRI* in **Last Letter**, I was able to improve all the metrics for both unstructured and structured outputs using a 0-shot prompt. For 3-shot prompts, I improved **GSM8k** and **Last Letter** across all methods, and *NL* in **Shuffle Object**.\n",
    "\n",
    "These counter-intuitive results of 3-shot prompting hurting performance are in line with previous [analyses](https://python.useinstructor.com/blog/2024/09/26/bad-schemas-could-break-your-llm-structured-outputs/?h=bad+sc#modes-and-models). I will dive deeper into this in a future article.\n",
    "\n",
    "**GSM8k** and **Last Letter** show very similar results between unstructured and structured outputs. I don't believe there's enough evidence to say that structured outputs are better than unstructured outputs in these cases.\n",
    "\n",
    "On the other hand, **Shuffle Object** shows a clear advantage for unstructured outputs over structured outputs. I wasn't expecting this, and even after adjusting the prompts, I couldn't reduce the gap significantly.\n",
    "\n",
    " So, even though Tam et al. had issues in their study, their conclusion seems to hold. There are cases where unstructured outputs are better than structured outputs.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "If you're a developer, you'll be pleased to know that the answer to wether you should use structured or unstructured outputs is: [it depends](https://www.reddit.com/r/orlybooks/comments/50meb5/it_depends/).\n",
    "\n",
    "I love using structured outputs in my daily work, because it makes it easier to work with the output of LLMs. I always encourage clients who aren't using them yet to give it a try.\n",
    "\n",
    "But, until there's sufficient evidence in favor of one approach over the other, it's best to test yourself what works best for your specific use case. Don't rely on random posts from random people on the internet. Run your own evals and see for yourself.\n",
    "\n",
    "There's a good chance that structured outputs will have comparable quality to unstructured outputs in most cases. But, especially in cases where you're not generating complex structured outputs, you might be losing out on easy performance gains by not testing out unstructured outputs.\n",
    "\n",
    "The takeaway of this article shouldn't be that structured outputs are better than or worse than unstructured outputs in every possible task. Just that, in some cases, one or the other might be better. \n",
    "\n",
    "So, yes, say what you mean... sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18dc8ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65549cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33194d54-582b-4b81-a195-8f1269a438a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T20:37:59.673511Z",
     "iopub.status.busy": "2024-11-17T20:37:59.673328Z",
     "iopub.status.idle": "2024-11-17T20:38:07.222035Z",
     "shell.execute_reply": "2024-11-17T20:38:07.221730Z",
     "shell.execute_reply.started": "2024-11-17T20:37:59.673491Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dcast/Documents/GitHub/blog/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "from asyncio import Semaphore\n",
    "from enum import Enum\n",
    "from functools import partial\n",
    "from textwrap import dedent\n",
    "from typing import Callable, List, Literal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from openai import AsyncOpenAI\n",
    "from openai.types.chat import ChatCompletion\n",
    "from pydantic import BaseModel, ConfigDict, Field\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini-2024-07-18\"\n",
    "USE_SAMPLE = False \n",
    "MAX_CONCURRENCY = 200\n",
    "\n",
    "client = wrap_openai(AsyncOpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39c919d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "class PromptType(Enum):\n",
    "    WITHOUT_STRUCTURED_OUTPUT = \"without_so\"\n",
    "    WITH_TOOL_CALLS = \"with_so_tool_calls\"\n",
    "    WITH_JSON_MODE = \"with_so_json_mode\"\n",
    "    WITH_STRICT_TOOL_CALLS = \"with_so_strict_tool_calls\"\n",
    "\n",
    "\n",
    "class ClientConfig(BaseModel):\n",
    "    name: str\n",
    "    col_name: str\n",
    "    score_col_name: str\n",
    "\n",
    "\n",
    "CONFIGS = [\n",
    "    ClientConfig(\n",
    "        name=PromptType.WITHOUT_STRUCTURED_OUTPUT.value,\n",
    "        col_name=f\"response_{PromptType.WITHOUT_STRUCTURED_OUTPUT.value}\",\n",
    "        score_col_name=f\"score_{PromptType.WITHOUT_STRUCTURED_OUTPUT.value}\",\n",
    "    ),\n",
    "    ClientConfig(\n",
    "        name=PromptType.WITH_TOOL_CALLS.value,\n",
    "        col_name=f\"response_{PromptType.WITH_TOOL_CALLS.value}\",\n",
    "        score_col_name=f\"score_{PromptType.WITH_TOOL_CALLS.value}\",\n",
    "    ),\n",
    "    ClientConfig(\n",
    "        name=PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "        col_name=f\"response_{PromptType.WITH_STRICT_TOOL_CALLS.value}\",\n",
    "        score_col_name=f\"score_{PromptType.WITH_STRICT_TOOL_CALLS.value}\",\n",
    "    ),\n",
    "    ClientConfig(\n",
    "        name=PromptType.WITH_JSON_MODE.value,\n",
    "        col_name=f\"response_{PromptType.WITH_JSON_MODE.value}\",\n",
    "        score_col_name=f\"score_{PromptType.WITH_JSON_MODE.value}\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eb319a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "class LLMEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        configs: List[ClientConfig],\n",
    "        create_prompt_fn: Callable,\n",
    "        parse_response_fn: Callable,\n",
    "        response_model: BaseModel,\n",
    "        concurrency: int = MAX_CONCURRENCY,\n",
    "    ):\n",
    "        self.configs = configs\n",
    "        self.create_prompt_fn = create_prompt_fn\n",
    "        self.parse_response_fn = parse_response_fn\n",
    "        self.response_model = response_model\n",
    "        self.concurrency = concurrency\n",
    "\n",
    "    def _create_tool_call_schema(\n",
    "        self,\n",
    "        strict: bool = False,\n",
    "    ) -> dict:\n",
    "        model_schema = self.response_model.model_json_schema()\n",
    "        if strict:\n",
    "            return {\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": model_schema[\"title\"],\n",
    "                    \"schema\": model_schema,\n",
    "                    \"strict\": True,\n",
    "                },\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": model_schema[\"title\"],\n",
    "                    \"description\": f\"Correctly extracted `{model_schema['title']}` with all the required parameters with correct types\",\n",
    "                    \"parameters\": model_schema,\n",
    "                },\n",
    "            }\n",
    "\n",
    "    @traceable(run_type=\"prompt\")\n",
    "    def create_prompt(\n",
    "        self,\n",
    "        question: str,\n",
    "        prompt_type: str,\n",
    "    ) -> List[dict]:\n",
    "        return self.create_prompt_fn(\n",
    "            question=question,\n",
    "            prompt_type=prompt_type,\n",
    "            response_model=self.response_model,\n",
    "        )\n",
    "\n",
    "    @traceable(run_type=\"parser\")\n",
    "    def parse_response(\n",
    "        self,\n",
    "        response: ChatCompletion,\n",
    "        prompt_type: str,\n",
    "    ) -> str | int:\n",
    "        return self.parse_response_fn(response, prompt_type)\n",
    "\n",
    "    @traceable(run_type=\"llm\")\n",
    "    async def call_llm(\n",
    "        self,\n",
    "        config: ClientConfig,\n",
    "        question: str,\n",
    "    ) -> ChatCompletion:\n",
    "        params = {\n",
    "            \"messages\": self.create_prompt(question=question, prompt_type=config.name),\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"timeout\": 120,\n",
    "        }\n",
    "\n",
    "        prompt_type_configs = {\n",
    "            PromptType.WITH_JSON_MODE.value: {\n",
    "                \"response_format\": {\"type\": \"json_object\"}\n",
    "            },\n",
    "            PromptType.WITH_TOOL_CALLS.value: {\n",
    "                \"tools\": [self._create_tool_call_schema(strict=False)]\n",
    "            },\n",
    "            PromptType.WITH_STRICT_TOOL_CALLS.value: {\n",
    "                \"response_format\": self._create_tool_call_schema(strict=True)\n",
    "            },\n",
    "        }\n",
    "\n",
    "        if config.name in prompt_type_configs:\n",
    "            params.update(prompt_type_configs[config.name])\n",
    "\n",
    "        completion = await client.chat.completions.create(**params)\n",
    "\n",
    "        if config.name == PromptType.WITH_TOOL_CALLS.value:\n",
    "            response_content = (\n",
    "                completion.choices[0].message.tool_calls[0].function.arguments\n",
    "            )\n",
    "        else:\n",
    "            response_content = completion.choices[0].message.content\n",
    "\n",
    "        return response_content\n",
    "\n",
    "    @traceable(run_type=\"chain\")\n",
    "    async def process_question(\n",
    "        self,\n",
    "        question: str,\n",
    "        config: ClientConfig,\n",
    "        semaphore: Semaphore,\n",
    "        max_attempts: int = 3,\n",
    "    ) -> str | int | None:\n",
    "        async with semaphore:\n",
    "            for _ in range(max_attempts):\n",
    "                try:\n",
    "                    answer = await self.call_llm(\n",
    "                        config=config,\n",
    "                        question=question,\n",
    "                    )\n",
    "                    parsed_answer = self.parse_response(answer, config.name)\n",
    "                    return parsed_answer\n",
    "                except Exception:\n",
    "                    print(f\"{config.name}, {question[:10]}: Retrying...\")\n",
    "                    await asyncio.sleep(1)\n",
    "                    continue\n",
    "            print(\n",
    "                f\"{config.name}, {question[:10]}: Failed to process question after 3 attempts. Set answer to null.\"\n",
    "            )\n",
    "        return None\n",
    "\n",
    "    @traceable(run_type=\"chain\")\n",
    "    async def process_questions(\n",
    "        self,\n",
    "        run_name: str,\n",
    "        questions: List[dict],\n",
    "        config: ClientConfig,\n",
    "    ) -> List[str | int | None]:\n",
    "        semaphore = Semaphore(self.concurrency)\n",
    "        tasks = [\n",
    "            self.process_question(\n",
    "                question=question[\"question\"],\n",
    "                config=config,\n",
    "                semaphore=semaphore,\n",
    "            )\n",
    "            for question in questions\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        return results\n",
    "\n",
    "    def generate_outputs(self, questions: List[dict]) -> pd.DataFrame:\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"id\": [i for i in range(len(questions))],\n",
    "                \"question\": [question[\"question\"] for question in questions],\n",
    "                \"answer\": [question[\"answer\"] for question in questions],\n",
    "            }\n",
    "        )\n",
    "        for config in self.configs:\n",
    "            responses = asyncio.run(\n",
    "                self.process_questions(\n",
    "                    run_name=config.name,\n",
    "                    questions=questions,\n",
    "                    config=config,\n",
    "                )\n",
    "            )\n",
    "            df[config.col_name] = responses\n",
    "        return df\n",
    "\n",
    "    def evaluate_outputs(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df_copy = df.copy()\n",
    "        for config in self.configs:\n",
    "            df_copy[config.score_col_name] = (\n",
    "                df_copy[\"answer\"] == df_copy[config.col_name]\n",
    "            ) * 1\n",
    "        return df_copy\n",
    "\n",
    "    def calculate_confidence_intervals(\n",
    "        self, df: pd.DataFrame, conf_level: float = 0.95\n",
    "    ) -> None:\n",
    "        print(\n",
    "            f\"Calculating confidence intervals ({conf_level}) with {len(df)} observations:\"\n",
    "        )\n",
    "        for config in self.configs:\n",
    "            score_col = config.score_col_name\n",
    "            scores = df[score_col]\n",
    "\n",
    "            if len(scores) == 0:\n",
    "                print(f\"No scores available for {score_col}\")\n",
    "                continue\n",
    "\n",
    "            mean_score = scores.mean()\n",
    "            se_score = scores.std() / np.sqrt(len(scores))\n",
    "\n",
    "            z_score = stats.norm.ppf((1 + conf_level) / 2)\n",
    "            margin_error = z_score * se_score\n",
    "            ci = [\n",
    "                max(0.0, mean_score - margin_error),\n",
    "                min(1.0, mean_score + margin_error),\n",
    "            ]\n",
    "            print(\n",
    "                f\"{score_col} - Mean: {mean_score * 100:.2f}% CI: {ci[0] * 100:.2f}% - {ci[1] * 100:.2f}%\"\n",
    "            )\n",
    "        print()\n",
    "\n",
    "    def run_paired_t_test(self, df: pd.DataFrame) -> None:\n",
    "        scores = {}\n",
    "\n",
    "        for config in self.configs:\n",
    "            score_col = config.score_col_name\n",
    "            scores[score_col] = df[score_col] * 1\n",
    "\n",
    "        for score_col_1, score_col_2 in [\n",
    "            (\"score_without_so\", \"score_with_so_tool_calls\"),\n",
    "            (\"score_without_so\", \"score_with_so_strict_tool_calls\"),\n",
    "            (\"score_without_so\", \"score_with_so_json_mode\"),\n",
    "        ]:\n",
    "            if score_col_1 in scores and score_col_2 in scores:\n",
    "                t_stat, p_value = stats.ttest_rel(\n",
    "                    scores[score_col_1], scores[score_col_2]\n",
    "                )\n",
    "                print(f\"{score_col_1} vs {score_col_2}\")\n",
    "                print(f\"t-statistic: {t_stat}, p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff85876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "class ResponseGSM8K(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\", title=\"Response\")\n",
    "    reasoning: str = Field(description=\"step by step reasoning about the answer\")\n",
    "    answer: int = Field(description=\"final answer\")\n",
    "\n",
    "\n",
    "def create_prompt_gsm8k(\n",
    "    question: str, \n",
    "    prompt_type: str, \n",
    "    response_model: ResponseGSM8K | None = None, \n",
    "    zero_shot: bool = False,\n",
    "):\n",
    "    if prompt_type in [\n",
    "        PromptType.WITH_JSON_MODE.value,\n",
    "        PromptType.WITH_TOOL_CALLS.value,\n",
    "        PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "    ]:\n",
    "        system_prompt = (\n",
    "            dedent(\"\"\"\n",
    "        You are an expert in solving grade school math tasks. You will be presented with a grade-school math word problem and be asked to solve it.\n",
    "\n",
    "        You will always respond with JSON matching the following schema:\n",
    "        \"\"\")\n",
    "            + json.dumps(response_model.model_json_schema(), indent=2)\n",
    "            + \"\"\"\\n\\nFirst, provide your step by step reasoning in the \"reasoning\" field. Then, in the \"answer\" field, provide an integer that corresponds to the correct answer to the question. Don't include any other text in the \"answer\" field.\"\"\"\n",
    "        )\n",
    "    else:\n",
    "        system_prompt = dedent(\"\"\"\n",
    "        You are an expert in solving grade school math tasks. You will be presented with a grade-school math word problem and be asked to solve it.\n",
    "        \n",
    "        You will always respond in the following format:\n",
    "        \n",
    "        <str, reasoning about the answer>\n",
    "        ANSWER: <int, final answer>\n",
    "        \n",
    "        First, provide your step by step reasoning. Then, in ANSWER, provide an integer that corresponds to the correct answer to the question. Don't include any other text in ANSWER.\n",
    "        \"\"\")\n",
    "\n",
    "    examples = [\n",
    "        (\n",
    "            \"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\",\n",
    "            \"There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6.\",\n",
    "            6,\n",
    "        ),\n",
    "        (\n",
    "            \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\",\n",
    "            \"There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5.\",\n",
    "            5,\n",
    "        ),\n",
    "        (\n",
    "            \"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\",\n",
    "            \"Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39.\",\n",
    "            39,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    if not zero_shot:\n",
    "        system_prompt += \"\\nExamples:\" if examples else \"\"\n",
    "        for i, (example_q, example_reason, example_ans) in enumerate(examples):\n",
    "            system_prompt += f\"\\n\\n**{i+1}**\\nQuestion: {example_q}\"\n",
    "            if prompt_type in [\n",
    "                PromptType.WITH_JSON_MODE.value,\n",
    "                PromptType.WITH_TOOL_CALLS.value,\n",
    "                PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "            ]:\n",
    "                response = f'{{\"reasoning\": \"{example_reason}\", \"answer\": {example_ans}}}'\n",
    "            else:\n",
    "                response = f\"{example_reason}\\nANSWER: {example_ans}\"\n",
    "            system_prompt += f\"\\nAssistant Response:\\n{response}\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question: {question}\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "create_prompt_gsm8k_zero_shot = partial(create_prompt_gsm8k, zero_shot=True)\n",
    "\n",
    "\n",
    "def parse_response_gsm8k(response: str, prompt_type: str) -> int | None:\n",
    "    if prompt_type in [\n",
    "        PromptType.WITH_JSON_MODE.value,\n",
    "        PromptType.WITH_TOOL_CALLS.value,\n",
    "        PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "    ]:\n",
    "        return ResponseGSM8K.model_validate_json(response).answer\n",
    "    else:\n",
    "        cleaned_response = (\n",
    "            response.split(\"\\nANSWER:\")[1].replace(\",\", \"\").rstrip(\".\").strip()\n",
    "        )\n",
    "        return int(cleaned_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8bb4c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "evals = [\n",
    "    {\n",
    "        \"question\": d[\"question\"],\n",
    "        \"answer\": int(d[\"answer\"].split(\"#### \")[1].replace(\",\", \"\").strip()),\n",
    "    }\n",
    "    for d in dataset[\"test\"]\n",
    "]\n",
    "\n",
    "if USE_SAMPLE:\n",
    "    evals = evals[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86cd8588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without_so, Jamal's ph: Retrying...\n",
      "without_so, Elliott is: Retrying...\n",
      "without_so, Tim has a : Retrying...\n",
      "without_so, A team of : Retrying...\n",
      "with_so_tool_calls, Morisette : Retrying...\n",
      "with_so_tool_calls, Rani has t: Retrying...\n",
      "with_so_tool_calls, Carl’s fav: Retrying...\n",
      "with_so_tool_calls, Mary is an: Retrying...\n",
      "with_so_tool_calls, A local ga: Retrying...\n",
      "with_so_tool_calls, Adrien's t: Retrying...\n",
      "with_so_tool_calls, Artie has : Retrying...\n",
      "with_so_tool_calls, John drive: Retrying...\n",
      "with_so_tool_calls, Anakin and: Retrying...\n",
      "with_so_tool_calls, Darren dec: Retrying...\n",
      "with_so_tool_calls, It takes 2: Retrying...\n",
      "with_so_tool_calls, Mark is a : Retrying...\n",
      "with_so_tool_calls, 4 adults a: Retrying...\n",
      "with_so_tool_calls, Mary buys : Retrying...\n",
      "with_so_tool_calls, Morisette : Retrying...\n",
      "with_so_tool_calls, Ten stalls: Retrying...\n",
      "with_so_tool_calls, Carl’s fav: Retrying...\n",
      "with_so_tool_calls, John drive: Retrying...\n",
      "with_so_tool_calls, Mark makes: Retrying...\n",
      "with_so_tool_calls, Twenty doz: Retrying...\n",
      "with_so_tool_calls, Mark is ma: Retrying...\n",
      "with_so_tool_calls, Jaime is a: Retrying...\n",
      "with_so_tool_calls, There were: Retrying...\n",
      "with_so_tool_calls, Artie has : Retrying...\n",
      "with_so_tool_calls, 15 gallons: Retrying...\n",
      "with_so_tool_calls, Annabelle : Retrying...\n",
      "with_so_tool_calls, Bill is si: Retrying...\n",
      "with_so_tool_calls, Miss Maria: Retrying...\n",
      "with_so_tool_calls, Michael is: Retrying...\n",
      "with_so_tool_calls, Maria buys: Retrying...\n",
      "with_so_tool_calls, Mary buys : Retrying...\n",
      "with_so_tool_calls, Peter purc: Retrying...\n",
      "with_so_tool_calls, Gerald wor: Retrying...\n",
      "with_so_tool_calls, Darren dec: Retrying...\n",
      "with_so_tool_calls, 4 adults a: Retrying...\n",
      "with_so_tool_calls, Ophelia an: Retrying...\n",
      "with_so_tool_calls, Marilyn wa: Retrying...\n",
      "with_so_tool_calls, The farm h: Retrying...\n",
      "with_so_tool_calls, Joey has 2: Retrying...\n",
      "with_so_tool_calls, A salesman: Retrying...\n",
      "with_so_tool_calls, It's straw: Retrying...\n",
      "with_so_tool_calls, A food cat: Retrying...\n",
      "with_so_tool_calls, Heather's : Retrying...\n",
      "with_so_tool_calls, When Marcu: Retrying...\n",
      "with_so_tool_calls, Maddy is b: Retrying...\n",
      "with_so_tool_calls, Carl’s fav: Retrying...\n",
      "with_so_tool_calls, 48 people : Retrying...\n",
      "with_so_tool_calls, In a jewel: Retrying...\n",
      "with_so_tool_calls, Carl’s fav: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Jaime is a: Retrying...\n",
      "with_so_tool_calls, 15 gallons: Retrying...\n",
      "with_so_tool_calls, Every 2 mi: Retrying...\n",
      "with_so_tool_calls, Jackson is: Retrying...\n",
      "with_so_tool_calls, Maria buys: Retrying...\n",
      "with_so_tool_calls, Manny sign: Retrying...\n",
      "with_so_tool_calls, Bill is si: Retrying...\n",
      "with_so_tool_calls, In a groce: Retrying...\n",
      "with_so_tool_calls, An ice cre: Retrying...\n",
      "with_so_tool_calls, James need: Retrying...\n",
      "with_so_tool_calls, Miss Maria: Retrying...\n",
      "with_so_tool_calls, There were: Retrying...\n",
      "with_so_tool_calls, Tomorrow, : Retrying...\n",
      "with_so_tool_calls, Bill is or: Retrying...\n",
      "with_so_tool_calls, Lee rears : Retrying...\n",
      "with_so_tool_calls, 90 single : Retrying...\n",
      "with_so_tool_calls, Hannah's c: Retrying...\n",
      "with_so_tool_calls, The farm h: Retrying...\n",
      "with_so_tool_calls, A salesman: Retrying...\n",
      "with_so_tool_calls, In the fir: Retrying...\n",
      "with_so_tool_calls, Colby want: Retrying...\n",
      "with_so_tool_calls, Anthony ha: Retrying...\n",
      "with_so_tool_calls, Chris has : Retrying...\n",
      "with_so_tool_calls, Leila buys: Retrying...\n",
      "with_so_tool_calls, Michael is: Retrying...\n",
      "with_so_tool_calls, John goes : Retrying...\n",
      "with_so_tool_calls, At 8:00, 5: Retrying...\n",
      "with_so_tool_calls, It's straw: Retrying...\n",
      "with_so_tool_calls, John drive: Retrying...\n",
      "with_so_tool_calls, Rong has b: Retrying...\n",
      "with_so_tool_calls, A pencil c: Retrying...\n",
      "with_so_tool_calls, A cat eats: Retrying...\n",
      "with_so_tool_calls, Mason is o: Retrying...\n",
      "with_so_tool_calls, Ophelia an: Retrying...\n",
      "with_so_tool_calls, Helga was : Retrying...\n",
      "with_so_tool_calls, Marilyn wa: Retrying...\n",
      "with_so_tool_calls, Julie, Mic: Retrying...\n",
      "with_so_tool_calls, Judy bough: Retrying...\n",
      "with_so_tool_calls, An ice cre: Retrying...\n",
      "with_so_tool_calls, An interio: Retrying...\n",
      "with_so_tool_calls, A food cat: Retrying...\n",
      "with_so_tool_calls, Mark works: Retrying...\n",
      "with_so_tool_calls, Andy's car: Retrying...\n",
      "with_so_tool_calls, Jack decid: Retrying...\n",
      "with_so_tool_calls, 15 gallons: Retrying...\n",
      "with_so_tool_calls, Greta want: Retrying...\n",
      "with_so_tool_calls, James love: Retrying...\n",
      "with_so_tool_calls, The glee c: Retrying...\n",
      "with_so_tool_calls, Nate is fe: Retrying...\n",
      "with_so_tool_calls, A local to: Retrying...\n",
      "with_so_tool_calls, Dylan atte: Retrying...\n",
      "with_so_tool_calls, The farm h: Retrying...\n",
      "with_so_tool_calls, John drive: Retrying...\n",
      "with_so_tool_calls, A company': Retrying...\n",
      "with_so_tool_calls, 15 gallons: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Tim has a : Retrying...\n",
      "with_so_tool_calls, Melanie fo: Retrying...\n",
      "with_so_tool_calls, A pencil c: Retrying...\n",
      "with_so_tool_calls, Judy bough: Retrying...\n",
      "with_so_tool_calls, Milly need: Retrying...\n",
      "with_so_tool_calls, A dance st: Retrying...\n",
      "with_so_tool_calls, The farm h: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, John drive: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, A pirate c: Retrying...\n",
      "with_so_tool_calls, Katherine : Retrying...\n",
      "with_so_tool_calls, In a groce: Retrying...\n",
      "with_so_tool_calls, Marilyn wa: Retrying...\n",
      "with_so_tool_calls, A farmer i: Retrying...\n",
      "with_so_tool_calls, Mike decid: Retrying...\n",
      "with_so_tool_calls, The Smith : Retrying...\n",
      "with_so_tool_calls, Chris has : Retrying...\n",
      "with_so_tool_calls, A salesman: Retrying...\n",
      "with_so_tool_calls, Marilyn wa: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Mason is o: Retrying...\n",
      "with_so_tool_calls, One meatba: Retrying...\n",
      "with_so_tool_calls, Henry need: Retrying...\n",
      "with_so_tool_calls, Gomer ate : Retrying...\n",
      "with_so_tool_calls, A salesman: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, The ratio : Retrying...\n",
      "with_so_tool_calls, Billy is v: Retrying...\n",
      "with_so_tool_calls, Valerie ea: Retrying...\n",
      "with_so_tool_calls, In Mr. Rop: Retrying...\n",
      "with_so_tool_calls, Faraday ow: Retrying...\n",
      "with_so_tool_calls, Doxa slice: Retrying...\n",
      "with_so_tool_calls, Terry sell: Retrying...\n",
      "with_so_tool_calls, Jack decid: Retrying...\n",
      "with_so_tool_calls, Emily can : Retrying...\n",
      "with_so_tool_calls, Judy bough: Retrying...\n",
      "with_so_tool_calls, A toy manu: Retrying...\n",
      "with_so_tool_calls, An ice cre: Retrying...\n",
      "with_so_tool_calls, Elly is or: Retrying...\n",
      "with_so_tool_calls, Judy bough: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, An ice cre: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Ryan’s all: Retrying...\n",
      "with_so_tool_calls, Henry need: Retrying...\n",
      "with_so_tool_calls, John had a: Retrying...\n",
      "with_so_tool_calls, Dr. Hugo G: Retrying...\n",
      "with_so_tool_calls, Conor live: Retrying...\n",
      "with_so_tool_calls, Rose bough: Retrying...\n",
      "with_so_tool_calls, In a groce: Retrying...\n",
      "with_so_tool_calls, Faraday ow: Retrying...\n",
      "with_so_tool_calls, Sheila cha: Retrying...\n",
      "with_so_tool_calls, In a groce: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Elly is or: Retrying...\n",
      "with_so_tool_calls, Terry sell: Retrying...\n",
      "with_so_tool_calls, Every day : Retrying...\n",
      "with_so_tool_calls, Milly need: Retrying...\n",
      "with_so_tool_calls, Ryan’s all: Retrying...\n",
      "with_so_tool_calls, Emily can : Retrying...\n",
      "with_so_tool_calls, Billy is v: Retrying...\n",
      "with_so_tool_calls, There are : Retrying...\n",
      "with_so_tool_calls, Nate's dog: Retrying...\n",
      "with_so_tool_calls, Billy is v: Retrying...\n",
      "with_so_tool_calls, Rose bough: Retrying...\n",
      "with_so_tool_calls, Billy is v: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Rose bough: Retrying...\n",
      "with_so_tool_calls, Sheila cha: Retrying...\n",
      "with_so_tool_calls, Rose bough: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Ryan’s all: Retrying...\n",
      "with_so_tool_calls, Ryan’s all: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Terry sell: Retrying...\n",
      "with_so_tool_calls, Terry sell: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, To partici: Retrying...\n",
      "with_so_json_mode, George has: Retrying...\n",
      "with_so_json_mode, Candy has : Retrying...\n",
      "with_so_json_mode, Bennet is : Retrying...\n",
      "with_so_json_mode, Melanie fo: Retrying...\n",
      "Calculating confidence intervals (0.95) with 1319 observations:\n",
      "score_without_so - Mean: 92.80% CI: 91.40% - 94.19%\n",
      "score_with_so_tool_calls - Mean: 91.43% CI: 89.92% - 92.94%\n",
      "score_with_so_strict_tool_calls - Mean: 93.63% CI: 92.31% - 94.95%\n",
      "score_with_so_json_mode - Mean: 93.56% CI: 92.23% - 94.88%\n",
      "\n",
      "score_without_so vs score_with_so_tool_calls\n",
      "t-statistic: 1.9899966834348262, p-value: 0.04679761672823959\n",
      "score_without_so vs score_with_so_strict_tool_calls\n",
      "t-statistic: -1.3442749695517333, p-value: 0.1790908108735601\n",
      "score_without_so vs score_with_so_json_mode\n",
      "t-statistic: -1.3367047014260847, p-value: 0.1815497341124042\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "# Zero-shot\n",
    "evaluator = LLMEvaluator(\n",
    "    configs=CONFIGS,\n",
    "    create_prompt_fn=create_prompt_gsm8k_zero_shot,\n",
    "    parse_response_fn=parse_response_gsm8k,\n",
    "    response_model=ResponseGSM8K,\n",
    ")\n",
    "\n",
    "df = evaluator.generate_outputs(evals)\n",
    "df_results = evaluator.evaluate_outputs(df)\n",
    "evaluator.calculate_confidence_intervals(df_results)\n",
    "evaluator.run_paired_t_test(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1072353e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without_so, Olivia upl: Retrying...\n",
      "with_so_tool_calls, Toby is re: Retrying...\n",
      "with_so_tool_calls, 20% of 50 : Retrying...\n",
      "with_so_tool_calls, 90 single : Retrying...\n",
      "with_so_tool_calls, Toby is re: Retrying...\n",
      "with_so_tool_calls, Jo has bee: Retrying...\n",
      "with_so_tool_calls, 90 single : Retrying...\n",
      "with_so_tool_calls, Jo has bee: Retrying...\n",
      "with_so_tool_calls, Randy has : Retrying...\n",
      "Calculating confidence intervals (0.95) with 1319 observations:\n",
      "score_without_so - Mean: 93.10% CI: 91.73% - 94.47%\n",
      "score_with_so_tool_calls - Mean: 93.18% CI: 91.82% - 94.54%\n",
      "score_with_so_strict_tool_calls - Mean: 93.18% CI: 91.82% - 94.54%\n",
      "score_with_so_json_mode - Mean: 93.63% CI: 92.31% - 94.95%\n",
      "\n",
      "score_without_so vs score_with_so_tool_calls\n",
      "t-statistic: -0.1347897773418711, p-value: 0.8927986689942902\n",
      "score_without_so vs score_with_so_strict_tool_calls\n",
      "t-statistic: -0.12798913051366037, p-value: 0.898177100089478\n",
      "score_without_so vs score_with_so_json_mode\n",
      "t-statistic: -0.8818427649934396, p-value: 0.3780226330588442\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "# Few-shot\n",
    "evaluator = LLMEvaluator(\n",
    "    configs=CONFIGS,\n",
    "    create_prompt_fn=create_prompt_gsm8k,\n",
    "    parse_response_fn=parse_response_gsm8k,\n",
    "    response_model=ResponseGSM8K,\n",
    ")\n",
    "\n",
    "df = evaluator.generate_outputs(evals)\n",
    "df_results = evaluator.evaluate_outputs(df)\n",
    "evaluator.calculate_confidence_intervals(df_results)\n",
    "evaluator.run_paired_t_test(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2ae3a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "class ResponseLastLetter(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\", title=\"Response\")\n",
    "    reasoning: str = Field(description=\"step by step reasoning about the answer\")\n",
    "    answer: str = Field(description=\"final answer\")\n",
    "\n",
    "\n",
    "def create_prompt_last_letter(\n",
    "    question: str, \n",
    "    prompt_type: str, \n",
    "    response_model: ResponseLastLetter | None = None, \n",
    "    zero_shot: bool = False\n",
    "):\n",
    "    if prompt_type in [\n",
    "        PromptType.WITH_JSON_MODE.value,\n",
    "        PromptType.WITH_TOOL_CALLS.value,\n",
    "        PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "    ]:\n",
    "        system_prompt = dedent(\"\"\"\n",
    "        You are an expert in solving simple word puzzles using reasoning steps. Your specific task is going to be to take a list of 4 names and reason about the last letter of each. Then, you will concatenate the last letters into a word. \n",
    "          \n",
    "        You will always respond with JSON matching the following schema:\n",
    "        \"\"\") + json.dumps(response_model.model_json_schema(), indent=2) + \"\"\"\\n\\nFirst, provide your step by step reasoning in the \"reasoning\" field. Then, in the \"answer\" field, provide the final answer. Don't include any other text in the \"answer\" field.\"\"\"\n",
    "    else:\n",
    "        system_prompt = dedent(\"\"\"\n",
    "        You are an expert in solving simple word puzzles using reasoning steps. Your specific task is going to be to take a list of 4 names and reason about the last letter of each. Then, you will concatenate the last letters into a word. \n",
    "        \n",
    "        You will always respond in the following format:\n",
    "        \n",
    "        <str, reasoning about the answer>\n",
    "        ANSWER: <str, final answer>\n",
    "        \n",
    "        First, provide your step by step reasoning. Then, in ANSWER, provide the final answer. Don't include any other text in ANSWER.\n",
    "        \"\"\")\n",
    "\n",
    "    fewshot_examples = [\n",
    "        (\n",
    "            \"Ian Peter Bernard Stephen\",\n",
    "            \"The last letter of 'Ian' is 'N'. The last letter of 'Peter' is 'R'. The last letter of 'Bernard' is 'D'. The last letter of 'Stephen' is 'N'. Concatenating them is 'NRDN'.\",\n",
    "            \"NRDN\",\n",
    "        ),\n",
    "        (\n",
    "            \"Javier Dylan Christopher Joseph\",\n",
    "            \"The last letter of 'Javier' is 'R'. The last letter of 'Dylan' is 'N'. The last letter of 'Christopher' is 'R'. The last letter of 'Joseph' is 'H'. Concatenating them is 'RNRH'.\",\n",
    "            \"RNRH\",\n",
    "        ),\n",
    "        (\n",
    "            \"Anthony Elizabeth Carlos Jesus\",\n",
    "            \"The last letter of 'Anthony' is 'Y'. The last letter of 'Elizabeth' is 'H'. The last letter of 'Carlos' is 'S'. The last letter of 'Jesus' is 'S'. Concatenating them is 'YHSS'.\",\n",
    "            \"YHSS\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    if not zero_shot:\n",
    "        system_prompt += \"\\nExamples:\" if fewshot_examples else \"\"\n",
    "        for i, (example_q, example_reason, example_ans) in enumerate(fewshot_examples):\n",
    "            system_prompt += f\"\\n\\n**{i+1}**\\nQuestion: Take the last letters of the words in '{example_q}' and concatenate them.\"\n",
    "            if prompt_type in [\n",
    "                PromptType.WITH_JSON_MODE.value,\n",
    "                PromptType.WITH_TOOL_CALLS.value,\n",
    "                PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "            ]:\n",
    "                response = f'{{\"reasoning\": \"{example_reason}\", \"answer\": \"{example_ans}\"}}'\n",
    "            else:\n",
    "                response = f\"{example_reason}\\nANSWER: {example_ans}\"\n",
    "            system_prompt += f\"\\nAssistant Response:\\n{response}\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question: {question}\",\n",
    "        },\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "create_prompt_last_letter_zero_shot = partial(create_prompt_last_letter, zero_shot=True)\n",
    "\n",
    "def parse_response_last_letter(response: str, prompt_type: str) -> str | None:\n",
    "    if prompt_type in [\n",
    "        PromptType.WITH_JSON_MODE.value,\n",
    "        PromptType.WITH_TOOL_CALLS.value,\n",
    "        PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "    ]:\n",
    "        return ResponseLastLetter.model_validate_json(response).answer.lower()\n",
    "    else:\n",
    "        return response.split(\"\\nANSWER:\")[1].rstrip(\".\").strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bfacd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "dataset = load_dataset(\"ChilleD/LastLetterConcat\")\n",
    "evals = [\n",
    "    {\"question\": d[\"question\"], \"answer\": d[\"answer\"].lower()} for d in dataset[\"test\"]\n",
    "]\n",
    "\n",
    "if USE_SAMPLE:\n",
    "    evals = evals[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d36bc930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating confidence intervals (0.95) with 150 observations:\n",
      "score_without_so - Mean: 83.33% CI: 77.35% - 89.32%\n",
      "score_with_so_tool_calls - Mean: 84.00% CI: 78.11% - 89.89%\n",
      "score_with_so_strict_tool_calls - Mean: 87.33% CI: 81.99% - 92.67%\n",
      "score_with_so_json_mode - Mean: 90.67% CI: 86.00% - 95.34%\n",
      "\n",
      "score_without_so vs score_with_so_tool_calls\n",
      "t-statistic: -0.1993588014488551, p-value: 0.8422538839198896\n",
      "score_without_so vs score_with_so_strict_tool_calls\n",
      "t-statistic: -1.2268049987877068, p-value: 0.22183090435484304\n",
      "score_without_so vs score_with_so_json_mode\n",
      "t-statistic: -2.5703013926014764, p-value: 0.01114153217659293\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "# Zero-shot\n",
    "evaluator = LLMEvaluator(\n",
    "    configs=CONFIGS,\n",
    "    create_prompt_fn=create_prompt_last_letter_zero_shot,\n",
    "    parse_response_fn=parse_response_last_letter,\n",
    "    response_model=ResponseLastLetter,\n",
    ")\n",
    "\n",
    "df = evaluator.generate_outputs(evals)\n",
    "df_results = evaluator.evaluate_outputs(df)\n",
    "evaluator.calculate_confidence_intervals(df_results)\n",
    "evaluator.run_paired_t_test(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "042dfdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating confidence intervals (0.95) with 150 observations:\n",
      "score_without_so - Mean: 90.00% CI: 85.18% - 94.82%\n",
      "score_with_so_tool_calls - Mean: 91.33% CI: 86.82% - 95.85%\n",
      "score_with_so_strict_tool_calls - Mean: 88.67% CI: 83.58% - 93.76%\n",
      "score_with_so_json_mode - Mean: 90.67% CI: 86.00% - 95.34%\n",
      "\n",
      "score_without_so vs score_with_so_tool_calls\n",
      "t-statistic: -0.7059233380455161, p-value: 0.4813380870329814\n",
      "score_without_so vs score_with_so_strict_tool_calls\n",
      "t-statistic: 0.6311859646135468, p-value: 0.5288860007980016\n",
      "score_without_so vs score_with_so_json_mode\n",
      "t-statistic: -0.3323434771513362, p-value: 0.7400969989055401\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "# Few-shot\n",
    "evaluator = LLMEvaluator(\n",
    "    configs=CONFIGS,\n",
    "    create_prompt_fn=create_prompt_last_letter,\n",
    "    parse_response_fn=parse_response_last_letter,\n",
    "    response_model=ResponseLastLetter,\n",
    ")\n",
    "\n",
    "df = evaluator.generate_outputs(evals)\n",
    "df_results = evaluator.evaluate_outputs(df)\n",
    "evaluator.calculate_confidence_intervals(df_results)\n",
    "evaluator.run_paired_t_test(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00b7f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "class ResponseShuffledObjects(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\", title=\"Response\")\n",
    "    reasoning: str = Field(description=\"reasoning about the answer\")\n",
    "    answer: Literal[\"A\", \"B\", \"C\", \"D\", \"E\"] = Field(description=\"final answer\")\n",
    "\n",
    "\n",
    "def create_prompt_shuffled_objects(\n",
    "    question,\n",
    "    prompt_type: str,\n",
    "    response_model: ResponseShuffledObjects | None = None,\n",
    "    zero_shot: bool = False\n",
    "):\n",
    "    if prompt_type in [\n",
    "        PromptType.WITH_JSON_MODE.value,\n",
    "        PromptType.WITH_TOOL_CALLS.value,\n",
    "        PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "    ]:\n",
    "        system_prompt = dedent(\"\"\"\n",
    "        You are an expert in performing common sense tasks involving the ordering of a sequence of events.\n",
    "        Each question will present you with a sequence of events involving 5 people (switching objects, partners, positions, etc.). Your task is to determine the correct answer from the options provided.\n",
    "          \n",
    "        You will always respond with JSON matching the following schema:\n",
    "        \"\"\") + json.dumps(response_model.model_json_schema(), indent=2) + \"\"\"\\n\\nFirst, provide your reasoning in the \"reasoning\" field. Then, in the \"answer\" field, provide only the single letter representing the correct choice you are presented with. Don't include any other text in the \"answer\" field.\"\"\"\n",
    "    else:\n",
    "        system_prompt = dedent(\"\"\"\n",
    "        You are an expert in performing common sense tasks involving the ordering of a sequence of events.\n",
    "        Each question will present you with a sequence of events involving 5 people (switching objects, partners, positions, etc.). Your task is to determine the correct answer from the options provided.\n",
    "        \n",
    "        You will always respond in the following format:\n",
    "        \n",
    "        <str, reasoning about the answer>\n",
    "        ANSWER: <str, final answer>\n",
    "        \n",
    "        First, provide your step by step reasoning. Then, in ANSWER, provide only the single letter representing the correct choice you are presented with. Don't include any other text in ANSWER.\n",
    "        \"\"\")\n",
    "\n",
    "    fewshot_examples = [\n",
    "        (\n",
    "            \"Alice, Bob, Claire, Dave, and Eve are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Patrick, Bob is dancing with Sam, Claire is dancing with Jamie, Dave is dancing with Lola, and Eve is dancing with Melissa.\\nThroughout the song, the dancers often trade partners. First, Dave and Eve switch partners. Then, Dave and Alice switch partners. Then, Eve and Alice switch partners. Then, Claire and Bob switch partners. Finally, Dave and Alice switch partners. At the end of the dance, Alice is dancing with\\nOptions:\\n(A) Patrick\\n(B) Sam\\n(C) Jamie\\n(D) Lola\\n(E) Melissa\",\n",
    "            \"Dave and Eve switch partners, so Dave's partner is now Melissa and Eve's partner is now Patrick. Then Dave and Alice switch partners so Dave's partner is now Patrick and Alice's partner is now Melissa. Then Eve and Alice switch partners so Eve's partner is now Melissa and Alice's partner is now Lola. Then Claire and Bob switch patners so Claire's partner is now Sam, and Bob's partner is now Jamie. Finally, Dave and Alice switch partners so Dave's new partner is Lola, and Alice's new partner is Patrick. Alice is dance in with Patrick, choice A.\",\n",
    "            \"A\",\n",
    "        ),\n",
    "        (\n",
    "            \"Alice, Bob, Claire, Dave, and Eve are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Ophelia, Bob is dancing with Jamie, Claire is dancing with Melissa, Dave is dancing with Rodrigo, and Eve is dancing with Patrick.\\nThroughout the song, the dancers often trade partners. First, Claire and Bob switch partners. Then, Claire and Eve switch partners. Then, Claire and Bob switch partners. Then, Eve and Dave switch partners. Finally, Claire and Alice switch partners. At the end of the dance, Alice is dancing with\\nOptions:\\n(A) Ophelia\\n(B) Jamie\\n(C) Melissa\\n(D) Rodrigo\\n(E) Patrick\",\n",
    "            \"Claire and Bob switch partners, so Claire's partner is now Jamie and Bob's partner is now Melissa. Then, Claire and Eve switch partners, so Claire's partner becomes Patrick and Eve's partner becomes Jamie. Next, Claire and Bob switch partners again, making Claire's partner Melissa and Bob's partner Patrick. After that, Eve and Dave switch partners, resulting in Eve's partner being Rodrigo and Dave's partner being Jamie. Finally, Claire and Alice switch partners, so Claire's partner is now Ophelia and Alice's partner becomes Melissa. Alice is dancing with Melissa, which is choice C.\",\n",
    "            \"C\",\n",
    "        ),\n",
    "        (\n",
    "            \"Alice, Bob, Claire, Dave, and Eve are friends and avid readers who occasionally trade books. At the start of the semester, they each buy one new book: Alice gets Catch-22, Bob gets Hound of the Baskervilles, Claire gets Frankenstein, Dave gets The Pearl, and Eve gets The Fellowship of the Ring.\\nAs the semester proceeds, they start trading around the new books. First, Eve and Alice swap books. Then, Alice and Claire swap books. Then, Alice and Bob swap books. Then, Dave and Alice swap books. Finally, Dave and Claire swap books. At the end of the semester, Dave has\\nOptions:\\n(A) Catch-22\\n(B) Hound of the Baskervilles\\n(C) Frankenstein\\n(D) The Pearl\\n(E) The Fellowship of the Ring\",\n",
    "            \"First, Eve and Alice swap, so Alice gets The Fellowship of the Ring and Eve gets Catch-22. Next, Alice and Claire swap, giving Claire The Fellowship of the Ring and Alice Frankenstein. Then, Alice and Bob swap, resulting in Bob holding Frankenstein and Alice having Hound of the Baskervilles. Dave and Alice then swap, so Dave takes Hound of the Baskervilles and Alice receives The Pearl. Finally, Dave and Claire swap books, which means Dave takes The Fellowship of the Ring from Claire. Therefore, at the end of all the swaps, Dave possesses The Fellowship of the Ring, making option E the correct answer.\",\n",
    "            \"E\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    if not zero_shot:\n",
    "        system_prompt += \"\\nExamples:\" if fewshot_examples else \"\"\n",
    "        for i, (example_q, example_reason, example_ans) in enumerate(fewshot_examples):\n",
    "            system_prompt += f\"\\n\\n**{i+1}**\\nQuestion: {example_q}\"\n",
    "\n",
    "            if prompt_type in [\n",
    "                PromptType.WITH_JSON_MODE.value,\n",
    "                PromptType.WITH_TOOL_CALLS.value,\n",
    "                PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "            ]:\n",
    "                response = f'{{\"reasoning\": \"{example_reason}\", \"answer\": \"{example_ans}\"}}'\n",
    "            else:\n",
    "                response = f\"{example_reason}\\nANSWER: {example_ans}\"\n",
    "            system_prompt += f\"\\nAssistant Response:\\n{response}\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question: {question}\",\n",
    "        },\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "create_prompt_shuffled_objects_zero_shot = partial(create_prompt_shuffled_objects, zero_shot=True)\n",
    "\n",
    "\n",
    "def parse_response_shuffled_objects(response: str, prompt_type: str) -> str:\n",
    "    if prompt_type in [\n",
    "        PromptType.WITH_JSON_MODE.value,\n",
    "        PromptType.WITH_TOOL_CALLS.value,\n",
    "        PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "    ]:\n",
    "        return ResponseShuffledObjects.model_validate_json(response).answer\n",
    "    else:\n",
    "        return response.split(\"\\nANSWER:\")[1].rstrip(\".\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11d87fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"openeval/BIG-Bench-Hard\", data_files=\"tracking_shuffled_objects_five_objects.json\"\n",
    ")\n",
    "evals = [\n",
    "    {\n",
    "        \"question\": d[\"input\"],\n",
    "        \"answer\": d[\"target\"].replace(\"(\", \"\").replace(\")\", \"\").strip(),\n",
    "    }\n",
    "    for d in dataset[\"train\"][\"examples\"][0][4:]  # first 3 are few-shot examples\n",
    "]\n",
    "\n",
    "if USE_SAMPLE:\n",
    "    evals = evals[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89cf3a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Alice, Bob: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Alice, Bob: Failed to process question after 3 attempts. Set answer to null.\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Retrying...\n",
      "with_so_tool_calls, Alice, Bob: Failed to process question after 3 attempts. Set answer to null.\n",
      "Calculating confidence intervals (0.95) with 246 observations:\n",
      "score_without_so - Mean: 94.31% CI: 91.41% - 97.21%\n",
      "score_with_so_tool_calls - Mean: 82.11% CI: 77.32% - 86.91%\n",
      "score_with_so_strict_tool_calls - Mean: 87.40% CI: 83.24% - 91.55%\n",
      "score_with_so_json_mode - Mean: 81.71% CI: 76.87% - 86.55%\n",
      "\n",
      "score_without_so vs score_with_so_tool_calls\n",
      "t-statistic: 4.397966037062802, p-value: 1.6302059975884974e-05\n",
      "score_without_so vs score_with_so_strict_tool_calls\n",
      "t-statistic: 2.9170522118292084, p-value: 0.0038616410626042263\n",
      "score_without_so vs score_with_so_json_mode\n",
      "t-statistic: 4.3280709635557075, p-value: 2.192817009461412e-05\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "# Zero-shot\n",
    "evaluator = LLMEvaluator(\n",
    "    configs=CONFIGS,\n",
    "    create_prompt_fn=create_prompt_shuffled_objects_zero_shot,\n",
    "    parse_response_fn=parse_response_shuffled_objects,\n",
    "    response_model=ResponseShuffledObjects,\n",
    ")\n",
    "\n",
    "df = evaluator.generate_outputs(evals)\n",
    "df_results = evaluator.evaluate_outputs(df)\n",
    "evaluator.calculate_confidence_intervals(df_results)\n",
    "evaluator.run_paired_t_test(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5951ae92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating confidence intervals (0.95) with 246 observations:\n",
      "score_without_so - Mean: 87.40% CI: 83.24% - 91.55%\n",
      "score_with_so_tool_calls - Mean: 70.73% CI: 65.03% - 76.43%\n",
      "score_with_so_strict_tool_calls - Mean: 67.48% CI: 61.61% - 73.35%\n",
      "score_with_so_json_mode - Mean: 63.41% CI: 57.38% - 69.45%\n",
      "\n",
      "score_without_so vs score_with_so_tool_calls\n",
      "t-statistic: 4.816493986159092, p-value: 2.561002821571171e-06\n",
      "score_without_so vs score_with_so_strict_tool_calls\n",
      "t-statistic: 5.359767575478789, p-value: 1.9228803996842006e-07\n",
      "score_without_so vs score_with_so_json_mode\n",
      "t-statistic: 6.31805795375796, p-value: 1.239309733027979e-09\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "evaluator = LLMEvaluator(\n",
    "    configs=CONFIGS,\n",
    "    create_prompt_fn=create_prompt_shuffled_objects,\n",
    "    parse_response_fn=parse_response_shuffled_objects,\n",
    "    response_model=ResponseShuffledObjects,\n",
    ")\n",
    "\n",
    "df = evaluator.generate_outputs(evals)\n",
    "df_results = evaluator.evaluate_outputs(df)\n",
    "evaluator.calculate_confidence_intervals(df_results)\n",
    "evaluator.run_paired_t_test(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
