{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9548f597",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Synthetic data for RAG\"\n",
    "date: 2025-07-11\n",
    "date-modified: 2025-07-11\n",
    "description-meta: \"How to use synthetic data to build a RAG system\"\n",
    "categories:\n",
    "  - llm\n",
    "  - python\n",
    "  - rag \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2a708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58b9d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import random\n",
    "from textwrap import dedent\n",
    "\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import Client, traceable\n",
    "from pydantic import BaseModel\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a8c9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\n",
    "    \"../data/synthetic-data-rag/people-group/\", glob=\"**/*.md\", loader_cls=TextLoader\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2fe2a3",
   "metadata": {},
   "source": [
    "## Index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7df6e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_ef = OpenAIEmbeddingFunction(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "client = chromadb.PersistentClient(\n",
    "    path=\"../data/synthetic-data-rag/chroma\",\n",
    ")\n",
    "collection = client.get_or_create_collection(\n",
    "    \"gitlab-handbook\", embedding_function=openai_ef\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10523d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownTextSplitter\n",
    "\n",
    "text_splitter = MarkdownTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4o\",\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cc250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(ids, documents, metadatas, batch_size=100):\n",
    "    batches = []\n",
    "    for i in range(0, len(ids), batch_size):\n",
    "        batch_ids = ids[i : i + batch_size]\n",
    "        batch_documents = documents[i : i + batch_size]\n",
    "        batch_metadatas = metadatas[i : i + batch_size]\n",
    "        batches.append((batch_ids, batch_metadatas, batch_documents))\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3287eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [f\"{str(i)}\" for i in range(len(splits))]\n",
    "documents = [doc.page_content for doc in splits]\n",
    "metadatas = [doc.metadata for doc in splits]\n",
    "\n",
    "\n",
    "if collection.count() > 0:\n",
    "    print(\"Collection already exists, skipping creation.\")\n",
    "else:\n",
    "    print(\"Adding documents...\")\n",
    "    batches = create_batches(ids=ids, documents=documents, metadatas=metadatas)\n",
    "    for i, batch in enumerate(batches):\n",
    "        print(f\"Adding batch {i} of size {len(batch[0])}\")\n",
    "        collection.add(ids=batch[0], metadatas=batch[1], documents=batch[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0baae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievedDoc(BaseModel):\n",
    "    id: str\n",
    "    path: str\n",
    "    page_content: str\n",
    "\n",
    "\n",
    "def get_similar_docs(text: str, top_k: int = 5) -> list[RetrievedDoc]:\n",
    "    results = collection.query(query_texts=[text], n_results=top_k)\n",
    "    docs = [results[\"documents\"][0][i] for i in range(top_k)]\n",
    "    metadatas = [results[\"metadatas\"][0][i] for i in range(top_k)]\n",
    "    ids = [results[\"ids\"][0][i] for i in range(top_k)]\n",
    "    return [\n",
    "        RetrievedDoc(id=id_, path=m[\"source\"], page_content=d)\n",
    "        for d, m, id_ in zip(docs, metadatas, ids)\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_doc_by_id(doc_id: str) -> RetrievedDoc:\n",
    "    results = collection.get(ids=[doc_id])\n",
    "    doc = results[\"documents\"][0]\n",
    "    metadata = results[\"metadatas\"][0]\n",
    "    return RetrievedDoc(id=doc_id, path=metadata[\"source\"], page_content=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa4ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(chunks: list[RetrievedDoc]) -> str:\n",
    "    return \"\\n\".join(\n",
    "        [f\"*** Filepath: {chunk.path} ***\\n{chunk.page_content}\\n\" for chunk in chunks]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db2a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_docs_idx = random.sample(range(len(splits)), 200)\n",
    "golden_docs = [get_doc_by_id(str(i)) for i in golden_docs_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf323e",
   "metadata": {},
   "source": [
    "## Generate QA Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582a5b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_generate = dedent(\n",
    "    \"\"\"\n",
    "    You are a helpful assistant generating synthetic QA pairs for retrieval evaluation.\n",
    "\n",
    "    Given a target chunk of text and a set of confounding chunks, you must extract a specific, self-contained fact from the target chunk that is not included in the confounding chunks. Then write a question that is directly and unambiguously answered by that fact. The question should only be answered by the fact extracted from the target chunk (and not by any of the confounding chunks) but it should also use themes or terminology that is present in the confounding chunks.\n",
    "\n",
    "    Always respond with a JSON object with the following keys (in that exact order):\n",
    "    1. \"fact\": \"<the fact extracted from the target chunk>\",\n",
    "    2. \"confounding_terms\": \"<a list of terms or themes from the confounding chunks that are relevant to the question>\",\n",
    "    3. \"question\": \"<the question that is directly and unambiguously answered by the fact>\",\n",
    "    \n",
    "    You should write the questions as if you're an employee looking for information in the handbook. The question should be as realistic and natural as possible, reflecting the kind of queries an employee might actually make when searching for information in the handbook.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "user_prompt_generate = dedent(\n",
    "    \"\"\"\n",
    "    TARGET CHUNK:\n",
    "    {target_chunk}\n",
    "\n",
    "    CONFOUNDING CHUNKS:\n",
    "    {confounding_chunks} \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aab841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response(BaseModel):\n",
    "    fact: str\n",
    "    confounding_terms: list[str] = []\n",
    "    question: str\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=1)\n",
    "llm_with_structured_output = llm.with_structured_output(Response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d155b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt_generate), (\"user\", user_prompt_generate)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248ec536",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_qa_pair(random_chunk):\n",
    "    similar_chunks = get_similar_docs(random_chunk.page_content)\n",
    "    compiled_messages = await messages.ainvoke(\n",
    "        {\n",
    "            \"target_chunk\": format_docs([similar_chunks[0]]),\n",
    "            \"confounding_chunks\": format_docs(similar_chunks[1:]),\n",
    "        }\n",
    "    )\n",
    "    output = await llm_with_structured_output.ainvoke(compiled_messages)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3cf7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [generate_qa_pair(random_split) for random_split in golden_docs]\n",
    "qa_pairs = await asyncio.gather(*tasks)\n",
    "\n",
    "df = pd.DataFrame([qa_pair.dict() for qa_pair in qa_pairs])\n",
    "df.to_excel(\"../data/synthetic-data-rag/files/qa_pairs.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38ffcc1",
   "metadata": {},
   "source": [
    "## Filtering QA pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeec315",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_rate = dedent(\n",
    "    \"\"\"\n",
    "    You are an AI assistant helping us curate a high-quality dataset of questions for evaluating an company's internal handbook. We have generated synthetic questions and need to filter out those that are unrealistic or not representative of typical user queries.\n",
    "\n",
    "    Here are examples of realistic and unrealistic user queries we have manually rated:\n",
    "\n",
    "    ### Realistic Queries (Good Examples)\n",
    "\n",
    "    * **Query:** \"What is the required process for creating a new learning hub for your team in Level Up at GitLab?\"\n",
    "        * **Explanation:** Very realistic user query. It's concise, information-seeking, and process-oriented.\n",
    "        * **Rating:** 5\n",
    "    * **Query:** \"Where is the People Operations internal handbook hosted, and how can someone gain access to it?\"\n",
    "        * **Explanation:** Realistic query but might be a bit too detailed for a typical user.\n",
    "        * **Rating:** 4\n",
    "    * **Query:** \"Who controls access to People Data in the data warehouse at GitLab, and what approvals are required for Analytics Engineers and Data Analysts to obtain access?\"\n",
    "        * **Explanation:** Seems reasonable but too lengthy for a typical user query. \n",
    "        * **Rating:** 3\n",
    "\n",
    "    ### Unrealistic Queries (Bad Examples)\n",
    "\n",
    "    * **Query:** \"If a GitLab team member has been with the company for over 3 months and is interested in participating in the Onboarding Buddy Program, what should they do to express their interest?\"\n",
    "        * **Explanation:** Overly specific and unnatural. No real user would ask this.\n",
    "        * **Rating:** 1\n",
    "    * **Query:** \"On what date did the 'Managing Burnout with Time Off with John Fitch' session occur as part of the FY21 Learning Speaker Series?\"\n",
    "        * **Explanation:** Irrelevant and overly specific. Not a typical user query. \n",
    "        * **Rating:** 2\n",
    "\n",
    "    ### Your Task\n",
    "\n",
    "    For the following generated question, please:\n",
    "\n",
    "    1.  Rate its realism as a typical user query for an internal handbook application on a scale of 1 to 5 (1 = Very Unrealistic, 3 = Neutral/Somewhat Realistic, 5 = Very Realistic).\n",
    "    2.  Provide a brief explanation for your rating, comparing it to the examples above if helpful.\n",
    "\n",
    "    ### Output Format\n",
    "\n",
    "    **Explanation:** `[Your brief explanation]`\n",
    "    **Rating:** `[Your 1–5 rating]`\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "user_prompt_rate = dedent(\n",
    "    \"\"\"\n",
    "    **Generated Question to Evaluate:**\n",
    "    `{question_to_evaluate}`\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0929e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseFiltering(BaseModel):\n",
    "    explanation: str\n",
    "    rating: int\n",
    "\n",
    "\n",
    "llm_with_structured_output_filtering = llm.with_structured_output(ResponseFiltering)\n",
    "\n",
    "messages_filtering = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt_rate), (\"user\", user_prompt_rate)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5b953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def rate_qa_pair(qa_pair):\n",
    "    compiled_messages = await messages_filtering.ainvoke(\n",
    "        {\"question_to_evaluate\": qa_pair.question}\n",
    "    )\n",
    "    output = await llm_with_structured_output_filtering.ainvoke(compiled_messages)\n",
    "    return output\n",
    "\n",
    "\n",
    "tasks = [rate_qa_pair(qa_pair) for qa_pair in qa_pairs]\n",
    "results = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a7e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rated_qa_pairs = [\n",
    "    {\n",
    "        \"rating\": result.rating,\n",
    "        \"explanation\": result.explanation,\n",
    "        \"question\": qa_pair.question,\n",
    "        \"answer\": qa_pair.fact,\n",
    "    }\n",
    "    for (result, qa_pair) in zip(results, qa_pairs)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1921d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rated_qa_pairs = pd.DataFrame(\n",
    "    rated_qa_pairs, columns=[\"Rating\", \"Explanation\", \"Question\"]\n",
    ")\n",
    "\n",
    "df_rated_qa_pairs.to_excel(\n",
    "    \"../data/synthetic-data-rag/files/rated_qa_pairs.xlsx\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa79d22",
   "metadata": {},
   "source": [
    "## Evaluating RAG system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdfe231",
   "metadata": {},
   "outputs": [],
   "source": [
    "langsmith_client = Client()\n",
    "dataset_name = \"Gitlab Handbook QA Evaluation 2\"\n",
    "\n",
    "try:\n",
    "    dataset = langsmith_client.create_dataset(dataset_name=dataset_name)\n",
    "    examples = [\n",
    "        {\n",
    "            \"inputs\": {\n",
    "                \"question\": h[\"question\"],\n",
    "            },\n",
    "            \"outputs\": {\n",
    "                \"answer\": h[\"answer\"],\n",
    "                \"doc\": {\n",
    "                    \"id\": chunk.id,\n",
    "                    \"path\": chunk.path,\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "        for h, chunk in zip(rated_qa_pairs, golden_docs)\n",
    "        if h[\"rating\"] >= 5\n",
    "    ]\n",
    "    langsmith_client.create_examples(dataset_id=dataset.id, examples=examples)\n",
    "except Exception:\n",
    "    print(\"Dataset already exists, skipping creation.\")\n",
    "    dataset = langsmith_client.read_dataset(dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a43d34",
   "metadata": {},
   "source": [
    "### Retrieval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07239efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr(inputs: dict, outputs: dict, reference_outputs: dict) -> list[dict]:\n",
    "    reference_docs = [str(reference_outputs[\"doc\"][\"id\"])]\n",
    "    docs = outputs.get(\"docs\", [])\n",
    "\n",
    "    if not docs:\n",
    "        return [{\"key\": \"MRR\", \"score\": 0.0}]\n",
    "\n",
    "    rank = next((i + 1 for i, doc in enumerate(docs) if doc in reference_docs), None)\n",
    "    mrr_value = 1.0 / rank if rank else 0.0\n",
    "\n",
    "    return [{\"key\": \"MRR\", \"score\": mrr_value}]\n",
    "\n",
    "\n",
    "def recall_at_k(inputs: dict, outputs: dict, reference_outputs: dict) -> list[dict]:\n",
    "    reference_docs = [str(reference_outputs[\"doc\"][\"id\"])]\n",
    "    docs = outputs.get(\"docs\", [])\n",
    "\n",
    "    if not docs:\n",
    "        return [\n",
    "            {\"key\": \"recall@1\", \"score\": 0.0},\n",
    "            {\"key\": \"recall@3\", \"score\": 0.0},\n",
    "            {\"key\": \"recall@5\", \"score\": 0.0},\n",
    "            {\"key\": \"recall@10\", \"score\": 0.0},\n",
    "        ]\n",
    "\n",
    "    recall_at_1 = docs[0] in reference_docs\n",
    "    recall_at_3 = any(doc in reference_docs for doc in docs[:3])\n",
    "    recall_at_5 = any(doc in reference_docs for doc in docs[:5])\n",
    "    recall_at_10 = any(doc in reference_docs for doc in docs[:10])\n",
    "\n",
    "    return [\n",
    "        {\"key\": \"recall@1\", \"score\": recall_at_1 * 1.0},\n",
    "        {\"key\": \"recall@3\", \"score\": recall_at_3 * 1.0},\n",
    "        {\"key\": \"recall@5\", \"score\": recall_at_5 * 1.0},\n",
    "        {\"key\": \"recall@10\", \"score\": recall_at_10 * 1.0},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c074a4",
   "metadata": {},
   "source": [
    "### Generation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604a2d2d",
   "metadata": {},
   "source": [
    "#### Answer accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f60491",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_answer_accuracy = dedent(\n",
    "    \"\"\"\n",
    "    You are an expert evaluator. Your task is to evaluate the accuracy of a User Answer against a Reference Answer, given a Question.\n",
    "\n",
    "    Say 2, if User Answer is full contained and equivalent to Reference Answer in all terms, topics, numbers, metrics, dates and units.\n",
    "    Say 1, if User Answer is partially contained and almost equivalent to Reference Answer in all terms, topics, numbers, metrics, dates and units.\n",
    "    Say 0, if User Answer is not contained in Reference Answer or not accurate in all terms, topics, numbers, metrics, dates and units or the User Answer do not answer the question.\n",
    "\n",
    "    Your rating must be only 2, 1 or 0 according to the instructions above.\n",
    "\n",
    "\n",
    "    Your answer must be a JSON object with the following keys:\n",
    "    1. \"explanation\": \"<a brief explanation of your rating>\",\n",
    "    2. \"rating\": \"<your rating, which must be one of the following:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "user_prompt_answer_accuracy = dedent(\n",
    "    \"\"\"\n",
    "    **Question:** `{question}`\n",
    "    **User Answer:** `{user_answer}`\n",
    "    **Reference Answer:** `{reference_answer}`\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "messages_answer_accuracy = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt_answer_accuracy), (\"user\", user_prompt_answer_accuracy)]\n",
    ")\n",
    "\n",
    "class ResponseAnswerAccuracy(BaseModel):\n",
    "    explanation: str\n",
    "    rating: int\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "\n",
    "llm_with_structured_output_answer_accuracy = llm.with_structured_output(ResponseAnswerAccuracy)\n",
    "\n",
    "async def answer_accuracy(inputs: dict, outputs: dict, reference_outputs: dict) -> list[dict]:\n",
    "    question = inputs[\"question\"]\n",
    "    user_answer = outputs[\"answer\"]\n",
    "    reference_answer = reference_outputs[\"answer\"]\n",
    "\n",
    "    compiled_messages = await messages_answer_accuracy.ainvoke(\n",
    "        {\n",
    "            \"question\": question,\n",
    "            \"user_answer\": user_answer,\n",
    "            \"reference_answer\": reference_answer,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    output  = await llm_with_structured_output_answer_accuracy.ainvoke(compiled_messages)\n",
    "\n",
    "    return output.rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f760b10",
   "metadata": {},
   "source": [
    "## Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb7581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_generation = dedent(\n",
    "    \"\"\"\n",
    "    You're a helpful assistant. Provided with a question and the most relevant documents, you must generate a concise and accurate answer based on the information in those documents.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "user_prompt_generation = dedent(\n",
    "    \"\"\"\n",
    "    QUESTION: {question}\n",
    "\n",
    "    RELEVANT DOCUMENTS:\n",
    "    {documents}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "messages_generation = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt_generation), (\"user\", user_prompt_generation)]\n",
    ")\n",
    "\n",
    "llm_generation = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ddd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "async def target(inputs: dict) -> dict:\n",
    "    relevant_docs = get_similar_docs(inputs[\"question\"], top_k=10)\n",
    "    formatted_docs = format_docs(relevant_docs)\n",
    "    messages = await messages_generation.ainvoke(\n",
    "        {\n",
    "            \"question\": inputs[\"question\"],\n",
    "            \"documents\": formatted_docs,\n",
    "        }\n",
    "    )\n",
    "    response = await llm_generation.ainvoke(messages)\n",
    "    return {\"answer\": response.content, \"docs\": [doc.id for doc in relevant_docs]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af2580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = await langsmith_client.aevaluate(\n",
    "    target,\n",
    "    data=dataset_name,\n",
    "    evaluators=[recall_at_k, mrr, answer_accuracy],\n",
    "    max_concurrency=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f196606e",
   "metadata": {},
   "source": [
    "## Improving retrieval with a reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef76069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "cross_encoder = CrossEncoder(\"mixedbread-ai/mxbai-rerank-xsmall-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481b2e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the process for creating a new learning hub for your team in Level Up at GitLab?\"\n",
    "hits = get_similar_docs(query, top_k=50)\n",
    "cross_inp = [[query, h.page_content] for h in hits]\n",
    "reranker_scores = cross_encoder.predict(cross_inp)\n",
    "sorted_hits = sorted(hits, key=lambda x: reranker_scores[hits.index(x)], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3411cd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reranked_docs(\n",
    "    query: str, similar_docs: list[RetrievedDoc]\n",
    ") -> list[RetrievedDoc]:\n",
    "    cross_inp = [[query, doc.page_content] for doc in similar_docs]\n",
    "    reranker_scores = cross_encoder.predict(cross_inp)\n",
    "    sorted_docs = sorted(\n",
    "        similar_docs, key=lambda x: reranker_scores[similar_docs.index(x)], reverse=True\n",
    "    )\n",
    "    return sorted_docs[:10]\n",
    "\n",
    "\n",
    "@traceable\n",
    "async def target_with_reranking(inputs: dict) -> dict:\n",
    "    relevant_docs = get_similar_docs(inputs[\"question\"], top_k=100)\n",
    "    reranked_docs = get_reranked_docs(inputs[\"question\"], relevant_docs)\n",
    "    formatted_docs = format_docs(reranked_docs)\n",
    "    messages = await messages_generation.ainvoke(\n",
    "        {\n",
    "            \"question\": inputs[\"question\"],\n",
    "            \"documents\": formatted_docs,\n",
    "        }\n",
    "    )\n",
    "    response = await llm.ainvoke(messages)\n",
    "    return {\"answer\": response, \"docs\": [doc.id for doc in reranked_docs]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29acba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = await langsmith_client.aevaluate(\n",
    "    target_with_reranking,\n",
    "    data=dataset_name,\n",
    "    evaluators=[recall_at_k, mrr],\n",
    "    max_concurrency=20,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
