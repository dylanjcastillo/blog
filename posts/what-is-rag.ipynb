{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "title: \"What is Retrieval Augmented Generation (RAG)?\"\n",
    "date: 2025-06-26\n",
    "description-meta: \"What is RAG and how does it work?\"\n",
    "categories:\n",
    "  - llm\n",
    "  - rag\n",
    "  - python\n",
    "---\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "It’s a technique to improve LLM answers by providing them with external information before they generate a response.\n",
    "\n",
    "1. **Retrieve:** The system starts by searching a specific knowledge base for relevant information about the query.\n",
    "2. **Augment:** This retrieved information is added to your original question/prompt.\n",
    "3. **Generate:** The LLM uses both your question and the provided information to create a better answer.\n",
    "\n",
    "It’s useful because it reduces \"hallucinations\", allows the use of current data, and builds trust with users by (potentially) providing citations.\n",
    "\n",
    "\n",
    "## Vector databases \n",
    "\n",
    "A vector database (VectorDB) is designed to store and query data as vector embeddings (numerical representations).\n",
    "\n",
    "Some popular vector databases are:\n",
    "\n",
    "1. New generation: [Qdrant](https://qdrant.tech/), [Chroma](https://www.trychroma.com/), and [Pinecone](https://www.pinecone.io/).\n",
    "2. Old generation: [Elasticsearch](https://www.elastic.co/) and [Postgres+PGVector](https://github.com/pgvector/pgvector).\n",
    "\n",
    "New generation doesn't really mean better. It just means more recent. Many of the new providers had to \"re-discover\" the same concepts that were already available in the old generation such as BM25-based retrieval.\n",
    "\n",
    "### Term-based retrieval\n",
    "\n",
    "Term-based retrieval is a technique that uses the terms in the query to find the most relevant documents in the vector database.\n",
    "\n",
    "TF-IDF:\n",
    "\n",
    "- Counts how often a term appears in this document (TF).\n",
    "- Measures how rare the word is across all documents (IDF).\n",
    "- Highlights terms important and unique to this specific document.\n",
    "\n",
    "Okapi BM25: Expands TF-IDF to introduce a weighting mechanism for term saturation and document length.\n",
    "\n",
    "### Embedding-based retrieval\n",
    "\n",
    "- Small dataset: Use k-NN.\n",
    "  - Calculate similarity score between the query vector and every other vector stored in the VectorDB.\n",
    "  - Sort all the vectors based on these similarity scores.\n",
    "  - Return the 'k' most similar vectors (relative to the query)\n",
    "- Big dataset: Use ANN such as LSH or HNSW.\n",
    "\n",
    "### Why use vector databases?\n",
    "\n",
    "If you have a small dataset, there's no real reason to use a vector database. But if you're dealing with thousands or millions of documents, you'll need to use a vector database to efficiently retrieve the most relevant documents.\n",
    "\n",
    "They're useful because:\n",
    "\n",
    "1. The more noise in the context provided to the LLM, the more likely it is to produce bad output.\n",
    "2. It takes more time to process a longer context\n",
    "3. It costs more to process a longer context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## RAG without vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import chromadb\n",
    "import tiktoken\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_text_splitters import (\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"assets/bbva.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = []\n",
    "\n",
    "for page in loader.lazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]\n\u001b[32m     20\u001b[39m context = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mpages\u001b[49m):\n\u001b[32m     22\u001b[39m     context += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--- PAGE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpage.page_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_response\u001b[39m(context: \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[31mNameError\u001b[39m: name 'pages' is not defined"
     ]
    }
   ],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant that can answer questions about the provided context.\n",
    "\n",
    "Please cite the page number used to answer the question. Write the page number in the format \"Page X\" at the end of your answer. \n",
    "\n",
    "If the answer is not found in the context, please say so.\n",
    "\"\"\"\n",
    "user_message = \"\"\"\n",
    "Please answer the following question based on the context provided:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Documents:\n",
    "{documents}\n",
    "\"\"\"\n",
    "\n",
    "messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]\n",
    "context = \"\"\n",
    "for i, page in enumerate(pages):\n",
    "    context += f\"--- PAGE {i + 1} ---\\n{page.page_content}\\n\\n\"\n",
    "\n",
    "\n",
    "def get_response(context: dict):\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_message.format(**context)),\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return response.content\n",
    "\n",
    "\n",
    "question = \"What is the main idea of the document?\"\n",
    "response = get_response({\"question\": question, \"documents\": context})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the daily transaction limits?\"\n",
    "response = get_response({\"question\": question, \"documents\": context})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_ef = OpenAIEmbeddingFunction(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "vector_db = chromadb.PersistentClient()\n",
    "\n",
    "try:\n",
    "    collection = vector_db.delete_collection(\"bbva\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "collection = vector_db.create_collection(\"bbva\", embedding_function=openai_ef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and index documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=[split.page_content for split in all_splits],\n",
    "    metadatas=[split.metadata for split in all_splits],\n",
    "    ids=[str(i) for i in range(len(all_splits))],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcollection\u001b[49m.query(\n\u001b[32m      2\u001b[39m     query_texts=[\u001b[33m\"\u001b[39m\u001b[33mWhat are the daily transaction limits?\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mIs there a monthly limit?\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      3\u001b[39m     n_results=\u001b[32m3\u001b[39m,\n\u001b[32m      4\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'collection' is not defined"
     ]
    }
   ],
   "source": [
    "collection.query(\n",
    "    query_texts=[\"What are the daily transaction limits?\", \"Is there a monthly limit?\"],\n",
    "    n_results=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant that can answer questions about the provided context.\n",
    "\n",
    "Please cite the page number used to answer the question. Write the page number in the format \"Page X\" at the end of your answer. \n",
    "\n",
    "If the answer is not found in the context, please say so.\n",
    "\"\"\"\n",
    "\n",
    "user_message = \"\"\"\n",
    "Please answer the following question based on the context provided:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Documents:\n",
    "{documents}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@traceable\n",
    "def get_relevant_docs(question: str):\n",
    "    relevant_docs = collection.query(query_texts=question, n_results=3)\n",
    "    documents = relevant_docs[\"documents\"][0]\n",
    "    metadatas = relevant_docs[\"metadatas\"][0]\n",
    "    return [\n",
    "        {\"page_content\": doc, \"type\": \"Document\", \"metadata\": metadata}\n",
    "        for doc, metadata in zip(documents, metadatas)\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_context(relevant_docs: list[dict]):\n",
    "    context = \"\"\n",
    "    for doc in relevant_docs:\n",
    "        context += f\"--- PAGE {doc['metadata']['page']} ---\\n{doc['page_content']}\\n\\n\"\n",
    "    return context\n",
    "\n",
    "\n",
    "@traceable\n",
    "def get_messages(question: str, relevant_docs: dict):\n",
    "    prompt_vars = {\"question\": question, \"documents\": get_context(relevant_docs)}\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_message.format(**prompt_vars)),\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "\n",
    "@traceable\n",
    "def get_response(question: str):\n",
    "    relevant_docs = get_relevant_docs(question)\n",
    "    messages = get_messages(question, relevant_docs)\n",
    "    response = model.invoke(messages)\n",
    "    return response.content\n",
    "\n",
    "\n",
    "question = \"What are the customer service channels?\"\n",
    "response = get_response(question)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
