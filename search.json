[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "These are some of my most popular side projects, from newest to oldest.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\n\nDescription\n\n\n\n\n\n\n\n\neconagents\n\n\nPython library that lets you simulate economic experiments using LLMs. \n\n\n\n\n\n\nKing of the Prompt\n\n\nA fun game to see if you‚Äôre a good prompt engineer. \n\n\n\n\n\n\nAre you a good estimator?\n\n\nA fun experiment to see if you‚Äôre a good estimator. It was featured in Financial Times. \n\n\n\n\n\n\nSofia\n\n\nA digital companion for your grandma. \n\n\n\n\n\n\nAItheneum\n\n\nA collection of classic books enhanced with AI to make them easier to read and understand. \n\n\n\n\n\n\nDjango HTMX Components\n\n\nA collection of components and UI patterns built with Django and HTMX that you can copy and paste into your projects. \n\n\n\n\n\n\nlystinct\n\n\nAI real estate listing description generator. It reduces the time it takes to write a listing from 1 hour to 2 minutes. \n\n\n\n\n\n\ndeepsheet\n\n\nAI tool that helps you analyze data and make graphs using ChatGPT. \n\n\n\n\n\n\nNamemancer\n\n\nAI tool that helps you come up with cool titles for your startup. It generates a list of 20 potential titles, accompanied by fitting slogans, and checks if the corresponding .com domains are available. Then, it scores and ranks the titles for you. \n\n\n\n\n\n\nAsk Seneca\n\n\nA GPT-based stoic philosopher who gives you life advice based on Seneca‚Äôs writings. It also gives you the sources it used to answer your questions. It was featured in The Economist. \n\n\n\n\n\n\nShell Genie\n\n\nA command-line tool that lets you interact with the terminal in plain English. You ask the genie what you want to do and it will give you the command you need. It uses GPT-3 in the backend. \n\n\n\n\n\n\nPython Data Visualization Cookbook\n\n\nAn open-source interactive cookbook with +35 recipes for creating commonly used graphs with pandas, matplotlib, seaborn, and plotly.express. \n\n\n\n\n\n\nPandas Cheat Sheet\n\n\nAn open-source interactive cheat sheet with code snippets demonstrating basic pandas operations. \n\n\n\n\n\n\nFast Flood\n\n\nA daily logic puzzle based on Flood. I made it after the NYT bought Wordle for more than $1 million. Fast Flood got more than 100k visitors, and was featured in The Hustle, and led to a five-figure offer. I turned it down and didn‚Äôt make any money :D \n\n\n\n\n\n\nplanMIR\n\n\nA web app that helps physicians prepare for the Spanish medical residence entry exam. I made it to help my wife prepare for the exam while learning more about Django. \n\n\n\n\n\n\nAnchoring\n\n\nAnother cognitive science experiment. The instructions were lengthier and a bit confusing, so it got less traction than the the 2-4-6 task. \n\n\n\n\n\n\nThe 2-4-6 Task\n\n\nA fun cognitive science experiment designed by Peter Wason in the 1970s. It sparked some insightful discussions on reddit. Try it yourself before reading about it! \n\n\n\n\n\n\nPolituits\n\n\nA web app that tracked the sentiment toward Spanish politicians on Twitter in near-real time. I trained the models, built the frontend and backend, and deployed the app on an Ubuntu server. I learned a lot in the process! \n\n\n\n\n\n\nStay Home & Learn\n\n\nA collection of free educational and self-improvement resources made free due to the pandemic. I created it with Google Sheets and a little HTML and CSS. \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dylan Castillo",
    "section": "",
    "text": "Hi there üëã, I‚Äôm Dylan.\nI‚Äôm an independent AI consultant and have been working in the AI & ML space for the past 8 years. I‚Äôve delivered projects for startups, large corporations, and government agencies. I also enjoy working on open-source projects.\nThis is my blog. Here, you‚Äôll find a collection of my articles. I usually write about technical topics, but every once in a while I publish posts about financial independence or my end-of-year reviews.\nIf you‚Äôd like me to send you an email when I write new posts, you can subscribe to my newsletter."
  },
  {
    "objectID": "index.html#posts",
    "href": "index.html#posts",
    "title": "Dylan Castillo",
    "section": "Posts",
    "text": "Posts\nThese are longer posts covering mostly technical topics and a few personal posts such as yearly reviews.\n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nJul 4, 2025\n\n\nBuilding ReAct agents with (and without) LangGraph\n\n\n8 min\n\n\n\n\n\n\nJul 3, 2025\n\n\nAgentic workflows from scratch with (and without) LangGraph\n\n\n16 min\n\n\n\n\n\n\nJul 1, 2025\n\n\nFunction calling and structured outputs in LLMs with LangChain and OpenAI\n\n\n9 min\n\n\n\n\n\n\nJun 29, 2025\n\n\nWhat is Retrieval Augmented Generation (RAG)?\n\n\n9 min\n\n\n\n\n\n\nJun 29, 2025\n\n\nKey parameters for LLMs\n\n\n5 min\n\n\n\n\n\n\nJun 26, 2025\n\n\nPrompt engineering 101\n\n\n10 min\n\n\n\n\n\n\nJun 25, 2025\n\n\nControlling randomness in LLMs: Temperature and Seed\n\n\n10 min\n\n\n\n\n\n\nJan 5, 2025\n\n\n2024: Personal Snapshot\n\n\n11 min\n\n\n\n\n\n\nDec 27, 2024\n\n\nThe good, the bad, and the ugly of Gemini‚Äôs structured outputs\n\n\n13 min\n\n\n\n\n\n\nDec 8, 2024\n\n\nStructured outputs can hurt the performance of LLMs\n\n\n14 min\n\n\n\n\n\n\nNov 9, 2024\n\n\nStructured outputs: don‚Äôt put the cart before the horse\n\n\n5 min\n\n\n\n\n\n\nSep 21, 2024\n\n\nDeploying a FastAPI app with Kamal, AWS ECR, and Github Actions\n\n\n13 min\n\n\n\n\n\n\nSep 15, 2024\n\n\nDeploying a Django app with Kamal 2, AWS ECR, and Github Actions\n\n\n14 min\n\n\n\n\n\n\nSep 8, 2024\n\n\nClassifying images with Gemini Flash 1.5\n\n\n7 min\n\n\n\n\n\n\nAug 11, 2024\n\n\nCreate a Kamal-ready VPS on Hetzner using Terraform\n\n\n3 min\n\n\n\n\n\n\nJan 5, 2024\n\n\n2023: Personal Snapshot\n\n\n9 min\n\n\n\n\n\n\nJun 9, 2023\n\n\nClustering Documents with OpenAI embeddings, HDBSCAN and UMAP\n\n\n6 min\n\n\n\n\n\n\nMay 12, 2023\n\n\nCreate a Code Interpreter Chatbot with Pyodide, LangChain, and OpenAI\n\n\n12 min\n\n\n\n\n\n\nApr 27, 2023\n\n\nSemantic Search with Elasticsearch, OpenAI, and LangChain\n\n\n10 min\n\n\n\n\n\n\nApr 11, 2023\n\n\nSemantic Search with OpenSearch, Cohere, and FastAPI\n\n\n12 min\n\n\n\n\n\n\nMar 3, 2023\n\n\nBuild an AI Search Engine Using FastAPI, Qdrant, and ChatGPT\n\n\n17 min\n\n\n\n\n\n\nFeb 17, 2023\n\n\nTips for Standing Out on LinkedIn\n\n\n10 min\n\n\n\n\n\n\nFeb 3, 2023\n\n\nHow to Securely Deploy a FastAPI app with NGINX and Gunicorn\n\n\n14 min\n\n\n\n\n\n\nJan 20, 2023\n\n\nMy Experience at Entrepreneur First\n\n\n11 min\n\n\n\n\n\n\nSep 21, 2022\n\n\nPyScript 101\n\n\n7 min\n\n\n\n\n\n\nAug 31, 2022\n\n\nHow to Use OpenSearch in Python\n\n\n10 min\n\n\n\n\n\n\nAug 23, 2022\n\n\nText Classification Using Python and Scikit-learn\n\n\n9 min\n\n\n\n\n\n\nAug 12, 2022\n\n\nHow to Use Elasticsearch in Python\n\n\n10 min\n\n\n\n\n\n\nJun 17, 2022\n\n\nEntrepreneurship as a Risk Management Strategy\n\n\n7 min\n\n\n\n\n\n\nJan 31, 2022\n\n\n2021: Personal Snapshot\n\n\n8 min\n\n\n\n\n\n\nOct 7, 2021\n\n\nHow to Use GitHub Deploy Keys\n\n\n5 min\n\n\n\n\n\n\nJul 3, 2021\n\n\nHow to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express\n\n\n100 min\n\n\n\n\n\n\nJan 18, 2021\n\n\nHow to Cluster Documents Using Word2Vec and K-means\n\n\n21 min\n\n\n\n\n\n\nDec 10, 2020\n\n\nClean and Tokenize Text With Python\n\n\n11 min\n\n\n\n\n\n\nMay 25, 2020\n\n\n4 Ways To Improve Your Graphs Using Plotly\n\n\n12 min\n\n\n\n\n\n\nApr 16, 2020\n\n\nUse Google Sheets, S3, and Python to Build a Website Quickly\n\n\n13 min\n\n\n\n\n\n\nMar 1, 2020\n\n\nFast & Asynchronous In Python: Accelerate Your Requests Using asyncio\n\n\n12 min\n\n\n\n\n\n\nDec 20, 2019\n\n\nMind-reading Algorithms: An Introduction to Recommender Systems\n\n\n15 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#today-i-learned-til",
    "href": "index.html#today-i-learned-til",
    "title": "Dylan Castillo",
    "section": "Today I Learned (TIL)",
    "text": "Today I Learned (TIL)\nI‚Äôm a big fan of Simon Willison‚Äôs ‚ÄúTIL‚Äù posts, so I copied his idea.\nThey‚Äôre a great way to force yourself to write more often by lowering the bar for what it‚Äôs worth sharing.\nThese are short, less polished posts about things I‚Äôve learned that I think others might find useful.\n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nJul 9, 2025\n\n\nParallelization and orchestrator-workers workflows with Pydantic AI\n\n\n3 min\n\n\n\n\n\n\nJul 9, 2025\n\n\nEvaluator-optimizer workflow with Pydantic AI\n\n\n3 min\n\n\n\n\n\n\nJul 8, 2025\n\n\nPrompt chaining workflow with Pydantic AI\n\n\n3 min\n\n\n\n\n\n\nJul 8, 2025\n\n\nRouting workflow with Pydantic AI\n\n\n3 min\n\n\n\n\n\n\nJul 4, 2025\n\n\nUsing Pydantic AI to build a ReAct agent\n\n\n2 min\n\n\n\n\n\n\nJun 27, 2025\n\n\nJapanese is the most expensive language in terms of input tokens\n\n\n2 min\n\n\n\n\n\n\nNov 23, 2024\n\n\nTransform any image to WebP from the terminal\n\n\n2 min\n\n\n\n\n\n\nAug 12, 2024\n\n\nFixing missing ‚Äòpython‚Äô error in macOS\n\n\n1 min\n\n\n\n\n\n\nJun 22, 2024\n\n\nA Dockerfile for a Django app using Poetry\n\n\n4 min\n\n\n\n\n\n\nJun 16, 2024\n\n\nMigrate a blog from Ghost to Quarto\n\n\n8 min\n\n\n\n\n\n\nJun 8, 2024\n\n\nInstalling Alacritty, Zellij, and Neovim in macOS\n\n\n6 min\n\n\n\n\n\n\nJan 28, 2024\n\n\nLive Components with Django and htmx\n\n\n10 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "til/transforming-images-to-webp.html",
    "href": "til/transforming-images-to-webp.html",
    "title": "Transform any image to WebP from the terminal",
    "section": "",
    "text": "I was annoyed by the file size of my photo in the About page, because it was slowing down the page load.\nIs it important? No.\nDon‚Äôt I have better things to do on a Saturday afternoon? Yes.\nBut it‚Äôs like going to bed with the closet door open‚Äîyou know there‚Äôs nothing in there, but you just can‚Äôt shake the feeling that the devil (or Diosdado Cabello) might jump out and kill you in your sleep unless you get up and shut it.\nSo I got o1-mini to write a simple script for me, and thought others might find it useful.\nHere it is:\nfunction img2webp() {\n\n  # Check if the input file is provided or if help is requested\n  if [[ $# -lt 1 || \"$1\" == \"--help\" || \"$1\" == \"-h\" ]]; then\n    echo \"Usage: img2webp input_image [quality]\"\n    echo \"  input_image: Path to the input image file\"\n    echo \"  quality: Quality of the output WebP image (0-100, default is 80)\"\n    return 1\n  fi\n\n  local input=\"$1\"\n  local quality=\"${2:-80}\"  # Default quality is 80 if not specified\n  local output=\"${input%.*}.webp\"\n\n  # Convert the image to WebP using ffmpeg\n  ffmpeg -i \"$input\" -qscale:v \"$quality\" \"$output\"\n\n  # Check if the conversion was successful\n  if [[ $? -eq 0 ]]; then\n    echo \"Successfully converted '$input' to '$output' with quality $quality.\"\n  else\n    echo \"Failed to convert '$input' to WebP.\"\n    return 1\n  fi\n}\nIf you‚Äôre using MacOS, you first need to install ffmpeg using Homebrew:\nbrew install ffmpeg\nThen you can add it to your .zshrc and use it by running img2webp &lt;path_to_image&gt; [quality].\nJust as reference, keeping the same quality, I decreased my profile picture from 234KB to 36KB by just changing from PNG to WebP.\nHope you found this useful.\n\n\n\nCitationBibTeX citation:@online{castillo2024,\n  author = {Castillo, Dylan},\n  title = {Transform Any Image to {WebP} from the Terminal},\n  date = {2024-11-23},\n  url = {https://dylancastillo.co/til/transforming-images-to-webp.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCastillo, Dylan. 2024. ‚ÄúTransform Any Image to WebP from the\nTerminal.‚Äù November 23, 2024. https://dylancastillo.co/til/transforming-images-to-webp.html."
  },
  {
    "objectID": "til/parallelization-orchestrator-workers-pydantic-ai.html",
    "href": "til/parallelization-orchestrator-workers-pydantic-ai.html",
    "title": "Parallelization and orchestrator-workers workflows with Pydantic AI",
    "section": "",
    "text": "I‚Äôve been re-implementing typical patterns for building agentic systems with Pydantic AI. In this post, I‚Äôll explore how to build a parallelization and orchestrator-worker workflow.\nIn previous TILs, I‚Äôve explored:\nYou can download this notebook here."
  },
  {
    "objectID": "til/parallelization-orchestrator-workers-pydantic-ai.html#what-is-parallelization",
    "href": "til/parallelization-orchestrator-workers-pydantic-ai.html#what-is-parallelization",
    "title": "Parallelization and orchestrator-workers workflows with Pydantic AI",
    "section": "What is parallelization?",
    "text": "What is parallelization?\nThis workflow is designed for tasks that can be easily divided into independent subtasks. The key trade-off is managing complexity and coordination overhead in exchange for significant speed improvements or diverse perspectives.\nIt looks like this:\n\n\n\n\n\nflowchart LR\n    In([In]) --&gt; LLM1[\"LLM Call 1\"]\n    In --&gt; LLM2[\"LLM Call 2\"]\n    In --&gt; LLM3[\"LLM Call 3\"]\n    LLM1 --&gt; Aggregator[\"Aggregator\"] \n    LLM2 --&gt; Aggregator[\"Aggregator\"] \n    LLM3 --&gt; Aggregator[\"Aggregator\"] \n    Aggregator --&gt; Out([Out])\n\n\n\n\n\n\nExamples:\n\nEvaluate multiple independent aspects of a text (safety, quality, relevance)\nProcess user query and apply guardrails in parallel\nGenerate multiple response candidates given a query for comparison"
  },
  {
    "objectID": "til/parallelization-orchestrator-workers-pydantic-ai.html#what-is-orchestrator-worker",
    "href": "til/parallelization-orchestrator-workers-pydantic-ai.html#what-is-orchestrator-worker",
    "title": "Parallelization and orchestrator-workers workflows with Pydantic AI",
    "section": "What is orchestrator-worker?",
    "text": "What is orchestrator-worker?\nThis workflow works well for tasks where you don‚Äôt know the required subtasks beforehand. The subtasks are determined by the orchestrator.\nHere‚Äôs a diagram:\n\n\n\n\n\nflowchart LR\n    In([In]) --&gt; Orch[Orchestrator]\n\n    Orch -.-&gt; LLM1[\"LLM Call 1\"]\n    Orch -.-&gt; LLM2[\"LLM Call 2\"]\n    Orch -.-&gt; LLM3[\"LLM Call 3\"]\n\n    LLM1 -.-&gt; Synth[Synthesizer]\n    LLM2 -.-&gt; Synth\n    LLM3 -.-&gt; Synth\n\n    Synth --&gt; Out([Out])\n\n\n\n\n\n\n\nExamples:\n\nCoding tools making changes to multiple files at once\nSearching multiple sources and synthesize the results\n\nThe difference between parallelization and orchestrator-worker is that in parallelization, the subtasks are known beforehand, while in orchestrator-worker, the subtasks are determined by the orchestrator."
  },
  {
    "objectID": "til/parallelization-orchestrator-workers-pydantic-ai.html#setup",
    "href": "til/parallelization-orchestrator-workers-pydantic-ai.html#setup",
    "title": "Parallelization and orchestrator-workers workflows with Pydantic AI",
    "section": "Setup",
    "text": "Setup\nPydantic AI uses asyncio under the hood, so you‚Äôll need to enable nest_asyncio to run this notebook:\n\nimport nest_asyncio\n\nnest_asyncio.apply()\n\nThen, you need to import the required libraries. I‚Äôm using Logfire to monitor the workflow.\n\nimport asyncio\nimport os\nfrom pprint import pprint\nfrom typing import Literal, Optional\n\nimport logfire\nimport requests\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai import Agent, RunContext\n\nload_dotenv()\n\nTrue\n\n\nPydanticAI is compatible with OpenTelemetry (OTel). So it‚Äôs pretty easy to use it with Logfire or with any other OTel-compatible observability tool (e.g., Langfuse).\nTo enable tracking, create a project in Logfire, generate a Write token and add it to the .env file. Then, you just need to run:\n\nlogfire.configure(\n    token=os.getenv(\"LOGFIRE_TOKEN\"),\n)\nlogfire.instrument_pydantic_ai()\n\nThe first time you run this, it will ask you to create a project in Logfire. From it, it will generate a logfire_credentials.json file in your working directory. In following runs, it will automatically use the credentials from the file."
  },
  {
    "objectID": "til/parallelization-orchestrator-workers-pydantic-ai.html#parallelization-example",
    "href": "til/parallelization-orchestrator-workers-pydantic-ai.html#parallelization-example",
    "title": "Parallelization and orchestrator-workers workflows with Pydantic AI",
    "section": "Parallelization example",
    "text": "Parallelization example\nIn this example, I‚Äôll show you how to build a workflow that runs the same evaluator in parallel and then aggregates the results.\n\nclass Evaluation(BaseModel):\n    explanation: str\n    is_appropiate: bool\n\n\nclass AggregatedResults(BaseModel):\n    summary: str\n    is_appropiate: bool\n\nThen you can create the agents and encapsulate the logic in a function:\n\nevaluator = Agent(\n    \"openai:gpt-4.1-mini\",\n    output_type=Evaluation,\n    system_prompt=(\n        \"You are an expert evaluator. Provided with a text, you will evaluate if it's appropriate for a general audience.\"\n    ),\n)\n\naggregator = Agent(\n    \"openai:gpt-4.1-mini\",\n    output_type=AggregatedResults,\n    system_prompt=(\n        \"You are an expert evaluator. Provided with a list of evaluations, you will summarize them and provide a final evaluation.\"\n    ),\n)\n\n@logfire.instrument(\"Run workflow\")\nasync def run_workflow(topic: str) -&gt; str:\n    tasks = [evaluator.run(f\"Evaluate the following text: {topic}\") for _ in range(3)]\n    evaluations = await asyncio.gather(*tasks)\n    aggregated_results = await aggregator.run(f\"Summarize the following evaluations:\\n\\n{[(eval.output.explanation, eval.output.is_appropiate) for eval in evaluations]}\")\n    return aggregated_results.output\n\noutput = await run_workflow(\"Athletes should consume enhancing drugs to improve their performance.\")\n\n15:28:36.289 Run workflow\n15:28:36.290   agent run\n15:28:36.290     chat gpt-4.1-mini\n15:28:36.291   agent run\n15:28:36.291     chat gpt-4.1-mini\n15:28:36.292   agent run\n15:28:36.292     chat gpt-4.1-mini\n\n\nLogfire project URL: https://logfire-us.pydantic.dev/dylanjcastillo/blog\n\n\n\n15:28:39.380   aggregator run\n15:28:39.380     chat gpt-4.1-mini\n\n\nFinally, you can run the workflow. You should get an output like this:\n\npprint(output.model_dump())\n\n{'is_appropiate': False,\n 'summary': 'All evaluations agree that the text promotes the consumption of '\n            'performance-enhancing drugs by athletes, which is a sensitive and '\n            'controversial topic. The main concerns highlighted are health '\n            'risks, ethical issues, fairness in sports, and legality. The '\n            'evaluations consistently indicate that encouraging or normalizing '\n            'the use of such drugs is inappropriate for a general audience as '\n            'it may promote illegal, harmful, or unsafe behavior. There is '\n            'consensus that the subject should be handled with caution.'}"
  },
  {
    "objectID": "til/parallelization-orchestrator-workers-pydantic-ai.html#orchestrator-workers-example",
    "href": "til/parallelization-orchestrator-workers-pydantic-ai.html#orchestrator-workers-example",
    "title": "Parallelization and orchestrator-workers workflows with Pydantic AI",
    "section": "Orchestrator-workers example",
    "text": "Orchestrator-workers example\nIn this example, I‚Äôll show you how to build a workflow that given a topic generates a table of contents, then writes each section of the article by making an individual request to an LLM.\nFirst, you must define the structures we‚Äôll use in the outputs of the workflow.\n\nclass Section(BaseModel):\n    name: str = Field(description=\"The name of the section\")\n    description: str = Field(description=\"The description of the section\")\n\n\nclass CompletedSection(BaseModel):\n    name: str = Field(description=\"The name of the section\")\n    content: str = Field(description=\"The content of the section\")\n\n\nclass Sections(BaseModel):\n    sections: list[Section] = Field(description=\"The sections of the article\")\n\nThen, we‚Äôll define the agents we‚Äôll use in the workflow.\n\norchestrator = Agent(\n    \"openai:gpt-4.1-mini\",\n    output_type=Sections,\n    system_prompt=(\n        \"You are an expert writer specialized in SEO. Provided with a topic, you will generate the sections for a short article.\"\n    ),\n)\n\nworker = Agent(\n    \"openai:gpt-4.1-mini\",\n    output_type=CompletedSection,\n    system_prompt=(\n        \"You are an expert writer specialized in SEO. Provided with a topic and a section, you will generate the content of the section.\"\n    ),\n)\n\ndef synthesizer(sections: list[CompletedSection]) -&gt; str:\n    completed_sections_str = \"\\n\\n\".join(\n        [section.content for section in sections]\n    )\n    return completed_sections_str\n\nThen, you can define a function that orchestrates the workflow:\n\n@logfire.instrument(\"Run workflow\")\nasync def run_workflow(topic: str) -&gt; str:\n    orchestrator_output = await orchestrator.run(f\"Generate the sections for a short article about {topic}\")\n    tasks = [worker.run(f\"Write the section {section.name} about {topic} with the following description: {section.description}\") for section in orchestrator_output.output.sections]\n    completed_sections = await asyncio.gather(*tasks)\n    full_article = synthesizer([c.output for c in completed_sections])\n    return full_article\n\noutput = await run_workflow(\"Artificial Intelligence\")\n\n15:32:13.934 Run workflow\n15:32:13.936   orchestrator run\n15:32:13.937     chat gpt-4.1-mini\n15:32:18.567   agent run\n15:32:18.568     chat gpt-4.1-mini\n15:32:18.569   agent run\n15:32:18.569     chat gpt-4.1-mini\n15:32:18.570   agent run\n15:32:18.571     chat gpt-4.1-mini\n15:32:18.572   agent run\n15:32:18.572     chat gpt-4.1-mini\n15:32:18.573   agent run\n15:32:18.573     chat gpt-4.1-mini\n\n\nThat‚Äôs all!\nIf you want to see the full code, you can download the notebook here."
  },
  {
    "objectID": "til/migrate-blog-from-ghost-to-quarto.html",
    "href": "til/migrate-blog-from-ghost-to-quarto.html",
    "title": "Migrate a blog from Ghost to Quarto",
    "section": "",
    "text": "When I started blogging five years ago, I read all reddit posts comparing blogging platforms and concluded that Ghost was the best choice because I needed a powerful tool for all those millions of visitors my blog would get.\nI saw myself as the Garc√≠a M√°rquez of technical writing.\nFast forward five years, and I‚Äôve paid $2,000 for hosting a blog that barely gets 8k visits per month. Plus, I‚Äôm forced to write it in an interface that I hate.\nWith that kind of money, I could have funded a moderately extravagant hamster-only summer party.\nNot that I should, but I could.\nYes, I‚Äôm not proud of that decision1. So I‚Äôm migrating my blog from Ghost to Quarto.\nHere‚Äôs a short guide on how to migrate your blog from Ghost to Quarto."
  },
  {
    "objectID": "til/migrate-blog-from-ghost-to-quarto.html#migrate-blog-from-ghost-to-quarto",
    "href": "til/migrate-blog-from-ghost-to-quarto.html#migrate-blog-from-ghost-to-quarto",
    "title": "Migrate a blog from Ghost to Quarto",
    "section": "Migrate blog from Ghost to Quarto",
    "text": "Migrate blog from Ghost to Quarto\n\nSetting up your blog\nFirst, install Quarto and create a blog in an empty repository:\nquarto create project blog myblog\nThe resulting myblog folder will contain the barebones configuration for a Quarto blog and a posts folder with some example posts. You can remove those posts. Later on, you‚Äôll add your own.\n\n\nExporting your Ghost‚Äôs blog content\nThen, you need to download a copy of your Ghost‚Äôs blog content.\nGo to &lt;YOUR_BLOG_URL&gt;/ghost/#/settings/migration and click on Export, then Export JSON.\n\n\n\nExporting my blog\n\n\nThis is pretty obvious, but remember to replace &lt;YOUR_BLOG_URL&gt; with your blog‚Äôs URL.\nYou‚Äôll get a JSON file with all the posts and pages in your blog. You‚Äôll need to process it to convert your posts to Quarto posts.\nThis small Python script did the heavy lifting for me:\n\n\nShow the code\n\nimport json\nimport os\nfrom datetime import datetime\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom markdownify import markdownify as md\n\n\nBLOG_URL = \"https://dylancastillo.co\" # Replace with your blog's URL\nBLOG_JSON_DUMP = \"./dylan-castillo.ghost.2024-05-28-10-39-09.json\" # Replace with the path to the JSON file you downloaded\nBLOG_AUTHOR_NAME = \"Dylan Castillo\" # Replace with your name\n\n\ndef download_images(markdown_content, post_slug):\n    soup = BeautifulSoup(markdown_content, \"html.parser\")\n    images = soup.find_all(\"img\")\n    if images:\n        os.makedirs(post_slug, exist_ok=True)\n        for img in images:\n            img_url_raw = img[\"src\"]\n            img_url = img_url_raw.replace(\"__GHOST_URL__\", BLOG_URL)\n            img_name = os.path.basename(img_url)\n            response = requests.get(img_url, stream=True)\n            if response.status_code == 200:\n                print(f\"Downloading image: {img_url} to {post_slug}/{img_name}\")\n                with open(os.path.join(post_slug, img_name), \"wb\") as f:\n                    f.write(response.content)\n                markdown_content = markdown_content.replace(\n                    img_url_raw, os.path.join(post_slug, img_name)\n                )\n            else:\n                print(f\"Failed to download image: {img_url}\")\n    return markdown_content\n\n\ndef process_posts(data):\n    posts = data[\"db\"][0][\"data\"][\"posts\"]\n    for post in posts:\n        print(\"Processing post:\", post[\"title\"])\n        title = post[\"title\"]\n        description = post[\"custom_excerpt\"]\n        author = BLOG_AUTHOR_NAME\n        date = (\n            datetime.strptime(post[\"published_at\"], \"%Y-%m-%dT%H:%M:%S.%fZ\").strftime(\n                \"%m/%d/%Y\"\n            )\n            if post[\"published_at\"]\n            else \"\"\n        )\n        date_modified = (\n            datetime.strptime(post[\"updated_at\"], \"%Y-%m-%dT%H:%M:%S.%fZ\").strftime(\n                \"%m/%d/%Y\"\n            )\n            if post[\"updated_at\"]\n            else \"\"\n        )\n\n        # Convert HTML content to Markdown\n        markdown_content = download_images(\n            post[\"html\"] if post[\"html\"] else \"\", post[\"slug\"]\n        )\n        markdown_content = md(markdown_content, code_language=\"python\")\n        markdown_content = markdown_content.replace(\"__GHOST_URL__\", BLOG_URL)\n        markdown_content = f\"\"\"---\\ntitle: \"{title}\"\\ndescription: \"{description}\"\\nauthor: \"{author}\"\\ndate: \"{date}\"\\ndate-modified: \"{date_modified}\"\\n---\\n\\n{markdown_content}\"\"\"\n\n        # Save the markdown content to a file\n        filename = f\"{post['slug']}.md\"\n        with open(filename, \"w\", encoding=\"utf-8\") as file:\n            file.write(markdown_content)\n\n\nif __name__ == \"__main__\":\n    with open(BLOG_JSON_DUMP) as file:\n        data = json.load(file)\n    process_posts(data)\n\nWhen you run the script, it will create a folder with all the posts in .md format and their images. Feel free to adapt it to your needs.\n\n\nCustomizing your blog\nThrough trial and error, I found some settings that helped me customize the look and feel of my blog.\nHere are some of the things I modified:\n\nAdded RSS, favicon, and customized the navbar:\n\n\n\n_quarto.yml\n\nwebsite:\n  title: # The title of your blog\n  site-url: # For the RSS feed that no one will read\n  favicon: # Add a favicon to the blog\n  navbar: # Customize the navbar if you want\n  page-footer: # Add a page footer like \"Copyright 2024, Saul Goodman\" to sound legit\n\n\nAdded custom CSS and JS and a custom theme:\n\n\n\n_quarto.yml\n\nformat:\n  html:\n    include-in-header:\n      - text: |\n          &lt;link href=\"&lt;YOUR_CUSTOM_FONT_URL&gt;\" rel=\"stylesheet\"&gt;\n          &lt;script src=\"&lt;YOUR_CUSTOM_JS_URL&gt;\" defer&gt;&lt;/script&gt;\n    page-layout: \"article\"\n    theme: # Pick a theme and customize it in `custom.scss`\n      - &lt;YOUR_THEME&gt;\n      - custom.scss # Add your custom CSS here\n    code-line-numbers: true # Add line numbers to code blocks\n\n\nFor each post, I used this front matter:\n\n\n\n&lt;POST_SLUG&gt;.md\n\n---\ntitle: \"&lt;POST_TITLE&gt;\"\naliases:\n  - /&lt;POST_SLUG&gt;/ # Add an alias to the previous post's URL\ndescription-meta: \"&lt;POST_DESCRIPTION&gt;\"\ndate: \"&lt;POST_DATE&gt;\"\ndate-modified: last-modified # Automatically set to the last modified date\ntoc: true\ntoc-depth: 3\nlightbox: true # For images\nfig-cap-location: margin # Captions for images\ncategories:\n  - &lt;CATEGORY&gt;\nauthor:\n  - name: &lt;AUTHOR_NAME&gt;\n    url: &lt;AUTHOR_URL&gt;\n    affiliation: &lt;AUTHOR_AFFILIATION&gt;\n    affiliation-url: &lt;AUTHOR_AFFILIATION_URL&gt;\ncitation: true\ncomments:\n  utterances: # For comments\n    repo: &lt;YOUR_GITHUB_USERNAME&gt;/&lt;YOUR_GITHUB_REPO&gt;\n    issue-term: pathname\n---\n\nSee my settings for an example and a recent post source for reference.\nFor the CSS, I copied darkly and created a custom custom.scss file to modify some Bootstrap styles. I just changed some colors and a couple of styles to make the blog look closer to my Ghost theme. It was super easy.\n\n\nDeployment using GitHub Pages + GitHub Actions\nQuarto offers multiple deployment options. I wanted one where I could push changes to a GitHub repository, and have the blog automatically deployed. I went with GitHub Pages combined with GitHub Actions.\nTo deploy the blog, I created a GitHub repository, added the blog‚Äôs content, updated .gitignore to ignore the /.quarto/ and /_site/ and updated _quarto.yml to only compute code locally (otherwise you‚Äôd need a Python kernel running on your GitHub Actions runner):\n\n\n_quarto.yml\n\nexecute:\n  freeze: auto\n\nThen I ran this command to automatically generate the workflow .github/workflows/publish.yml for me:\nquarto publish gh-pages\nSince then, every time I push changes to the main branch, GitHub Actions automatically renders the website and updates the gh-pages branch.\n\n\nUsing a custom domain\nThat seemed to work at first, but very quickly I noticed that whenever I pushed changes to the main branch, the site would no longer be served from my custom domain dylancastillo.co.\nWhen you render your website, Quarto recreates the CNAME file in the gh-pages branch, which seems to break the custom domain setup in GitHub Pages.\nI found a solution in this discussion and added a CNAME file to the root of the repository with my custom domain:\n\n\nCNAME\n\ndylancastillo.co\n\nThen, I added this to _quarto.yml:\n\n\n_quarto.yml\n\nproject:\n  type: website\n  resources: # New\n    - CNAME\n\nAnd that worked!"
  },
  {
    "objectID": "til/migrate-blog-from-ghost-to-quarto.html#conclusion",
    "href": "til/migrate-blog-from-ghost-to-quarto.html#conclusion",
    "title": "Migrate a blog from Ghost to Quarto",
    "section": "Conclusion",
    "text": "Conclusion\nThere you go, my friend.\nNow you can also break free from Ghost.\nSee you in the next post."
  },
  {
    "objectID": "til/migrate-blog-from-ghost-to-quarto.html#footnotes",
    "href": "til/migrate-blog-from-ghost-to-quarto.html#footnotes",
    "title": "Migrate a blog from Ghost to Quarto",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nChoosing Ghost. No regrets about the hypothetical hamster party.‚Ü©Ô∏é"
  },
  {
    "objectID": "til/counting-tokens.html",
    "href": "til/counting-tokens.html",
    "title": "Japanese is the most expensive language in terms of input tokens",
    "section": "",
    "text": "OpenAI mentions in their documentation that 1 token corresponds to roughly 4 characters.\nI was curious how this would work for different languages, so:"
  },
  {
    "objectID": "til/counting-tokens.html#code",
    "href": "til/counting-tokens.html#code",
    "title": "Japanese is the most expensive language in terms of input tokens",
    "section": "Code",
    "text": "Code\nHere‚Äôs the code:\n\nimport tiktoken\n\ndef read_text(file_path):\n    with open(file_path, \"r\") as file:\n        return file.read()\n\ntext_en = read_text(\"../_extras/counting-tokens/en.md\")\ntext_es = read_text(\"../_extras/counting-tokens/es.md\")\ntext_fr = read_text(\"../_extras/counting-tokens/fr.md\")\ntext_de = read_text(\"../_extras/counting-tokens/de.md\")\ntext_jp = read_text(\"../_extras/counting-tokens/jp.md\")\ntext_zh = read_text(\"../_extras/counting-tokens/zh.md\")\ntext_hi = read_text(\"../_extras/counting-tokens/hi.md\")\ntext_ru = read_text(\"../_extras/counting-tokens/ru.md\")\ntext_pt = read_text(\"../_extras/counting-tokens/pt.md\")\n\ndef count_tokens(text):\n    return len(tiktoken.encoding_for_model(\"gpt-4o\").encode(text))\n\nchars_count = {\n    \"en\": len(text_en),\n    \"es\": len(text_es),\n    \"fr\": len(text_fr),\n    \"de\": len(text_de),\n    \"jp\": len(text_jp),\n    \"zh\": len(text_zh),\n    \"hi\": len(text_hi),\n    \"ru\": len(text_ru),\n    \"pt\": len(text_pt),\n}\n\ntokens_count = {\n    \"en\": count_tokens(text_en),\n    \"es\": count_tokens(text_es),\n    \"fr\": count_tokens(text_fr),\n    \"de\": count_tokens(text_de),\n    \"jp\": count_tokens(text_jp),\n    \"zh\": count_tokens(text_zh),\n    \"hi\": count_tokens(text_hi),\n    \"ru\": count_tokens(text_ru),\n    \"pt\": count_tokens(text_pt),\n}\n\nThis reads the text from the file, and uses tiktoken to count the tokens. I also counted the number of characters in the text.\nThen I calculated the ratio of tokens to characters for each language.\n\nfor lang in [\"en\", \"es\", \"fr\", \"de\", \"jp\", \"zh\", \"hi\", \"ru\", \"pt\"]:\n    chars = chars_count[lang]\n    tokens = tokens_count[lang]\n    print(f\"{lang}: {chars / tokens:.2f} chars per token, {chars} chars, {tokens} tokens\")\n\nen: 4.75 chars per token, 2053 chars, 432 tokens\nes: 4.56 chars per token, 2271 chars, 498 tokens\nfr: 4.69 chars per token, 2689 chars, 573 tokens\nde: 4.46 chars per token, 2479 chars, 556 tokens\njp: 1.41 chars per token, 1081 chars, 767 tokens\nzh: 1.33 chars per token, 707 chars, 531 tokens\nhi: 3.51 chars per token, 2194 chars, 625 tokens\nru: 4.02 chars per token, 2275 chars, 566 tokens\npt: 4.63 chars per token, 2200 chars, 475 tokens"
  },
  {
    "objectID": "til/counting-tokens.html#results",
    "href": "til/counting-tokens.html#results",
    "title": "Japanese is the most expensive language in terms of input tokens",
    "section": "Results",
    "text": "Results\nI found this interesting:\n\nEnglish is the most efficient language in terms of characters per token, with 4.75 characters per token.\nMandarin Chinese (1.33 characters per token) is the least efficient language in terms of characters per token, followed by Japanese (1.41 characters per token).\nThe same text in Japanese uses 77% more tokens than in English, making it the most expensive language in terms of input tokens.\nEven though Chinese is less efficient than Japanese in terms of characters per token, it‚Äôs more efficient in terms of information conveyed per character. The article took 2053 characters in English, 707 characters in Chinese, and 1081 characters in Japanese. This explains why Chinese isn‚Äôt also the most expensive language.\nLanguages that use a latin alphabet (English, Spanish, French, German, Portuguese) are more efficient than languages that use a non-latin alphabet (Japanese, Chinese, Hindi, Russian). Russian is the most efficient language of these, with 4.02 characters per token."
  },
  {
    "objectID": "til/counting-tokens.html#limitations",
    "href": "til/counting-tokens.html#limitations",
    "title": "Japanese is the most expensive language in terms of input tokens",
    "section": "Limitations",
    "text": "Limitations\nThis analysis has some clear limitations:\n\nThe text might not be a good example of the types of texts you‚Äôre working with.\nThe translations might not be good enough to truly reflect the information conveyed per character."
  },
  {
    "objectID": "til/live-components-with-django-and-htmx.html",
    "href": "til/live-components-with-django-and-htmx.html",
    "title": "Live Components with Django and htmx",
    "section": "",
    "text": "I discovered django-components late last year and I quickly realized it was the missing piece in my Django + htmx workflow. It made my developer experience so much better, that I even started contributing to it.\ndjango-components lets you build components that combine HTML, JS, and CSS in a single place. Plus, it now lets you use components as views. This feature allows you to keep all the logic for a part of your application in one place, giving you great locality of behavior.\nA click-to-load component would look something like this:\nYou can use this component in any view using {% component 'click_to_load' page_obj=page_obj %} or render it outside of a view by adding it to urls.py:\nShort and sweet, just like the best things in life."
  },
  {
    "objectID": "til/live-components-with-django-and-htmx.html#django-live-components",
    "href": "til/live-components-with-django-and-htmx.html#django-live-components",
    "title": "Live Components with Django and htmx",
    "section": "Django Live Components",
    "text": "Django Live Components\nI thought it‚Äôd be fun to use the library for something it wasn‚Äôt designed for: streaming component changes through server-sent events (SSE).\nIt took me a few hours and several reads of V√≠√∞ir‚Äôs tutorial to figure it out, but it worked. It‚Äôs a bit hacky but all the pieces were there. I just had to find a way to put them together.\nThe code is available here.\nI had a simple idea: set up a Redis pub/sub channel for server notifications. When the client loads the page, it subscribes to this notification channel. Each time the server publishes a new notification, the system reads it from the channel. Then, it renders the HTML and sends it to the client using Server-Sent Events (SSE).\nFirst, you need a notification component, with a streaming view that updates the client whenever a new notification occurs, and a way to subscribe to new notifications sent from the server.\nHere‚Äôs what I came up with:\nimport asyncio\nimport json\nfrom typing import AsyncGenerator\n\nimport redis.asyncio as redis\nfrom django.http import StreamingHttpResponse\nfrom django.utils.decorators import classonlymethod\nfrom django_components import component\n\nr = redis.from_url(\"redis://localhost\")\n\n\ndef sse_message(event_id: int, event: str, data: str) -&gt; str:\n    data = data.replace(\"\\n\", \"\")\n    return f\"id: {event_id}\\n\" f\"event: {event}\\n\" f\"data: {data.strip()}\\n\\n\"\n\n\nclass NotificationComponent(component.Component):\n\n    @classonlymethod\n    def as_live_view(cls, **initkwargs):\n        view = super().as_view(**initkwargs)\n        view._is_coroutine = asyncio.coroutines._is_coroutine\n        return view\n\n    template = \"\"\"\n    &lt;div style=\"color: {{color}};\" role=\"alert\"&gt;\n        &lt;span style=\"font-weight: bold;\"&gt;{{ title }}&lt;/span&gt; {{ message }}\n    &lt;/div&gt;\n    \"\"\"\n\n    async def streaming_response(self, *args, **kwargs) -&gt; AsyncGenerator[str, None]:\n        async with r.pubsub() as pubsub:\n            await pubsub.subscribe(\"notifications_channel\")\n            try:\n                while True:\n                    message = await pubsub.get_message(\n                        ignore_subscribe_messages=True, timeout=1\n                    )\n                    if message is not None:\n                        notification_data = json.loads(message[\"data\"].decode())\n                        sse_message_rendered = sse_message(\n                            notification_data[\"id\"],\n                            \"notification\",\n                            self.render(\n                                {\n                                    \"title\": notification_data[\"title\"],\n                                    \"message\": notification_data[\"message\"],\n                                    \"color\": notification_data[\"color\"],\n                                }\n                            ),\n                        )\n                        yield sse_message_rendered\n                    await asyncio.sleep(0.1)\n            finally:\n                await r.aclose()\n\n    async def get(self, request, *args, **kwargs):\n        return StreamingHttpResponse(\n            streaming_content=self.streaming_response(),\n            content_type=\"text/event-stream\",\n        )\nAnd you should include this in your urls.py:\nfrom django.urls import path\nfrom components.notification import NotificationComponent\n\nurlpatterns = [\n    path(\n        \"notification/\",\n        NotificationComponent.as_live_view(),\n        name=\"stream_notification\",\n    ),\n]\nThen, you need a simple HTML template to show these notifications. I used the htmx SSE extension to handle the SSE connection on the client. This was my template:\n&lt;!-- src/templates/index.html --&gt;\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"UTF-8\" /&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" /&gt;\n    &lt;title&gt;Django Live Components&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;div\n      hx-ext=\"sse\"\n      sse-connect=\"{% url 'stream_notification' %}\"\n      sse-swap=\"notification\"\n    &gt;&lt;/div&gt;\n    &lt;script\n      src=\"https://unpkg.com/htmx.org@1.9.10\"\n      integrity=\"sha384-D1Kt99CQMDuVetoL1lrYwg5t+9QdHe7NLX/SoJYkXDFfX37iInKRy5xLSi8nO7UC\"\n      crossorigin=\"anonymous\"\n    &gt;&lt;/script&gt;\n    &lt;script src=\"https://unpkg.com/htmx.org/dist/ext/sse.js\"&gt;&lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\nFinally, you need a script to simulate these server notifications:\n# random_notifications.py\nimport redis\nimport json\nimport random\nimport time\n\nREDIS_HOST = \"localhost\"\nREDIS_PORT = 6379\nREDIS_CHANNEL = \"notifications_channel\"\n\nr = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)\n\n\ndef create_random_notification():\n    \"\"\"Create a random notification message\"\"\"\n    return {\n        \"id\": random.randint(1, 1000),\n        \"title\": \"Notification \" + str(random.randint(1, 100)),\n        \"message\": \"This is a random message \" + str(random.randint(1, 100)),\n        \"color\": random.choice([\"blue\", \"green\", \"red\", \"black\", \"gray\", \"purple\"]),\n        \"timestamp\": time.ctime(),\n    }\n\n\ndef publish_notification():\n    \"\"\"Publish a random notification to the Redis channel\"\"\"\n    notification = create_random_notification()\n    r.publish(REDIS_CHANNEL, json.dumps(notification))\n    print(f\"Published: {notification}\")\n\n\nif __name__ == \"__main__\":\n    try:\n        while True:\n            publish_notification()\n            time.sleep(3)\n    except KeyboardInterrupt:\n        print(\"Stopped notification publisher\")\nYou can run Redis on Docker to run this script. It‚Äôll start adding notifications to the Redis channel, that you‚Äôll see flash on the page.\nThis was fun. I ended up using a similar pattern in AItheneum."
  },
  {
    "objectID": "til/prompt-chaining-pydantic-ai.html",
    "href": "til/prompt-chaining-pydantic-ai.html",
    "title": "Prompt chaining workflow with Pydantic AI",
    "section": "",
    "text": "To get more familiar with Pydantic AI, I‚Äôve been re-implementing typical patterns for building agentic systems.\nIn this post, I‚Äôll explore how to build a prompt chaining. I won‚Äôt cover the basics of agentic workflows, so if you‚Äôre not familiar with the concept, I recommend you to read this post first.\nI‚Äôve also written other TILs about Pydantic AI: - Routing - Evaluator-optimizer - ReAct agent - Parallelization and Orchestrator-workers\nYou can download this notebook here."
  },
  {
    "objectID": "til/prompt-chaining-pydantic-ai.html#what-is-prompt-chaining",
    "href": "til/prompt-chaining-pydantic-ai.html#what-is-prompt-chaining",
    "title": "Prompt chaining workflow with Pydantic AI",
    "section": "What is prompt chaining?",
    "text": "What is prompt chaining?\nPrompt chaining is a workflow pattern that splits a complex task into multiple subtasks. This gives you better results, but at the cost of longer completion times (higher latency).\nIt looks like this:\n\n\n\n\n\nflowchart LR\n    In --&gt; LLM1[\"LLM Call 1\"]\n    LLM1 -- \"Output 1\" --&gt; Gate{Gate}\n    Gate -- Pass --&gt; LLM2[\"LLM Call 2\"]\n    Gate -- Fail --&gt; Exit[Exit]\n    LLM2 -- \"Output 2\" --&gt; LLM3[\"LLM Call 3\"]\n    LLM3 --&gt; Out\n\n\n\n\n\n\nExamples:\n\nGenerating content in a pipeline by generating table of contents, content, revisions, translations, etc.\nGenerating a text through a multi-step process to evaluate if it matches certain criteria\n\nLet‚Äôs get to it!"
  },
  {
    "objectID": "til/prompt-chaining-pydantic-ai.html#setup",
    "href": "til/prompt-chaining-pydantic-ai.html#setup",
    "title": "Prompt chaining workflow with Pydantic AI",
    "section": "Setup",
    "text": "Setup\nI went with a simple example to implement a content generation workflow composed of three steps:\n\nGenerate a table of contents for the article\nGenerate the content of the article\nUpdate the content of the article if it‚Äôs too long\n\nBecause Pydantic AI uses asyncio under the hood, you need to enable nest_asyncio to use it in a notebook:\n\nimport nest_asyncio\n\nnest_asyncio.apply()\n\nThen, you need to import the required libraries. Logfire is part of the Pydantic ecosystem, so I thought it‚Äôd be good to use it for observability.\n\nimport os\nfrom typing import Literal\n\nimport logfire\nimport requests\nfrom dotenv import load_dotenv\nfrom pydantic_ai import Agent, RunContext\n\nload_dotenv()\n\nTrue\n\n\nPydanticAI is compatible with OpenTelemetry (OTel). So it‚Äôs pretty easy to use it with Logfire or with any other OTel-compatible observability tool (e.g., Langfuse).\nTo enable tracking, create a project in Logfire, generate a Write token and add it to the .env file. Then, you just need to run:\n\nlogfire.configure(\n    token=os.getenv('LOGFIRE_TOKEN'),\n)\nlogfire.instrument_pydantic_ai()\n\n\nLogfire project URL: ]8;id=13458;https://logfire-us.pydantic.dev/dylanjcastillo/blog\\https://logfire-us.pydantic.dev/dylanjcastillo/blog]8;;\\\n\n\n\n\nThe first time you run this, it will ask you to create a project in Logfire. From it, it will generate a logfire_credentials.json file in your working directory. In following runs, it will automatically use the credentials from the file."
  },
  {
    "objectID": "til/prompt-chaining-pydantic-ai.html#prompt-chaining-workflow",
    "href": "til/prompt-chaining-pydantic-ai.html#prompt-chaining-workflow",
    "title": "Prompt chaining workflow with Pydantic AI",
    "section": "Prompt chaining workflow",
    "text": "Prompt chaining workflow\nAs mentioned before, the workflow is composed of three steps: generate a table of contents, generate the content of the article and update the content if it‚Äôs too long.\nSo I created three Agent instances. Each one takes care of one of the steps.\nHere‚Äôs the code:\n\ntoc_agent = Agent(\n    'openai:gpt-4.1-mini',\n    system_prompt=(\n        \"You are an expert writer specialized in SEO. Provided with a topic, you will generate the table of contents for a short article.\"\n    ),\n)\n\narticle_agent = Agent(\n    'openai:gpt-4.1-mini',\n    system_prompt=(\n        \"You are an expert writer specialized in SEO. Provided with a topic and a table of contents, you will generate the content of the article.\"\n    ),\n)\n\neditor_agent = Agent(\n    'openai:gpt-4.1-mini',\n    system_prompt=(\n        \"You are an expert writer specialized in SEO. Provided with a topic, a table of contents and a content, you will revise the content of the article to make it less than 1000 characters.\"\n    ),\n)\n\n\n@logfire.instrument(\"Run workflow\")\ndef run_workflow(topic: str) -&gt; str:\n    toc = toc_agent.run_sync(f\"Generate the table of contents of an article about {topic}\")\n    content = article_agent.run_sync(f\"Generate the content of an article about {topic} with the following table of contents: {toc.output}\")\n    if len(content.output) &gt; 1000:\n        revised_content = editor_agent.run_sync(f\"Revise the content of an article about {topic} with the following table of contents: {toc.output} and the following content: {content.output}\")\n        return revised_content.output\n    return content.output\n\noutput = run_workflow(\"Artificial Intelligence\")\n\n18:54:00.987 Run workflow\n18:54:00.988   toc_agent run\n18:54:00.989     chat gpt-4.1-mini\n18:54:02.911   article_agent run\n18:54:02.911     chat gpt-4.1-mini\n18:54:18.621   editor_agent run\n18:54:18.622     chat gpt-4.1-mini\n\n\nThis code creates the agents and puts them together in a workflow. I used @logfire.instrument to make sure all the traces related to the workflow are logged within the same span. See example below:\n\n\n\nPrompt chaining workflow\n\n\nAnd here‚Äôs the output:\n\nprint(output)\n\nArtificial Intelligence (AI) simulates human intelligence in machines capable of learning, problem-solving, and decision-making. Originating as a formal discipline in the 1950s, AI evolved from rule-based systems to advanced machine learning and deep learning, now integral to daily life. AI types include Narrow AI for specific tasks, General AI with human-level cognition, and theoretical Superintelligent AI. Key technologies include machine learning, deep learning, natural language processing, and computer vision. AI transforms sectors like healthcare, finance, transportation, and education by automating tasks and improving decisions. Benefits include increased efficiency and innovation, while challenges involve data privacy, bias, job displacement, and transparency. Future trends highlight explainable AI, edge computing, and human-AI collaboration. Ethical concerns focus on accountability, fairness, and user privacy. Responsible AI development promises a transformative, inclusive future.\n\n\nThat‚Äôs all!\nYou can access this notebook here.\nIf you have any questions or feedback, please let me know in the comments below."
  },
  {
    "objectID": "posts/entrepreneurship-as-a-risk-management-strategy.html",
    "href": "posts/entrepreneurship-as-a-risk-management-strategy.html",
    "title": "Entrepreneurship as a Risk Management Strategy",
    "section": "",
    "text": "Entrepreneurship, contrary to popular belief, is a great risk management strategy. Thoughtful entrepreneurs take on similar risks as employees while getting a chance to win a much larger pot.\nAlso, it‚Äôs more fun!\nIf you‚Äôve ever read a book about how to start a business, you‚Äôll know that most business failures can be attributed to a few factors:\nAs a result, avoiding those mistakes should be your number one priority. This will increase your chances of success and ensure you stay in the entrepreneurship game long enough to give yourself a fair chance of winning.\nYou don‚Äôt have to have those three things perfectly covered before you begin. Along the way, you could learn how to build products and pick the right market.\nBut one thing is certain, starting a business without a financial safety net is like going to a pistol duel with a nail clipper. It might work, and stories will be written about you if you succeed, but there‚Äôs a good chance you‚Äôll get your brains blown out.\nOnce you‚Äôve taken steps to improve your odds of success and ensured that failure won‚Äôt mean financial ruin, there‚Äôs little downside to ‚Äútaking the risk‚Äù of starting your own business. Allow me to explain.\nLet‚Äôs start with a favorable scenario. You start a business, and it succeeds. In the best case, you could reap most of the financial benefits because you own most of the company‚Äôs equity. This could mean life-changing money. While if you‚Äôre an employee at a successful company, you either own very little equity or none at all. Only extremely rare circumstances, such as being an early employee in a tech unicorn, could result in comparable results.\nTherefore, starting your own business will put you in a better position to build wealth than working for someone else. It is no surprise that surveys of wealthy citizens reveal that most of them make their income from businesses, not salaries.\nConsider the inverse scenario. You are the owner of a failing business. Suppose you funded your business with your savings rather than debt. In that case, the worst-case scenario is that you‚Äôll lose some of your savings and the time you‚Äôve invested in the company. But, since, in any case, you‚Äôll likely work thirty to forty years of your life, getting out of the rat race for a few years and using some of your savings before retirement won‚Äôt make a huge difference in the end result.\nFurthermore, you would‚Äôve gained valuable skills you can use in your next ventures or in a regular job. You could go back to full-time employment and potentially earn more money than before!\nThink about what would happen to an employee in the same situation (a failing business). That‚Äôs when you most need job security, but also the time when many learn that it doesn‚Äôt exist. A company fighting for survival will not hesitate to let you go (ask Coinbase). In the best-case scenario, you‚Äôll be overworked because you‚Äôll take on the work of those who were let go. In the worst-case scenario, you‚Äôll have to return to the job market and look for another job, just like the failed entrepreneur.\nIn conclusion, if all goes well, the advantages of being an entrepreneur far outweigh the benefits of being an employee. If things don‚Äôt go as planned, being an entrepreneur is just slightly worse than being an employee. With similar risk profiles, entrepreneurship produces better results. Not bad, is it?\nAllow me to clarify. This article isn‚Äôt intended to convince you to start your own business. I‚Äôm writing it because these were some of the factors I considered before embarking on my entrepreneurial journey.\nRight now, I‚Äôm in the early stages. Read on if you‚Äôd like to learn more."
  },
  {
    "objectID": "posts/entrepreneurship-as-a-risk-management-strategy.html#the-plan",
    "href": "posts/entrepreneurship-as-a-risk-management-strategy.html#the-plan",
    "title": "Entrepreneurship as a Risk Management Strategy",
    "section": "The Plan",
    "text": "The Plan\nI had considered starting my own business for a long time but didn‚Äôt feel prepared. Also, I didn‚Äôt have enough savings and worried that not having a job would risk my family‚Äôs wellbeing.\nIn 2019, I devised a strategy to get into a position where I could devote all my time to building a business. First, I‚Äôd maximize my savings rate and learn as much as possible about businesses until I had at least one year worth of savings. Then, I‚Äôd take time away from work to build a business.\nI thought a good starting point could be freelancing for companies outside of Spain. That would allow me to learn the ins and outs of running a one-man business and save money at a higher rate compared to full-time employment or freelancing with local companies.\nLast year, I landed my first freelancing contract at the European Commission, and, a few months later, at Deliveroo. I‚Äôll finish my contract with Deliveroo next month and have saved enough money to take time away from work.\nIn addition to freelancing, I also spent a lot of time learning about businesses. I read, listened to podcasts, and consumed any useful material I could find. More importantly, I created a few small products, launched them, and talked to users.\nI haven‚Äôt decided for how long I will be away from work. Most likely, I‚Äôll start with 6 months and will correct course depending on my burn rate, inflation, and whether I feel I‚Äôm progressing toward my goal.\nRegarding business ideas, I‚Äôll concentrate on projects that could help me decouple the amount of time I work from how much money I make. That is, I‚Äôll either build products or develop productized services.\nI have a list of +45 ideas that I‚Äôve started keeping since I devised this plan. Most suck. But these are the ones I‚Äôve shortlisted to tackle in the short-term:\n\nFast Flood: I built a small game called Fast Flood. It got good traction, and I even got a low five-figure offer for it. I have a few improvements that I‚Äôd like to try, and I hope, later on, to sell it for more.\n2X your salary using LinkedIn course: In the last 6 years, I‚Äôve increased my income by 30% on average every year. I have an okayish educational background but have found good-paying jobs at international organizations, a top-tier consulting firm, and a top-tech European company. I believe I can help other Data scientists learn how to use LinkedIn to make more money and find better jobs.\nData Visualization with Python course: I teach a course about Data Science at Nuclio Digital School. I‚Äôve received pretty positive feedback, so I believe I could build an online course and sell it through Gumroad or Udemy.\nCareer coaching for Data scientists: Instead of doing a course, I would personally help Data scientists get a higher-paying jobs or help them get into freelancing. I‚Äôd charge them a variable fee depending on the salary I help them achieve. There‚Äôs already at least one startup doing that.\nGrammarly + Quillbot in Spanish: Grammarly is a $10bn company. There might be 364 million Spanish speakers users on the internet. I believe there‚Äôs a market for a tool that helps you write better in that language.\n\nI‚Äôll work on these ideas in the order listed and ¬†dedicate up to a month to each of them. If they don‚Äôt get any traction, I‚Äôll move on to the next one. It‚Äôs also possible that, along the way, I replace some of them with more promising ideas.\nAlso, I recently received some leads for projects that I believe could become productized services. So I‚Äôm going to make some time for that in parallel, while working in the other ideas."
  },
  {
    "objectID": "posts/entrepreneurship-as-a-risk-management-strategy.html#whats-next",
    "href": "posts/entrepreneurship-as-a-risk-management-strategy.html#whats-next",
    "title": "Entrepreneurship as a Risk Management Strategy",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nThis week I‚Äôm starting with Fast Flood. I already have a list of improvements I want to make, so I will focus on that for now.\nYou can follow my updates on Twitter and LinkedIn.\nThanks to Luis for his feedback on this piece."
  },
  {
    "objectID": "posts/2024-personal-snapshot.html",
    "href": "posts/2024-personal-snapshot.html",
    "title": "2024: Personal Snapshot",
    "section": "",
    "text": "This is my annual review. I use it as a way to reflect on the past year and get a snapshot of ‚Äúwho I am‚Äù at the time of writing.\nIf it‚Äôs me rereading this, welcome back. This is Dylan from 2024.\nIn previous snapshots, I listed what went well and what didn‚Äôt. This year, I‚Äôm trying something new: a chronological approach. 2024 went through three distinct phases, and I found it easier to reflect on each one."
  },
  {
    "objectID": "posts/2024-personal-snapshot.html#early-2024",
    "href": "posts/2024-personal-snapshot.html#early-2024",
    "title": "2024: Personal Snapshot",
    "section": "Early 2024",
    "text": "Early 2024\nThis was my ‚ÄúI‚Äôm lost, and I don‚Äôt know what to do‚Äù phase.\nIn my last snapshot, I said that I was going to focus on building a consulting practice in 2024. Then I came up with a new product idea üòÖ and spent a big chunk of the first two months of the year building a prototype.\n\n\n\nAt least I generated some cool AI book covers.\n\n\nAItheneum1 is a tool that helps you read classic philosophy books by making them easier to understand using AI. I pitched it as a way to make classic books more accessible to modern readers.\nI had fun working on it, but it wasn‚Äôt really solving anyone‚Äôs problem. It was a vitamin, not a painkiller. By the time I released it, it was clear nobody cared about it (not even me!).\nDuring this time, I was also contributing to django-components and half-assing talking to potential clients for freelance work and doing cold outreach to get clients. The former was fun, the latter wasn‚Äôt.\nLater in the year, after reading Alex Hormozi‚Äôs 100M Leads, I realized that cold outreach at this stage was mostly a waste of time. When you‚Äôre just starting out, you should rely on warm introductions or referrals. If you haven‚Äôt built credibility yet, you‚Äôll waste a lot of time trying to convince strangers to work with you.\nI essentially spent the first two months of the year focusing on the wrong things.\nOn a more positive note, I was also doing open mic sets almost every week. I‚Äôm not sure if this was ‚Äúgood‚Äù, but I had a lot of fun performing stand-up comedy.\n\n\n\nMy last stand-up set\n\n\nAs you might expect, I bombed a few times. But I also had a few good sets. A professional (but not famous) comedian even invited me to open for him at a show in a nearby city. I didn‚Äôt go, but it was flattering.\nOne thing that surprised me about stand-up comedy is that it follows a somewhat scientific process. Your joke is a hypothesis about what your audience will find funny. You test it by performing it in front of people. If it doesn‚Äôt work, you adjust it and test it again. You do this over and over until you find something that works.\nThe other thing I liked about it is that it‚Äôs very meritocratic. If you‚Äôre funny, you‚Äôll get laughs. There‚Äôs no way to fake it. It‚Äôs very easy to figure out who‚Äôs good and who‚Äôs not. You might not like someone‚Äôs style, but if they‚Äôre good you cannot deny it.\nHowever, after a few months, I realized it wasn‚Äôt sustainable:\n\nTo get good at stand-up, you have to make time for it. That means writing everyday, and performing as often as possible.\nOpen mics take a lot of time. To practice an 8 minute set, I‚Äôd often have to commute for 1 hour and stay for 1-2 hours of show (during which I did my 8 minutes).\nAs a beginner, you‚Äôll only get spots during weekday evenings. Which isn‚Äôt great if you want to get enough sleep to be productive the next day.\n\nEven so, I didn‚Äôt fully decide to quit until we had a health crisis at home. My wife and I weren‚Äôt prioritizing our health in the first months of the year, and it took its toll.\nStand-up was fun, but it was distracting me from more important things. So I stopped.\nI‚Äôm not sad or regretful about it, I‚Äôm happy that I‚Äôve tried it and that I‚Äôve learned that it‚Äôs not what I want to do right now. I‚Äôd love to return to it in the future, once I have less pressing things to focus on."
  },
  {
    "objectID": "posts/2024-personal-snapshot.html#mid-2024",
    "href": "posts/2024-personal-snapshot.html#mid-2024",
    "title": "2024: Personal Snapshot",
    "section": "Mid 2024",
    "text": "Mid 2024\nThis was the ‚Äúwork hard and mental health struggles‚Äù phase.\nI worked with clients on AI projects in various industries, including legal, construction, consumer packaged goods, and finance. I also started blogging again.\nI frequently put in 12-hour workdays and worked most weekends. I stopped going to the gym, and switched to home workouts. I rarely went to social events. Life passed by rapidly, and looking back, it all feels like a blur.\nI struggled with my mental health. I experienced episodes of depression and anxiety.\nDepression has been part of my life for the past twenty years. Though, I‚Äôm much better at dealing with it than I used to be. I‚Äôve learned to recognize early warning signs and adjust my approach accordingly.\nFor that reason, I try to keep track of the ‚Äúbad days‚Äù. Because when they occur too often, my brain is telling me I should take action. During this phase of the year, I had a lot more of those than I‚Äôd like.\nEventually, things settled down. I became more conscious of my own limits. I learned to regulate the intensity I put into things. I could push myself hard when I needed to, but also learned to feel comfortable with taking a break and not do anything when I needed to.\nI also met some clients in person. While I‚Äôm a fan of remote work, seeing people face to face makes you more empathetic toward the people you‚Äôre working with. This was a lot more fun than I expected.\nDuring this time I also hired external support to help me develop some of my product ideas and support my consulting work."
  },
  {
    "objectID": "posts/2024-personal-snapshot.html#late-2024",
    "href": "posts/2024-personal-snapshot.html#late-2024",
    "title": "2024: Personal Snapshot",
    "section": "Late 2024",
    "text": "Late 2024\nThis was the ‚Äúforce myself to stay focused and in motion‚Äù phase.\nThe last few months were still intense, but less focused. I was working with clients while also making time to think about the future. I had a few moments of ‚Äúwhat if I drop the consulting thing and go back to product development?‚Äù, but decided against it every time.\nI did more sales outreach, managed multiple projects, and kept writing blog posts. I wrote a post about using Structured Outputs in LLMs and fixed a security vulnerability in a Python package that gets more than 1M downloads per month.\nI also traveled to Venezuela for the first time in eight years. It was both wonderful and unsettling.\nDuring the first week of the trip, we stayed in a nice hotel in Margarita, with great food, gym, pools, and a private beach. But if you talked with anyone working at the hotel, you‚Äôd learn they were making less than $200/month and did not routinely have access to clean water or electricity. After almost a decade of living in Europe, it felt strange to witness once again such a disparity.\nI also met old friends and family there. It made me nostalgic for my childhood and university days, but it also made me realize how much I‚Äôve changed since then. When I was a teenager, I often thought I might end up alone and without friends, as I had extreme social anxiety. Now I‚Äôm married, have a great group of friends, and can even tell jokes in public!\nAfter I came back from Venezuela, I started going through more academic research and refreshing my math skills. I‚Äôve always felt that some of my base math skills were lacking and I wanted to fix that. So far, it‚Äôs going great.\nMy father-in-law also moved with us for this part of the year. It was a big change for us, but it‚Äôs been a great experience."
  },
  {
    "objectID": "posts/2024-personal-snapshot.html#stats-photos",
    "href": "posts/2024-personal-snapshot.html#stats-photos",
    "title": "2024: Personal Snapshot",
    "section": "Stats & Photos",
    "text": "Stats & Photos\nHere are the stats for the year:\n\nüíª Code and blogging:\n\nI committed code on 322 out of 365 days.\nI wrote 14 blog posts.\n\nüí∞ Financials:\n\nRevenue doubled from last year. So far, my best year.\nCosts increased by 155% from last year.\n66% of my revenue comes from time-based billing (daily, hourly).\nMy biggest client represented 33% of my revenue.\n\nüìù Sales:\n\nI worked on 9 projects with 7 different clients.\nI got 3 new clients.\nSuccess rate of proposals: 67%.\n\nüí™ Health & Fitness:\n\n112 strength training sessions (2.2 sessions/week).\n3,488 minutes of cardio (~60 minutes/week).\nVO2 max: 46 ml/kg/min.\nRHR: 57 bpm.\n\n\nThese are some photos from the year:\n\n\n\n\n\n\nI don‚Äôt like taking pictures, but looking back, I regret not capturing more key moments from the year. That‚Äôs a small lesson I‚Äôll take into next year."
  },
  {
    "objectID": "posts/2024-personal-snapshot.html#lessons-learned",
    "href": "posts/2024-personal-snapshot.html#lessons-learned",
    "title": "2024: Personal Snapshot",
    "section": "Lessons Learned",
    "text": "Lessons Learned\nThe three biggest lessons I took away from 2024 are focus, intensity, and action.\nFocus is about figuring out what are the right things to prioritize. It‚Äôs easy to say that, but it‚Äôs hard to do. Life is full of distractions, and it‚Äôs easy to get lost in the noise.\nWhen I‚Äôm not sure what to do, I use one heuristic that works well: I prioritize things that will help me on my way regardless of their outcome. For example, if I dedicate more time to writing technical posts, I learn through the act of writing, but I also attract clients who are interested in those posts. The same with sales: I might not get a client from a sales call, but I learn a lot about the process and get better at it. So, regardless of the outcome, I win.\nIntensity is about working hard and taking ownership. This year I handled more work than ever. My brain fought back. At times I felt anxious, depressed, out of energy, and unable to focus. Eventually, my brain accepted that I wouldn‚Äôt back down. It stopped fighting against me and began working for me.\nIt‚Äôs not pretty. It‚Äôs deeply uncomfortable. But it‚Äôs worth it.\nAction is about staying in motion. Being too busy is better than being too idle. When you‚Äôre busy, you‚Äôre constantly learning, forced to prioritize, and often become more creative under pressure.\nI used to think that being ‚Äútoo busy‚Äù would prevent me from focusing on the right things. But I eventually realized it was the other way around. The busier I got, the clearer my priorities became. I learned more, and the better I became at what I was doing, the more time I had for the things I really wanted to do.\nBeing busy made me better and more productive."
  },
  {
    "objectID": "posts/2024-personal-snapshot.html#whats-next",
    "href": "posts/2024-personal-snapshot.html#whats-next",
    "title": "2024: Personal Snapshot",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nI‚Äôve realized I‚Äôm not great at predicting what I‚Äôll want to do, nor am I a great planner.\nRight now, my biggest priority is growing my consulting practice. I also finally found a painkiller product idea that I‚Äôve been slowly working on. If I find enough traction, I‚Äôll pivot to it. But until then, my focus is on consulting.\nThe only thing I‚Äôm sure is that I‚Äôll keep myself busy."
  },
  {
    "objectID": "posts/2024-personal-snapshot.html#footnotes",
    "href": "posts/2024-personal-snapshot.html#footnotes",
    "title": "2024: Personal Snapshot",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nyes, I know, what a terrible name.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/opensearch-python.html",
    "href": "posts/opensearch-python.html",
    "title": "How to Use OpenSearch in Python",
    "section": "",
    "text": "OpenSearch is a free and open-source search platform created by Amazon that makes it easy to search and analyze data. Developers use it for search in apps, log analytics, data observability, data ingestion, and more.\nOpenSearch began with a lot of controversy. It is a fork of Elasticsearch and Kibana that was created in January 2021 due to a change in the license of Elastic‚Äôs products. Elastic changed its policy because it believed AWS was not using its products fairly. Both companies faced significant backlash as a result of this event.\nFollowing this conflict, AWS stopped providing clusters with the most recent version of Elasticsearch. The most recent version available is 7.10. Since then, OpenSearch has been the default option (currently on version 1.3).\nGiven the prevalence of AWS in many industries, it‚Äôs likely that you‚Äôll end up using OpenSearch at some point. If you work in data, you should familiarize yourself with this technology.\nMany data scientists struggle to set up a local environment or understand how to interact with OpenSearch in Python, and there aren‚Äôt many resources available to help. This is why I created this tutorial.\nYou‚Äôll learn how to:\nLet‚Äôs get to it!"
  },
  {
    "objectID": "posts/opensearch-python.html#prerequisites",
    "href": "posts/opensearch-python.html#prerequisites",
    "title": "How to Use OpenSearch in Python",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore you begin, a few things must be in place. Follow these steps:\n\nInstall Docker.\nDownload the data.\nCreate a virtual environment and install the required packages. You can create one with venv by running these commands in the terminal:\n\npython3 -m venv .venv\nsource .venv/bin/activate\npython3 -m pip install pandas==1.5.3 notebook==6.5.3 opensearch-py==2.2.0\nLet‚Äôs move on to the next section."
  },
  {
    "objectID": "posts/opensearch-python.html#run-a-local-opensearch-cluster",
    "href": "posts/opensearch-python.html#run-a-local-opensearch-cluster",
    "title": "How to Use OpenSearch in Python",
    "section": "Run a Local OpenSearch Cluster",
    "text": "Run a Local OpenSearch Cluster\nUsing Docker is the simplest method for running OpenSearch locally. Run the following command in a terminal to launch a single-node cluster:\ndocker run --rm -p 9200:9200 -p 9600:9600 -e \"discovery.type=single-node\" opensearchproject/opensearch:2.6.0\nYou will see a lot of text on your terminal after running this command. But it‚Äôs okay, don‚Äôt worry!\nLet me describe what the command mentioned above does:\n\n**docker run**: It‚Äôs the command you use to run an image inside -a container.\n**--rm**: This flag instructs Docker to clear the container‚Äôs contents and file system after it shuts down.\n**-p 9200:9200 -p 9600:9600** : This instructs Docker to open specific ports on the container‚Äôs network interface.\n**-e \"discovery.type=single-node\"**: This instructs Docker to set up a cluster using a single node."
  },
  {
    "objectID": "posts/opensearch-python.html#connect-to-your-cluster",
    "href": "posts/opensearch-python.html#connect-to-your-cluster",
    "title": "How to Use OpenSearch in Python",
    "section": "Connect to Your Cluster",
    "text": "Connect to Your Cluster\nCreate a new Jupyter Notebook, and run the following code, to connect to your newly created OpenSearch cluster.\nfrom opensearchpy import OpenSearch\n\nclient = OpenSearch(\n    hosts = [{\"host\": \"localhost\", \"port\": 9200}],\n    http_auth = (\"admin\", \"admin\"),\n    use_ssl = True,\n    verify_certs = False,\n    ssl_assert_hostname = False,\n    ssl_show_warn = False,\n)\nclient.info()\nThis will connect to your local cluster. You specify the following parameters:\n\n**hosts = [{\"host\": \"localhost\", \"port\": 9200}]**: this tells OpenSearch to connect to the cluster running in your local machine in port 9200.\n**http_auth = (\"admin\", \"admin\")**: this provides the username and password to use when connecting to the cluster. You‚Äôll use admin for both as this is only for local development.\nuse_ssl=True: this tells the client to use an SSL connection. OpenSearch‚Äôs docker images use SSL connections by default, so you‚Äôll need to set this to True even if you‚Äôre using self signed certificates.\n**verify_certs**, **ssl_assert_hostname**, and **ssl_show_warn**: The first two security parameters that you‚Äôll set to false, as this is a cluster running locally only used for development purposes. The last parameter prevents the client from raising a warning when initiating the connection with the cluster.\n\nIf everything went well, your output should look like this:\nNext, you‚Äôll read the data you‚Äôll use in the tutorial."
  },
  {
    "objectID": "posts/opensearch-python.html#read-the-data",
    "href": "posts/opensearch-python.html#read-the-data",
    "title": "How to Use OpenSearch in Python",
    "section": "Read the Data",
    "text": "Read the Data\nYou‚Äôll use pandas to read the dataset and get a sample of 5,000 observations. Feel free to use dataset if you want to, but not that if you do, it‚Äôll take longer to index the data.\nimport pandas as pd\n\ndf = (\n    pd.read_csv(\"wiki_movie_plots_deduped.csv\")\n    .dropna()\n    .sample(5000, random_state=42)\n    .reset_index(drop=True)\n)\nNext, let‚Äôs create an index to store your data."
  },
  {
    "objectID": "posts/opensearch-python.html#create-an-index",
    "href": "posts/opensearch-python.html#create-an-index",
    "title": "How to Use OpenSearch in Python",
    "section": "Create an Index",
    "text": "Create an Index\nOpenSearch stores and represents your data using a data structure known as an inverted index. This data structure identifies the documents in which each unique word appears.\nThis inverted index is why OpenSearch can form very quick full-text searches.\nTo index documents, you must first create an index. Here‚Äôs how you do it:\nbody = {\n    \"mappings\":{\n        \"properties\": {\n            \"title\": {\"type\": \"text\", \"analyzer\": \"english\"},\n            \"ethnicity\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n            \"director\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n            \"cast\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n            \"genre\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n            \"plot\": {\"type\": \"text\", \"analyzer\": \"english\"},\n            \"year\": {\"type\": \"integer\"},\n            \"wiki_page\": {\"type\": \"keyword\"}\n        }\n    }\n}\nresponse = client.indices.create(\"movies\", body=body)\nThis code will create a new index called movies using the cluster you set up earlier.\nLines 1 to 14 define the request‚Äôs body, specifying configuration settings used when the index is created. The mapping, which tells the index how to store the documents, is the only specified setting in this case.\nThere are two ways to map data fields in OpenSearch: dynamic mapping and explicit mapping. With dynamic mapping, the engine automatically detects the data type for each field. With explicit mapping, you manually define the data type for each field. In this example, you‚Äôll use the latter.\nNow you‚Äôll start adding data to your index."
  },
  {
    "objectID": "posts/opensearch-python.html#add-data-to-your-index",
    "href": "posts/opensearch-python.html#add-data-to-your-index",
    "title": "How to Use OpenSearch in Python",
    "section": "Add Data to Your Index",
    "text": "Add Data to Your Index\nThere are two ways to add data to an index: client.index() and bulk(). client.index() lets you add one item at a time while bulk() lets you add multiple items simultaneously.\nYou can use any of the two methods to add data to your index:\n\nUsing client.index()\nHere‚Äôs how you use client.index() to index your data:\nfor i, row in df.iterrows():\n    body = {\n            \"title\": row[\"Title\"],\n            \"ethnicity\": row[\"Origin/Ethnicity\"],\n            \"director\": row[\"Director\"],\n            \"cast\": row[\"Cast\"],\n            \"genre\": row[\"Genre\"],\n            \"plot\": row[\"Plot\"],\n            \"year\": row[\"Release Year\"],\n            \"wiki_page\": row[\"Wiki Page\"]\n    }\n    client.index(index=\"movies\", id=i, body=body)\nThis code iterates through the rows of the dataset you read earlier and indexes the relevant information from each row using client.index(). You use three parameters of that method:\n\nindex=\"movies\": this tells OpenSearch which index to use to store the data, as you can have multiple indexes in a cluster.\nid=i: this is the document‚Äôs identifier when you add it to the index. In this case, you set it to be the row number.\ndocument=doc: this tells the engine what information it should store.\n\n\n\nUsing bulk()\nHere‚Äôs how you use bulk() to store your data:\nfrom opensearchpy.helpers import bulk\n\nbulk_data = []\nfor i,row in df.iterrows():\n    bulk_data.append(\n        {\n            \"_index\": \"movies\",\n            \"_id\": i,\n            \"_source\": {\n                \"title\": row[\"Title\"],\n                \"ethnicity\": row[\"Origin/Ethnicity\"],\n                \"director\": row[\"Director\"],\n                \"cast\": row[\"Cast\"],\n                \"genre\": row[\"Genre\"],\n                \"plot\": row[\"Plot\"],\n                \"year\": row[\"Release Year\"],\n                \"wiki_page\": row[\"Wiki Page\"],\n            }\n        }\n    )\nbulk(client, bulk_data)\nbulk() requires the same information as client.index(): the index‚Äôs name, the document‚Äôs ID, and the document itself. But instead of adding each item one by one, you must create a list of dictionaries with all the documents you want to add to the index. Then, you pass this information and the client to bulk().\nAfter you add the data, you can make sure it worked by counting the number of items in the index:\nclient.indices.refresh(index=\"movies\")\nclient.cat.count(index=\"movies\", format=\"json\")\nYour output should look like this:"
  },
  {
    "objectID": "posts/opensearch-python.html#search-your-data",
    "href": "posts/opensearch-python.html#search-your-data",
    "title": "How to Use OpenSearch in Python",
    "section": "Search Your Data",
    "text": "Search Your Data\nFinally, you‚Äôll want to start running searches using your index. OpenSearch comes with a query domain-specific Language (DSL) that lets you tailor your searches to your needs.\nHere‚Äôs an example of a search that looks for movies starring Jack Nicholson but whose director isn‚Äôt Tim Burton:\nresp = client.search(\n    index=\"movies\",\n    body={\n        \"query\": {\n            \"bool\": {\n                \"must\": {\n                    \"match_phrase\": {\n                        \"cast\": \"jack nicholson\",\n                    }\n                },\n                \"filter\": {\"bool\": {\"must_not\": {\"match_phrase\": {\"director\": \"tim burton\"}}}},\n            },\n        },\n    }\n)\nresp\nWhen you run this code, you should get a very long response that looks something like this:\n\nThere are many ways to tailor your search queries. To learn more about it, check out the official documentation."
  },
  {
    "objectID": "posts/opensearch-python.html#delete-documents-from-the-index",
    "href": "posts/opensearch-python.html#delete-documents-from-the-index",
    "title": "How to Use OpenSearch in Python",
    "section": "Delete Documents From the Index",
    "text": "Delete Documents From the Index\nYou can use the following code to remove documents from the index:\nclient.delete(index=\"movies\", id=\"2500\")\nThe code above will delete the document with ID 2500 from the index movies."
  },
  {
    "objectID": "posts/opensearch-python.html#delete-an-index",
    "href": "posts/opensearch-python.html#delete-an-index",
    "title": "How to Use OpenSearch in Python",
    "section": "Delete an Index",
    "text": "Delete an Index\nFinally, if, for whatever reason, you‚Äôd like to delete an index (and all of its documents), here‚Äôs how you do it:\nclient.indices.delete(index='movies')"
  },
  {
    "objectID": "posts/opensearch-python.html#conclusion",
    "href": "posts/opensearch-python.html#conclusion",
    "title": "How to Use OpenSearch in Python",
    "section": "Conclusion",
    "text": "Conclusion\nThis tutorial taught you the basics of OpenSearch and how to use it in Python. OpenSearch is now the default offering in AWS, so if you‚Äôre a data professional, you should familiarize yourself with it.\nIn this tutorial, you‚Äôve learned:\n\nHow to set up a local OpenSearch cluster\nHow to create an index and store data in it\nHow to search your data using OpenSearch\nHow to delete documents and an index\n\nIf you have any questions or feedback, let me know in the comments!\nAll the code for this tutorial is available on GitHub."
  },
  {
    "objectID": "posts/fastapi-nginx-gunicorn.html",
    "href": "posts/fastapi-nginx-gunicorn.html",
    "title": "How to Securely Deploy a FastAPI app with NGINX and Gunicorn",
    "section": "",
    "text": "Deploying a FastAPI web app to a Virtual Private Server (VPS) is tricky. If you‚Äôre not familiar with technologies such as NGINX, Gunicorn, and Uvicorn, it can easily overwhelm you. I wrote this tutorial so you won‚Äôt have to spend as much time on your first deployment as I did.\nFastAPI is one of the most popular Python libraries for developing APIs, thanks to its performance and ease of use. If you‚Äôre using machine learning models in your web app, it‚Äôs likely the go-to tool for you.\nNGINX , Gunicorn, and Uvicorn are battle-tested technologies that are frequently used as a reverse proxy and ASGI server when deploying Python web apps. If you‚Äôre familiar with Django or Flask, you‚Äôve probably heard about some of them before.\nIn this tutorial, I‚Äôll show you how to combine these tools to deploy a FastAPI web app. You will:\nLet‚Äôs get to it!"
  },
  {
    "objectID": "posts/fastapi-nginx-gunicorn.html#prerequisites",
    "href": "posts/fastapi-nginx-gunicorn.html#prerequisites",
    "title": "How to Securely Deploy a FastAPI app with NGINX and Gunicorn",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou should have access to a Debian-based VPS. I will use Ubuntu 20.04.\nYou should be familiar with basic shell commands, such as sudo, mkdir, or cd.\nYou should know how to exit vim üòú"
  },
  {
    "objectID": "posts/fastapi-nginx-gunicorn.html#tech-stack",
    "href": "posts/fastapi-nginx-gunicorn.html#tech-stack",
    "title": "How to Securely Deploy a FastAPI app with NGINX and Gunicorn",
    "section": "Tech Stack",
    "text": "Tech Stack\nBefore we go any further, I‚Äôll give you a quick rundown of the technologies you‚Äôll be using:\n\nFastAPI is one of the most popular Python frameworks for building APIs.\nIt‚Äôs built on top of Starlette and Pydantic and uses standard Python type hints. It‚Äôs loved by developers because of it is performant, easy to learn, and provides a great developer experience.\nGunicorn is a popular web server used to deploy Python web apps. Typically, it‚Äôs used as a WSGI server, but it‚Äôs possible to combine it with Uvicorn to work as an ASGI server.\nUvicorn is an ASGI web server implementation for Python. It‚Äôs the recommended web server for Starlette and FastAPI.\nNGINX is an open-source tool with many uses. It started out as a web server but can now be used as a reverse proxy server, a load balancer, and more.\nNGINX is often used as a reverse proxy in front of the app‚Äôs web server when working with Python web frameworks.\n\nNow, let‚Äôs get to the interesting part!"
  },
  {
    "objectID": "posts/fastapi-nginx-gunicorn.html#optional-step-1-secure-your-server",
    "href": "posts/fastapi-nginx-gunicorn.html#optional-step-1-secure-your-server",
    "title": "How to Securely Deploy a FastAPI app with NGINX and Gunicorn",
    "section": "(Optional) Step 1: Secure Your Server",
    "text": "(Optional) Step 1: Secure Your Server\nThis step isn‚Äôt required, but it‚Äôs still a good idea to at least skim it. Even more so if you‚Äôre not sure what you‚Äôre doing. This will make your application more secure.\n\nEnable Automatic Updates\nFirst, you should make sure your server has the latest software:\nsudo apt update && sudo apt upgrade -y\nThese are common commands you‚Äôll see when working with Debian-based servers:\n\nsudo apt update updates the package list index on the user‚Äôs system.\nsudo apt upgrade upgrades the installed packages to their latest versions. You provide the -y flag to proceed with the installation without requiring confirmation.\n\nNext, you should set up automatic security updates, so that you don‚Äôt have to do them manually. For that, you‚Äôll need to install and enable unnattended-upgrades:\nsudo apt install unattended-upgrades\nOnce the installation is finished, edit /etc/apt/apt.conf.d/20auto-upgrades to include the following lines:\nAPT::Periodic::Update-Package-Lists \"1\";\nAPT::Periodic::Unattended-Upgrade \"1\";\nAPT::Periodic::AutocleanInterval \"7\";\nThese lines configure unattended-upgrades so that it runs automatically. Here‚Äôs what they do:\n\nAPT::Periodic::Update-Package-Lists \"1\" means that the list of packages will be automatically updated every day.\nAPT::Periodic::Unattended-Upgrade \"1\" means that the system will be updated to the latest version of the packages without the user having to intervene.\n\"APT::Periodic::AutocleanInterval \"7\" means that the auto-clean operation, which gets rid of old and unnecessary package files, will run once a week.\n\nLastly, edit /etc/apt/apt.conf.d/50unattended-upgrades to make sure the system automatically reboots when kernel updates require it:\nUnattended-Upgrade::Automatic-Reboot \"true\"; # change to true\nYou can also configure your system to send emails when there are issues with the upgrades. If you want to do that, take a look at this article.\nWhew! You‚Äôve now ensured that your system is up to date and will remain so. Next, you‚Äôll create a user to make sure you don‚Äôt give your app more permissions than it needs to run.\n\n\nCreate a Non-root User\nIf your server ever gets hacked, having a non-root user reduces the damage the malicious actor can do. That, among other reasons, justifies the creation of a non-root user.\nsudo adduser fastapi-user # replace fastapi-user with your preferred name\nsudo gpasswd -a fastapi-user sudo # add to sudoers\nsu - fastapi-user # login as fastapi-user\nThese commands will create a user name fastapi-user, add it to the sudo group (which contains all users with root privileges), and then log in as that user.\nThen, you will set up your server so that you connect to it using an SSH key instead of a password. It‚Äôs safer and faster, so you have nothing to lose.\nIf you don‚Äôt already have an SSH key, open a new terminal on your local machine and run the following command. Otherwise, skip this step, and move directly to copy your public SSH key.\nssh-keygen -t ed25519 -C \"username@email.com\"\n‚ÄåThis command will create and store an SSH key in your local machine. You employ two parameters:\n\n-t ed25519 to specify which algorithm to use to generate the key. You went with ED25519, which is a very safe and efficient algorithm.\n-C username@email.com to append your email as a comment at the end of the key. Make sure to replace username@email.com it with your actual email.\n\nThen, copy your public SSH key by using this command and copying the output:\ncat ~/.ssh/id_ed25519.pub\nGo back to the remote server‚Äôs terminal and type in the following commands:\nmkdir ~/.ssh/\nchmod 700 -R ~/.ssh/\nsudo vim ~/.ssh/authorized_keys\nThese commands will:\n\nCreate a .ssh directory\nSet the necessary permissions (the owner of .ssh/ has full read, write, and execute permissions, but other users and groups shouldn‚Äôt).\nOpen authorized_keys with an editor\n\nPaste your public SSH key into authorized_keys. Save the changes and close the editor. Make sure the changes worked by closing the terminal and logging back into your machine using the following command:\nssh fastapi-user@your-server-ip\nOnce you‚Äôve tested that it works, you should disable the root login and use password authentication for SSH connections. To do this, you‚Äôll have to update the following values in /etc/ssh/sshd_config using vim (or any other editor) using sudo privileges:\nPermitRootLogin no # change to no\n...\nPasswordAuthentication no # change to no\nThese modifications will prohibit users from logging in as root and also disable the option of authenticating using a password rather than an SSH key.\n\n\nOther Security Measures\nMost cloud providers offer firewall services, but if yours doesn‚Äôt, you should configure one and only allow incoming traffic to the necessary ports: 80, 443, and 22.\nAlso, you can install fail2ban to prevent brute-force authentication attacks. To learn more about the best practices to secure a Linux server, check out this guide from Linode."
  },
  {
    "objectID": "posts/fastapi-nginx-gunicorn.html#step-2-install-software-tools",
    "href": "posts/fastapi-nginx-gunicorn.html#step-2-install-software-tools",
    "title": "How to Securely Deploy a FastAPI app with NGINX and Gunicorn",
    "section": "Step 2: Install Software Tools",
    "text": "Step 2: Install Software Tools\nYou will require a few software tools. Begin by running the following commands to install Python:\nsudo add-apt-repository ppa:deadsnakes/ppa\nsudo apt update\nsudo apt install python3.11 python3.11-venv -y\nThen, install Supervisor and NGINX:\nsudo apt install supervisor nginx -y\nSupervisor is a process control system for Unix-like operating systems, including Linux. It‚Äôs intended to monitor and manage the processes of programs, ensuring that they are always running and restarting them if they crash or shut down.\nNGINX,as mentioned before, is a popular multifaceted software, that‚Äôs often used as a reverse proxy when deploying web applications.\nEnable and start Supervisor:\nsudo systemctl enable supervisor\nsudo systemctl start supervisor\nenable will make sure Supervisor starts on boot, and start will start Supervisor right away."
  },
  {
    "objectID": "posts/fastapi-nginx-gunicorn.html#step-3-set-up-your-fastapi-app",
    "href": "posts/fastapi-nginx-gunicorn.html#step-3-set-up-your-fastapi-app",
    "title": "How to Securely Deploy a FastAPI app with NGINX and Gunicorn",
    "section": "Step 3: Set Up Your FastAPI App",
    "text": "Step 3: Set Up Your FastAPI App\nStart by cloning the sample app into /home/fastapi-user:\ngit clone https://github.com/dylanjcastillo/fastapi-nginx-gunicorn\nThis will work with public repositories. If you want to deploy an app from a private GitHub repository, you should set up a GitHub deploy key and clone the repository using it.\nNext, create a virtual environment and activate it in the project directory:\ncd /home/fastapi-user/fastapi-nginx-gunicorn\npython3.11 -m venv .venv\nsource .venv/bin/activate\nThese commands will change your current location to the project directory, create a virtual environment in it, and activate it. From now on, you should see a (.venv) prefix in your command line.\nNow, use pip to install the libraries specified in requirements.txt:\npip install -r requirements.txt\nThis will install the packages in requirements.txt: fastapi, gunicorn, and uvicorn, in your current virtual environment.\nVerify that everything went well by running the application:\nuvicorn main:app\nYou shouldn‚Äôt get any errors when you run this command. You can also verify that it‚Äôs working by opening a new terminal window, connecting to the server, and making a request with curl:\ncurl http://localhost:8000\nYou should get the following response:\n{\"message\":\"It's working!\"}\nYou‚Äôre halfway there. You got your FastAPI app running, next you‚Äôll configure Gunicorn to serve as a WSGI server."
  },
  {
    "objectID": "posts/fastapi-nginx-gunicorn.html#step-4-configure-gunicorn",
    "href": "posts/fastapi-nginx-gunicorn.html#step-4-configure-gunicorn",
    "title": "How to Securely Deploy a FastAPI app with NGINX and Gunicorn",
    "section": "Step 4: Configure Gunicorn",
    "text": "Step 4: Configure Gunicorn\nThere are two parts to configuring Gunicorn. First, specifying the configuration requirements of gunicorn. Second, setting up a Supervisor program to run it.\n\nSet Up Gunicorn\nYou‚Äôll first create a file to define the parameters you‚Äôll use when running Gunicorn. For that, create a file called gunicorn_start in the project directory:\nvim gunicorn_start\nThen, add the following content to it:\n#!/bin/bash\n\nNAME=fastapi-app\nDIR=/home/fastapi-user/fastapi-nginx-gunicorn\nUSER=fastapi-user\nGROUP=fastapi-user\nWORKERS=3\nWORKER_CLASS=uvicorn.workers.UvicornWorker\nVENV=$DIR/.venv/bin/activate\nBIND=unix:$DIR/run/gunicorn.sock\nLOG_LEVEL=error\n\ncd $DIR\nsource $VENV\n\nexec gunicorn main:app \\\n  --name $NAME \\\n  --workers $WORKERS \\\n  --worker-class $WORKER_CLASS \\\n  --user=$USER \\\n  --group=$GROUP \\\n  --bind=$BIND \\\n  --log-level=$LOG_LEVEL \\\n  --log-file=-\nHere‚Äôs what you‚Äôre defining:\n\nLine 1 indicates that the script is to be run by the bash shell.\nLines 3 to 11 specify the configuration options that you‚Äôll pass to Gunicorn. Most parameters are self-explanatory, except for WORKERS, WORKER_CLASS, and BIND:\n\nWORKERS: defines the number of workers that Gunicorn will use, it‚Äôs usually recommended to use the number of CPU cores + 1.\nWORKER_CLASS: type of worker used. In this case, you specify Uvicorn workers, which allows you to use it as an ASGI server.\nBIND: Specifies the server socket that Gunicorn binds to.\n\nLines 13 and 14 change the location to the project directory and activate the virtual environment.\nLines 16 to 24 run Gunicorn with the specified parameters.\n\nSave and close the fine. Then, make it executable by running the following:\nchmod u+x gunicorn_start\nFinally, make a run folder in your project directory for the Unix socket file you defined in the BIND parameter:\nmkdir run\nWhen you have multiple servers communicating on the same machine, using a Unix socket file is better.\n\n\nConfigure Supervisor\nFirst, create a directory called logs in the project directory to store your application‚Äôs error logs:\nmkdir logs\nNext, create a Supervisor‚Äôsconfiguration file by running the following command:\nsudo vim /etc/supervisor/conf.d/fastapi-app.conf\nThere copy and paste the following:\n[program:fastapi-app]\ncommand=/home/fastapi-user/fastapi-nginx-gunicorn/gunicorn_start\nuser=fastapi-user\nautostart=true\nautorestart=true\nredirect_stderr=true\nstdout_logfile=/home/fastapi-user/fastapi-nginx-gunicorn/logs/gunicorn-error.log\nThis configuration file runs the file you created earlier, gunicorn_start, using the fastapi-user. Supervisor will start the application anytime the server starts, and will also restart it if it fails.\nThis configuration file executes the gunicorn_start file you created earlier using fastapi-user as the user. Supervisor will launch the application whenever the server boots up and will restart it if the application fails. The errors are logged into gunicorn-error.log in logs in the project directory.\nReread Supervisor‚Äôs configuration file and restart the service by running these commands:\nsudo supervisorctl reread\nsudo supervisorctl update\nFinally, you can check the status of the program by running this command:\nsudo supervisorctl status fastapi-app\nIf everything went well, the fastapi-app service status should be set to RUNNING.\nYou can also test it by opening a new terminal window, connecting to the server, and issuing a GET request using curl:\ncurl --unix-socket /home/fastapi-user/fastapi-nginx-gunicorn/run/gunicorn.sock localhost\nYou should see the following output:\n{ \"message\": \"It's working!\" }\nFinally, if you make changes to the code, you can restart the service to apply to changes by running this command:\nsudo supervisorctl restart fastapi-app\nWay to go! You‚Äôve got an ASGI server running using Gunicorn and Uvicorn. Next, you‚Äôll set up a reverse proxy server using NGINX."
  },
  {
    "objectID": "posts/fastapi-nginx-gunicorn.html#step-5-configure-nginx",
    "href": "posts/fastapi-nginx-gunicorn.html#step-5-configure-nginx",
    "title": "How to Securely Deploy a FastAPI app with NGINX and Gunicorn",
    "section": "Step 5: Configure NGINX",
    "text": "Step 5: Configure NGINX\nCreate a new NGINX configuration file for your project:\nsudo vim /etc/nginx/sites-available/fastapi-app\nOpen the NGINX configuration file and paste the following content:\nupstream app_server {\n    server unix:/home/fastapi-user/fastapi-nginx-gunicorn/run/gunicorn.sock fail_timeout=0;\n}\n\nserver {\n    listen 80;\n\n    # add here the ip address of your server\n    # or a domain pointing to that ip (like example.com or www.example.com)\n    server_name XXXX;\n\n    keepalive_timeout 5;\n    client_max_body_size 4G;\n\n    access_log /home/fastapi-user/fastapi-nginx-gunicorn/logs/nginx-access.log;\n    error_log /home/fastapi-user/fastapi-nginx-gunicorn/logs/nginx-error.log;\n\n    location / {\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header Host $http_host;\n        proxy_redirect off;\n\n        if (!-f $request_filename) {\n            proxy_pass http://app_server;\n            break;\n        }\n    }\n}\nThis is the NGINX configuration file. Here‚Äôs how it works:\n\nLines 1 to 3 define a cluster of servers called app_server that NGINX will proxy requests to. The requests are redirected to the Unix socket file located at /home/fastapi-user/fastapi-nginx-gunicorn/run/gunicorn.sock. Setting fail_timeout=0 tells NGINX not to consider the server as failed even if it does not respond.\nLines 1 to 5 define the configuration for the virtual server that NGINX will use to serve requests. In this case, it listens on port 80. Replace XXXX by the IP or the site‚Äôs name.\nLines 12 and 13 specify keepalive_timeout to set the maximum amount of time that a client can keep a persistent connection open, and client_max_body_size to set a limit to size of the client request body that NGINX will allow.\nLines 15 and 16 specify the locations where NGINX will write its access and error logs.\nLines 18 to 27 defines how NGINX will handle requests to the root directory /. You provide some specifications to handle headers, and set a directive to proxy the requests to the app_server you defined earlier.\n\nEnable the configuration of your site by creating a symbolic link from the file in sites-available into sites-enabled by running this command:\nsudo ln -s /etc/nginx/sites-available/fastapi-app /etc/nginx/sites-enabled/\nTest that the configuration file is OK and restart NGINX:\nsudo nginx -t\nsudo systemctl restart nginx\nIf everything went well, now you should be able to make a GET request successfully to the IP of your server from your browser or using curl. Once again, you should see the following output:\n{ \"message\": \"It's working!\" }\nYou should have your FastAPI app running by now, as well as Gunicorn+Uvicorn as an ASGI server and NGINX in front of them as a reverse proxy.\n\nPermissions Error\nIf you get a permission error telling you that NGINX cannot access the unix socket, you can add the www-data user (which typically is the user running the NGINX processes) to the fastapi-user group. You can use the following command:\nsudo usermod -aG fastapi-user www-data\nGood job! If you haven‚Äôt bought a domain name for your API, you can stop reading here. If you have one, proceed to the next step to obtain an SSL certificate and enable HTTPS."
  },
  {
    "objectID": "posts/fastapi-nginx-gunicorn.html#optional-step-6-obtain-a-free-ssl-certificate-using-certbot",
    "href": "posts/fastapi-nginx-gunicorn.html#optional-step-6-obtain-a-free-ssl-certificate-using-certbot",
    "title": "How to Securely Deploy a FastAPI app with NGINX and Gunicorn",
    "section": "(Optional) Step 6: Obtain a Free SSL Certificate Using Certbot",
    "text": "(Optional) Step 6: Obtain a Free SSL Certificate Using Certbot\nThis only applies if you have a domain for which you want to obtain an SSL certificate.\nIf you‚Äôre using Ubuntu, you can skip this step. Otherwise, you first need to install snapd:\nsudo apt install snapd\nNext, make sure you have the latest version available:\nsudo snap install core; sudo snap refresh core\nInstall certbot and make sure the cerbot command is executable:\nsudo snap install --classic certbot\nsudo ln -s /snap/bin/certbot /usr/bin/certbot\nNext, generate a certificate for your domain interactively by running the following command:\nsudo certbot --nginx\nFinally, Certbot will automatically handle the renewal of your certificate. To test that it works run the following:\nsudo certbot renew --dry-run\nIf it worked as expected, you should see a Congratulations, all simulated renewals succeeded... message.\nIf everything went well, you should be able to make a successful get request using HTTPS."
  },
  {
    "objectID": "posts/fastapi-nginx-gunicorn.html#conclusion",
    "href": "posts/fastapi-nginx-gunicorn.html#conclusion",
    "title": "How to Securely Deploy a FastAPI app with NGINX and Gunicorn",
    "section": "Conclusion",
    "text": "Conclusion\nThat‚Äôs all there is to it! This tutorial showed you how to use NGINX, Gunicorn, and Uvicorn to deploy a FastAPI application. FastAPI is one of the most popular Python web frameworks. It‚Äôs become the go-to option for deploying machine learning-powered web apps, so becoming acquainted with it is a wise career move.\nIn this article you‚Äôve learned:\n\nWhy and when should you use FastAPI, NGINX, Gunicorn, and Uvicorn.\nHow to set up Gunicorn+Uvicorn as an ASGI server.\nHow to use Supervisor to run Gunicorn.\nHow to configure NGINX and generate a free SSL certificate using certbot.\n\nIf you have any questions or feedback, let me know in the comments!\nAll the code for this tutorial is available on GitHub."
  },
  {
    "objectID": "posts/nlp-snippets-clean-and-tokenize-text-with-python.html",
    "href": "posts/nlp-snippets-clean-and-tokenize-text-with-python.html",
    "title": "Clean and Tokenize Text With Python",
    "section": "",
    "text": "Every time I start a new project, I promise to save the most useful code snippets for the future, but I never do.\nThe old ways are too compelling. I end up copying code from old projects, looking for the same questions in Stack Overflow, or reviewing the same Kaggle notebooks for the hundredth time. At this point, I don‚Äôt know how many times I‚Äôve googled for a variant of ‚Äúremove extra spaces in a string using Python.‚Äù\nSo, finally, I‚Äôve decided to compile snippets and small recipes for frequent tasks. I‚Äôm starting with Natural Language Processing (NLP) because I‚Äôve been involved in several projects in that area in the last few years.\nFor now, I‚Äôm planning on compiling code snippets and recipes for the following tasks:\nThis article contains 20 code snippets you can use to clean and tokenize text using Python. I‚Äôll continue adding new ones whenever I find something useful. They‚Äôre based on a mix of ¬†Stack Overflow answers, books, and my experience.\nIn the next section, you can see an example of how to use the code snippets. Then, you can check the snippets on your own and take the ones you need."
  },
  {
    "objectID": "posts/nlp-snippets-clean-and-tokenize-text-with-python.html#how-to-use",
    "href": "posts/nlp-snippets-clean-and-tokenize-text-with-python.html#how-to-use",
    "title": "Clean and Tokenize Text With Python",
    "section": "How to use",
    "text": "How to use\nI‚Äôd recommend you combine the snippets you need into a function. Then, you can use that function for pre-processing or tokenizing text. If you‚Äôre using pandas, you can apply that function to a specific column using the .map method of pandas‚Äô Series.\nTake a look at the example below:\nimport re\nimport pandas as pd\n\nfrom string import punctuation\n\ndf = pd.DataFrame({\n    \"text_col\": [\n        \"This TEXT needs \\t\\t\\tsome cleaning!!!...\",\n        \"This text too!!...       \",\n        \"Yes, you got it right!\\n This one too\\n\"\n    ]\n})\n\ndef preprocess_text(text):\n    text = text.lower()  # Lowercase text\n    text = re.sub(f\"[{re.escape(punctuation)}]\", \"\", text)  # Remove punctuation\n    text = \" \".join(text.split())  # Remove extra spaces, tabs, and new lines\n    return text\n\ndf[\"text_col\"].map(preprocess_text)"
  },
  {
    "objectID": "posts/nlp-snippets-clean-and-tokenize-text-with-python.html#code-snippets",
    "href": "posts/nlp-snippets-clean-and-tokenize-text-with-python.html#code-snippets",
    "title": "Clean and Tokenize Text With Python",
    "section": "Code snippets",
    "text": "Code snippets\nBefore testing the snippets, copy the following function at the top of your Python script or Jupyter notebook.\ndef print_text(sample, clean):\n    print(f\"Before: {sample}\")\n    print(f\"After: {clean}\")\n\nCleaning text\nThese are functions you can use to clean text using Python. Most of them just use Python‚Äôs standard libraries like re or string.\n\nLowercase text\nIt‚Äôs fairly common to lowercase text for NLP tasks. Luckily, Python strings include a .lower() method that makes that easy for you. Here‚Äôs how you use it:\nsample_text = \"THIS TEXT WILL BE LOWERCASED. THIS WON'T: √ü√ü√ü\"\nclean_text = sample_text.lower()\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: THIS TEXT WILL BE LOWERCASED. THIS WON'T: √ü√ü√ü\n# After: this text will be lowercased. this won't: √ü√ü√ü\n\n\nRemove cases (useful for caseless matching)\nCase folding is a common approach for matching strings (especially in languages other than English). Python strings provide you with .casefold() for that. Here‚Äôs how to use it:\nsample_text = \"THIS TEXT WILL BE LOWERCASED. THIS too: √ü√ü√ü\"\nclean_text = sample_text.casefold()\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: THIS TEXT WILL BE LOWERCASED. THIS too: √ü√ü√ü\n# After: this text will be lowercased. this too: ssssss\n\n\nRemove hyperlinks\nIf you‚Äôre web scrapping, you‚Äôll often deal with hyperlinks. It‚Äôs possible that you‚Äôd like to remove those to analyze the text. The easiest way to do that is by using regular expressions.\nPython‚Äôs re library is handy for those cases. This is how you‚Äôd remove hyperlinks using it:\nimport re\n\nsample_text = \"Some URLs: https://example.com http://example.io http://exam-ple.com More text\"\nclean_text = re.sub(r\"https?://\\S+\", \"\", sample_text)\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: Some URLs: https://example.com http://example.io http://exam-ple.com More text\n# After: Some URLs:    More text\n\n\nRemove &lt;a&gt; tags but keep their content\nSimilar to the previous case, if you‚Äôre doing web scrapping, you might often find dealing with tags. In some cases, such as &lt;a&gt;, you may want to remove the tag and its attributes but not its contents (e.g., the text it contains).\nYou can use Python‚Äôs re for that. Here‚Äôs how you‚Äôd remove the &lt;a&gt; tag and its attributes while keeping its content:\nimport re\n\nsample_text = \"Here's &lt;a href='https://example.com'&gt; a tag&lt;/a&gt;\"\nclean_text = re.sub(r\"&lt;a[^&gt;]*&gt;(.*?)&lt;/a&gt;\", r\"\\1\", sample_text)\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: Here's &lt;a href='https://example.com'&gt; a tag&lt;/a&gt;\n# After: Here's  a tag\nYou can also use this snippet as a starting point to remove other types of tags.\n\n\nRemove all HTML tags but keep their contents\nIf you‚Äôre dealing with web pages and want to remove all the tags in a document, you can use a generalized version of the previous snippet. Here‚Äôs how you remove all the HTML tags using Python‚Äôs re library:\nimport re\n\nsample_text = \"\"\"\n&lt;body&gt;\n&lt;div&gt; This is a sample text with &lt;b&gt;lots of tags&lt;/b&gt; &lt;/div&gt;\n&lt;br/&gt;\n&lt;/body&gt;\n\"\"\"\nclean_text = re.sub(r\"&lt;.*?&gt;\", \" \", sample_text)\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before:\n# &lt;body&gt;\n# &lt;div&gt; This is a sample text with &lt;b&gt;lots of tags&lt;/b&gt; &lt;/div&gt;\n# &lt;br/&gt;\n# &lt;/body&gt;\n\n# After:\n\n#  This is a sample text with lots of tags\n\n\nRemove extra spaces, tabs, and line breaks\nYou might think that the best approach to remove extra spaces, tabs, and line breaks would depend on regular expressions. But it doesn‚Äôt.\nThe best approach consists of using a clever combination two string methods: .split() and .join(). First, you apply the .split() method to the string you want to clean. It will split the string by any whitespace and output a list. Then, you apply the .join() method on a string with a single whitespace (‚Äù ‚Äú), using as input the list you generated. This will put back together the string you split but use a single whitespace as separator.\nYes, I know it sounds a bit confusing. But, in reality, it‚Äôs fairly simple. Here‚Äôs how it looks in code:\nsample_text = \"     \\t\\tA      text\\t\\t\\t\\n\\n sample       \"\nclean_text = \" \".join(sample_text.split())\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before:           A      text\n\n#  sample\n# After: A text sample\n\n\nRemove punctuation\nMany NLP applications won‚Äôt work very well if you include punctuation. So it‚Äôs common to remove them. The easiest approach consists in using the string and re standard libraries are as follows:\nimport re\nfrom string import punctuation\n\nsample_text = \"A lot of !!!! .... ,,,, ;;;;;;;?????\"\nclean_text = re.sub(f\"[{re.escape(punctuation)}]\", \"\", sample_text)\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: A lot of !!!! .... ,,,, ;;;;;;;?????\n# After: A lot of\n\n\nRemove numbers\nIn some cases, you might want to remove numbers from text, when you don‚Äôt feel they‚Äôre very informative. You can use a regular expression for that:\nimport re\n\nsample_text = \"Remove these numbers: 1919191 2229292 11.233 22/22/22. But don't remove this one H2O\"\nclean_text = re.sub(r\"\\b[0-9]+\\b\\s*\", \"\", sample_text)\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: Remove these numbers: 1919191 2229292 11.233 22/22/22. But don't remove this one H2O\n# After: Remove these numbers: .//. But don't remove this one H2O\n\n\nRemove digits\nThere are cases where you might want to remove digits instead of any number. For instance, when you want to remove numbers but not dates. Using a regular expression gets a bit trickier.\nIn those cases, you can use the .isdigit() method of strings:\nsample_text = \"I want to keep this one: 10/10/20 but not this one 222333\"\nclean_text = \" \".join([w for w in sample_text.split() if not w.isdigit()]) # Side effect: removes extra spaces\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: I want to keep this one: 10/10/20 but not this one 222333\n# After: I want to keep this one: 10/10/20 but not this one\n\n\nRemove non-alphabetic characters\nSometimes, you‚Äôd like to remove non-alphabetic characters like numbers or punctuation. The .isalpha() method of Python strings will come in handy in those cases.\nHere‚Äôs how you use it:\nsample_text = \"Sample text with numbers 123455 and words !!!\"\nclean_text = \" \".join([w for w in sample_text.split() if w.isalpha()]) # Side effect: removes extra spaces\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: Sample text with numbers 123455 and words !!!\n# After: Sample text with numbers and words\n\n\nRemove all special characters and punctuation\nIn cases where you want to remove all characters except letters and numbers, you can use a regular expression.\nHere‚Äôs a quick way to do it:\nimport re\n\nsample_text = \"Sample text 123 !!!! Haha.... !!!! ##$$$%%%%\"\nclean_text = re.sub(r\"[^A-Za-z0-9\\s]+\", \"\", sample_text)\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: Sample text 123 !!!! Haha.... !!!! ##$$$%%%%\n# After: Sample text 123  Haha\n\n\nRemove stopwords from a list\nThere‚Äôs the case where you‚Äôd like to exclude words using a predefined list. A quick way to do it is by using list comprehensions. Here‚Äôs one way to do it:\nstopwords = [\"is\", \"a\"]\nsample_text = \"this is a sample text\"\ntokens = sample_text.split()\nclean_tokens = [t for t in tokens if not t in stopwords]\nclean_text = \" \".join(clean_tokens)\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: this is a sample text\n# After: this sample text\n\n\nRemove short tokens\nIn some cases, you may want to remove tokens with few characters. In this case, using list comprehensions will make it easy:\nsample_text = \"this is a sample text. I'll remove the a\"\ntokens = sample_text.split()\nclean_tokens = [t for t in tokens if len(t) &gt; 1]\nclean_text = \" \".join(clean_tokens)\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: this is a sample text. I'll remove the a\n# After: this is sample text. I'll remove the\n\n\nTransform emojis into characters\nIf you‚Äôre processing social media data, there might be cases where you‚Äôd like to extract the meaning of emojis instead of simply removing them. An easy way to do that is by using the emoji library.\nHere‚Äôs how you do it:\nfrom emoji import demojize\n\nsample_text = \"I love ü•ë\"\nclean_text = demojize(sample_text)\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: I love ü•ë\n# After: I love :avocado:\n\n\nRemove repeated characters\nIn some cases, you may want to remove repeated characters, so instead of ‚Äúhelloooo‚Äù you use ‚Äúhello‚Äù. Here‚Äôs how you can do it (I got this code snippet from The Kaggle Book):\nimport re\n\nsample_text = \"hellooooo\"\nclean_text = re.sub(r'(.)\\1{3,}',r'\\1', sample_text)\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: hellooooo\n# After: hello\n\n\n\nNLTK\nBefore using NLTK‚Äôs snippets, you need to install NLTK. You can do that as follows: pip install nltk.\n\nTokenize text using NLTK\nfrom nltk.tokenize import word_tokenize\n\nsample_text = \"this is a text ready to tokenize\"\ntokens = word_tokenize(sample_text)\nprint_text(sample_text, tokens)\n\n# ----- Expected output -----\n# Before: this is a text ready to tokenize\n# After: ['this', 'is', 'a', 'text', 'ready', 'to', 'tokenize']\n\n\nTokenize tweets using NLTK\nfrom nltk.tokenize import TweetTokenizer\n\ntweet_tokenizer = TweetTokenizer()\nsample_text = \"This is a tweet @jack #NLP\"\ntokens = tweet_tokenizer.tokenize(sample_text)\nprint_text(sample_text, tokens)\n\n# ----- Expected output -----\n# Before: This is a tweet @jack #NLP\n# After: ['This', 'is', 'a', 'tweet', '@jack', '#NLP']\n\n\nSplit text into sentences using NLTK\nfrom nltk.tokenize import sent_tokenize\n\nsample_text = \"This is a sentence. This is another one!\\nAnd this is the last one.\"\nsentences = sent_tokenize(sample_text)\nprint_text(sample_text, sentences)\n\n# ----- Expected output -----\n# Before: This is a sentence. This is another one!\n# And this is the last one.\n# After: ['This is a sentence.', 'This is another one!', 'And this is the last one.']\n\n\n\nRemove stopwords using NLTK\nimport nltk\n\nfrom nltk.corpus import stopwords\n\nnltk.download(\"stopwords\")\n\nstopwords_ = set(stopwords.words(\"english\"))\n\nsample_text = \"this is a sample text\"\ntokens = sample_text.split()\nclean_tokens = [t for t in tokens if not t in stopwords_]\nclean_text = \" \".join(clean_tokens)\nprint_text(sample_text, clean_text)\n\n# ----- Expected output -----\n# Before: this is a sample text\n# After: sample text\n\n\nspaCy\nBefore using spaCy‚Äôs snippets, you need to install the library as follows: pip install spacy. You also need to download a language model. For English, here‚Äôs how you do it: python -m spacy download en_core_web_sm.\n\nTokenize text using spaCy\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nsample_text = \"this is a text ready to tokenize\"\ndoc = nlp(sample_text)\ntokens = [token.text for token in doc]\nprint_text(sample_text, tokens)\n\n# ----- Expected output -----\n# Before: this is a text ready to tokenize\n# After: ['this', 'is', 'a', 'text', 'ready', 'to', 'tokenize']\n\n\nSplit text into sentences using spaCy\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nsample_text = \"This is a sentence. This is another one!\\nAnd this is the last one.\"\ndoc = nlp(sample_text)\nsentences = [sentence.text for sentence in doc.sents]\nprint_text(sample_text, sentences)\n\n# ----- Expected output -----\n# Before: This is a sentence. This is another one!\n# And this is the last one.\n# After: ['This is a sentence.', 'This is another one!\\n', 'And this is the last one.']\n\n\n\nKeras\nBefore using Keras‚Äô snippets, you need to install the library as follows: pip install tensorflow && pip install keras.\n\nTokenize text using Keras\nfrom keras.preprocessing.text import text_to_word_sequence\n\nsample_text = 'This is a text you want to tokenize using KERAS!!'\ntokens = text_to_word_sequence(sample_text)\nprint_text(sample_text, tokens)\n\n# ----- Expected output -----\n# Before: This is a text you want to tokenize using KERAS!!\n# After: ['this', 'is', 'a', 'text', 'you', 'want', 'to', 'tokenize', 'using', 'keras']"
  },
  {
    "objectID": "posts/say-what-you-mean-sometimes.html",
    "href": "posts/say-what-you-mean-sometimes.html",
    "title": "Structured outputs can hurt the performance of LLMs",
    "section": "",
    "text": "When I read Let Me Speak Freely? by Tam et al.¬†I thought they raised an interesting question: does constraining LLM outputs to structured formats impact the quality of their responses?\nIn both the original study and their recent update, Tam et al.¬†concluded that is the case. They found that ‚Äústructured generation constraints significantly impact LLM performance across various tasks‚Äù.\nBut the study had major flaws. The .txt team wrote a very compelling rebuttal to the paper. For Llama-3-8B-Instruct, they demonstrate that Tam, et al.¬†results were mostly due to poor prompting, unfair comparisons and the improper use of an ‚ÄúAI parser‚Äù rather than the use of structured outputs.\nI liked the rebuttal but it still left me wondering how well their results generalize. They focused on a single model1, which represents a small fraction of the LLMs powering applications in production today. Open-weight models offer more flexibility on how to structure your output, such as letting users specify regex expressions to constrain the output. Proprietary models lack this. Right now, JSON is the only structured output format guaranteed to work across most popular providers.\nGiven this constraint, would the .txt team‚Äôs results still hold?\nPlus, both the original study and the rebuttal focused on tasks that might not be a good proxy for the full range of tasks people use LLMs for. Would the rebuttal results be different in settings outside of simple reasoning tasks?\nSo I decided to:\nThis article presents the results of the first two steps. All the code is available on Github."
  },
  {
    "objectID": "posts/say-what-you-mean-sometimes.html#results",
    "href": "posts/say-what-you-mean-sometimes.html#results",
    "title": "Structured outputs can hurt the performance of LLMs",
    "section": "Results",
    "text": "Results\nIf you‚Äôre short on time, here are the key findings:\n\nTam et al.‚Äôs conclusions about structured outputs might still hold, even if they did not properly test for it. There are tasks where structured outputs perform worse than unstructured ones.\n.txt‚Äôs rebuttal is accurate. It shows that structured outputs are as good or better than unstructured outputs for LLaMA3-8B-Instruct in the tasks considered. But a similar approach did not hold for GPT-4o-mini (and possibly other models).\n\nThe figure below shows the results for GPT-4o-mini using .txt‚Äôs prompt fixes, along with additional improvements I made.\n\n\n\n\n                                                \n\n\nFigure¬†1: Overall results for GPT-4o-mini.\n\n\n\n\nFor GSM8K and Last Letter, structured and unstructured methods scored similarly. But for Shuffled Objects, unstructured outputs clearly surpassed a structured format.\nThe rest of the article will explain the approach I took to get these results."
  },
  {
    "objectID": "posts/say-what-you-mean-sometimes.html#study-design",
    "href": "posts/say-what-you-mean-sometimes.html#study-design",
    "title": "Structured outputs can hurt the performance of LLMs",
    "section": "Study design",
    "text": "Study design\nTam et al.¬†evaluated structured and unstructured outputs across three reasoning tasks and six classification tasks. They used exact match to evaluate the reasoning tasks and accuracy to evaluate the classification tasks. They ran the experiments using the following models:\n\nProprietary models: gpt-3.5-turbo-0125, claude-3-haiku-20240307, gemini-1.5-flash, and gpt-4o-mini-2024-07-18.\nOpen-weight models: LLaMA3-8B-Instruct, and Gemma-2-9B-Instruct.\n\n.txt used a similar setup, but only focused on the reasoning tasks using LLaMA3-8B-Instruct. They did not include classification tasks because Tam et al.¬†observed that structured outputs resulted in better performance in these tasks, so there was no need to test for it.\nI also believe that structured outputs are better for classification tasks. So, I excluded them from my analysis as well.\nThe reasoning tasks were:\n\nGSM8K: A dataset from of grade school math word problems.\nLast Letter: A dataset of simple word puzzles that require concatening the last letters of a list of names.\nShuffled Objects: A dataset that requires reasoning about the state of a system after a sequence of shuffling operations.\n\nThe rest of the article details the process of replicating .txt‚Äôs rebuttal on these tasks and evaluating the same tasks using GPT-4o-mini."
  },
  {
    "objectID": "posts/say-what-you-mean-sometimes.html#replicating-.txts-rebuttal",
    "href": "posts/say-what-you-mean-sometimes.html#replicating-.txts-rebuttal",
    "title": "Structured outputs can hurt the performance of LLMs",
    "section": "Replicating .txt‚Äôs rebuttal",
    "text": "Replicating .txt‚Äôs rebuttal\n.txt made it very easy to reproduce their results by sharing their code on Github. I just set up a machine at Modal and ran the code.\nWhile going through the code, I noticed some small issues with the prompts. So I decided to tweak them a bit.\nBelow are .txt‚Äôs original results compared to mine, after the prompt adjustments:\n\n\n\n\n\n\n\n\n\n\nTask\n.txt\nme, 3-shot\n\n\n\nUnstructured\nStructured\nUnstructured\nStructured\n\n\n\n\nGSM8K\n77.18\n77.79\n79.98\n79.45\n\n\nLast Letter\n73.33\n77.33\n74.00\n78.00\n\n\nShuffled Objects\n40.72\n44.35\n42.68\n43.90\n\n\n\n\n\n\nTable¬†1: Results for LLaMA3-8B-Instruct.\n\n\n\n\nExcept for Structured in the Shuffled Objects task, I was able to improve all the metrics. In GSM8K‚Äôs case, even reversing .txt‚Äôs result, with Unstructured outperforming Structured by a small margin.\nBut I don‚Äôt think this matters much.\nTheir conclusion still holds: structured outputs are either as good as or better than unstructured outputs, in the tasks considered.\nI‚Äôll explain the prompt changes I made below, so that you can judge for yourself if they make sense.\n\nFormatting few-shot examples\nIn the GSM8K and Last Letter tasks, the few-shot prompt for both unstructured and structured used examples formatted as JSON objects and asked the LLM to produce the output in the same format, from which the answer was extracted.\nThat felt unfair. Even though you‚Äôre not formally constraining the LLM to produce a JSON object, you‚Äôre still asking it to format its response in somewhat unnatural way.\nI adjusted the prompts to be as similar as possible for both unstructured and structured outputs while still trying to get the most out of each approach.\nFor example, in GSM8K, the unstructured prompt is:\n\nYou are an expert in solving grade school math tasks. You will be presented with a grade-school math word problem and be asked to solve it. You will always respond in the following format:\n&lt;str, reasoning about the answer&gt;\nANSWER: &lt;int, final answer&gt;\nFirst, provide your step by step reasoning. Then, in ANSWER, provide an integer that corresponds to the correct answer to the question. Don‚Äôt include any other text in ANSWER.\n\nAnd the structured prompt is:\n\nYou are an expert in solving grade school math tasks. You will be presented with a grade-school math word problem and be asked to solve it. You will always respond in the following format:\n{‚Äúreasoning‚Äù: &lt;str, reasoning about the answer&gt;, ‚Äúanswer‚Äù: &lt;int, final answer&gt;}\nFirst, provide your step by step reasoning in the ‚Äúreasoning‚Äù field. Then, in the ‚Äúanswer‚Äù field, provide an integer that corresponds to the correct answer to the question. Don‚Äôt include any other text in the ‚Äúanswer‚Äù field.\n\nFinally, for all the tasks, I used a 3-shot prompt.\n\n\nClarifying the task\nI also tried to make the prompts clearer. The description of the task in the original Last Letter prompt was:\n\nYou are an expert in solving simple word puzzles using reasoning steps. Your specific task is going to be to take a list of 4 names and reason about the last letter of each ., then you will concatenate those letters into a word.\n\nI changed it to:\n\nYou are an expert in solving word puzzles. Your specific task is going to be to take a list of 4 names, get the last letter of each and concatenate these letters into a word.\n\nThe original prompt was reasonable, but I thought the new version was clearer. Through trial and error, I‚Äôve learned that when working with LLMs, it‚Äôs best to be as clear and direct as possible."
  },
  {
    "objectID": "posts/say-what-you-mean-sometimes.html#evaluating-gpt-4o-mini",
    "href": "posts/say-what-you-mean-sometimes.html#evaluating-gpt-4o-mini",
    "title": "Structured outputs can hurt the performance of LLMs",
    "section": "Evaluating GPT-4o-mini",
    "text": "Evaluating GPT-4o-mini\nUsing the same setup as before, I ran the same tasks with gpt-4o-mini-2024-07-18.\nIn the table below, you can see the results, including the original results from Tam et al.¬†for comparison:\n\n\n\n\n\n\n\n\n\n\n¬†\n¬†\nNL\nFRI\nJSON-Mode\nJSON-Schema\n\n\nTask\nMethod\n¬†\n¬†\n¬†\n¬†\n\n\n\n\nGSM8K\nTam et al.\n94.57\n87.17\n86.95\n91.71\n\n\nMe, 0-shot\n94.31\n92.12\n93.33\n93.48\n\n\nMe, 3-shot\n93.86\n92.72\n93.25\n92.95\n\n\nLast Letter\nTam et al.\n83.11\n84.73\n76.00\n86.07\n\n\nMe, 0-shot\n87.33\n88.00\n90.00\n87.33\n\n\nMe, 3-shot\n92.00\n94.67\n90.00\n93.33\n\n\nShuffled Obj.\nTam et al.\n82.85\n81.46\n76.43\n81.77\n\n\nMe, 0-shot\n95.12\n79.67\n81.71\n89.84\n\n\nMe, 3-shot\n92.68\n69.51\n62.60\n65.85\n\n\n\n\n\n\nTable¬†2: Results for GPT-4o-mini.\n\n\n\n\nNL stands for ‚ÄúNatural Language‚Äù, which would correspond to the Unstructured method in the previous table.\nFRI stands for ‚ÄúFormat Restricting Instructions‚Äù, which is a JSON generated through the OpenAI‚Äôs function calling. JSON-Mode is a JSON generated through the OpenAI‚Äôs JSON mode. JSON-Schema is a JSON generated using constrained decoding.\nJSON-Schema is the closest equivalent to Structured as referenced in the previous table. But, in real-life applications, you don‚Äôt really care about how the output was generated. You just want to get the output in the format you want. So, for the sake of comparison, I will consider the three other methods equivalent to Structured as well.\n\nAdjusting for proprietary models\nIn this case, I allowed for 3 retries in the case of parsing errors. I allowed for this because function calling had high error rates in the zero-shot prompting scenario.\nThese retries primarily affected FRI results. This might make the comparisons in Last Letter biased in favor of structured outputs (FRI was the best method in this case). But since JSON-Schema also outperformed NL in this case, this adjustment does not alter the overall conclusions. The other methods maintained error rates of &lt;0.5% in GSM8K and 0% in Last Letter and Shuffled Objects.\nI used slightly different parsing functions for Unstructured and Structured outputs. The Unstructured parser was more lenient, removing commas and periods at the end of responses. But I believe this remains a fair comparison given that in the Structured cases you provide a JSON schema which is more informative.\n\n\nAnalyzing the results\nSimilar to what the .txt team found, after adjusting the prompts, the performance of structured outputs increases substantially compared to Tam et al.\nExcept for NL in GSM8k and FRI in Last Letter, I was able to improve all the metrics for both unstructured and structured outputs using a 0-shot prompt. For 3-shot prompts, I improved GSM8k and Last Letter across all methods, and NL in Shuffled Objects.\nFor GSM8k and Last Letter, the results were very similar between unstructured and structured outputs. There was a slight edge for unstructured outputs in GSM8k and for structured outputs in Last Letter. In these cases, it‚Äôs not clear that one approach definitively outperforms the other.\nOn the other hand, Shuffled Objects shows a clear advantage for unstructured outputs over structured outputs. This was unexpected, and even after tweaking the prompts, I couldn‚Äôt fully close the gap.\nDespite the issues in Tam et al.‚Äôs study, their conclusion appears to hold. In this particular scenario, using a fairly popular model with reasonable prompts, there is a significant difference in performance between structured and unstructured outputs.\n\n\n\n\n\n\nNote\n\n\n\nIn GSM8k and Last Letter, few-shot prompting generally decreased performance. This is in line with other analyses."
  },
  {
    "objectID": "posts/say-what-you-mean-sometimes.html#conclusion",
    "href": "posts/say-what-you-mean-sometimes.html#conclusion",
    "title": "Structured outputs can hurt the performance of LLMs",
    "section": "Conclusion",
    "text": "Conclusion\nYou‚Äôre here because you want to know whether to use structured or unstructured outputs. As a developer, I‚Äôm glad to say the answer is: it depends.\nI love using structured outputs in my daily work, because it makes it much easier to work with the output of LLMs. I always encourage clients who aren‚Äôt using them yet to give them a try.\nThat said, until there‚Äôs stronger evidence showing that both approaches are equivalent, the best course of action is to test things for yourself. Run your own evals and make a decision based on data.\nI expect that in most cases, structured outputs will have similar performance to unstructured outputs. But, if you blindly assume that structured outputs are always equal to or better than unstructured ones, you might be missing out on easy performance gains.\nTake the example of Shuffled Objects with GPT-4o-mini. You could potentially reduce the gap between the two methods by continuing improving the prompts or by switching to a more powerful model. But the costs, in terms of time and effort, might be more than simply switching to unstructured outputs.\nAnd this cuts both ways. Unstructured outputs aren‚Äôt inherently better or worse than structured ones. Again, the right choice depends on your task, the model, and your prompt engineering skills. Test both approaches, identify if there are differences, and choose what works best."
  },
  {
    "objectID": "posts/say-what-you-mean-sometimes.html#footnotes",
    "href": "posts/say-what-you-mean-sometimes.html#footnotes",
    "title": "Structured outputs can hurt the performance of LLMs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlthough, they‚Äôve also shared results of other open-weight models using a different setup.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/prompt-engineering-101.html",
    "href": "posts/prompt-engineering-101.html",
    "title": "Prompt engineering 101",
    "section": "",
    "text": "I‚Äôve tried every trick in the book to get Large Language Models (LLMs) to do what I want them to do.\nI‚Äôve resorted to threats of physical violence. I‚Äôve offered bribes. I‚Äôve even made Cursor agents call me big daddy to ensure they follow my repo‚Äôs rules1.\nAll this trial and error has taught me a trick or two about writing prompts. This is a key part of using LLMs, but it‚Äôs also one of the most hyped and abused techniques. There are so many AI influencers selling and sharing their ‚Äúultimate prompt‚Äù that it often feels closer to astrology than to engineering.\nThis article is a no-BS guide to help you get the basics right. It won‚Äôt solve all the problems in your agentic workflow or LLM-based applications, but will avoid you making obvious mistakes.\nLet‚Äôs get started!"
  },
  {
    "objectID": "posts/prompt-engineering-101.html#what-is-a-prompt",
    "href": "posts/prompt-engineering-101.html#what-is-a-prompt",
    "title": "Prompt engineering 101",
    "section": "What is a prompt?",
    "text": "What is a prompt?\nPrompts are instructions sent as text to an LLM. Most models work with two types of instructions:\n\nSystem/developer prompt: Sets the ‚Äúbig picture‚Äù or high-level rules for the entire conversation. Examples: ‚ÄúYou are a helpful assistant.‚Äù; ‚ÄúAlways answer in haiku.‚Äù\nUser prompt: The actual question end-user types and any additional context. Examples: ‚ÄúWhat‚Äôs today‚Äôs weather in Dublin?‚Äù; ‚ÄúSummarize the following documents‚Äù\n\nThe system prompt gives the assistant a ‚Äúrole‚Äù, while the user prompt requests specific content within that framework.\nPrompts are usually provided as messages, which are a list of objects with a role and a content:\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}\n]\nThen, these are passed through a chat template and converted into a single string that is sent to the model. For example, this is the resulting message text used by Qwen3, after combining the system and user prompts:\n&lt;|im_start|&gt;system\nYou are a helpful assistant.\n&lt;|im_end|&gt;\n\n&lt;|im_start|&gt;user\nWhat's the weather in Tokyo?\n&lt;|im_end|&gt;\nAfter you make the request, the model will respond with an assistant message that contains the model‚Äôs response2. It looks like this:\n&lt;|im_start|&gt;assistant\nThe weather in Tokyo is sunny.\n&lt;|im_end|&gt;\nYou‚Äôll generally use the system and user prompts to instruct the model. But for some prompting techniques, such as few-shot prompting, people often use assistant messages to simulate model responses."
  },
  {
    "objectID": "posts/prompt-engineering-101.html#components-of-a-good-prompt",
    "href": "posts/prompt-engineering-101.html#components-of-a-good-prompt",
    "title": "Prompt engineering 101",
    "section": "Components of a good prompt",
    "text": "Components of a good prompt\nThere are many useful free resources online you can use to learn more about prompt engineering. I recommend this article by Anthropic or this one by OpenAI.\nMost of the advice boils down to these 6 principles:\n\nBe clear and specific\nProvide examples\nLet models think\nStructure prompts into sections and use clear delimiters\nSplit complex tasks into smaller steps\nRepeat instructions when the context is long\n\nLet‚Äôs go through each of these principles in more detail.\n\nPrinciple 1: Be clear and specific\nThis is the most important principle. If you cannot describe in detail the task you want to perform, the model will not be able to perform it. Whenever you write a prompt, ask yourself: ‚ÄúIf I didn‚Äôt have any background knowledge in this domain, could I complete this task based on the text that I just wrote in this prompt?‚Äù\nIn addition to describing the task, you should also provide a role for the model. For example, if you‚Äôre classifying documents you can use a role like ‚ÄúYou‚Äôre an expert in document classification‚Äù or if you‚Äôre dealing with financial data you can use a role like ‚ÄúYou‚Äôre an expert in financial analysis‚Äù.\nHere‚Äôs an example of a clear and specific prompt:\nYou're an expert in business writing. Please review and improve this email by addressing the following issues:\n\n- Fix any grammatical errors and typos\n- Improve clarity and conciseness\n- Ensure professional tone throughout\n- Strengthen the subject line to be more specific\n- Add a clear call-to-action if missing\n- Format for better readability (bullets, paragraphs, etc.)\n\n&lt;EMAIL CONTENT&gt;\nThis information will let the LLM know which specific task you want it to perform. This should go in the system prompt.\n\n\nPrinciple 2: Provide examples\nOne of the lowest hanging fruit in prompt engineering is to provide examples. It‚Äôs as simple as showing the model a few input and output pairs.\nThis technique is formally known as ‚Äúfew shot prompt‚Äù. It‚Äôs a simple but effective way to improve the quality of the output in many tasks.\nHere‚Äôs an example of a few-shot prompt:\nYou are an expert in solving simple word puzzles using reasoning steps. Provided with a list of 4 names, you will concatenate the last letters into a word.\nExamples:\n\n**Example 1**:\n\nInput: 'Ian Peter Bernard Stephen'\n\nOutput: 'NRDN'\n\n**Example 2**:\n\nInput: 'Javier Dylan Christopher Joseph'\n\nOutput: 'RNRH'\nIn my experience, it‚Äôs better to provide these examples directly in the system prompt because it‚Äôs easier to read and keep everything close together. However, as mentioned above, some people prefer to use assistant messages to provide examples.\n\n\nPrinciple 3: Let models think\nLLMs think in tokens. If you want them to achieve better results, you should let them use tokens to reason about the problem before generating the final answer.\nThis process is formally known as ‚ÄúChain of Thought‚Äù (CoT) prompting. Similar to few shot prompts, it‚Äôs a powerful way to improve the quality of results in many tasks.\nA 0-shot CoT prompt means that you explicitly ask the model to think step by step to solve the problem but don‚Äôt provide any examples of how it should reason about it. A few-shot CoT prompt means that you provide examples of how the model should reason about the problem (e.g., 1-shot means you provide one example, 2-shot means you provide two examples, etc.).\nHere are two examples of CoT prompts:\n\n0-Shot CoT Prompt\n**Question:** A cinema sold 120 tickets at $8 each. What was the total revenue?\n\n**Note:** think about your answer step by step\n\n\n1-Shot CoT Prompt\n&lt;example&gt;\n**Question:** Emily buys 3 notebooks at $4 each and 2 pens at $1.50 each. What's her total cost?\n**Reasoning:**\n\n1. Cost of notebooks = 3 √ó $4 = $12\n2. Cost of pens = 2 √ó $1.50 = $3\n3. Total cost = $12 + $3 = $15\n\n**Answer:** $15\n&lt;/example&gt;\n\n**Question:** A cinema sold 120 tickets at $8 each. What was the total revenue?\nThese days, most providers have options to let models think without explicitly asking them to do so in the prompt. With OpenAI models you can use models from the o-family (e.g., o3, o3-mini, o4-mini). For Anthropic and Gemini, you can configure Claude 3.7/4 or Gemini 2.5 models to use thinking tokens by setting a specific parameter. However, as I‚Äôm writing this, only Gemini gives you access to the full thinking tokens in the response. OpenAI will give you a summarized version of the thinking process and Anthropic will only give you the final answer.\n\n\n\nPrinciple 4: Structure prompts into sections\nIt‚Äôs a common practice to structure system and user prompts into sections. Some people like to use markdown formatting to make the prompt more readable, others use xml tags. You can also use reverse backticks (```) to delimit code blocks or JSON objects. Regardless of the method you use, make sure to do it consistently.\nI really haven‚Äôt checked if there is any hard evidence that proves this really improves performance because it just feels right. It also helps with readability. You will spend a lot of time iterating on prompts, so making them easy to read is already a good investment on its own.\n\nSystem prompt\nFor system prompts, you can use the following structure:\n**Role and objective**\n\nYou‚Äôre an expert document classifier. Your goal is to classify this document‚Ä¶\n\n**Rules**\n\n1. Documents that contain information about medical treatments should be classified as ‚Ä¶\n2. Do not classify documents into multiple categories\n3. ‚Ä¶\n\n**Examples**\n\nInput: [document text]\nClassification: [document category]\n‚Ä¶\n\n**Output**\n\nYou should generate a JSON object with this structure: [JSON schema]\n\n**(Optional) Reiterate objective and elicit thinking**\n\nYour goal is to XYZ‚Ä¶ Before writing your answers, write your reasoning step by step.\nThe headers are for reference only, you can skip them if you want. You also don‚Äôt need to include all of these sections in your prompt.\n\n\nUser prompt\nI‚Äôd recommend to keep user prompts short:\n**Context**\n\nInput: [document text]\n\n**Briefly reiterate objective**\n\nPlease classify this document into a category.\nIn it‚Äôs simplest form, you just provide the context the LLM needs to work with and reiterate the objective.\n\n\n\nPrinciple 5: Split complex tasks into smaller steps\nLLMs often get confused when the context is too long and/or the instructions are complex.\nFor example, you might have a document classifier that precedes a conditional entity extraction. Instead of doing a single LLM call with a prompt that does the document classification and the entity extraction, you can split the task into two steps:\n\nFirst, you classify the document into a category.\nThen, you extract the entities from the document, based on the category.\n\nHere‚Äôs an example of the same task split into two steps.\n\nBig complex prompt\nThis prompt tries (and likely fails) to do two complex tasks at once.\nYou're an expert document classifier. First, classify the document into the following categories:\n\n- Medical\n- Financial\n- Legal\n\nThen, if the document is classified as \"Medical\", extract the following entities:\n...\n\nIf the document is classified as \"Financial\", extract the following entities:\n...\n\nIf the document is classified as \"Legal\", extract the following entities:\n...\n\n\nSmaller, simpler prompts\nThis second approach splits the task into two smaller prompts that do each task separately.\nPrompt 1: Document classification\nYou're an expert document classifier. First, classify the document into the following categories:\n\n- Medical\n- Financial\n- Legal\nPrompt N: Entity extraction (one for each category)\nYou're an expert in entity extraction in the &lt;CATEGORY&gt; domain. Your goal is to extract the entities from the document.\n...\n\n\n\nPrinciple 6: With long contexts, repeat instructions\nLLMs often get confused when the context is too long and the instructions are complex. When context gets above a certain length, it‚Äôs better to repeat the instructions at the bottom of the prompt. Anthropic has reported up to 30% performance improvement when using this technique.\nYou can use the following structure:\nYou're an expert in entity extraction in the &lt;CATEGORY&gt; domain. Your goal is to classify the document into the following categories: \"Medical\", \"Financial\", \"Legal\".\n\n&lt;VERY LONG CONTEXT&gt;\n\nRemember, your goal is to classify the document into the following categories: \"Medical\", \"Financial\", \"Legal\".\nWhen dealing with long contexts, I generally reiterate the objective at the bottom of the system and user prompts."
  },
  {
    "objectID": "posts/prompt-engineering-101.html#other-advanced-prompting-techniques",
    "href": "posts/prompt-engineering-101.html#other-advanced-prompting-techniques",
    "title": "Prompt engineering 101",
    "section": "Other advanced prompting techniques",
    "text": "Other advanced prompting techniques\nAfter you‚Äôve mastered the basics, you‚Äôre 90% of the way there. There are other more advanced techniques that might also be worth trying out:\n\nExemplar Selection KNN (ES-KNN): Instead of having a fixed selection of examples, you can embed the user query and the examples, and then use a KNN algorithm to select the most relevant examples. This has shown to improve the quality of the results in many tasks.\nSelf-consistency (SC): You use the model to generate multiple responses and then select the most consistent one by marginalizing over the noise. This has shown to boost the performance of CoT prompting.\nThread of Thought (ThoT): It‚Äôs a two-tiered prompting system that first asks the LLM to do an analytical dissection of the context, step by step, and summarizing intermediate results. Then, it uses another prompt to distill the analysis into a final answer.\n\nThere are more advanced techniques that I‚Äôm not going to cover here. This paper by E.G. Santana et al is a good starting point."
  },
  {
    "objectID": "posts/prompt-engineering-101.html#conclusion",
    "href": "posts/prompt-engineering-101.html#conclusion",
    "title": "Prompt engineering 101",
    "section": "Conclusion",
    "text": "Conclusion\nThis article is a short guide to help you write better prompts. Good prompt engineering can be summarized in 6 key principles:\n\nBe clear and specific\nProvide examples\nLet models think\nStructure prompts into sections and use clear delimiters\nSplit complex tasks into smaller steps\nRepeat instructions when the context is long\n\nThese principles will not solve all the problems in your agentic workflow or LLM-based applications. But they will help you get started.\nI hope you found this article useful. If you have any questions or feedback, leave a comment below."
  },
  {
    "objectID": "posts/prompt-engineering-101.html#footnotes",
    "href": "posts/prompt-engineering-101.html#footnotes",
    "title": "Prompt engineering 101",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt didn‚Äôt work, but I felt much better about myself.‚Ü©Ô∏é\nI removed the thinking tokens for brevity.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/how-to-use-github-deploy-keys.html",
    "href": "posts/how-to-use-github-deploy-keys.html",
    "title": "How to Use GitHub Deploy Keys",
    "section": "",
    "text": "Deployment is more of an art than science. Ask ten developers, and you‚Äôll end up with ten different ways to deploy your app. However, there are a few topics on which everyone agrees. One of those is using deploy keys.\nA GitHub deploy key is an SSH key that gives read ‚Äìand optionally write‚Äì access to a single repository on GitHub. It makes it easy to pull your app‚Äôs code to a server automatically.\nWith a deploy key, you just connect to your server, do a git fetch, and you‚Äôre done! Forget about connecting with an SFTP and uploading your files manually.\nThis tutorial will show you how to use GitHub deploy keys in your next project. Let‚Äôs get started!"
  },
  {
    "objectID": "posts/how-to-use-github-deploy-keys.html#prerequisites",
    "href": "posts/how-to-use-github-deploy-keys.html#prerequisites",
    "title": "How to Use GitHub Deploy Keys",
    "section": "Prerequisites",
    "text": "Prerequisites\nIf you‚Äôre planning on using deploy keys, I suppose you have something to deploy, right? ü§î\nBefore you continue with the next sections, make sure that:\n\nYou have a repository with your app‚Äôs code on GitHub.\nYou have access to a Linux server with git installed.\n\nIf so, continue to the next section."
  },
  {
    "objectID": "posts/how-to-use-github-deploy-keys.html#create-an-ssh-key-on-your-server",
    "href": "posts/how-to-use-github-deploy-keys.html#create-an-ssh-key-on-your-server",
    "title": "How to Use GitHub Deploy Keys",
    "section": "Create an SSH Key on Your Server",
    "text": "Create an SSH Key on Your Server\nA GitHub deploy key is, ultimately, an SSH key. So you‚Äôll start by creating an SSH key on your server.\nFirst, connect to your server and open a terminal. Once you‚Äôre in, create an SSH key by executing the following command:\nssh-keygen -t ed25519 -C \"USERNAME@EMAIL.com\"\n‚ÄåThis command will create and store an SSH key. You use two parameters:\n\n-t ed25519 to specify which algorithm to use to generate the key. You chose ¬†ED25519, a very secure and efficient algorithm.\n-C USERNAME@EMAIL.com to append your email as a comment at the end of the key. Make sure to replace USERNAME@EMAIL.com with your actual email.\n\nYou can learn more about these parameters here. But don‚Äôt worry, you don‚Äôt need to know much more to use GitHub deploy keys.\nAfter running the command, you‚Äôll get a couple of questions asking you to specify a path and set a passphrase for the key. You can save the key at the default path and skip setting a passphrase. So just hit Enter when prompted."
  },
  {
    "objectID": "posts/how-to-use-github-deploy-keys.html#add-the-key-to-ssh-config",
    "href": "posts/how-to-use-github-deploy-keys.html#add-the-key-to-ssh-config",
    "title": "How to Use GitHub Deploy Keys",
    "section": "Add the Key to SSH Config",
    "text": "Add the Key to SSH Config\nNext, you‚Äôll create an SSH config file. This file allows you to define the instructions required to connect to remote servers using SSH. It‚Äôs an easy way to manage multiple servers‚Äô SSH keys or to keep deploy keys of different repositories in the same server.\nYou can create and edit the SSH config file by running these commands:\ntouch ~/.ssh/config\nchmod 600 ~/.ssh/config\nvim ~/.ssh/config # Or: nano ~/.ssh/config\nThese commands will create an SSH config file, set the correct permissions, and open it using vim. You can also use nano to open the file if you‚Äôre unfamiliar with vim‚Äôs shortcuts.\nAn SSH config file consists of sections specifying the instructions required for each server you‚Äôll connect to. So, in this case, you‚Äôll provide the instructions needed to connect to your repository. For that, copy this text to the file:\nHost github-YOUR-APP\n    HostName github.com\n    AddKeysToAgent yes\n    PreferredAuthentications publickey\n    IdentityFile ~/.ssh/id_ed25519\n‚ÄåIn the code snippet above, you provide the settings required to connect to GitHub using the SSH key you just created. You specify the following parameters:\n\nHost: the name you‚Äôll use in the terminal when referring to this server. Choose a name that‚Äôs easy to remember.\nHostName: the real hostname that you‚Äôll connect to. In this case, github.com.\nAddKeysToAgent: specifies if it should add the private key to the ssh-agent.\nPreferredAuthentications: order in which the client tries authentication methods. In this case, you only use your publickey.\nIdentityFile: specifies a file from which the key is read. You need to specify the name of the private key you generated earlier. If you used the default name, it should be ~/.ssh/id_ed25519.\n\nThat‚Äôs all you need to do on the server. Next, you‚Äôll use your SSH key to create a deploy key on GitHub."
  },
  {
    "objectID": "posts/how-to-use-github-deploy-keys.html#create-a-deploy-key-on-github",
    "href": "posts/how-to-use-github-deploy-keys.html#create-a-deploy-key-on-github",
    "title": "How to Use GitHub Deploy Keys",
    "section": "Create a Deploy Key on GitHub",
    "text": "Create a Deploy Key on GitHub\nFirst, copy the public key from the SSH key you created earlier. The easiest way of doing that is to run:\ncat ~/.ssh/id_ed25519.pub\nThen select and copy the resulting text from the command line. It should look something like this:\nssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAILs35pzG5jZakTEHDWeRErgkAmabhQj2yj/onxlIQgli USERNAME@EMAIL.com\n‚ÄåNext, open your preferred browser and go to the repository with your app‚Äôs code on GitHub. Click on Settings, select Deploy keys, and then click on Add deploy key.\n\nAdd deploy key on GitHub\nCopy the key in the Key textbox and set a title to the key. You can leave Allow write access unchecked and click on Add key. Allow write access allows you to make changes to the repository using the deploy key. For security reasons, you don‚Äôt want to do that in most cases.\n\nNext, you can go back to your server‚Äôs terminal and clone your repository. You can do that by running this command:\ngit clone git@github-YOUR-APP:dylanjcastillo/random.git\nThis will clone your app‚Äôs repository to your server. Please remember to replace github-YOUR-APP by the Host you specified in the SSH config file. Otherwise, it won‚Äôt work.\nThat‚Äôs all!"
  },
  {
    "objectID": "posts/how-to-use-github-deploy-keys.html#conclusion",
    "href": "posts/how-to-use-github-deploy-keys.html#conclusion",
    "title": "How to Use GitHub Deploy Keys",
    "section": "Conclusion",
    "text": "Conclusion\nThis tutorial showed you how to create and use GitHub deploy keys. Using a deploy key will save you time and provide you with a safer way to deploy your app.\nIn this tutorial, you‚Äôve learned:\n\nHow to create an SSH key on your server.\nWhat an SSH config file is and how to use it.\nHow to add a deploy key to your GitHub repository.\n\nIf you have any questions or feedback, let me know in the comments!"
  },
  {
    "objectID": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html",
    "href": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html",
    "title": "Deploying a Django app with Kamal 2, AWS ECR, and Github Actions",
    "section": "",
    "text": "Every other night, my wife wakes me up to tell me I‚Äôm muttering unintelligible phrases in my sleep: ‚Äúrestart nginx,‚Äù ‚Äúthe SSL certificate failed to validate,‚Äù or ‚Äúhow do I exit vim?‚Äù\nI still suffer from PTSD from the days of manually deploying web apps. But since switching to Kamal, I‚Äôve been sleeping like a baby1.\nKamal is sort of a lightweight version of Kubernetes that you can use to deploy containerized apps to a VPS. It has a bit of a learning curve, but once you get the hang of it, it‚Äôll take you less than 5 minutes to get an app in production with a CI/CD pipeline.\nA single push to main, and that green GitHub Actions checkmark confirms that your 2-pixel padding change is live for the world to admire.\nIn this tutorial, I‚Äôll walk you through the process of deploying a Django app with Kamal, AWS ECR, and Github Actions.\nYou can find the code for this tutorial in this repository."
  },
  {
    "objectID": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#prerequisites",
    "href": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#prerequisites",
    "title": "Deploying a Django app with Kamal 2, AWS ECR, and Github Actions",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo make the most of this tutorial, you should:\n\nHave an AWS account and its CLI installed.\nBe comfortable with Docker.\nHave a basic understanding of Kamal. You‚Äôll need to install version 1.9.0 for this tutorial.\nHave a basic understanding of Github Actions.\nHave a VPS with Ubuntu ready to host your app.\n\nIdeally, you should also have a Django project ready to deploy. But if you don‚Äôt have one, you can use this sample Django project for the tutorial."
  },
  {
    "objectID": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#prepare-the-vps-for-kamal",
    "href": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#prepare-the-vps-for-kamal",
    "title": "Deploying a Django app with Kamal 2, AWS ECR, and Github Actions",
    "section": "Prepare the VPS for Kamal",
    "text": "Prepare the VPS for Kamal\nAt a minimum, you‚Äôll need to install docker, curl, git, and snapd on your VPS, and create a non-root user called kamal that can sudo. That user should have a 1000 UID and GID.\nI have a terraform script that will take care of this for you if you‚Äôre using Hetzner.\nBut if you‚Äôd like to do it manually, you can run these commands on your VPS‚Äôs terminal:\n# Install docker, curl, and git, and snapd\napt-get update\napt-get install -y docker.io curl git snapd\n\n# Start and enable the docker service\nsystemctl start docker\nsystemctl enable docker\n\n# Create a non-root user called kamal\nuseradd -m -s /bin/bash -u 1000 kamal\nusermod -aG sudo kamal\necho \"kamal ALL=(ALL) NOPASSWD:ALL\" &gt;&gt; /etc/sudoers.d/kamal\n\n# SSH key to login as kamal user\nmkdir -p /home/kamal/.ssh\necho \"&lt;YOUR_PUBLIC_SSH_KEY&gt;\" &gt;&gt; /home/kamal/.ssh/authorized_keys\nchmod 700 /home/kamal/.ssh\nchmod 600 /home/kamal/.ssh/authorized_keys\nchown -R kamal:kamal /home/kamal/.ssh\n\n# Disable root login\nsed -i '/PermitRootLogin/d' /etc/ssh/sshd_config\necho \"PermitRootLogin no\" &gt;&gt; /etc/ssh/sshd_config\nsystemctl restart sshd\n\n# Add the kamal user to the docker group\nusermod -aG docker kamal\n\n# Create a folder for the Let's Encrypt ACME JSON\nmkdir -p /letsencrypt && touch /letsencrypt/acme.json && chmod 600 /letsencrypt/acme.json\nchown -R kamal:kamal /letsencrypt\n\n# Create a folder for the SQLite database (skip this if you're using a different database)\nmkdir -p /db\nchown -R 1000:1000 /db\n\n# Create a folder for the redis data (skip this if you're not using redis)\nmkdir -p /data\nchown -R 1000:1000 /data\n\nreboot\nThis assumes that you‚Äôre using a root user to connect to your server and that there isn‚Äôt a non-root user with UID 1000 already. Otherwise, adjust the commands accordingly.\nAlso, if you don‚Äôt have a public SSH key for the ‚ÄúAdd SSH key‚Äù step, you can generate one with the following command:\nssh-keygen -t ed25519 -C \"your-email@example.com\"\nThese commands will:\n\nInstall docker, curl, git, and snapd\nStart and enable the docker service\nCreate a non-root user called kamal\nRemove the root login\nAdd the kamal user to the docker group\nCreate a bridge network for Traefik, SQLite, and redis\nCreate a folder for the Let‚Äôs Encrypt ACME JSON\nMake the Let‚Äôs Encrypt ACME JSON folder writable by the kamal user\nCreate a folder for the SQLite database and redis data\nMake the SQLite database and redis data folders writable by the kamal user\nRestart the server\n\nIf you‚Äôre not using SQLite or redis, you can skip the database and redis data folder steps.\nFinally, configure the SSH key in your local .ssh/config file so you can login as the kamal user without using the root account.\nHost kamal\n  HostName &lt;YOUR_VPS_IP&gt;\n  User kamal\n  IdentityFile ~/.ssh/&lt;YOUR_PRIVATE_SSH_KEY&gt;"
  },
  {
    "objectID": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#create-a-dockerfile-for-your-app",
    "href": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#create-a-dockerfile-for-your-app",
    "title": "Deploying a Django app with Kamal 2, AWS ECR, and Github Actions",
    "section": "Create a Dockerfile for your app",
    "text": "Create a Dockerfile for your app\nKamal is meant to deploy containerized apps, so you‚Äôll need to have a Dockerfile for your app. I also recommend using an entrypoint.sh script to run the application.\n\nDockerfile\nHere‚Äôs the Dockerfile I‚Äôm using for my projects. You can use this as a template and adjust it to your needs.\n\n\nDockerfile\n\nFROM python:3.10-slim AS base\n\nENV POETRY_HOME=/opt/poetry\nENV POETRY_VERSION=1.8.3\nENV PATH=${POETRY_HOME}/bin:${PATH}\n\nRUN apt-get update \\\n    && apt-get install --no-install-recommends -y \\\n    curl \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\nRUN curl -sSL https://install.python-poetry.org | python3 - && poetry --version\n\nFROM base AS builder\n\nWORKDIR /app\n\nCOPY poetry.lock pyproject.toml ./\n\nRUN poetry config virtualenvs.in-project true && \\\n    poetry install --only main --no-interaction\n\nFROM base AS runner\n\nWORKDIR /app\nCOPY --from=builder /app/.venv/ /app/.venv/\n\nCOPY . /app\nRUN mkdir -p /data /db\n\nRUN chmod +x /app/src/entrypoint.sh\n\nFROM runner AS production\n\nEXPOSE 8000\n\nARG user=django\nARG group=django\nARG uid=1000\nARG gid=1000\nRUN groupadd -g ${gid} ${group} && \\\n    useradd -u ${uid} -g ${group} -s /bin/sh -m ${user} && \\\n    chown -R ${uid}:${gid} /app /data /db\n\nUSER ${uid}:${gid}\n\nWORKDIR /app/src\nCMD [ \"/app/src/entrypoint.sh\" , \"app\"]\n\nThis is a multi-stage Dockerfile that:\n\nInstalls poetry and sets up the virtual environment\nCreates the user django with the UID and GID 1000 and runs the application with that user. It‚Äôs important that this user has the same UID and GID as the owner of the folders outside the container. Otherwise, you‚Äôll have issues with file permissions and the app won‚Äôt persist data.\nExposes port 8000 and runs the application by executing the entrypoint.sh script. By exposing the port, Kamal will automatically detect that is the port the app runs on and will use that to set up the reverse proxy.\n\nFeel free to adjust this Dockerfile to your needs. If you are not planning on using redis or a SQLite database in your same VPS, you can remove those parts from the Dockerfile.\n\n\nentrypoint.sh script\nI use an entrypoint.sh script to run the application because that makes it easier to collect static files, run migrations when the container starts, and also running commands in the container.\nHere‚Äôs an example of a simple entrypoint.sh script:\n\n\nentrypoint.sh\n\n#!/bin/sh\n\nset -e\n\nif [ \"$1\" = \"app\" ]; then\n    echo \"Collecting static files\"\n    poetry run python manage.py collectstatic --clear --noinput\n\n    echo \"Running migrations\"\n    poetry run python manage.py migrate\n\n    echo \"Running in production mode\"\n    exec poetry run gunicorn -c gunicorn.conf.py\nelse\n    exec \"$@\"\nfi\n\nThis script just collects static files, runs migrations, and starts the Gunicorn server with the configuration in the gunicorn.conf.py file. You can add or remove commands to the script as needed."
  },
  {
    "objectID": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#configure-an-ecr-registry-in-aws",
    "href": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#configure-an-ecr-registry-in-aws",
    "title": "Deploying a Django app with Kamal 2, AWS ECR, and Github Actions",
    "section": "Configure an ECR registry in AWS",
    "text": "Configure an ECR registry in AWS\nNext, you‚Äôll need a place to push and pull your Docker images. I like using AWS, so that‚Äôs what I‚Äôll show you how to do. If you prefer other services, take a look at the instructions for other registries in the Kamal documentation.\nLog in to the AWS Management Console and go to Amazon ECR. Click on Create repository and set a name for your repository.\nThen, create a new IAM user in your AWS account by going to Services &gt; IAM &gt; Users &gt; Add user.\nInstead of using a predefined policy, create a new one with the following JSON and attach it to the user:\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"ListImagesInRepository\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"ecr:ListImages\"],\n      \"Resource\": [\n        \"arn:aws:ecr:&lt;REGION&gt;:&lt;ACCOUNT_ID&gt;:repository/&lt;REPOSITORY_NAME&gt;\"\n      ]\n    },\n    {\n      \"Sid\": \"GetAuthorizationToken\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"ecr:GetAuthorizationToken\"],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"ManageRepositoryContents\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ecr:BatchCheckLayerAvailability\",\n        \"ecr:GetDownloadUrlForLayer\",\n        \"ecr:GetRepositoryPolicy\",\n        \"ecr:DescribeRepositories\",\n        \"ecr:ListImages\",\n        \"ecr:DescribeImages\",\n        \"ecr:BatchGetImage\",\n        \"ecr:InitiateLayerUpload\",\n        \"ecr:UploadLayerPart\",\n        \"ecr:CompleteLayerUpload\",\n        \"ecr:PutImage\"\n      ],\n      \"Resource\": [\n        \"arn:aws:ecr:&lt;REGION&gt;:&lt;ACCOUNT_ID&gt;:repository/&lt;REPOSITORY_NAME&gt;\"\n      ]\n    }\n  ]\n}\nThis policy allows the user to list, get, and manage the ECR repository you created earlier and get the authorization token to push and pull the image. You will need to replace the &lt;REGION&gt;, &lt;ACCOUNT_ID&gt;, and &lt;REPOSITORY_NAME&gt; with the values for your repository.\nNext, select the user you created and go to Security credentials &gt; Access keys &gt; Create access key. Download the CSV file and keep it in a secure location.\nYou will use those credentials in your Github Actions pipeline to push and pull the image from the ECR registry."
  },
  {
    "objectID": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#set-up-kamal-in-your-project",
    "href": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#set-up-kamal-in-your-project",
    "title": "Deploying a Django app with Kamal 2, AWS ECR, and Github Actions",
    "section": "Set up Kamal in your project",
    "text": "Set up Kamal in your project\nOpen your Django project in your favorite code editor. Create a folder called deploy in the root directory. Then go into the folder and initialize Kamal:\nkamal init\nThis will create two folders (.kamal/ and config/), a deploy.yml file, and a secrets file, and a .hooks/ folder that contains the hooks for the deployment. This is where you‚Äôll provide the instructions for Kamal to build and deploy your app.\nYou can use the following deploy.yml file as a template for your Django app:\n\n\ndeploy.yml\n\nservice: example\n\nimage: example\n\nssh:\n  user: kamal\n\nenv:\n  secret:\n    - DJANGO_SECRET_KEY\n\nproxy:\n  ssl: true\n  host: example.iwanalabs.com\n  app_port: 8000\n  healthcheck:\n    path: /kamal/up/\n    interval: 2\n    timeout: 2\n\nservers:\n  web:\n    - 128.140.0.209\n\naccessories:\n  redis:\n    image: redis:7.0\n    roles:\n      - web\n    cmd: --maxmemory 200m --maxmemory-policy allkeys-lru\n    volumes:\n      - /var/redis/data:/data/redis\n    options:\n      memory: 250m\n      network: private_network\n\nvolumes:\n  - \"/db/:/app/db/\"\n\nregistry:\n  server: &lt;account_id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com\n  username: AWS\n  password:\n    - KAMAL_REGISTRY_PASSWORD\n\nbuilder:\n  arch: amd64\n  dockerfile: \"../Dockerfile\"\n  context: \"../\"\n  cache:\n    type: gha\n\nThis will set up your app and a Kamal‚Äôs proxy server (with automatic SSL certificates using Let‚Äôs Encrypt), a Redis database, and a volume to persist the SQLite database. It will also do a healthcheck on /kamal/up/ on port 8000. Remember to replace the placeholders with your own values.\nTo pass the healthcheck, you will need to add a small middleware that bypasses Django‚Äôs built-in security middleware.\n\n\nsrc/config/middleware.py\n\nfrom django.http import HttpResponse\n\n\nclass HealthCheckMiddleware:\n    def __init__(self, get_response):\n        self.get_response = get_response\n\n    def __call__(self, request):\n        if request.path == \"/kamal/up/\":\n            response = HttpResponse(\"OK\")\n        else:\n            response = self.get_response(request)\n\n        return response\n\nAnd add it to the MIDDLEWARE list in the settings.py file.\n\n\nsrc/config/settings.py\n\nMIDDLEWARE = [\n    \"config.middleware.HealthCheckMiddleware\",\n    # other middlewares...\n]\n\n\nTest the configuration locally\nTo test it locally, first, you‚Äôll have to define the required environment variables in the .env file, such as the Django secret key, OpenAI API key, and any other secrets you need.\nYou‚Äôll also need to get a temporary password for the ECR registry. You can get this password by running the following command:\naws ecr get-login-password --region &lt;YOUR_REGION&gt;\nYou should copy the output of this command and paste it in the KAMAL_REGISTRY_PASSWORD field in the .env file.\nThen, you should define the required environment variables in .kamal/secrets as follows:\n\n\n.kamal/secrets\n\nKAMAL_REGISTRY_PASSWORD=$KAMAL_REGISTRY_PASSWORD\nDJANGO_SECRET_KEY=$DJANGO_SECRET_KEY\nOPENAI_API_KEY=$OPENAI_API_KEY\n\nThen, run the following command to deploy your application to your VPS:\nkamal deploy\nThe first command will push the environment variables to the VPS. The second command will build the Docker image, push it to the ECR registry, and deploy it to your VPS.\nAfter a few minutes, your app should be live at https://&lt;YOUR_DOMAIN&gt;.\nIf you see any errors, there are two things you can do:\n\nRun kamal app logs to see the logs of the app.\nOpen a terminal in the container by running kamal app exec -it bash.\n\nThis is how I usually debug the app."
  },
  {
    "objectID": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#automate-the-deployment-with-github-actions",
    "href": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#automate-the-deployment-with-github-actions",
    "title": "Deploying a Django app with Kamal 2, AWS ECR, and Github Actions",
    "section": "Automate the deployment with Github Actions",
    "text": "Automate the deployment with Github Actions\nNow that you have a working deployment process in your local environment, you can automate the deployment with Github Actions.\nCreate a new file in the .github/workflows folder called deploy.yml and add the following code:\nname: Deploy webapp to VPS\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\non:\n  push:\n    branches: [\"main\"]\n  workflow_dispatch:\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n\n      - uses: webfactory/ssh-agent@v0.7.0\n        with:\n          ssh-private-key: ${{ secrets.VPS_SSH_PRIVATE_KEY }}\n\n      - name: Set up Ruby and install kamal\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: 3.2.2\n      - run: gem install kamal -v 1.9.0\n\n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID_ECR }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY_ECR }}\n          aws-region: us-east-1\n          mask-aws-account-id: false\n\n      - name: Login to AWS ECR\n        id: login-ecr\n        uses: aws-actions/amazon-ecr-login@v2\n\n      - name: Set up Docker Buildx for cache\n        uses: docker/setup-buildx-action@v3\n\n      - name: Expose GitHub Runtime for cache\n        uses: crazy-max/ghaction-github-runtime@v3\n\n      - name: Kamal Deploy\n        id: kamal-deploy\n        env:\n          KAMAL_REGISTRY_PASSWORD: ${{ steps.login-ecr.outputs.docker_password_&lt;account_id&gt;_dkr_ecr_&lt;region&gt;_amazonaws_com }}\n          DJANGO_SECRET_KEY: ${{ secrets.DJANGO_SECRET_KEY }}\n        run: |\n          cd deploy\n          kamal lock release\n          kamal deploy\nThis workflow will:\n\nCheckout the code\nSet up the Ruby environment and install Kamal\nConfigure the AWS credentials\nLogin to the AWS ECR registry\nSet up Docker Buildx for cache\nExpose GitHub Runtime for cache\nRun Kamal deploy with the secrets defined in the environment\n\nIt will run everytime you make a push to the main branch or by manually triggering the workflow. It‚Äôll cancel any in-progress runs to avoid conflicts.\nAlso, before you push your code to the repository, you‚Äôll need to add the following secrets to the repository:\n\nVPS_SSH_PRIVATE_KEY: The private key to connect to your VPS\nAWS_ACCESS_KEY_ID_ECR: The access key ID for the AWS ECR registry\nAWS_SECRET_ACCESS_KEY_ECR: The secret access key for the AWS ECR registry\nDJANGO_SECRET_KEY: The Django secret key\n\nFinally, to speed up the deployment, add these options to the builder section of the deploy.yml file:\nbuilder:\n  dockerfile: \"../Dockerfile\"\n  context: \"../\"\n  multiarch: false # new\n  cache: # new\n    type: gha # new\nThis will enable the Docker Buildx cache for the build process in Github Actions. You can set multiarch to false if your CI pipeline shares the same architecture as your VPS, which was the case for me."
  },
  {
    "objectID": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#conclusion",
    "href": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#conclusion",
    "title": "Deploying a Django app with Kamal 2, AWS ECR, and Github Actions",
    "section": "Conclusion",
    "text": "Conclusion\nYou now have a fully automated deployment pipeline for your Django app. A push to the main branch will trigger the workflow, which will build the Docker image, push it to the ECR registry, and deploy it to your VPS.\nBreak free from the tyranny of manual deployments and expensive cloud services. Sleep like a baby and let Kamal handle your deployments.\nIf you have any questions or feedback, please feel free to leave a comment below."
  },
  {
    "objectID": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#footnotes",
    "href": "posts/deploy-a-django-app-with-kamal-aws-ecr-and-github-actions.html#footnotes",
    "title": "Deploying a Django app with Kamal 2, AWS ECR, and Github Actions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ncrying and sh*tting my diapers?‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/4-ways-to-improve-your-plotly-graphs.html",
    "href": "posts/4-ways-to-improve-your-plotly-graphs.html",
    "title": "4 Ways To Improve Your Graphs Using Plotly",
    "section": "",
    "text": "These last weeks I‚Äôve been working on an application using Dash and Plotly. These tools are great if you want to get something out quickly. But, as usual, there‚Äôs no magical make_beautiful_graphs parameter you can set to True by default.\nIf you want to have beautiful and customized visualizations in your application, you‚Äôll need to spend some time playing around with Plotly‚Äôs extensive list of figures‚Äô attributes. I wanted to have something better than the default looks, so I went through Plotly‚Äôs documentation, old code snippets I had, and Stack Overflow questions.\nThis is not the first time I found myself doing that. However, this time I decided to keep track of the things I frequently do when making graphs with Plotly. That way, I wouldn‚Äôt need to read the same documentation or browse the same Stack Overflow questions next time.\nIn this article I‚Äôm compiling a list of things I frequently do when building data visualizations using Plotly. Rest assured, I‚Äôve been working in data-related positions for a while, so you will not find outrageous things like how to make 3D pie charts. These improvements are based on a sample of one, but I‚Äôve frequently seen others applying similar ideas.\nI‚Äôm focusing on practical and simple improvements that apply to most of the basic charts: scatter plots, line charts, bar charts, and some statistical charts. Here you will find things like removing gridlines and not things like selecting the best colors for your 4D contour plot.\nFirst, I‚Äôll do a brief introduction on how you build graphs using Plotly. Next, I‚Äôll provide a list of improvements and the reasoning behind them. Last, I‚Äôll give additional recommendations I‚Äôve found useful when working with Plotly and other plotting libraries."
  },
  {
    "objectID": "posts/4-ways-to-improve-your-plotly-graphs.html#how-to-make-a-graph-using-plotly",
    "href": "posts/4-ways-to-improve-your-plotly-graphs.html#how-to-make-a-graph-using-plotly",
    "title": "4 Ways To Improve Your Graphs Using Plotly",
    "section": "How to Make a Graph Using Plotly",
    "text": "How to Make a Graph Using Plotly\nThere are three things you need to know about the inner workings of Plotly:\nFirst, making a graph in Plotly is essentially populating a Python dictionary. This dictionary is usually referred to as figure.\nSecond, there are two keys in the figure dictionary: layout and data. In layout, you define the looks of your graph like the typography, titles, and axes. In the data key, you set the values and information of the traces you‚Äôll be plotting. That could be something like [1, 2, 3] for X, [5, 3, 9] for Y and bar chart type.\nFinally, once you populate the figure dictionary, it is serialized (transformed) into a JSON structure. This resulting data structure is then used by the Plotly JavaScript library to plot your chart.\nThat‚Äôs it.\nSo, how do you make a figure?\nThere are multiple ways to do it. The lowest-level approach is to use Python dictionaries, and the highest-level one is using the Plotly Express interface. I tend to use a mid-level interface called Figure Constructor. It‚Äôs easier to debug than using Python dictionaries, and it‚Äôs more flexible than Plotly Express.\nThe code for making a graph using the Figure Constructor looks as follows:\nimport plotly.graph_objects as go\nimport numpy as np\nnp.random.seed(42)\n\n# Simulate data\nreturns = np.random.normal(0.01, 0.2, 100)\nprice = 100 * np.exp(returns.cumsum())\ntime = np.arange(100)\n\n# Generate graph using Figure Constructor\nlayout = go.Layout(\n    title=\"Historic Prices\",\n    xaxis_title=\"time\",\n    yaxis_title=\"price\"\n)\n\nfig = go.Figure(\n    data=go.Scatter(x=time, y=price),\n    layout=layout\n)\nfig.show()\nHow to make make a line chart using Plotly\nThis is the resulting graph:\n\n\n\nBasic plot in Plotly\n\nFor the code snippets listed below, I used the Figure Constructor approach. You may need to adjust the code to make it work for your case if you are using a different interface.\nSo let‚Äôs get down to the meat and potatoes."
  },
  {
    "objectID": "posts/4-ways-to-improve-your-plotly-graphs.html#list-of-improvements",
    "href": "posts/4-ways-to-improve-your-plotly-graphs.html#list-of-improvements",
    "title": "4 Ways To Improve Your Graphs Using Plotly",
    "section": "List of Improvements",
    "text": "List of Improvements\nHere‚Äôs the list of things I usually do to improve Plotly graphs:\n\n#1: Remove gridlines and background color\n#2: Keep consistent colors across graphs\n#3: Use spikelines to compare data points\n#4: Remove floating menu, disable zoom and adjust click behavior\n\n\n#1: Remove gridlines and background color\nGridlines are lines that cross the chart to show axis divisions. They help the viewer visualize the value represented by an unlabeled data point fast. However, gridlines are not very useful when working with interactive graphs. You can hover over a data point and see its value. So more often than not, I remove gridlines when working with Plotly.\nHere‚Äôs how you can do it:\nimport plotly.graph_objects as go\nimport numpy as np\nnp.random.seed(42)\n\n# Simulate data\nreturns = np.random.normal(0.01, 0.2, 100)\nprice = 100 * np.exp(returns.cumsum())\ntime = np.arange(100)\n\nlayout = go.Layout(\n    title=\"Historic Prices\",\n    plot_bgcolor=\"#FFF\",  # Sets background color to white\n    xaxis=dict(\n        title=\"time\",\n        linecolor=\"#BCCCDC\",  # Sets color of X-axis line\n        showgrid=False  # Removes X-axis grid lines\n    ),\n    yaxis=dict(\n        title=\"price\",\n        linecolor=\"#BCCCDC\",  # Sets color of Y-axis line\n        showgrid=False,  # Removes Y-axis grid lines\n    )\n)\n\nfig = go.Figure(\n    data=go.Scatter(x=time, y=price),\n    layout=layout\n)\nfig.show()\nCode snippet to remove grid lines\nAnd this is how it looks:\n\n\n\nLine chart without gridlines\n\n\n\n#2: Keep consistent colors across graphs\nWhen working with categories, there are two things people usually like to do. First, they want to assign some specific colors to each group. For instance, if you are analyzing electoral results in the US, you probably want to use particular blue and red variations to identify the Democratic and Republican parties.\nSecond, you want this color to remain consistent across all the graphs you do. For example, if you are analyzing some real-world companies, you may want to use their distinctive colors to plot their prices but also when you analyze their returns.\nHere‚Äôs how you can do that using Plotly:\nimport plotly.graph_objects as go\nimport numpy as np\nnp.random.seed(42)\n\n# Simulate data\nreturns_A = np.random.normal(0.01, 0.2, 100)\nreturns_B = np.random.normal(0.01, 0.2, 100)\nreturns = np.append(returns_A, returns_B)\n\nprices_A = 100 * np.exp(returns_A.cumsum())\nprices_B = 100 * np.exp(returns_B.cumsum())\nprices = np.append(prices_A, prices_B)\n\ncompanies = [\"A\"] * 100 + [\"B\"] * 100\ntime = np.append(np.arange(100), np.arange(100))\n\ndf = pd.DataFrame({\n    \"company\": companies,\n    \"time\": time,\n    \"price\": prices,\n    \"returns\": returns\n})\n\n# Build graph\nCOLORS_MAPPER = {\n    \"A\": \"#38BEC9\",\n    \"B\": \"#D64545\"\n}\n\nlayout = go.Layout(\n    title=\"Performance of A vs. B\",\n    plot_bgcolor=\"#FFFFFF\",\n    barmode=\"stack\",\n    xaxis=dict(\n        domain=[0, 0.5],\n        title=\"time\",\n        linecolor=\"#BCCCDC\",\n    ),\n    yaxis=dict(\n        title=\"price\",\n        linecolor=\"#BCCCDC\"\n    ),\n    xaxis2=dict(\n        domain=[0.6, 1],\n        title=\"returns\",\n        linecolor=\"#BCCCDC\",\n    ),\n    yaxis2=dict(\n        anchor=\"x2\",\n        linecolor=\"#BCCCDC\"\n    )\n)\n\ndata = []\nfor company,col in COLORS_MAPPER.items():\n    time = df.loc[df.company == company, \"time\"]\n    price = df.loc[df.company == company, \"price\"]\n    returns = df.loc[df.company == company, \"returns\"]\n    line_chart = go.Scatter(\n        x=time,\n        y=price,\n        marker_color=col,  # Defines specific color for a trace\n        legendgroup=company,  # Groups traces belonging to the same group in the legend\n        name=company\n    )\n    histogram = go.Histogram(\n        x=returns,\n        marker_color=col,  # Defines specific color for a trace\n        legendgroup=company,  # Groups traces belonging to the same group in the legend\n        xaxis=\"x2\",\n        yaxis=\"y2\",\n        showlegend=False\n    )\n    data.append(line_chart)\n    data.append(histogram)\n\nfig = go.Figure(data=data, layout=layout)\nfig.show()\n\n\n\nConsistent colors across graphs\n\nThe snippet above allows you to keep consistent colors when working with multiple graphs that share the same categories. The critical part is the COLOR_MAPPER dictionary and its use when adding new traces. This dictionary is the mapping of the categories and colors you‚Äôll be using across your charts.\nWhenever you add a trace to a graph, you can assign the right color to the marker_color attribute by getting it from the COLOR_MAPPER dictionary.\nThe resulting graph looks as follows:\nConsistent colors across graphs\n\n\n#3: Use spike lines to compare data points\nA spike line is a vertical or horizontal line that appears when hovering on data. It‚Äôs useful for comparing values in line charts and scatter plots. This is how you can add those using Plotly:\nimport plotly.graph_objects as go\nimport numpy as np\nnp.random.seed(42)\n\n# Simulate data\nreturns_A = np.random.normal(0.01, 0.2, 100)\nreturns_B = np.random.normal(0.01, 0.2, 100)\nreturns = np.append(returns_A, returns_B)\n\nprices_A = 100 * np.exp(returns_A.cumsum())\nprices_B = 100 * np.exp(returns_B.cumsum())\nprices = np.append(prices_A, prices_B)\n\ncompanies = [\"A\"] * 100 + [\"B\"] * 100\ntime = np.append(np.arange(100), np.arange(100))\n\ndf = pd.DataFrame({\n    \"company\": companies,\n    \"time\": time,\n    \"price\": prices,\n    \"returns\": returns\n})\n\n# Build graph\nlayout = go.Layout(\n    title=\"Performance of A vs. B\",\n    plot_bgcolor=\"#FFFFFF\",\n    hovermode=\"x\",\n    hoverdistance=100, # Distance to show hover label of data point\n    spikedistance=1000, # Distance to show spike\n    xaxis=dict(\n        title=\"time\",\n        linecolor=\"#BCCCDC\",\n        showspikes=True, # Show spike line for X-axis\n        # Format spike\n        spikethickness=2,\n        spikedash=\"dot\",\n        spikecolor=\"#999999\",\n        spikemode=\"across\",\n    ),\n    yaxis=dict(\n        title=\"price\",\n        linecolor=\"#BCCCDC\"\n    )\n)\n\ndata = []\nfor company in [\"A\", \"B\"]:\n    time = df.loc[df.company == company, \"time\"]\n    price = df.loc[df.company == company, \"price\"]\n    returns = df.loc[df.company == company, \"returns\"]\n    line_chart = go.Scatter(\n        x=time,\n        y=price,\n        name=company\n    )\n    data.append(line_chart)\n\nfig = go.Figure(data=data, layout=layout)\nfig.show()\nCode snippet to add spike lines to chart\nThis is the resulting graph:\n\n\n\nChart with spike lines\n\n\n\n#4: Remove floating menu, disable zoom and adjust click behavior\nI‚Äôm not too fond of the floating menu that Plotly adds to your chart by default. It makes graphs look cool, but I‚Äôve rarely seen people using it. It has so many options that it‚Äôs just confusing for someone looking at a graph for the first time. Usually, I remove it.\nAlso, I like to re-define two other user interaction parameters. I prefer to limit the users‚Äô ability to zoom in and change the behavior of clicking on a trace in the legend. In Plotly, by default, if you want to inspect a trace on its own, you have to double-click on the trace, instead of just clicking on it. That‚Äôs not very intuitive, so I tend to invert that behavior.\nThis is how you can apply those changes:\nimport plotly.graph_objects as go\nimport numpy as np\nnp.random.seed(42)\n\n# Simulate data\nreturns_A = np.random.normal(0.01, 0.2, 100)\nreturns_B = np.random.normal(0.01, 0.2, 100)\nreturns = np.append(returns_A, returns_B)\n\nprices_A = 100 * np.exp(returns_A.cumsum())\nprices_B = 100 * np.exp(returns_B.cumsum())\nprices = np.append(prices_A, prices_B)\n\ncompanies = [\"A\"] * 100 + [\"B\"] * 100\ntime = np.append(np.arange(100), np.arange(100))\n\ndf = pd.DataFrame({\n    \"company\": companies,\n    \"time\": time,\n    \"price\": prices,\n    \"returns\": returns\n})\n\n# Build graph\nlayout = go.Layout(\n    title=\"Performance of A vs. B\",\n    plot_bgcolor=\"#FFFFFF\",\n    legend=dict(\n        # Adjust click behavior\n        itemclick=\"toggleothers\",\n        itemdoubleclick=\"toggle\",\n    ),\n    xaxis=dict(\n        title=\"time\",\n        linecolor=\"#BCCCDC\",\n        fixedrange=True\n    ),\n    yaxis=dict(\n        title=\"price\",\n        linecolor=\"#BCCCDC\",\n        fixedrange=True\n    )\n)\n\ndata = []\nfor company in [\"A\", \"B\"]:\n    time = df.loc[df.company == company, \"time\"]\n    price = df.loc[df.company == company, \"price\"]\n    returns = df.loc[df.company == company, \"returns\"]\n    line_chart = go.Scatter(\n        x=time,\n        y=price,\n        name=company\n    )\n    data.append(line_chart)\n\nfig = go.Figure(data=data, layout=layout)\nfig.show(config={\"displayModeBar\": False, \"showTips\": False}) # Remove floating menu and unnecesary dialog box\nThis is the resulting graph:\n\n\n\nChart without floating menu, no-zoom and adjusted click behavior"
  },
  {
    "objectID": "posts/4-ways-to-improve-your-plotly-graphs.html#additional-recommendations",
    "href": "posts/4-ways-to-improve-your-plotly-graphs.html#additional-recommendations",
    "title": "4 Ways To Improve Your Graphs Using Plotly",
    "section": "Additional recommendations",
    "text": "Additional recommendations\nThere are three things that I‚Äôve found useful for learning how to make better data visualizations:\n\nGet feedback from your audience: This is not always possible. But if you can do it, always prioritize getting input from those who will use your data visualizations. If you are working on a dashboard, the first thing you should do is understand what problem your dashboard solves. Then see users interacting with it. That has the highest ROI for your time.\nCheck out Storytelling with Data by Cole Knaflic: It‚Äôs a great book if you want to level-up your data visualization design skills. It provides a lot of practical advice and compelling use cases.\nPlotly‚Äôs Figure Reference: Get used to Plotly‚Äôs Figure Reference and documentation. You‚Äôll be using it a lot. Though, there‚Äôs nothing to worry about. Plotly has great documentation!"
  },
  {
    "objectID": "posts/4-ways-to-improve-your-plotly-graphs.html#closing-words",
    "href": "posts/4-ways-to-improve-your-plotly-graphs.html#closing-words",
    "title": "4 Ways To Improve Your Graphs Using Plotly",
    "section": "Closing Words",
    "text": "Closing Words\nI hope you‚Äôve find these ideas useful. There might be some things that do not resonate with you, or others that you feel are missing. If that‚Äôs the case, please let me know in the comments below. I‚Äôll be happy to update this and add other valuable advice.\nIf you want to keep up-to-date with what I‚Äôm doing you can follow me on twitter."
  },
  {
    "objectID": "posts/clustering-documents-with-openai-langchain-hdbscan.html",
    "href": "posts/clustering-documents-with-openai-langchain-hdbscan.html",
    "title": "Clustering Documents with OpenAI embeddings, HDBSCAN and UMAP",
    "section": "",
    "text": "In the past, the most common way to cluster documents was by building vectors with traditional Machine Learning methods such as bag-of-words or smaller pre-trained NLP models, like BERT, and then creating groups out of them. But LLMs have changed that.\nWhile older methods are still relevant, if I had to cluster text data today, I‚Äôd start using the OpenAI or Cohere (embeddings and generation) APIs. It‚Äôs faster, easier, and gives you additional goodies such as coming up with fitting titles for each cluster.\nI haven‚Äôt seen many tutorials on this topic, so I wrote one. In this tutorial, I‚Äôll show you how to cluster news articles using OpenAI embeddings, and HDBSCAN.\nLet‚Äôs get to it!"
  },
  {
    "objectID": "posts/clustering-documents-with-openai-langchain-hdbscan.html#prerequisites",
    "href": "posts/clustering-documents-with-openai-langchain-hdbscan.html#prerequisites",
    "title": "Clustering Documents with OpenAI embeddings, HDBSCAN and UMAP",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo make the most of this tutorial, you should be familiar with the following concepts:\n\nHow to cluster text data using traditional ML methods.\nWhat are OpenAI Embeddings\nHow HDBSCAN works\n\nIn addition, you‚Äôll need an OpenAI account."
  },
  {
    "objectID": "posts/clustering-documents-with-openai-langchain-hdbscan.html#set-up-your-local-environment",
    "href": "posts/clustering-documents-with-openai-langchain-hdbscan.html#set-up-your-local-environment",
    "title": "Clustering Documents with OpenAI embeddings, HDBSCAN and UMAP",
    "section": "Set Up Your Local Environment",
    "text": "Set Up Your Local Environment\n\nCreate a virtual environment using venv:\n\npython3.10 -m venv .venv\n\nCreate a requirements.txt file that contains the following packages:\n\nhdbscan\nopenai\npandas\nnumpy\npython-dotenv\ntiktoken\nnotebook\nplotly\numap-learn\n\nActivate the virtual environment and install the packages:\n\nsource .venv/bin/activate\npip3 install -r requirements.txt\n\nCreate a file called .env, and add the your OpenAI key:\n\nOPENAI_API_KEY=&lt;your key&gt;\n\nCreate an empty notebook file. For the rest of this tutorial, you‚Äôll work on it."
  },
  {
    "objectID": "posts/clustering-documents-with-openai-langchain-hdbscan.html#clustering-documents",
    "href": "posts/clustering-documents-with-openai-langchain-hdbscan.html#clustering-documents",
    "title": "Clustering Documents with OpenAI embeddings, HDBSCAN and UMAP",
    "section": "Clustering Documents",
    "text": "Clustering Documents\nYou should think of the clustering process in three steps:\n\nGenerate numerical vector representations of documents using OpenAI‚Äôs embedding capabilities.\nApply a clustering algorithm on the vectors to group the documents.\nGenerate a title for each cluster summarizing the articles contained in it.\n\nThat‚Äôs it! Now, you‚Äôll see how that looks in practice.\n\nImport the Required Packages\nStart by importing the required Python libraries. Copy the following code in your notebook:\nimport os\n\nimport hdbscan\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport requests\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom umap import UMAP\n\nload_dotenv()\nThis code imports the libraries you‚Äôll use throughout the tutorial. Here‚Äôs the purpose of each one:\n\nos helps you read the environment variables.\nhdbscan gives you a wrapper of HDBSCAN, the clustering algorithm you‚Äôll use to group the documents.\nopenai to use OpenAI LLMs.\numap loads UMAP for dimensionality reduction and visualizing clusters.\ndotenv load the environment variables you define in .env.\n\nNext, you‚Äôll get a sample of news articles to cluster.\n\n\nDownload the data and generate embeddings\nDownload, read these articles, and generate documents you‚Äôll use to create the embeddings:\ndf = pd.read_csv(\"news_data_dedup.csv\")\ndocs = [\n    f\"{title}\\n{description}\"\n    for title, description in zip(df.title, df.description)\n]\nThen, initialize the OpenAI client and generate the embeddings:\nclient = OpenAI()\nresponse = client.embeddings.create(input=docs, model=\"text-embedding-3-small\")\nembeddings = [np.array(x.embedding) for x in response.data]\n\n\nCluster documents\nOnce you have the embeddings, you can cluster them using hdbscan:\nhdb = hdbscan.HDBSCAN(min_samples=3, min_cluster_size=3).fit(embeddings)\nThis code will generate clusters using the embeddings generated, and then create a DataFrame with the results. Itfits the hdbscan algorithm. In this case, I set min_samples and min_cluster_size to 3, but depending on your data this may change. Check HDBSCAN‚Äôs documentation to learn more about these parameters.\nNext, you‚Äôll create topic titles for each cluster based on their contents.\n\n\nVisualize the clusters\nAfter you‚Äôve generated the clusters, you can visualize them using UMAP:\numap = UMAP(n_components=2, random_state=42, n_neighbors=80, min_dist=0.1)\n\ndf_umap = (\n    pd.DataFrame(umap.fit_transform(np.array(embeddings)), columns=['x', 'y'])\n    .assign(cluster=lambda df: hdb.labels_.astype(str))\n    .query('cluster != \"-1\"')\n    .sort_values(by='cluster')\n)\n\nfig = px.scatter(df_umap, x='x', y='y', color='cluster')\nfig.show()\nYou should get something similar to this graph:\n\nThis will give you a sense of how good are the clusters generated.\n\n\nCreate a Topic Title per Cluster\nFor each cluster, you‚Äôll generate a topic title summarizing the articles in that cluster. Copy the following code to your notebook:\ndf[\"cluster_name\"] = \"Uncategorized\"\n\ndef generate_topic_titles():\n    system_message = \"You're an expert journalist. You're helping me write short but compelling topic titles for groups of news articles.\"\n    user_template = \"Using the following articles, write a 4 to 5 word title that summarizes them.\\n\\nARTICLES:\\n\\n{}\\n\\nTOPIC TITLE:\"\n\n    for c in df.cluster.unique():\n        sample_articles = df.query(f\"cluster == '{c}'\").to_dict(orient=\"records\")\n        articles_str = \"\\n\\n\".join(\n            [\n                f\"[{i}] {article['title']}\\n{article['description'][:200]}{'...' if len(article['description']) &gt; 200 else ''}\"\n                for i, article in enumerate(\n                    sample_articles, start=1\n                )\n            ]\n        )\n        messages = [\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": user_template.format(articles_str)},\n        ]\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\", messages=messages, temperature=0.7, seed=42\n        )\n\n        topic_title = response.choices[0].message.content\n        df.loc[df.cluster == c, \"cluster_name\"] = topic_title\nThis code takes all the articles per cluster and uses gpt-3.5-turbo to generate a relevant topic title from them. Itgoes through each cluster, takes the articles in it, and makes a prompt using that to generate a topic title for that cluster.\nFinally, you can check the resulting clusters and topic titles, as follows:\nc = 6\nwith pd.option_context(\"display.max_colwidth\", None):\n    print(df.query(f\"cluster == '{c}'\").topic_title.values[0])\n    display(df.query(f\"cluster == '{c}'\").drop(columns=[\"topic_title\"]).head())\nIn my case, running this code produces the following articles and topic titles:\n\nAll articles seem to be related to the topic title. Yay!\n\n\nConclusion\nIn this short tutorial, you‚Äôve learned how to cluster documents using OpenAI embeddings, HDBSCAN, and UMAP. I hope you find this useful. Let me know in the comments if you have any questions.\nCheck out the code on GitHub. You can also check this project I built with Iswar which shows this in practice."
  },
  {
    "objectID": "posts/semantic-search-elasticsearch-openai-langchain.html",
    "href": "posts/semantic-search-elasticsearch-openai-langchain.html",
    "title": "Semantic Search with Elasticsearch, OpenAI, and LangChain",
    "section": "",
    "text": "LangChain is the new cool kid on the block. It‚Äôs a library designed to help you interact with Large Language Models (LLMs). Up until recently, I was building most things from scratch when working with LLMs, so I decided to give LangChain a try.\nAfter a few projects using it, I‚Äôm truly impressed. It simplifies many of the routine tasks associated with working with LLMs, such as extracting text from documents or indexing them in a vector database. If you‚Äôre working with LLMs today, LangChain can save you hours of work.\nHowever, one drawback is that its documentation, despite being extensive, can be scattered and difficult for newcomers to comprehend. Moreover, most of the content online focuses on the latest generation of vector databases. Since many organizations still use older, but battle-tested technologies such as Elasticsearch and I decided to write a tutorial using it.\nI combined LangChain and Elasticsearch in one of the most common LLM applications: semantic search. In this tutorial, I‚Äôll walk you through building a semantic search service using Elasticsearch, OpenAI, LangChain, and FastAPI. You‚Äôll create an application that lets users ask questions about Marcus Aurelius‚Äô Meditations and provides them with concise answers by extracting the most relevant content from the book.\nLet‚Äôs dive in!"
  },
  {
    "objectID": "posts/semantic-search-elasticsearch-openai-langchain.html#prerequisites",
    "href": "posts/semantic-search-elasticsearch-openai-langchain.html#prerequisites",
    "title": "Semantic Search with Elasticsearch, OpenAI, and LangChain",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou should be familiar with these topics to make the most out of this tutorial:\n\nWhat semantic search is.\nHow to use Elasticsearch in Python.\nWhat text embeddings are.\n\nIn addition, you must install Docker and create an account at OpenAI."
  },
  {
    "objectID": "posts/semantic-search-elasticsearch-openai-langchain.html#designing-a-semantic-search-service",
    "href": "posts/semantic-search-elasticsearch-openai-langchain.html#designing-a-semantic-search-service",
    "title": "Semantic Search with Elasticsearch, OpenAI, and LangChain",
    "section": "Designing a Semantic Search Service",
    "text": "Designing a Semantic Search Service\nYou‚Äôre going to build a service with three components:\n\nIndexer: This creates the index, generates the embeddings and the metadata (source and title of the book, in this case), and adds them to the vector database.\nVector database: This is a database that you use to store and retrieve the embeddings you generate.\nSearch app: This is a backend service that uses the user‚Äôs search term, generates an embedding from it, and then looks for the most similar embeddings in the vector database.\n\nHere‚Äôs a diagram of this architecture:\n\nIf you‚Äôve read my previous tutorial on semantic search, you might have noticed that the vectorizer isn‚Äôt part of this architecture. I‚Äôm purposely skipping it because langchain takes care of that part for you, so you can have the indexer and vectorizer in the same place.\nNext, you‚Äôll set up your local environment."
  },
  {
    "objectID": "posts/semantic-search-elasticsearch-openai-langchain.html#set-up-your-local-environment",
    "href": "posts/semantic-search-elasticsearch-openai-langchain.html#set-up-your-local-environment",
    "title": "Semantic Search with Elasticsearch, OpenAI, and LangChain",
    "section": "Set Up Your Local Environment",
    "text": "Set Up Your Local Environment\nFollow these steps to set up your local environment:\n\nInstall Python 3.10.\nInstall Poetry. It‚Äôs optional but highly recommended.\nClone the project‚Äôs repository:\n\ngit clone https://github.com/dylanjcastillo/semantic-search-elasticsearch-openai-langchain\n\nFrom the root folder of the project, install the dependencies:\n\nUsing Poetry: Create the virtual environment in the same directory as the project and install the dependencies:\n\npoetry config virtualenvs.in-project true\npoetry install\n\nUsing venv and pip: Create a virtual environment and install the dependencies listed in requirements.txt:\n\npython3.10 -m venv .venv && source .venv/bin/activate\npip install -r requirements.txt\nOpen src/.env-example, add your secret OpenAI key, and save the file as .env.\n\nBy now, you‚Äôll have a virtual environment set up with the required libraries and a local copy of the repository. Your project structure should look like this:\nsemantic-search-elasticsearch-openai-langchain\n‚îú‚îÄ‚îÄ LICENSE\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ poetry.lock\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îú‚îÄ‚îÄ requirements.txt\n‚îú‚îÄ‚îÄ run_elasticsearch.sh\n‚îú‚îÄ‚îÄ src\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ app.py\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ config.py\n‚îÇ   ‚îú‚îÄ‚îÄ data\n‚îÇ¬†¬† ‚îÇ   ‚îî‚îÄ‚îÄ Marcus_Aurelius_Antoninus...\n‚îÇ¬†¬† ‚îÇ       ‚îú‚îÄ‚îÄ index.html\n‚îÇ¬†¬† ‚îÇ       ‚îú‚îÄ‚îÄ metadata.opf\n‚îÇ¬†¬† ‚îÇ       ‚îî‚îÄ‚îÄ style.css\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ indexer.py\n‚îÇ   ‚îî‚îÄ‚îÄ .env-example\n‚îî‚îÄ‚îÄ .venv/\nThese are the most relevant files and directories in the project:\n\npoetry.lock and pyproject.toml: These files contain the project‚Äôs specifications and dependencies and are used by Poetry to create a virtual environment.\nrequirements.txt: This file contains a list of Python packages required by the project.\nrun_elasticsearch_docker.sh: This file contains a bash script used to run an Elasticsearch cluster locally.\nsrc/app.py: This file contains the code of the search application.\nsrc/config.py: This file contains project configuration specifications such as OpenAI‚Äôs API key (read from a .env file), the paths to the data, and the name of the index.\nsrc/data/: This directory contains Meditations as originally downloaded from Wikisource. You‚Äôll use that as the corpus of text for this tutorial.\nsrc/indexer.py: This file contains the code you use to create an index and insert the documents in OpenSearch.\n.env-example: This file is typically used for environment variables. In this case, you use it to pass OpenAI‚Äôs API key to your application.\n.venv/: This directory contains the project‚Äôs virtual environment.\n\nAll done! Let‚Äôs get going."
  },
  {
    "objectID": "posts/semantic-search-elasticsearch-openai-langchain.html#start-a-local-elasticsearch-cluster",
    "href": "posts/semantic-search-elasticsearch-openai-langchain.html#start-a-local-elasticsearch-cluster",
    "title": "Semantic Search with Elasticsearch, OpenAI, and LangChain",
    "section": "Start a Local Elasticsearch Cluster",
    "text": "Start a Local Elasticsearch Cluster\nBefore we get into the code, you should start a local Elasticsearch cluster. Open a new terminal, navigate to the project‚Äôs root folder, and run:\nsh run_elasticsearch_docker.sh\nIf everything went well, a lengthy text string will appear on the terminal. For the rest of the tutorial, keep this terminal window open in the background."
  },
  {
    "objectID": "posts/semantic-search-elasticsearch-openai-langchain.html#split-and-index-the-book",
    "href": "posts/semantic-search-elasticsearch-openai-langchain.html#split-and-index-the-book",
    "title": "Semantic Search with Elasticsearch, OpenAI, and LangChain",
    "section": "Split and Index the Book",
    "text": "Split and Index the Book\nIn this step, you‚Äôll do two things:\n\nProcess the text from the book by splitting it into chunks of 1,000 tokens.\nIndex the text chunks you‚Äôve generated (from now on called documents) in your Elasticsearch cluster.\n\nTake a look at src/indexer.py:\nfrom langchain.document_loaders import BSHTMLLoader\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import ElasticVectorSearch\n\nfrom config import Paths, openai_api_key\n\n\ndef main():\n    loader = BSHTMLLoader(str(Paths.book))\n    data = loader.load()\n\n    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n        chunk_size=1000, chunk_overlap=0\n    )\n    documents = text_splitter.split_documents(data)\n\n    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n    db = ElasticVectorSearch.from_documents(\n        documents,\n        embeddings,\n        elasticsearch_url=\"http://localhost:9200\",\n        index_name=\"elastic-index\",\n    )\n    print(db.client.info())\n\n\nif __name__ == \"__main__\":\n    main()\nThis code takes Meditations, splits it into text chunks of 1,000 tokens, and then indexes those chunks in your Elasticsearch cluster. Here‚Äôs a detailed breakdown:\n\nLines 1 to 4 import the required components from langchain:\n\nBSHTMLLoader: This loader uses BeautifulSoup4 to parse the documents.\nOpenAIEmbeddings: This component is a wrapper around OpenAI embeddings. It helps you generate embeddings for documents and queries.\nRecursiveCharacterTextSplitter: This utility function divides the input text by attempting various characters in an order designed to maintain semantically similar content in proximity. The characters used for splitting, in the following order, are: \"\\n\\n\", \"\\n\", \" \", \"\".\nElasticSearchVector: This is a wrapper around the Elasticsearch client, that simplifies interacting with your cluster.\n\nLine 6 imports the relevant configurations from config.py\nLines 11 and 12 extract the book‚Äôs text using BSHTMLLoader.\nLines 13 to 16 initialize the text splitter and split the text in chunks of no more than 1,000 tokens. In this case, you use tiktoken to count the tokens but you can also use different length functions, such as counting the number of characters instead of tokens or a different tokenizing function.\nLines 18 to 25 initialize the embeddings function, create a new index, and index the documents generated by the text splitter. In elasticsearch_url, you specify the port where your application is running locally, and in index_name you specify the name of the index you‚Äôll use. Finally, you print the Elasticsearch client information.\n\nTo run this script, open a terminal, activate the virtual environment, and from src folder of your project, run the following command:\n# ../src/\npython indexer.py\nIf everything went well, you should get an output that looks similar to this (but not properly formatted):\n{\n  \"name\": \"0e1113eb2915\",\n  \"cluster_name\": \"docker-cluster\",\n  \"cluster_uuid\": \"og6mFMqwQtaJiv_3E_q2YQ\",\n  \"version\": {\n    \"number\": \"8.7.0\",\n    \"build_flavor\": \"default\",\n    \"build_type\": \"docker\",\n    \"build_hash\": \"09520b59b6bc1057340b55750186466ea715e30e\",\n    \"build_date\": \"2023-03-27T16:31:09.816451435Z\",\n    \"build_snapshot\": false,\n    \"lucene_version\": \"9.5.0\",\n    \"minimum_wire_compatibility_version\": \"7.17.0\",\n    \"minimum_index_compatibility_version\": \"7.0.0\"\n  },\n  \"tagline\": \"You Know, for Search\"\n}\nNext, let‚Äôs create a simple FastAPI app, to interact with your cluster."
  },
  {
    "objectID": "posts/semantic-search-elasticsearch-openai-langchain.html#create-a-search-application",
    "href": "posts/semantic-search-elasticsearch-openai-langchain.html#create-a-search-application",
    "title": "Semantic Search with Elasticsearch, OpenAI, and LangChain",
    "section": "Create a Search Application",
    "text": "Create a Search Application\nIn this step, you‚Äôll create a simple application to interact with Meditations. You‚Äôll connect to the Elasticsearch cluster, initialize the Retrieval Questioning/Answering Chain, and create an /ask endpoint that lets the user interact with the app.\nTake a look at the code of src/app.py:\nfrom fastapi import FastAPI\nfrom langchain.chains import RetrievalQA\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores import ElasticVectorSearch\n\nfrom config import openai_api_key\n\nembedding = OpenAIEmbeddings(openai_api_key=openai_api_key)\n\ndb = ElasticVectorSearch(\n    elasticsearch_url=\"http://localhost:9200\",\n    index_name=\"elastic-index\",\n    embedding=embedding,\n)\nqa = RetrievalQA.from_chain_type(\n    llm=ChatOpenAI(temperature=0),\n    chain_type=\"stuff\",\n    retriever=db.as_retriever(),\n)\n\napp = FastAPI()\n\n\n@app.get(\"/\")\ndef index():\n    return {\n        \"message\": \"Make a post request to /ask to ask questions about Meditations by Marcus Aurelius\"\n    }\n\n\n@app.post(\"/ask\")\ndef ask(query: str):\n    response = qa.run(query)\n    return {\n        \"response\": response,\n    }\nThis code lets the user ask questions about Marcus Aurelius‚Äô Meditations and provides answers back to the users. Let me show you how it works:\n\nLines 1 to 5 import the required libraries:\n\nFastAPI: This class initializes the app.\nRetrievalQA: This is a chain that allows you to ask questions about documents in a vector database. It finds the most relevant documents based on your question and generates an answer from them.\nChatOpenAI: This is a wrapper around OpenAI‚Äôs chat models.\nOpenAIEmbeddings and ElasticVectorSearch: These are the same wrappers discussed in the previous section.\n\nLine 7 imports the OpenAI secret key.\nLines 9 to 15 initialize the Elasticsearch cluster using OpenAI embeddings.\nLines 16 to 20 initialize the RetrievalQA chain with the following parameters:\n\nllm: Specifies the LLM used to run prompts defined in the chain.\nchain_type: Defines how documents are retrieved and processed from the vector database. By specifying stuff, documents will be retrieved and passed to the chain to answer the question as-is. Alternatively, you can use map_reduce or map_rerank for additional processing before answering the question, but these methods use more API calls. For more information, consult the langchain documentation.\nretriever: Specifies the vector database used by the chain to retrieve documents.\n\nLines 22 to 36 initialize the FastAPI app and define two endpoints. The / endpoint provides users with information on how to use the application. The /ask endpoint takes a user‚Äôs question (query parameter) and returns an answer using the previously initialized chain.\n\nFinally, you can run the app from the terminal (using your virtual environment):\nuvicorn app:app --reload\nThen, visit http://127.0.0.1:8000/docs, and test /ask by making a question about the book:\n\nIf everything went well, you should get something like this:\n\nThat‚Äôs it! You now have your own semantic search service up and running, based on Elasticsearch, OpenAI, Langchain, and FastAPI."
  },
  {
    "objectID": "posts/semantic-search-elasticsearch-openai-langchain.html#going-to-production",
    "href": "posts/semantic-search-elasticsearch-openai-langchain.html#going-to-production",
    "title": "Semantic Search with Elasticsearch, OpenAI, and LangChain",
    "section": "Going to Production",
    "text": "Going to Production\nMoving to production can vary greatly for different software products. However, at a minimum, you should:\n\nBuild appropriate data ingestion and processing pipelines. When adding new data, you generally do not want to rebuild the index, as you did in this tutorial.\nUse a production-ready Elasticsearch-managed instance or a Docker image optimized for production. The instance used in this tutorial was intended for testing and development purposes only.\nChoose a deployment strategy that suits your organization‚Äôs needs. For example, I often use NGINX with Gunicorn and Uvicorn workers for small projects."
  },
  {
    "objectID": "posts/semantic-search-elasticsearch-openai-langchain.html#conclusion",
    "href": "posts/semantic-search-elasticsearch-openai-langchain.html#conclusion",
    "title": "Semantic Search with Elasticsearch, OpenAI, and LangChain",
    "section": "Conclusion",
    "text": "Conclusion\nWay to go! In this tutorial, you‚Äôve learned how to build a semantic search engine using Elasticsearch, OpenAI, and Langchain.\nIn particular, you‚Äôve learned:\n\nHow to structure a semantic search service.\nHow to use LangChain to split and index documents.\nHow to use Elasticsearch as a vector database with LangChain.\nHow to use the Retrieval Questioning/Answering Chain to answer questions with a vector database.\nWhat you should consider when productizing this type of application.\n\nHope you found this tutorial useful. Let me know if you have any questions!\nAll the code for this tutorial is available on GitHub."
  },
  {
    "objectID": "posts/gemini-structured-outputs.html",
    "href": "posts/gemini-structured-outputs.html",
    "title": "The good, the bad, and the ugly of Gemini‚Äôs structured outputs",
    "section": "",
    "text": "For a long time, I didn‚Äôt pay much attention to the idea that structured outputs could have an impact on the performance of LLMs. But after reading Let Me Speak Freely? and .txt‚Äôs rebuttal, I started to question my assumptions.\nI decided to run some benchmarks myself using popular proprietary models, starting with GPT-4o-mini. During my analysis, I found that structured outputs did, in fact, decrease performance on some tasks.\nAfter that, I was curious to see if the same was true for Gemini 1.5 Flash. This time, the answer wasn‚Äôt so straightforward, which is why I decided to write a separate post about it. In the process, I ran into an issue in Gemini‚Äôs Generative AI SDK that can break your application if you‚Äôre not careful.\nIn this article, I‚Äôll share the results from running various benchmarks for Gemini 1.5 Flash comparing structured and unstructured outputs and the learnings I had along the way.\nYou can find the full code to run the benchmarks in this GitHub repository."
  },
  {
    "objectID": "posts/gemini-structured-outputs.html#results-tldr",
    "href": "posts/gemini-structured-outputs.html#results-tldr",
    "title": "The good, the bad, and the ugly of Gemini‚Äôs structured outputs",
    "section": "Results (TLDR)",
    "text": "Results (TLDR)\n\n\n\n\n\n\nNote\n\n\n\nGoogle recently released a new Python SDK for Gemini, which seems to address the automatically sorting keys issue.\n\n\nIf you‚Äôre in a hurry, here are my key findings:\n\nThe good: Overall, Gemini‚Äôs structured outputs performed on par with unstructured outputs.1\nThe bad: This only holds for the less rigid interpretation of ‚Äústructured outputs‚Äù. When testing constrained decoding specifically, Gemini‚Äôs structured outputs performed worse than unstructured outputs.\nThe ugly: Function calling and constrained decoding have a big design flaw. The order of the keys specified in the schema is not preserved when using the Generative AI Python SDK. This will break chain-of-thought reasoning in your applications unless you use a workaround (that only works for constrained decoding).\n\nThe figure below shows the overall results for Gemini 1.5 Flash:\n\n\n\n\n                                                \n\n\nFigure¬†1: Best results for Gemini 1.5 Flash.\n\n\n\n\nThe figure above compares the performance of Gemini‚Äôs structured outputs to unstructured outputs. NL stands for Natural Language, which means the model writes the output in a free-form manner. In contrast, JSON-Prompt and JSON-Schema involve structured outputs that follow a predefined JSON schema.\nFor JSON-Prompt, the JSON schema is included in the prompt, instructing the model to generate JSON formatted output based on its MIME type configuration. JSON-Schema works similarly, but the schema is set directly in the model‚Äôs configuration instead of being included in the prompt.\nWhen considering both JSON-Prompt and JSON-Schema, Gemini‚Äôs structured outputs performed comparably to unstructured outputs. However, with JSON-Schema alone (i.e., constrained decoding), performance drops compared to unstructured outputs. This difference is most evident in the Shuffled Objects task, where NL achieved a score of 97.15%, while JSON-Schema scored 86.18%."
  },
  {
    "objectID": "posts/gemini-structured-outputs.html#structured-outputs-101",
    "href": "posts/gemini-structured-outputs.html#structured-outputs-101",
    "title": "The good, the bad, and the ugly of Gemini‚Äôs structured outputs",
    "section": "Structured outputs 101",
    "text": "Structured outputs 101\nIn case you‚Äôre not familiar with the concept, structured outputs are a group of methods that ‚Äúensure that the model outputs adhere to a specific structure‚Äù. In Open weight models, a structure can mean anything from a JSON schema to a specific regex pattern. With proprietary models, it usually means a JSON schema.\nIn its less rigid interpretation, this includes any method that can generate LLM outputs adhering to a structured format, such as prompting, function calling, JSON mode, or constrained decoding.\nIn its more rigid interpretation, this includes only constrained decoding, as it is the only method that will guarantee the output adheres to the schema you provide."
  },
  {
    "objectID": "posts/gemini-structured-outputs.html#study-design",
    "href": "posts/gemini-structured-outputs.html#study-design",
    "title": "The good, the bad, and the ugly of Gemini‚Äôs structured outputs",
    "section": "Study design",
    "text": "Study design\nIn Let Me Speak Freely?, Tam et al.¬†evaluated structured and unstructured outputs across three reasoning tasks and six classification tasks. They used exact match to evaluate the reasoning tasks and accuracy to evaluate the classification tasks. They ran the experiments using the following models:\n\nProprietary models: gpt-3.5-turbo-0125, claude-3-haiku-20240307, gemini-1.5-flash, and gpt-4o-mini-2024-07-18.\nOpen-weight models: LLaMA3-8B-Instruct, and Gemma-2-9B-Instruct.\n\nFor this article, I just focused on Gemini 1.5 Flash and the reasoning tasks. I already ran the benchmarks for GPT-4o-mini and Llama3-8B-Instruct in my previous post.\nI excluded the classification tasks because I believe structured outputs perform better in classification tasks, and this is also in line with the study‚Äôs original findings. So I just focused on the three reasoning tasks:\n\nGSM8K: A dataset from of grade school math word problems.\nLast Letter: A dataset of simple word puzzles that require concatening the last letters of a list of names.\nShuffled Objects: A dataset that requires reasoning about the state of a system after a sequence of shuffling operations.\n\nThe rest of the article details the process of re-evaluating these benchmarks using Gemini-1.5-Flash."
  },
  {
    "objectID": "posts/gemini-structured-outputs.html#structured-outputs-with-gemini",
    "href": "posts/gemini-structured-outputs.html#structured-outputs-with-gemini",
    "title": "The good, the bad, and the ugly of Gemini‚Äôs structured outputs",
    "section": "Structured outputs with Gemini",
    "text": "Structured outputs with Gemini\nGemini has three ways of generating structured outputs:\n\nForced function calling (FC): You force the model to call a ‚Äúfunction‚Äù and that makes the model generate a JSON schema that matches the function‚Äôs arguments.\nSchema in prompt (JSON-Prompt): You include a JSON schema in the prompt, specify mime_type='application/json' and the model generates a response that matches the schema.\nSchema in model configuration (JSON-Schema): You provide a JSON schema in the model configuration, specify mime_type='application/json' in the request and the model generates a response that matches the schema. This is the only method that seems to use controlled generation.\n\nI‚Äôve included JSON-Prompt and JSON-Schema in the analysis, but had to exclude FC because it‚Äôs not possible to use it for chain-of-thought reasoning, which is a requirement for the benchmarks."
  },
  {
    "objectID": "posts/gemini-structured-outputs.html#issues-with-geminis-structured-outputs",
    "href": "posts/gemini-structured-outputs.html#issues-with-geminis-structured-outputs",
    "title": "The good, the bad, and the ugly of Gemini‚Äôs structured outputs",
    "section": "Issues with Gemini‚Äôs structured outputs",
    "text": "Issues with Gemini‚Äôs structured outputs\nWhen running the three benchmarks, I quickly ran into a performance issue with FC and JSON-Schema. In all tasks, both showed double-digit performance drops compared to NL.\nThis didn‚Äôt make a lot of sense, so I started investigating.\nI was using the following response schema for all structured outputs:\nclass Response(BaseModel):\n    reasoning: str\n    answer: str\nThe prompts were similar to the one below, adjusted according to the task:\n\nYou are an expert in solving grade school math tasks. You will be presented with a grade-school math word problem and be asked to solve it.\nYou will always respond with JSON matching the following schema:\n[RESPONSE_SCHEMA]\nFirst, provide your step by step reasoning in the ‚Äúreasoning‚Äù field. Then, in the ‚Äúanswer‚Äù field, provide your final answer. Don‚Äôt include any other text in the ‚Äúanswer‚Äù field.\n\nI eventually realized that the performance drop in JSON-Schema was due to the keys in the schema being reversed when generating the response. I then noticed that Tam et al.¬†briefly mentioned in Let Me Speak Freely? that JSON-Schema responses failed to produce valid JSON due to this exact problem, so they did not include it in their analysis.\nI didn‚Äôt want to exclude it from the analysis, so I started looking for a way to control the order of the keys in the schema. I found that the order of the keys in the schema gets sorted alphabetically before the response is generated. To confirm this, I ran a test with 100 randomly generated schemas. Every resulting output had its keys sorted alphabetically, so it‚Äôs clear this is not by chance.\nI also found that the Vertex AI documentation mentions a propertyOrdering parameter, which should allow control over the order of keys. However, this feature doesn‚Äôt appear to work with the Generative AI Python SDK.\nUnable to use the propertyOrdering parameter, I resorted to a quick workaround: I named the keys in a way that forced the desired order alphabetically. Instead of using reasoning and answer, I used reasoning and solution. This preserved the chain-of-thought reasoning in the responses, and resolved the performance drop in JSON-Schema.\nBut FC was a different story. Unlike JSON-Schema, the order of the keys follow a less predictable pattern, and I couldn‚Äôt find a way to control it. So I decided to exclude FC from the analysis."
  },
  {
    "objectID": "posts/gemini-structured-outputs.html#benchmarks-of-gemini-1.5-flash",
    "href": "posts/gemini-structured-outputs.html#benchmarks-of-gemini-1.5-flash",
    "title": "The good, the bad, and the ugly of Gemini‚Äôs structured outputs",
    "section": "Benchmarks of Gemini 1.5 Flash",
    "text": "Benchmarks of Gemini 1.5 Flash\nAfter applying the key ordering workaround, and additional improvements discussed in my previous post, I recomputed the benchmarks.\nThe table below shows the results for Gemini 1.5 Flash compared to the original results from Tam et al.\n\n\n\n\n\n\n\n\n\n\n¬†\n¬†\nNL\nJSON-Prompt\nJSON-Schema\n\n\nTask\nMethod\n¬†\n¬†\n¬†\n\n\n\n\nGSM8K\nTam et al.\n89.33\n89.21\n47.78\n\n\nMe, 0-shot\n93.71\n93.78\n93.03\n\n\nMe, 3-shot\n94.84\n94.16\n93.63\n\n\nLast Letter\nTam et al.\n65.45\n77.02\n0.67\n\n\nMe, 0-shot\n82.67\n80.00\n81.33\n\n\nMe, 3-shot\n80.00\n82.00\n80.67\n\n\nShuffled Obj.\nTam et al.\n58.21\n65.07\nN/A\n\n\nMe, 0-shot\n97.15\n92.28\n86.18\n\n\nMe, 3-shot\n92.68\n98.37\n84.96\n\n\n\n\n\n\nTable¬†1: Results for Gemini 1.5 Flash.\n\n\n\n\nUsing a 0-shot prompt and 3-shot prompts, I was able to improve all the metrics on the tasks and methods Tam et al.¬†used for their benchmarks. Which is great!\nNL and JSON-Prompt are tied, without a clear winner between them. Each method got a slight edge over in 3 out of 6 tasks. On the other hand, JSON-Schema performed worst than NL in 5 out of 6 tasks. Plus, in Shuffled Objects, it did so with a gap of more than 10 percentage points: 97.15% for NL vs.¬†86.18% for JSON-Schema.\nTam et al.¬†defined structured outputs as any method that ‚Äúinvolves providing output in standardized formats like JSON or XML through format restriction.‚Äù which is in line with the less rigid interpretation of structured outputs. Using this definition, the results show no performance gap between structured and unstructured outputs. This directly contradicts the study‚Äôs original claim.\nBut if you take the also common interpretation that constrained decoding is the only form of structured generation, then the study‚Äôs original conclusion still applies: there is indeed a significant performance gap between structured and unstructured outputs.\nWeird, I know. But that‚Äôs the way it is."
  },
  {
    "objectID": "posts/gemini-structured-outputs.html#conclusion",
    "href": "posts/gemini-structured-outputs.html#conclusion",
    "title": "The good, the bad, and the ugly of Gemini‚Äôs structured outputs",
    "section": "Conclusion",
    "text": "Conclusion\nResults are mixed for Gemini 1.5 Flash.\nThe good news is that structured outputs perform on par with unstructured ones. The bad news is that this only holds if you adopt the more flexible definition of ‚Äústructured outputs.‚Äù And the ugly news is that Gemini‚Äôs Generative AI SDK has a major issue in how it handles the order of keys in the provided schema.\nBased on the results, I‚Äôd suggest the following when using Gemini:\n\nAvoid FC for any tasks that require chain-of-thought reasoning.\nDefault to JSON-Prompt over JSON-Schema for reasoning tasks.\n\nFinally, I want to emphasize that I love working with structured outputs. They save a lot of time. But I know there are tasks where they might perform worse (or better!) than unstructured outputs. There‚Äôs not enough evidence to support one or the other, so what I should just run my own evals and decide based on that.\nThat‚Äôs the real takeaway: run your own evals and choose the approach that works best for you. Don‚Äôt blindly trust random posts online.\nYou can find the code to replicate my results in this GitHub repository."
  },
  {
    "objectID": "posts/gemini-structured-outputs.html#footnotes",
    "href": "posts/gemini-structured-outputs.html#footnotes",
    "title": "The good, the bad, and the ugly of Gemini‚Äôs structured outputs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAssuming the less rigid interpretation of ‚Äústructured outputs‚Äù.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/mind-reading-algorithms.html",
    "href": "posts/mind-reading-algorithms.html",
    "title": "Mind-reading Algorithms: An Introduction to Recommender Systems",
    "section": "",
    "text": "Tonight‚Äôs game plan: A hot meal, a warm bed, and a couple of hours of Netflix. Best of all, you are about to make it happen.\nYou put the key in the front door, turn the lock, and push the door open‚Ä¶\n‚Ä¶That last episode from Black Mirror was somewhat disturbing. Maybe I should give a chance to Better Call Saul tonight‚Ä¶\nYou are halfway through the front door‚Ä¶\n‚Ä¶Probably won‚Äôt be as good as Breaking Bad but‚Ä¶ Wait, what‚Äôs that?\nThere is music playing nearby. You step back outside and listen. It‚Äôs coming from that new store next to your building. It is a song you‚Äôve had in your mind for the last couple of days. There is a red carpet laid down in front of the store.¬π\n‚ÄúE-Store,‚Äù you whisper to yourself, reading what‚Äôs on the glowing red neon sign on top of the store. You hadn‚Äôt paid much attention to this place the few times you passed by.\nYou will do a quick inspection of what is on display. After all, a few minutes of window shopping hasn‚Äôt harmed anyone.\nYou enter the store and notice that it is warmly lit. There are various shelves with items and some clerks moving and sorting stuff out. There is a small stage at the center of the store with a couch and a tiny shelf.\nUpon closer examination, you noticed that there are only running shoes of your size and your favorite sports brand on the tiny shelf on the stage. You had been thinking about starting running again.\nIs it time I do something about it?\nYou select a pair of shoes and head to the cashier. On your way there, you spot a shelf with running shirts and pants¬≤. A glance at them reveals that some would make for a great outfit when combined with your new shoes. You‚Äôll take a quick look.\nI cannot go out there looking like an amateur!\nWith your now complete running outfit, you resume your way to the cashier. While handing in the items to the clerk, you notice a poster of your favorite actors running a marathon¬≥. They are wearing almost the same outfit you are about to buy. The only difference is that you are missing the smartwatch they are wearing.\nWhat are the odds?\nThe clerk smiles and points toward a smartwatch on display next to the cashier. You happily abide.\nAfter leaving the store, a few hours later, you start to reflect on your spending spree. Besides the outfit and smartwatch, you bought a pair of sunglasses, 1 kg of protein powder, a gym subscription, and a health plan.\nMaybe that bit of window shopping was not that harmless after all‚Ä¶\nWhat seems like a far-fetched story for brick-and-mortar stores is the bread-and-butter of many internet services. The strategies used by the story‚Äôs E(vil)-store to capture your attention, have an existing digital analogous you can check in the references of this article. Most of these techniques are part of a field referred to as Recommender Systems.\nNowadays, Recommenders are ubiquitous. Chances are that you are reading this article because of a suggestion generated by one. They are responsible for 35% of what users buy on Amazon‚Å¥, 75% of what people watch on Netflix, and 70% of watch time on YouTube‚Åµ.\nThese algorithms are so ingrained in our society that people have even started to get wary of their risks‚Å∂. Anti-vaxxers, flat-earth proponents, and other conspiracy theorists have learned to manipulate these systems to recommended their content at disproportionately high rates. Those who argued for a New Enlightenment Era driven by the internet most likely did not have present-day YouTube in mind.\nSo far, this article is not helping in rehabilitating the Recommendation Systems‚Äô image. But my goal is to focus on their positive side. Recommenders provide us with a valuable service: enable decision-making by decreasing uncertainty over choices. Furthermore, as we will see in the next sections, in a digital world of endless options, this is not an easy task.\nThrough this article I try to share an intuitive idea on the What, Why, and How of Recommenders. I am not aiming to cover implementation nor technical details. So beware if that is what you are looking for.\nIn a nutshell, if you need to explain what is a recommender to your boss, this article might help. Conversely, if you need to build a recommender for your boss, this article might help to distract him while you search for other articles!"
  },
  {
    "objectID": "posts/mind-reading-algorithms.html#recommendation-systems-101",
    "href": "posts/mind-reading-algorithms.html#recommendation-systems-101",
    "title": "Mind-reading Algorithms: An Introduction to Recommender Systems",
    "section": "Recommendation Systems 101",
    "text": "Recommendation Systems 101\nA Recommendation System or a Recommender is a set of techniques used for suggesting users the most suitable items based on their needs. This definition sounds simple, yet it conceals many details.\nIn the context of recommenders, an item is a very malleable idea. It could go from movies or songs in entertainment applications to possible love or mating partners in a dating app. Based on the items‚Äô qualities, the recommender tries to guess which items are the most suitable to suggest to a given user. Thus, if you have a history of watching action movies, it‚Äôs fair to assume that you‚Äôll prefer movies like Fast & Furious than the latest romantic drama on Netflix.\nSuitability is also a subjective matter. From a user perspective, you expect that a recommender provides you with the best possible option for your needs, as fast as possible, and paying the least. On the other hand, a business is trying to make a living, thus the way in which the recommender will provide suggestions will need to reflect that end. One can expect then, that the needs of both the users and businesses will sometimes clash.\nCharles Duhigg popularized a telling example of recommendations going too far. In the book, The Power of Habit, he points out the case of an angry father who found out his teenage daughter was pregnant through a targeted ad. The advertising company, using his daughter‚Äôs purchasing history, thought she could soon need baby clothes and sent coupons for it. The unsuspecting father received the mail and found the coupons. Shortly after complaining to a representative of the company, learn from his daughter that she was indeed pregnant.\nFor this article, I deliberately chose a definition of Recommender Systems that was not limited to software or computer systems. That is because these systems are not technical matters that only big technology companies can build. Moreover, they are not limited to the digital world or even to human affairs.\nHunter-gatherers civilizations needed to recommend to others the best foraging places for their survival. Kings had panels of ministers for suggesting courses of action in essential areas of government. Even in the animal kingdom, ants leave traces behind to suggest to others in the colony the best routes towards food‚Å∑.\nIt‚Äôs in recent times that the use of Recommender Systems has extended to a wide range of digital services. In applications where the number of choices was excessive, it became necessary. The research into such systems started at Duke University in the seventies. Yet, it took two decades for the arrival of the first known software-based Recommender System, Tapestry. It was developed at Xerox Palo Alto Research Center (PARC) and published in the journal Communications of the ACM in 1992‚Å∏.\n\n\n\nXerox PARC researchers\n\n\nXerox PARC researchers during an informal meeting‚Åπ. Probably complaining about all the cat images filling their inboxes.\nUsing Tapestry, the Xerox PARC‚Äôs researchers, tried to handle all the unnecessary documents they were receiving due to the increasing use of electronic mail. This system employed people‚Äôs collaboration to tag documents based on their reactions. Then, those tags were used to create personal filters that reduced the amount of incoming documents per user. For instance, Alice could create filters to only receive documents tagged as Funny by Bob and Joe, and to receive documents tagged as Important from Mary¬π‚Å∞.\nBut how does an algorithm which started as a filter for documents became so rooted in our present-day digital services? That is what we will go through in the next section.\nFor the sake of simplicity, from now on we will focus solely on software-based Recommenders and will refer to them using the broad terms Recommendation Systems, Recommender Systems, and Recommenders.\n\nThe Problems with Small Bookshelves and Infinite Bookshelves\nImagine you are about to open a bookstore in your town. It feels like a terrible idea now that Amazon dominates the market. Even so, nobody will stop your entrepreneurial drive. You‚Äôve already signed a lease for a small but well-located place and are also planning on offering your signature espresso to customers.\nA while ago, you received catalogs of books from a few publishing houses, and today you need to decide which books will fill the shelves. But, as you read through the first catalog, making a decision feels more and more daunting.\nShould I order the latest book by Paulo Coelho?‚Ä¶\nWhat about The Hunger Games series?‚Ä¶\nAnd the recently-published Memories of a Ranch Dresser Expert from my friend Derek¬π¬π?\nShelf space limits how many books you can have at a time. As you probably want to survive over the long-term, it is sensible to focus on the most popular books. Sorry, Derek‚Ä¶\nIn this case, caring for the individual desires of customers is impossible. You don‚Äôt have enough space for so many books. If you want to make money, you need to put on display what you know is on demand. Some clients will not find what they are looking for, but the majority will be happy just by buying the most popular offerings.\nNow, imagine that a few years have gone by. Your strategy is working like a charm. Customers are very happy with your selection of books and signature espresso. So much, that a major bookstore chain recently made a huge offer for your store. They want to name you as CEO to drive their newly established digital strategy.\nFinally, you don‚Äôt have to worry about limited shelf space anymore. The company‚Äôs homepage is an infinite and fully-customizable bookshelf. Also, you have access to an inventory as big as Amazon‚Äôs. You just need to figure out which books to show, out of the fifty million available, to each of the ten million customers you are expecting next month‚Ä¶ Hmm‚Ä¶\nYou could stick to your previous strategy of showing each customer the most popular books on the homepage. Yet, millions of customers will have little interest in what you are showing to them. Besides, you will not exploit your vast inventory‚Äôs potential. The end result could be millions of angry customers and under-performing sales.\nAnother option could be to show all the available books on the homepage. Nonetheless, you are at risk of the Paradox of Choice. Humans, when faced with abundant choices, instead of feeling happier, get irritated, and anxious¬π¬≤. Thus, you might end up with angrier customers and fewer sales.\nIt has been couple of weeks and the Board of Directors are already having second thoughts about your designation.\nDesperate, you step out of the office and into the rainy night. Looking at the skies, you scream, asking for a way out‚Ä¶\nSuddenly, your mobile phone vibrates. You spend a few seconds struggling to unlock your phone under the rain until you can read the notification.\n‚ÄúWondering what to watch next? We suggest Black Mirror: Bandersnatch‚Äù reads on a small banner from Netflix on your dripping-wet screen.\nUgh‚Ä¶ Thanks, but right now is not the best of times‚Ä¶\nOr‚Ä¶ Is it?\nThat is when recommendation systems step in. Usually, in a less dramatic manner.\nThere is a middle road between providing all possible choices to a user or generic choices to all users. It is possible to provide a few but well-thought suggestions to each user by using a recommender.\nFor that, you do not need to care about the popularity of books. You could match each customers‚Äô interests with books‚Äô attributes as genre, length, and author. For instance, you might find that a few customers would react better to The Lightbringer series instead of the Game of Thrones (GoT). In an online bookstore, that is something you can and need to care for. In a physical store, those same customers will most likely need to settle with the GoT series.\nThe difference in which customers demands are met in the online and physical world is referred to as The Long Tail. The figure below is a visual aid to understanding this phenomenon. Each bar on the horizontal axis represents an item. These bars are ordered in a decreasing manner by popularity (represented in the vertical axis).\n\n\n\nThe Long Tail phenomenon\n\n\nThe Long Tail. Physical stores define what they show to users by their shelf space limitations. Online stores use Recommenders to define what to¬†show.\nThe bars to the left of the dotted vertical line are the items that a physical store may display given its space limitations. In contrast, an online store could display the entire range of items: the tail as well as the popular ones¬π¬≥. Recommenders are meant to solve the issue of displaying an excessive number of options in an online context.\nSo far we have seen what recommenders are and the problems they solve. Now we will review what are the different ways Recommenders generate suggestions."
  },
  {
    "objectID": "posts/mind-reading-algorithms.html#fantastic-recommenders-and-where-to-find-them",
    "href": "posts/mind-reading-algorithms.html#fantastic-recommenders-and-where-to-find-them",
    "title": "Mind-reading Algorithms: An Introduction to Recommender Systems",
    "section": "Fantastic Recommenders and Where To Find Them",
    "text": "Fantastic Recommenders and Where To Find Them\nBesides the What and Why of recommenders, it also makes sense to get an idea of how these systems are usually build. For that, we will review the standard six categories of recommenders¬π‚Å¥ and which technological companies have made use of them¬π‚Åµ:\n\nContent-based (CB): recommends items similar to the ones that the user has liked. For identifying the similarity, the recommender uses characteristics or features from the items. For a books‚Äô recommender the algorithm could use genre, author, or book-length as features to recommend similar books. Used by: Facebook, and Amazon\nCollaborative filtering (CF): recommends the user items that other users with similar tastes liked in the past. The reasoning behind CF is that ‚Äútwo or more individuals sharing some similar interests in one area tend to get inclined towards similar items or products from some other areas too.‚Äù Used by: Amazon, Facebook, LinkedIn, and Twitter\nDemographic: recommends items based on the demographic profile of the user. These systems usually segment users following business-specific rules and generate recommendations based on those segments. Used by: eBay\nKnowledge-based: recommends items by matching explicit user‚Äôs needs to items‚Äô features. For instance, you specify the number of bedrooms, floor space, and the website returns a list of the best matches of houses.\nCommunity-based: recommends items using the user‚Äôs friends‚Äô preferences: Tell me who your friends are, and I‚Äôll tell you what you like.\nHybrid: this type of recommender suggests items combining two or more of the previous techniques. A typical case is to combine a collaborative filtering approach with a content-based system. Used by: Amazon, and Netflix\n\nOut of these six types of recommenders, the first two, Content-based and Collaborative Filtering, are the most popular. There is ample material on both available online. Start there, if you would like to dig deeper into recommenders or build one yourself."
  },
  {
    "objectID": "posts/mind-reading-algorithms.html#closing-words",
    "href": "posts/mind-reading-algorithms.html#closing-words",
    "title": "Mind-reading Algorithms: An Introduction to Recommender Systems",
    "section": "Closing Words",
    "text": "Closing Words\nThis article started as a technical introduction to Recommender Systems. Yet, after a bit of research, I noticed there were already hundreds of articles with a similar goal.\nAs I was not very motivated to do the same, I made this Frankenstein article by mixing a bit of narrative and theory. I hope it was useful for understanding Recommenders and maybe gave you a pity laugh.\nWe are now in an era where these algorithms are shaping a significant part of our daily lives. We should care to understand what is behind what we see in our social media feeds, online shopping suggestions, and other digital services. This article tried to fill that gap in an accessible manner.\nI hope you enjoyed the article. Feel free to drop me a note if you have questions or comments."
  },
  {
    "objectID": "posts/mind-reading-algorithms.html#next-steps",
    "href": "posts/mind-reading-algorithms.html#next-steps",
    "title": "Mind-reading Algorithms: An Introduction to Recommender Systems",
    "section": "Next Steps",
    "text": "Next Steps\nFinally, if you want to learn more about Recommenders, I have a couple of suggestions for starting points:\nTheory\n\nMining Massive Datasets, Chapter 9\nRecommender Systems Handbook\n\nApplications\n\nBeginner‚Äôs Recommendation Systems with Python\nRecommendation System Based on PySpark\nBuilding A Collaborative Filtering Recommender System with TensorFlow\n\nDatasets\n\nMovieLens\nTMDB 5000\nRestaurant Data with Consumer Ratings"
  },
  {
    "objectID": "posts/mind-reading-algorithms.html#references",
    "href": "posts/mind-reading-algorithms.html#references",
    "title": "Mind-reading Algorithms: An Introduction to Recommender Systems",
    "section": "References",
    "text": "References\n[1] Mailchimp, What is Retargeting? (2019, date of access)\n[2] R. Reshef, Understanding Collaborative Filtering Approach (2015)\n[3] A. Chandrashekar, F. Amat, J. Basilico and T. Jebara, Netflix‚Äôs Artwork Personalization (2017)\n[4] I. MacKenzie, C. Meyer, and S. Noble, How Retailers Can Keep Up With Consumers (2013)\n[5] A. Rodriguez, YouTube‚Äôs recommendations drive 70% of what we watch (2018)\n[6] G. Chalot, Twitter Thread on YouTube‚Äôs Recommendations (2019)\n[7] R. Sharma, R. Singh, Evolution of Recommender Systems from Ancient Times to Modern Era: A Survey (2016)\n[8] R. Sharma, R. Singh, Evolution of Recommender Systems from Ancient Times to Modern Era: A Survey (2016)\n[9] Computer History, Xerox PARC (2019, date of access)\n[10] Huttner, Joseph, From Tapestry to SVD: A Survey of the Algorithms That Power Recommender Systems (2009)\n[11] The_Curly_Council, This is a profession I can see myself getting into (2013)\n[12] P. Hiebert, The Paradox Of Choice, 10 Years Later (2017)\n[13] J. Leskovec, A. Rajaraman, J. Ullman, Mining Massive Datasets, Chapter 9 (2014)\n[14] F. Ricci, L. Rokach, B. Shapira, Introduction to Recommender Systems Handbook, Chapter 1 (2011)\n[15] R. Sharma, R. Singh, Evolution of Recommender Systems from Ancient Times to Modern Era: A Survey (2016)"
  },
  {
    "objectID": "posts/react-agent-langgraph.html",
    "href": "posts/react-agent-langgraph.html",
    "title": "Building ReAct agents with (and without) LangGraph",
    "section": "",
    "text": "As Large Language Models (LLMs) have become more powerful, I‚Äôve started to see increasing interest from clients in building agents.\nThe problem is that most of the use cases clients have in mind for agents are better suited for agentic workflows. Agents are a good fit for tasks without a predefined path and where the order of steps is not known beforehand. Agentic workflows, on the other hand, are the right choice for tasks where both of these things are known.\nNonetheless, agents are still a good fit for many use cases, such as coding assistants and support agents. You should definitely spend some time learning about them.\nIn this post, I‚Äôll show you how to build a Reasoning and Acting (ReAct) agent with (and without) LangGraph.\nLet‚Äôs start by defining some key concepts."
  },
  {
    "objectID": "posts/react-agent-langgraph.html#what-is-an-agent",
    "href": "posts/react-agent-langgraph.html#what-is-an-agent",
    "title": "Building ReAct agents with (and without) LangGraph",
    "section": "What is an agent?",
    "text": "What is an agent?\nThe biggest players in the ecosystem have converged on similar definitions of what constitutes an ‚Äúagent.‚Äù Anthropic describes them as systems where LLMs ‚Äúdynamically direct their own processes and tool usage,‚Äù while OpenAI calls them ‚Äúsystems that independently accomplish tasks on behalf of users.‚Äù LangChain similarly defines them as systems using an LLM to ‚Äúdecide the control flow of an application.‚Äù\nIn essence, agents are systems that can independently make decisions, use tools, take actions, and pursue a goal without direct human guidance. The most well-known agent implementation are ReAct Agents."
  },
  {
    "objectID": "posts/react-agent-langgraph.html#whats-a-react-agent",
    "href": "posts/react-agent-langgraph.html#whats-a-react-agent",
    "title": "Building ReAct agents with (and without) LangGraph",
    "section": "What‚Äôs a ReAct agent?",
    "text": "What‚Äôs a ReAct agent?\nReAct (Reasoning and Acting) Agents are AI systems that merge the reasoning of Large Language Models (LLMs) with the ability to perform actions. They follow an iterative ‚Äúthink, act, observe‚Äù cycle to solve problems and achieve user goals. For example, a ReAct agent would:\n\nTake a user query.\nThink about the query and decide on an action.\nExecute the action using available tools (environment).\nAnalyzes the result of that action (environment).\nContinues the ‚ÄúReason, Act, Observe‚Äù loop until it reaches the final answer.\n\nHere‚Äôs a diagram of a ReAct agent:\n\n\n\n\n\ngraph LR\n    Human &lt;--&gt; LLM[LLM]\n    LLM --&gt;|Action| Environment\n    Environment --&gt;|Feedback| LLM\n    LLM -.-&gt; Stop\n\n\n\n\n\n\nThe first generation of ReAct agents used a prompt technique of ‚ÄúThought, Action, Observation‚Äù. Current agents rely on function-calling to implement the ‚Äúthink, act, observe‚Äù loop."
  },
  {
    "objectID": "posts/react-agent-langgraph.html#what-is-langgraph",
    "href": "posts/react-agent-langgraph.html#what-is-langgraph",
    "title": "Building ReAct agents with (and without) LangGraph",
    "section": "What is LangGraph?",
    "text": "What is LangGraph?\nLangGraph is a graph-based framework for building complex LLM applications, designed for stateful workflows. It makes it easier to build complex agent architectures.\nGraphs are composed of nodes, edges, state, and reducers. Nodes are the units of work (functions, tools) and edges define the paths between nodes. State is persistent data passed between nodes and updated through reducers (functions that define how the state is updated).\nI like LangGraph because it provides you with easy-to-use components, a simple API, and it lets you visualize your workflow. It also integrates well with LangSmith, a tool for monitoring and debugging LLM applications.\nIn this tutorial, I‚Äôll show you how to build a ReAct agent with (and without) LangGraph. I‚Äôll also use LangChain as a thin wrapper on top of OpenAI models."
  },
  {
    "objectID": "posts/react-agent-langgraph.html#prerequisites",
    "href": "posts/react-agent-langgraph.html#prerequisites",
    "title": "Building ReAct agents with (and without) LangGraph",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo follow this tutorial you‚Äôll need to:\n\nSign up and generate an API key in OpenAI.\nSet the API key as an environment variable called OPENAI_API_KEY.\nCreate a virtual environment in Python and install the requirements:\n\npython -m venv venv\nsource venv/bin/activate\npip install langchain langchain-openai langchain-community langgraph jupyter\nOnce you‚Äôve completed the steps above, you can run the code from this article. You can also download the notebook from here."
  },
  {
    "objectID": "posts/react-agent-langgraph.html#implementation",
    "href": "posts/react-agent-langgraph.html#implementation",
    "title": "Building ReAct agents with (and without) LangGraph",
    "section": "Implementation",
    "text": "Implementation\nAs usual, you must start by importing the necessary libraries and loading the environment variables. You‚Äôll use the same model in all the examples, so you‚Äôll define it once here:\n\nfrom typing import Literal\n\nfrom dotenv import load_dotenv\nfrom IPython.display import Image, display\nfrom langchain_core.messages import HumanMessage, SystemMessage, ToolMessage\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import END, START, MessagesState, StateGraph\n\nload_dotenv()\n\nmodel = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n\nThis code:\n\nImports the necessary libraries.\nLoads environment variables from a .env file using load_dotenv().\nDefines a model (gpt-4.1-mini) that you‚Äôll use in all the examples.\n\n\nVanilla ReAct agent\nYou‚Äôll build an agent that takes a question from a user and has access to a a tool. The tool is a Python REPL that it can use to answer the question.\nYou should not use this in production. This tool can run arbitrary Python code on your device, and that‚Äôs not something you want to expose to random people on the internet.\nFirst, let‚Äôs define the run_python_code tool.\n\n@tool\ndef run_python_code(code: str) -&gt; str:\n    \"\"\"Run arbitrary Python code including imports, assignments, and statements. Do not use any external libraries. Save your results as a variable.\n\n    Args:\n        code: Python code to run\n    \"\"\"\n    import sys\n    from io import StringIO\n\n    old_stdout = sys.stdout\n    sys.stdout = captured_output = StringIO()\n\n    namespace = {}\n\n    try:\n        exec(code, namespace)\n\n        output = captured_output.getvalue()\n\n        if not output.strip():\n            user_vars = {\n                k: v\n                for k, v in namespace.items()\n                if not k.startswith(\"__\") and k not in [\"StringIO\", \"sys\"]\n            }\n            if user_vars:\n                if len(user_vars) == 1:\n                    output = str(list(user_vars.values())[0])\n                else:\n                    output = str(user_vars)\n\n        return output.strip() if output.strip() else \"Code executed successfully\"\n\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n    finally:\n        sys.stdout = old_stdout\n\n\ntools = [run_python_code]\ntools_mapping = {tool.name: tool for tool in tools}\nmodel_with_tools = model.bind_tools(tools)\n\nThis code defines a tool. In LangChain tools are defined as functions decorated with @tool. These functions must have a docstring because it will be used to describe the tool to the LLM.\nrun_python_code is a function that takes a code string and returns the result of executing that code.\nNext, you provide the model with a mapping of the tool to its name by creating tools_mapping. This is often a point of confusion. The LLM doesn‚Äôt run the tools on its own. It only decides if a tool should be used. Then, your own code must make the actual tool call.\nThe mapping is more useful when there are multiple tools, and not a single tool, like in this case. However, I‚Äôm showing it here to illustrate how you‚Äôd usually do this in a real-world application.\nFinally, you bind the tool to the model. The binding makes the model aware of the tool, so that it can use it.\nThen, let‚Äôs define a function that encapsulates the logic of the agent.\n\ndef run_agent(question: str):\n    messages = [\n        SystemMessage(\n            \"You're a helpful assistant. Use the tools provided when relevant.\"\n        ),\n        HumanMessage(question),\n    ]\n    ai_message = model_with_tools.invoke(messages)\n    messages.append(ai_message)\n\n    while ai_message.tool_calls:\n        for tool_call in ai_message.tool_calls:\n            selected_tool = tools_mapping[tool_call[\"name\"]]\n            tool_msg = selected_tool.invoke(tool_call)\n            messages.append(tool_msg)\n        ai_message = model_with_tools.invoke(messages)\n        messages.append(ai_message)\n\n    return messages\n\nThis function takes a question from a user, comes up with a python script, uses run_python_code to execute it and returns the result.\nIt works as follows:\n\nLines 2 to 9 set up the prompts and call the assistant.\nLines 11 to 17 is where the magic happens. This is a loop that will check if there‚Äôs been a tool call in the response from the model. If there is, it will call (invoke) the tool and add the result to the messages. It will then send the results back to the assistant, and repeat this process until there are no more tool calls.\n\nThis is the core idea behind how agents work. You provide the assistant with a question and one or more tools. Then you let the assistant decide which tool to use, until it has all the information it needs to answer the question.\nYou can try it out by running the following code:\n\nmessages = run_agent(\"Generate 10 random numbers between 1 and 100\")\n\nfor m in messages:\n    m.pretty_print()\n\n\n================================ System Message ================================\n\n\n\nYou're a helpful assistant. Use the tools provided when relevant.\n\n================================ Human Message =================================\n\n\n\nGenerate 10 random numbers between 1 and 100\n\n================================== Ai Message ==================================\n\nTool Calls:\n\n  run_python_code (call_twMJ1JkZK81ofm3842P3jNtb)\n\n Call ID: call_twMJ1JkZK81ofm3842P3jNtb\n\n  Args:\n\n    code: import random\n\nrandom_numbers = [random.randint(1, 100) for _ in range(10)]\n\nrandom_numbers\n\n================================= Tool Message =================================\n\nName: run_python_code\n\n\n\n{'random': &lt;module 'random' from '/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/random.py'&gt;, 'random_numbers': [74, 75, 58, 19, 90, 45, 44, 52, 90, 33]}\n\n================================== Ai Message ==================================\n\n\n\nHere are 10 random numbers between 1 and 100: 74, 75, 58, 19, 90, 45, 44, 52, 90, 33.\n\n\n\n\nYou can see that the agent took the user‚Äôs request, used the run_python_code tool to generate the numbers, and then returned the result.\nNow, let‚Äôs see how you can build a ReAct agent with LangGraph.\n\n\nLangGraph ReAct agent\nLike, in the previous example, you start by defining the tools you want to use. You‚Äôll use the same run_python_code tool.\n\n@tool\ndef run_python_code(code: str) -&gt; str:\n    \"\"\"Run arbitrary Python code including imports, assignments, and statements. Do not use any external libraries. Save your results as a variable.\n\n    Args:\n        code: Python code to run\n    \"\"\"\n    import sys\n    from io import StringIO\n\n    old_stdout = sys.stdout\n    sys.stdout = captured_output = StringIO()\n\n    namespace = {}\n\n    try:\n        exec(code, namespace)\n\n        output = captured_output.getvalue()\n\n        if not output.strip():\n            user_vars = {\n                k: v\n                for k, v in namespace.items()\n                if not k.startswith(\"__\") and k not in [\"StringIO\", \"sys\"]\n            }\n            if user_vars:\n                if len(user_vars) == 1:\n                    output = str(list(user_vars.values())[0])\n                else:\n                    output = str(user_vars)\n\n        return output.strip() if output.strip() else \"Code executed successfully\"\n\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n    finally:\n        sys.stdout = old_stdout\n\n\ntools = [run_python_code]\ntools_by_name = {tool.name: tool for tool in tools}\nmodel_with_tools = model.bind_tools(tools)\n\nWith langgraph you also need to you define the tools in the same way: set up the functions, add @tool, and create the tool mapping. Then, bind the tools to the model.\nNext, you need to define the functions (nodes) that correspond to the steps the agent will take:\n\ndef call_llm(state: MessagesState):\n    messages = [\n        SystemMessage(content=\"You are a helpful assistant that can run python code.\"),\n    ] + state[\"messages\"]\n    return {\"messages\": [model_with_tools.invoke(messages)]}\n\n\ndef call_tool(state: MessagesState):\n    result = []\n    for tool_call in state[\"messages\"][-1].tool_calls:\n        tool = tools_by_name[tool_call[\"name\"]]\n        observation = tool.invoke(tool_call[\"args\"])\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n    return {\"messages\": result}\n\n\ndef should_continue(state: MessagesState) -&gt; Literal[\"environment\", END]:\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    if last_message.tool_calls:\n        return \"Action\"\n    return END\n\nThis is how it works:\n\ncall_llm: Sends the conversation history to the LLM to get the next response.\ncall_tool: If the LLM‚Äôs response is a request to use a tool, this function executes the tool with the specified arguments.\nshould_continue: This is the control logic. It checks the LLM‚Äôs last message. If it‚Äôs a tool request, it routes to the call_tool; otherwise, it ends the conversation.\n\ncall_llm and call_tool take MessagesState as input and return a the message key with the new message. This updates the messages key in the MessagesState. should_continue takes MessagesState act as a router, so it decides if a tool should be executed or if the conversation should end.\n\nagent_builder = StateGraph(MessagesState)\n\nagent_builder.add_node(\"llm\", call_llm)\nagent_builder.add_node(\"environment\", call_tool)\n\nagent_builder.add_edge(START, \"llm\")\nagent_builder.add_conditional_edges(\n    \"llm\",\n    should_continue,\n    {\n        \"Action\": \"environment\",\n        END: END,\n    },\n)\nagent_builder.add_edge(\"environment\", \"llm\")\n\nagent = agent_builder.compile()\n\nThis code sets up the logic of the agent. It starts with a call to the LLM. The LLM then decides whether to use a tool or to finish the task. If it uses a tool, the tool‚Äôs output is sent back to the LLM for the next step. This ‚Äúthink-act‚Äù loop continues until the agent decides the task is complete.\nYou can use LangGraph to visualize the agent‚Äôs flow:\n\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n\n\n\n\n\n\n\n\nFinally, you can run the agent by invoking the graph. For that, you‚Äôll need to pass a list of messages to the graph.\n\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant that can run python code.\"),\n    HumanMessage(content=\"Generate 10 random numbers between 1 and 100\"),\n]\n\nmessages = agent.invoke({\"messages\": messages})\n\nYou can review the process by printing the messages:\n\nfor m in messages[\"messages\"]:\n    m.pretty_print()\n\n\n================================ System Message ================================\n\n\n\nYou are a helpful assistant that can run python code.\n\n================================ Human Message =================================\n\n\n\nGenerate 10 random numbers between 1 and 100\n\n================================== Ai Message ==================================\n\nTool Calls:\n\n  run_python_code (call_bsBRC5aL7kgHjeGHXaLR85TC)\n\n Call ID: call_bsBRC5aL7kgHjeGHXaLR85TC\n\n  Args:\n\n    code: import random\n\nrandom_numbers = [random.randint(1, 100) for _ in range(10)]\n\nrandom_numbers\n\n================================= Tool Message =================================\n\n\n\n{'random': &lt;module 'random' from '/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/random.py'&gt;, 'random_numbers': [77, 37, 97, 26, 22, 29, 58, 82, 17, 80]}\n\n================================== Ai Message ==================================\n\n\n\nHere are 10 random numbers between 1 and 100: 77, 37, 97, 26, 22, 29, 58, 82, 17, 80.\n\n\n\n\nThat‚Äôs all!"
  },
  {
    "objectID": "posts/react-agent-langgraph.html#conclusion",
    "href": "posts/react-agent-langgraph.html#conclusion",
    "title": "Building ReAct agents with (and without) LangGraph",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tutorial, you‚Äôve learned what agents are, how they work, and how to build a simple ReAct agent with and without LangGraph. We covered:\n\nAgent fundamentals: How agents differ from agentic workflows by dynamically directing their own processes\nReAct pattern: The core ‚Äúthink, act, observe‚Äù loop that enables agents to take actions based on the information they have\nVanilla and LangGraph implementation: Understanding how to implement agents with and without LangGraph\n\nAgents are great for open-ended tasks where the path isn‚Äôt predetermined. If you‚Äôre working on one of those, this article provides a good starting point. On the other hand, for tasks that have predefined steps, consider agentic workflows instead.\nHope you find this tutorial useful. If you have any questions, let me know in the comments below."
  },
  {
    "objectID": "posts/text-classification-using-python-and-scikit-learn.html",
    "href": "posts/text-classification-using-python-and-scikit-learn.html",
    "title": "Text Classification Using Python and Scikit-learn",
    "section": "",
    "text": "Text classification is the task of automatically assigning labels to pieces of text, such as articles, blog posts, or reviews. Many businesses use text classification algorithms to save time and money by reducing the amount of manual labor needed to organize and analyze their text data.\nThese algorithms are extremely powerful tools when used correctly. Text classification models keep your email free of spam, assist authors in detecting plagiarism, and help your grammar checker understand the various parts of speech.\nIf you want to build a text classifier, you have many options to choose from. You can use traditional methods such as bag of words, advanced methods like Word2Vec, or cutting-edge approaches like BERT or GPT-3.\nBut if your goal is to get something up and running quickly and at no cost, you should build your text classification model with Python and Scikit-learn. I‚Äôll show you how in this tutorial.\nSo let‚Äôs get started!"
  },
  {
    "objectID": "posts/text-classification-using-python-and-scikit-learn.html#prerequisites",
    "href": "posts/text-classification-using-python-and-scikit-learn.html#prerequisites",
    "title": "Text Classification Using Python and Scikit-learn",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore we start, you need to install some libraries. The best way to do that is to create a new virtual environment and install the packages there.\nIf you‚Äôre using venv, run these commands:\npython3 -m venv .textcl\nsource .textcl/bin/activate\npython3 -m pip install pandas==1.4.3 notebook==6.3.0 numpy==1.23.2 scikit-learn==1.1.2\nIf you‚Äôre using conda, this is how you do it:\nconda create --name textcl\nconda activate textcl\nconda install pandas==1.4.3 notebook==6.3.0 numpy==1.23.2 scikit-learn==1.1.2\nThat‚Äôs it! These commands will create a virtual environment, activate it, and install the required packages.\nFinally, start a Jupyter Notebook session by executing jupyter notebook on the same terminal where you ran the previous commands and create a new notebook."
  },
  {
    "objectID": "posts/text-classification-using-python-and-scikit-learn.html#import-the-required-libraries",
    "href": "posts/text-classification-using-python-and-scikit-learn.html#import-the-required-libraries",
    "title": "Text Classification Using Python and Scikit-learn",
    "section": "Import the Required Libraries",
    "text": "Import the Required Libraries\nThe first step, as always, is to import the necessary libraries. Create a new cell in your notebook, paste the following code in it, and run the cell:\nimport joblib\nimport re\nimport string\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, classification_report\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nThis code will import the required libraries. Here‚Äôs why you need them:\n\nYou import joblib to save your model artifacts.\nYou import re and string to process the text.\nYou import numpy and pandas to read and transform the data.\nYou import multiple components of scikit-learn to extract features from the text, calculate the evaluation metrics, split the data into a training and a test set, and use the Multinomial Naive Bayes algorithm.\n\nNext, you‚Äôll read and process the data."
  },
  {
    "objectID": "posts/text-classification-using-python-and-scikit-learn.html#read-the-data",
    "href": "posts/text-classification-using-python-and-scikit-learn.html#read-the-data",
    "title": "Text Classification Using Python and Scikit-learn",
    "section": "Read the Data",
    "text": "Read the Data\nStart by reading the data. You‚Äôll use a dataset included in scikit-learn called 20 newsgroups. This dataset consists of roughly 20,000 newsgroup documents, split into 20 categories. For the sake of simplicity, you‚Äôll only use five of those categories in this example.\nCreate a new cell and paste this code to read the data:\ncategories = [\n    \"alt.atheism\",\n    \"misc.forsale\",\n    \"sci.space\",\n    \"soc.religion.christian\",\n    \"talk.politics.guns\",\n]\n\nnews_group_data = fetch_20newsgroups(\n    subset=\"all\", remove=(\"headers\", \"footers\", \"quotes\"), categories=categories\n)\n\ndf = pd.DataFrame(\n    dict(\n        text=news_group_data[\"data\"],\n        target=news_group_data[\"target\"]\n    )\n)\ndf[\"target\"] = df.target.map(lambda x: categories[x])\nThis code reads the 20 newsgroups dataset. Here‚Äôs how it works:\n\nLines 1 to 7: Define a list of categories, which are the different newsgroup categories used in the analysis: alt.atheism, misc.forsale, sci.space, soc.religion.christian, and talk.politics.guns.\nLines 9 to 11: Use fetch_20newsgroupsto get the data from the 20 newsgroups dataset. This function removes the headers, footers, and quotes from the data and only gets data from the categories specified in the categories list.\nLines 13 and 19: Create a data frame from the data fetched. The data frame has two columns, one for the text of the newsgroup post and one for the category (target) of the newsgroup. You change the target column to display the actual category name instead of a number.\n\nThat‚Äôs it. Now, you‚Äôll do a bit of cleaning of the data."
  },
  {
    "objectID": "posts/text-classification-using-python-and-scikit-learn.html#prepare-the-data",
    "href": "posts/text-classification-using-python-and-scikit-learn.html#prepare-the-data",
    "title": "Text Classification Using Python and Scikit-learn",
    "section": "Prepare the Data",
    "text": "Prepare the Data\nBefore you build your text classification model, you need to prepare the data. You‚Äôll do it in three steps: clean the text column, create the training and testing splits, and generating bag of words features from the documents.\n\nClean the Text Column\nUse this code to clean the text. It‚Äôll remove the punctuation marks and multiple adjacent spaces:\ndef process_text(text):\n    text = str(text).lower()\n    text = re.sub(\n        f\"[{re.escape(string.punctuation)}]\", \" \", text\n    )\n    text = \" \".join(text.split())\n    return text\n\ndf[\"clean_text\"] = df.text.map(process_text)\nThis code lowercases the text and removes any punctuation marks or duplicated spaces, and stores the results in a new column called clean_text. For that, you use process_text, which takes a string as input, lowercases it, replaces all punctuation marks with spaces, and removes the duplicated spaces.\n\n\nSplit the Data Into Train and Test Sets\nNext, you‚Äôll split the dataset into a training and a testing set:\ndf_train, df_test = train_test_split(df, test_size=0.20, stratify=df.target)\ntrain_test_split is used to split a dataset into training and testing sets. You provide the data frame you wish to split to the function and specify the following parameters:\n\ntest_size=0.20: this defines the size of the test set to 20% of the total.\nstratify=df.target: ensures that the training and testing sets are split in a stratified manner using target. This is important because it prevents bias.\n\nNext, you‚Äôll use these datasets to train and evaluate your model.\n\n\nCreate Bag of Words Features\nMachine Learning models cannot handle text features directly. To train a model, you first need to turn your text into numerical features. One popular approach to do that is called bag of words, and that‚Äôs what you‚Äôll use in this example.\nIn the bag of words approach, each document is represented as a row in a matrix, with each word or token appearing in the document represented by a column.\nFor example, consider these two sentences:\n\nI like reading books\nI do not like cooking\n\nThe simplest bag of words representation for these two sentences will look like this:\n\n\n\nid_doc\nI\nlike\nreading\nbooks\ndo\nnot\ncooking\n\n\n\n\n1\n1\n1\n1\n1\n0\n0\n0\n\n\n2\n1\n1\n0\n0\n1\n1\n1\n\n\n\nOnce you have this numerical representation, you can pass this dataset to your machine learning model. This is what you‚Äôll do with the documents in the 20 newsgroup dataset. Keep in mind that because the dataset has so many documents, you‚Äôll end up with a matrix with many more columns than the example above.\nTo create a bag of words representation in scikit-learn , you must use CountVectorizer. You can use this code:\nvec = CountVectorizer(\n    ngram_range=(1, 3),\n    stop_words=\"english\",\n)\n\nX_train = vec.fit_transform(df_train.clean_text)\nX_test = vec.transform(df_test.clean_text)\n\ny_train = df_train.target\ny_test = df_test.target\nCountVectorizer turns text into numerical features. Here‚Äôs what‚Äôs happening in the code above:\n\nLines 1 to 4: You use CountVectorizer to build a bag of words representation of clean_text. You specify two parameters: ngram_range and stop_words. ngram_range is the range of n-grams that the function will use. An n-gram is a sequence of n words. (1, 3) means that the function will use sequences of 1, 2, and 3 words to generate the counts. stop_words is a list of words that the function will ignore. In this case, the list ‚Äúenglish‚Äù means that the function will ignore the most common words in English.\nLines 6 and 7: You generate the matrices of token counts for your training and testing set and save them into X_train and X_test.\nLines 9 and 10: You save the response variable from the training and testing set into y_train and y_test.\n\nNext, you‚Äôll train your text classification model."
  },
  {
    "objectID": "posts/text-classification-using-python-and-scikit-learn.html#train-and-evaluate-the-model",
    "href": "posts/text-classification-using-python-and-scikit-learn.html#train-and-evaluate-the-model",
    "title": "Text Classification Using Python and Scikit-learn",
    "section": "Train and Evaluate the Model",
    "text": "Train and Evaluate the Model\nFinally, you can train the model by running this code:\nnb = MultinomialNB()\nnb.fit(X_train, y_train)\n\npreds = nb.predict(X_test)\nprint(classification_report(y_test, preds))\nIn lines 1 and 2, you train a Multinomial Naive Bayes model. This simple probabilistic model is commonly used in cases with discrete features such as word counts.\nThen, in lines 4 and 5, you evaluate the model‚Äôs results by computing the precision, recall, and f1 scores.\nAfter you run the code, you‚Äôll get an output that will look something like this:\nYou‚Äôve trained the model and obtained the relevant evaluation metrics. You achieved an 0.83 f1 score, which is not bad!\nNext, you‚Äôll learn how to save and load your model so that you can use it for inference."
  },
  {
    "objectID": "posts/text-classification-using-python-and-scikit-learn.html#saving-and-loading-the-model",
    "href": "posts/text-classification-using-python-and-scikit-learn.html#saving-and-loading-the-model",
    "title": "Text Classification Using Python and Scikit-learn",
    "section": "Saving and Loading the Model",
    "text": "Saving and Loading the Model\nIf you‚Äôd like to save the model for later, then you can use joblib. You‚Äôll need to save all the artifacts required to run the model, which in this case would be the vectorizer vec and the model nb.\nYou can use the following code to save your model artifacts:\njoblib.dump(nb, \"nb.joblib\")\njoblib.dump(vec, \"vec.joblib\")\nIf you want to reuse your model later, simply read it and use it to classify new data samples as follows:\nnb_saved = joblib.load(\"nb.joblib\")\nvec_saved = joblib.load(\"vec.joblib\")\n\nsample_text = [\"Space, Stars, Planets and Astronomy!\"]\n\n# Process the text in the same way you did when you trained it!\nclean_sample_text = process_text(sample_text)\nsample_vec = vec_saved.transform(sample_text)\nnb_saved.predict(sample_vec)\nThe code above will read the previously saved artifacts into nb_saved and vec_saved. Then you can apply them to new samples of text you‚Äôd like to classify.\nThat‚Äôs all! You‚Äôve learned how to use Python and Scikit-learn to train a text classification model. Let me offer some suggestions for what you could do next."
  },
  {
    "objectID": "posts/text-classification-using-python-and-scikit-learn.html#next-steps",
    "href": "posts/text-classification-using-python-and-scikit-learn.html#next-steps",
    "title": "Text Classification Using Python and Scikit-learn",
    "section": "Next Steps",
    "text": "Next Steps\nIf you want to take your modeling skills to the next level, here are some ideas to explore:\n\nUsing Cross-validation to ensure that your results generalize well.\nGet the best out of your model by tuning your model‚Äôs hyperparameters.\nUsing scikit-learn‚Äôs pipelines to generate fewer artifacts and simplify deployment.\n\nAlso, given the latest advances in Natural Language Processing (NLP), transformer-based approaches are becoming the go-to options for many problems that use text features. A good starting point is huggingface‚Äôs NLP course."
  },
  {
    "objectID": "posts/text-classification-using-python-and-scikit-learn.html#conclusion",
    "href": "posts/text-classification-using-python-and-scikit-learn.html#conclusion",
    "title": "Text Classification Using Python and Scikit-learn",
    "section": "Conclusion",
    "text": "Conclusion\nIn Machine Learning, text classification is the task of labeling pieces of text through automated methods. This tutorial showed you how to build your first text classification model using Python and Scikit-learn.\nYou‚Äôve learned:\n\nHow to clean text data and create features for your model.\nHow to train a text classification model and generate evaluation metrics.\nHow to save and load your model for future use.\n\nIf you have any questions or feedback, let me know in the comments!\nAll the code for this tutorial is available on GitHub."
  },
  {
    "objectID": "posts/elasticseach-python.html",
    "href": "posts/elasticseach-python.html",
    "title": "How to Use Elasticsearch in Python",
    "section": "",
    "text": "Elasticsearch (ES) is a technology used by many companies, including GitHub, Uber, and Facebook. It‚Äôs not often taught in Data Science courses, but it‚Äôs something you‚Äôll likely come across in your career.\nMany data scientists have trouble setting up a local environment or understanding how to interact with Elasticsearch in Python. Furthermore, there aren‚Äôt many up-to-date resources.\nThat‚Äôs why I decided to create this tutorial. It will teach you the basics, and you will be able to set up an Elasticsearch cluster on your machine for local development in no time. You‚Äôll also learn how to create an index, store data in it, and use it to search your data.\nLet‚Äôs get started!"
  },
  {
    "objectID": "posts/elasticseach-python.html#whats-elasticsearch",
    "href": "posts/elasticseach-python.html#whats-elasticsearch",
    "title": "How to Use Elasticsearch in Python",
    "section": "What‚Äôs Elasticsearch?",
    "text": "What‚Äôs Elasticsearch?\nElasticsearch is a distributed, fast, and easy-to-scale search engine capable of handling textual, numerical, geospatial, structured, and unstructured data. It‚Äôs a popular search engine for apps, websites, and log analytics. It is also a key component of the Elastic Stack (also known as ELK), which includes Logstash and Kibana.\nTo understand the inner workings of Elasticsearch, think of it as two distinct processes. One is ingestion, which normalizes and enriches the raw data before indexing using an inverted index. The second is retrieval, which enables users to retrieve data by writing queries that run against the index.\nThat‚Äôs all you need to know for now. Next, you‚Äôll prepare your local environment to run an ES cluster."
  },
  {
    "objectID": "posts/elasticseach-python.html#prerequisites",
    "href": "posts/elasticseach-python.html#prerequisites",
    "title": "How to Use Elasticsearch in Python",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou need to set up a few things before you get started. Make sure you have these covered, and I‚Äôll see you in the next section:\n\nInstall docker.\nDownload the necessary data.\nCreate a virtual environment and install the required packages. If you like venv, you can run these commands:\n\npython3 -m venv .venv\nsource .venv/bin/activate\npython3 -m pip install pandas==1.4.3 notebook==6.3.0 elasticsearch==8.7.0\nAll good? Let‚Äôs continue."
  },
  {
    "objectID": "posts/elasticseach-python.html#create-a-local-elasticsearch-cluster",
    "href": "posts/elasticseach-python.html#create-a-local-elasticsearch-cluster",
    "title": "How to Use Elasticsearch in Python",
    "section": "Create a Local Elasticsearch Cluster",
    "text": "Create a Local Elasticsearch Cluster\nThe easiest way to run Elasticsearch locally is by using docker.\nOpen a terminal and run this code to start a single-node ES cluster you can use for local development:\ndocker run --rm -p 9200:9200 -p 9300:9300 -e \"xpack.security.enabled=false\" -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:8.7.0\nOnce you run this command, you‚Äôll see a lot of text on your terminal. But don‚Äôt worry, that‚Äôs fine!\nThis command will start an Elasticsearch cluster in your machine. There are a few things to unpack here:\n\ndocker run: It‚Äôs the command you use to run an image inside -a container.\n--rm: This parameter lets Docker know to clean up the container and remove the file system when the container exits.\n-p 9200:9200 -p 9300:9300 : This tells Docker which ports to open on the container‚Äôs network interface.\n-e \"xpack.security.enabled=false\": This tells Docker to start with the security features disabled. This parameter should be set to true (or excluded) when running in production.\n-e \"discovery.type=single-node\": This tells Docker to create a cluster with a single node."
  },
  {
    "objectID": "posts/elasticseach-python.html#connect-to-your-cluster",
    "href": "posts/elasticseach-python.html#connect-to-your-cluster",
    "title": "How to Use Elasticsearch in Python",
    "section": "Connect to Your Cluster",
    "text": "Connect to Your Cluster\nCreate a new Jupyter Notebook, and run the following code, to connect to your newly created ES cluster.\nfrom elasticsearch import Elasticsearch\n\nes = Elasticsearch(\"http://localhost:9200\")\nes.info().body\nThis will connect to your local cluster. Make sure to use http instead of https. If you don‚Äôt, you‚Äôll get an error as you don‚Äôt have a valid SSL/TLS certificate. Note that in production, you‚Äôll want to use https.\nIf everything went well, you should see an output similar to mine:\n\nNow let‚Äôs get some data in your newly created index!"
  },
  {
    "objectID": "posts/elasticseach-python.html#read-the-dataset",
    "href": "posts/elasticseach-python.html#read-the-dataset",
    "title": "How to Use Elasticsearch in Python",
    "section": "Read the Dataset",
    "text": "Read the Dataset\nUse pandas to read the dataset and get a sample of 5,000 rows from it. You‚Äôll use a sample because otherwise, it‚Äôll take a long time to index the documents.\nimport pandas as pd\n\ndf = (\n    pd.read_csv(\"wiki_movie_plots_deduped.csv\")\n    .dropna()\n    .sample(5000, random_state=42)\n    .reset_index()\n)\nNext, you‚Äôll create an index to store this data."
  },
  {
    "objectID": "posts/elasticseach-python.html#create-an-index",
    "href": "posts/elasticseach-python.html#create-an-index",
    "title": "How to Use Elasticsearch in Python",
    "section": "Create an Index",
    "text": "Create an Index\nAn index is a collection of documents that Elasticsearch stores and represents through a data structure called an inverted index. This data structure identifies the documents in which each unique word appears.\nElasticsearch creates this inverted index when you index documents. This is how it can perform quick full-text searches.\nAs you can imagine, you must first create an index before you can begin indexing documents. This is how you do it:\nmappings = {\n        \"properties\": {\n            \"title\": {\"type\": \"text\", \"analyzer\": \"english\"},\n            \"ethnicity\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n            \"director\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n            \"cast\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n            \"genre\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n            \"plot\": {\"type\": \"text\", \"analyzer\": \"english\"},\n            \"year\": {\"type\": \"integer\"},\n            \"wiki_page\": {\"type\": \"keyword\"}\n    }\n}\n\nes.indices.create(index=\"movies\", mappings=mappings)\nThis code will create a new index called movies using the cluster you set up earlier.\nLines 1 to 12 define a mapping, which tells the index how the documents should be stored. A mapping specifies the data types assigned to each field in the documents stored in your index.\nYou can use either a dynamic or explicit mapping. In a dynamic mapping, Elasticsearch detects which data type should be used for each field. In an explicit mapping, each data type is manually defined. The latter allows you greater freedom in defining each field, which is why you used one in the code above.\nNow you‚Äôll start adding data to your index."
  },
  {
    "objectID": "posts/elasticseach-python.html#add-data-to-your-index",
    "href": "posts/elasticseach-python.html#add-data-to-your-index",
    "title": "How to Use Elasticsearch in Python",
    "section": "Add Data to Your Index",
    "text": "Add Data to Your Index\nYou can use es.index() or bulk() to add data to an index. es.index() adds one item at a time while bulk() lets you add multiple items at the same time.\nYou can use any of the two methods to add data to your index:\n\nUsing es.index()\nHere‚Äôs how you use es.index() to store your data:\nfor i, row in df.iterrows():\n    doc = {\n        \"title\": row[\"Title\"],\n        \"ethnicity\": row[\"Origin/Ethnicity\"],\n        \"director\": row[\"Director\"],\n        \"cast\": row[\"Cast\"],\n        \"genre\": row[\"Genre\"],\n        \"plot\": row[\"Plot\"],\n        \"year\": row[\"Release Year\"],\n        \"wiki_page\": row[\"Wiki Page\"]\n    }\n\n    es.index(index=\"movies\", id=i, document=doc)\nThis code iterates through the rows of the dataset you read earlier and adds to the index the relevant information from each row using es.index(). You use three parameters of that method:\n\nindex=\"movies\": this tells Elasticsearch which index to use to store the data. You can have multiple indexes in a cluster.\nid=i: this is the document‚Äôs identifier when you add it to the index. In this case, you set it to be the row number.\ndocument=doc: this specifies to Elasticsearch what information it should store.\n\n\n\nUsing bulk()\nHere‚Äôs how you use bulk() to store your data:\nfrom elasticsearch.helpers import bulk\n\nbulk_data = []\nfor i,row in df.iterrows():\n    bulk_data.append(\n        {\n            \"_index\": \"movies\",\n            \"_id\": i,\n            \"_source\": {\n                \"title\": row[\"Title\"],\n                \"ethnicity\": row[\"Origin/Ethnicity\"],\n                \"director\": row[\"Director\"],\n                \"cast\": row[\"Cast\"],\n                \"genre\": row[\"Genre\"],\n                \"plot\": row[\"Plot\"],\n                \"year\": row[\"Release Year\"],\n                \"wiki_page\": row[\"Wiki Page\"],\n            }\n        }\n    )\nbulk(es, bulk_data)\nbulk() requires the same information as .index(): the index‚Äôs name, the document‚Äôs ID, and the document itself. But instead of adding each item one by one, you must create a list of dictionaries with all the documents you want to add to the index. Then, you pass this information and the cluster object to bulk().\nAfter you add the data, you can make sure it worked by counting the number of items in the index:\nes.indices.refresh(index=\"movies\")\nes.cat.count(index=\"movies\", format=\"json\")\nYour output should look like this:"
  },
  {
    "objectID": "posts/elasticseach-python.html#search-your-data-using-elasticsearch",
    "href": "posts/elasticseach-python.html#search-your-data-using-elasticsearch",
    "title": "How to Use Elasticsearch in Python",
    "section": "Search Your Data Using Elasticsearch",
    "text": "Search Your Data Using Elasticsearch\nFinally, you‚Äôll want to start running searches using your index. Elasticsearch has a powerful DSL that lets you build many types of queries.\nHere‚Äôs an example of a search that looks for movies starring Jack Nicholson but whose director isn‚Äôt Roman Polanski:\nresp = es.search(\n    index=\"movies\",\n    query={\n            \"bool\": {\n                \"must\": {\n                    \"match_phrase\": {\n                        \"cast\": \"jack nicholson\",\n                    }\n                },\n                \"filter\": {\"bool\": {\"must_not\": {\"match_phrase\": {\"director\": \"roman polanski\"}}}},\n            },\n        },\n)\nresp.body\nWhen you run this code, you should get a very long response that looks something like this:\nNow it‚Äôs time for you to try and build your own searches. A good starting point is the query DSL documentation."
  },
  {
    "objectID": "posts/elasticseach-python.html#delete-documents-from-the-index",
    "href": "posts/elasticseach-python.html#delete-documents-from-the-index",
    "title": "How to Use Elasticsearch in Python",
    "section": "Delete Documents From the Index",
    "text": "Delete Documents From the Index\nYou can use the following code to remove documents from the index:\nes.delete(index=\"movies\", id=\"2500\")\nThe code above will delete the document with ID 2500 from the index movies."
  },
  {
    "objectID": "posts/elasticseach-python.html#delete-an-index",
    "href": "posts/elasticseach-python.html#delete-an-index",
    "title": "How to Use Elasticsearch in Python",
    "section": "Delete an Index",
    "text": "Delete an Index\nFinally, if for whatever reason, you‚Äôd like to delete an index (and all of its documents), here‚Äôs how you do it:\nes.indices.delete(index='movies')"
  },
  {
    "objectID": "posts/elasticseach-python.html#conclusion",
    "href": "posts/elasticseach-python.html#conclusion",
    "title": "How to Use Elasticsearch in Python",
    "section": "Conclusion",
    "text": "Conclusion\nThis tutorial taught you the basics of Elasticsearch and how to use it. This will be useful in your career, as you will surely come across Elasticsearch at some point.\nIn this tutorial, you‚Äôve learned:\n\nHow to set up an Elasticsearch cluster in your machine\nHow to create an index and store data in it\nHow to search your data using Elasticsearch\n\nIf you have any questions or feedback, let me know in the comments!\nAll the code for this tutorial is available on GitHub."
  },
  {
    "objectID": "posts/seed-temperature-llms.html",
    "href": "posts/seed-temperature-llms.html",
    "title": "Controlling randomness in LLMs: Temperature and Seed",
    "section": "",
    "text": "Temperature and seed are commonly used parameters when interacting with Large Language Models (LLMs). They‚Äôre also a source of confusion for many people. In this post, I‚Äôll show you what they are and how they work.\nTemperature is a parameter that controls the randomness of the output by scaling the logits of the tokens before applying the softmax function. Seed is also a parameter that controls the randomness of how the model selects tokens during text generation. It sets the initial state of the random number generator, which is then used for the sampling of the tokens during the generation process.\nTemperature is available for most providers, while seed is only available for OpenAI, Gemini on Vertex AI, and open-weight models (that I know of).\nLet‚Äôs get started."
  },
  {
    "objectID": "posts/seed-temperature-llms.html#how-llms-generate-text",
    "href": "posts/seed-temperature-llms.html#how-llms-generate-text",
    "title": "Controlling randomness in LLMs: Temperature and Seed",
    "section": "How LLMs generate text",
    "text": "How LLMs generate text\nTo understand how seed and temperature work, we first need to understand how LLMs generate text. Provided with a prompt, a model uses what‚Äôs called a decoding strategy to generate the next token.\nThere are many strategies, but for this post, we‚Äôll focus on just two: greedy search and sampling.\nIn greedy search, the model picks the token with the highest probability at each step. In sampling, the model picks a token based on the probability distribution of the tokens in the vocabulary. In both cases, the model will calculate the probability of each token in the vocabulary1, and use that to pick the next token. Let‚Äôs see an example.\nTake the following prompt:\n\nWhat‚Äôs the favorite dish of Chuck Norris?\n\nThese might be the top 5 most likely next tokens:\n\n\n\nRank\nToken\nProbability\n\n\n\n\n1\n‚ÄòDynamite‚Äô\n0.5823\n\n\n2\n‚ÄòVenom‚Äô\n0.2891\n\n\n3\n‚ÄòHimself‚Äô\n0.0788\n\n\n4\n‚ÄòRadiation‚Äô\n0.0354\n\n\n5\n‚ÄòYou‚Äô\n0.0144\n\n\n\nIf the model uses greedy search, it will pick the token with the highest probability, which is ‚ÄòDynamite‚Äô.\nIf it uses sampling, it will make a random selection based on those probabilities. So, the model has a 58% chance of picking ‚ÄòDynamite‚Äô, a 29% chance of picking ‚ÄòVenom‚Äô, a 8% chance of picking ‚ÄòHimself‚Äô, a 4% chance of picking ‚ÄòRadiation‚Äô, and a 1% chance of picking ‚ÄòYou‚Äô."
  },
  {
    "objectID": "posts/seed-temperature-llms.html#temperature",
    "href": "posts/seed-temperature-llms.html#temperature",
    "title": "Controlling randomness in LLMs: Temperature and Seed",
    "section": "Temperature",
    "text": "Temperature\nTemperature is a parameter that usually goes from 0 to 1 or 0 to 2, and it‚Äôs used to influence the randomness of the output. It does so by scaling the logits of the tokens by the temperature value.\nLogits are the raw scores that the model assigns to each token. To go from logits to probabilities, you must apply the softmax function:\n\\[\\text{P}(w_i) = \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}\\]\nWhere:\n\n\\(P(w_i)\\) is the probability of token \\(w_i\\)\n\\(z_i\\) is the logit for token \\(w_i\\)\n\\(n\\) is the total number of possible tokens\n\nThis is the non-scaled version of the probabilities. If you use Temperature (\\(T\\)) to scale the logits, you will change the probabilities of the tokens, as shown below:\n\\[P(w_i) = \\frac{e^{z_i / T}}{\\sum_{j=1}^{n} e^{z_j / T}}\\]\nEven though you cannot know for sure how proprietary providers (OpenAI, Anthropic, etc.) implement temperature, you can get a good idea of how it works by looking at TemperatureLogitWrapper in the transformers library.\nLet‚Äôs see a practical example of how temperature affects the probabilities of the tokens:\n\nimport numpy as np\n\ntokens = ['Dynamite', 'Venom', 'Himself', 'Radiation', 'You']\nlogits = np.array([2.5, 1.8, 0.5, -0.3, -1.2])\n\ntemperatures = [0.1, 0.5, 1.0, 1.5, 1.999999999]\n\nfor temperature in temperatures:\n    probs = np.exp(logits / temperature) / np.sum(np.exp(logits / temperature))\n    print(f\"\\nTemperature: {temperature:.2f}\")\n    print(\"What's the favorite dish of Chuck Norris?\")\n    print(\"Rank | Token      | Probability\")\n    print(\"-----|------------|------------\")\n    for i, (token, prob) in enumerate(zip(tokens, probs), 1):\n        print(f\"{i:4d} | '{token:10s}' | {prob:.4f}\")\n    print(f\"Sum of probabilities: {np.sum(probs):.4f}\")\n\nThis code simulates the impact of different temperature values on the next token probability. Given some initial logits and assuming this is the full vocabulary, we can calculate the probabilities of the tokens for a given temperature.\nFor a temperature of 0.1, you get the following probabilities:\n\n\n\nRank\nToken\nProbability\n\n\n\n\n1\n‚ÄòDynamite‚Äô\n0.9991\n\n\n2\n‚ÄòVenom‚Äô\n0.0009\n\n\n3\n‚ÄòHimself‚Äô\n0.0000\n\n\n4\n‚ÄòRadiation‚Äô\n0.0000\n\n\n5\n‚ÄòYou‚Äô\n0.0000\n\n\n\nFor a temperature of 2, you get the following probabilities:\n\n\n\nRank\nToken\nProbability\n\n\n\n\n1\n‚ÄòDynamite‚Äô\n0.4038\n\n\n2\n‚ÄòVenom‚Äô\n0.2846\n\n\n3\n‚ÄòHimself‚Äô\n0.1486\n\n\n4\n‚ÄòRadiation‚Äô\n0.0996\n\n\n5\n‚ÄòYou‚Äô\n0.0635\n\n\n\nYou can see that for lower temperature values, the model becomes more deterministic. For temperature 0.1, the probability of picking ‚ÄòDynamite‚Äô is &gt;99.9%, while for temperature 2, it‚Äôs only 40%.\nIn essence, temperature impacts the randomness of the output by changing the probabilities of selecting the next token. This should give you a good idea of how temperature works. But let‚Äôs try it with a real LLM instead of a simulation.\nFirst, let‚Äôs import the required libraries and load the model.\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"unsloth/Qwen3-1.7B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\nFor the sake of this example, we‚Äôll use unsloth/Qwen3-1.7B. But what you see here is applicable to most LLMs. We‚Äôll use generate_text as our text generation function.\n\ndef generate_text(prompt, temperature, seed=None, print_top_k=False):\n    if seed:\n        torch.manual_seed(seed)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed(seed)\n\n    messages = [\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n        enable_thinking=False\n    )\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    if temperature &gt; 0:\n        model_params = {\n            \"do_sample\": True,\n            \"temperature\": temperature if temperature &lt; 2 else 1.9999999,\n        }\n    else:\n        model_params = {\n            \"do_sample\": False,\n        }\n    outputs = model.generate(\n        **model_inputs,\n        **model_params,\n        max_new_tokens=1,\n        output_scores=True,\n        return_dict_in_generate=True,\n        pad_token_id=tokenizer.eos_token_id\n    )\n\n    output_token_id = outputs.sequences[0][-1].tolist()\n    selected_token = tokenizer.decode([output_token_id])\n\n    if not print_top_k:\n        return selected_token\n    \n    probs = F.softmax(outputs.scores[0][0], dim=-1)\n    top_k_probs, top_k_indices = torch.topk(probs, 10)\n\n    print(\"Top-10 most likely tokens:\")\n    for i, (prob, idx) in enumerate(zip(top_k_probs, top_k_indices)):\n        token_text = tokenizer.decode([idx.item()])\n        is_selected = \"‚Üê SELECTED\" if idx.item() == output_token_id else \"\"\n        print(f\"  {i+1}. '{token_text}' (prob: {prob.item():.4f}, logit: {outputs.scores[0][0][idx.item()].item():.4f}) {is_selected}\")\n\n    return selected_token\n\nOn a high-level, this function takes a prompt, a temperature value, and a seed (which we‚Äôll ignore for now), and returns the top 10 most likely tokens with their probabilities and logits. The implementation looks a bit complicated, so let‚Äôs break it down.\n\nLines 2 to 16: It takes a prompt, a temperature value, and optionally a seed. If a seed is provided, it sets the random number generator to that value. Then, it processes the prompt to create the required input for the model.\nLines 18 to 37: It chooses to sample from the model or not, based on the temperature value. If temperature is 0, the model will use to a greedy search strategy.\nLines 39 to 50: It returns the completion token and optinally prints the top 10 most likely tokens with their probabilities and logits.\n\nSimilar to what you saw in the previous example, you can try low and high temperature values.\nThis is what you get for a temperature of 0.1:\n\ntoken = generate_text(\"Tell me a joke about dogs\", temperature=0.1, print_top_k=True)\n\nTop-10 most likely tokens:\n  1. 'Why' (prob: 1.0000, logit: 330.0000) ‚Üê SELECTED\n  2. '!' (prob: 0.0000, logit: -inf) \n  3. '\"' (prob: 0.0000, logit: -inf) \n  4. '#' (prob: 0.0000, logit: -inf) \n  5. '$' (prob: 0.0000, logit: -inf) \n  6. '%' (prob: 0.0000, logit: -inf) \n  7. '&' (prob: 0.0000, logit: -inf) \n  8. ''' (prob: 0.0000, logit: -inf) \n  9. '(' (prob: 0.0000, logit: -inf) \n  10. ')' (prob: 0.0000, logit: -inf) \n\n\n\ntoken = generate_text(\"Tell me a joke about dogs\", temperature=1.99, print_top_k=True)\n\nTop-10 most likely tokens:\n  1. 'Why' (prob: 0.5742, logit: 16.5829) ‚Üê SELECTED\n  2. 'Sure' (prob: 0.3939, logit: 16.2060) \n  3. 'Here' (prob: 0.0319, logit: 13.6935) \n  4. '!' (prob: 0.0000, logit: -inf) \n  5. '\"' (prob: 0.0000, logit: -inf) \n  6. '#' (prob: 0.0000, logit: -inf) \n  7. '$' (prob: 0.0000, logit: -inf) \n  8. '%' (prob: 0.0000, logit: -inf) \n  9. '&' (prob: 0.0000, logit: -inf) \n  10. ''' (prob: 0.0000, logit: -inf) \n\n\nYou should see similar results. For the ‚ÄúTell me a joke about dogs‚Äù prompt, when using a temperature of 0.1, the model had ~100% probability of picking ‚ÄòWhy‚Äô, while for temperature 2, it‚Äôs only 57%.\nNote, that when temperature is 0, the model will use to a greedy search strategy, which is the same as picking the most likely token. So no sampling is done and results are deterministic."
  },
  {
    "objectID": "posts/seed-temperature-llms.html#seed",
    "href": "posts/seed-temperature-llms.html#seed",
    "title": "Controlling randomness in LLMs: Temperature and Seed",
    "section": "Seed",
    "text": "Seed\nThe seed parameter controls the randomness of how a model selects tokens. It sets the initial state for the random number generator used in the token sampling process.\nLet‚Äôs revisit the example from the previous section to see this in action. By setting the seed to a fixed value, you ensure the generation process is deterministic. This means you will get an identical result on every run, provided all other parameters (like temperature) remain the same in those runs.\nWe can start by setting our seed to 42 and temperature to 1 to verify which token is generated.\n\ngenerate_text(\"Tell me a joke about dogs\", temperature=1, seed=42, print_top_k=True)\n\nTop-10 most likely tokens:\n  1. 'Why' (prob: 0.6792, logit: 33.0000) \n  2. 'Sure' (prob: 0.3208, logit: 32.2500) ‚Üê SELECTED\n  3. '!' (prob: 0.0000, logit: -inf) \n  4. '\"' (prob: 0.0000, logit: -inf) \n  5. '#' (prob: 0.0000, logit: -inf) \n  6. '$' (prob: 0.0000, logit: -inf) \n  7. '%' (prob: 0.0000, logit: -inf) \n  8. '&' (prob: 0.0000, logit: -inf) \n  9. ''' (prob: 0.0000, logit: -inf) \n  10. '(' (prob: 0.0000, logit: -inf) \n\n\n'Sure'\n\n\nIn this case, the model selected ‚ÄúSure‚Äù as the next token, even though its probability is lower than ‚ÄòWhy‚Äô. Now, we can verify that this stays the same over multiple runs.\n\ntokens = []\nfor i in range(100):\n    token = generate_text(\"Tell me a joke about dogs\", temperature=1, seed=42)\n    tokens.append(token)\nassert len(set(tokens)) == 1\nprint(set(tokens))\n\n{'Sure'}\n\n\nThis code runs the text generation process 100 times and verifies that ‚ÄúSure‚Äù was picked in all runs. Next, we should verify that this consistency is lost when we omit the seed parameter.\n\ntokens = []\nfor i in range(100):\n    token = generate_text(\"Tell me a joke about dogs\", temperature=1)\n    tokens.append(token)\nassert len(set(tokens)) &gt; 1\nprint(set(tokens))\n\n{'Sure', 'Why'}\n\n\nIn this case, you see that after the 100 generations, the model picked two different tokens: ‚ÄòSure‚Äô and ‚ÄòWhy‚Äô. This is expected due to not setting a seed.\nYou can also use test this with a propietary model. Let‚Äôs try it with gpt-4.1-nano from OpenAI.\n\nimport openai\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclient = openai.OpenAI()\n\ndef generate_text_openai(prompt, temperature, seed=None, print_top_k=False):\n    response = client.chat.completions.create(\n        model=\"gpt-4.1-nano\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=temperature,\n        seed=seed,\n        max_tokens=1,\n        logprobs=True,\n        top_logprobs=10,\n    )\n    selected_token = response.choices[0].message.content\n    if print_top_k:\n        logprobs = response.choices[0].logprobs.content[0].top_logprobs\n        print(\"Top 10 most likely tokens:\")\n        for idx, token_info in enumerate(logprobs):\n            token = token_info.token\n            logprob = token_info.logprob\n            prob = np.round(np.exp(logprob)*100,2)\n            token_text = f\"{idx+1}. '{token}': {prob:.4f} ({logprob:.4f})\"\n            is_selected = \"‚Üê SELECTED\" if token_info.token == selected_token else \"\"\n            print(f\"{token_text} {is_selected}\")\n    return selected_token\n\nSimilar to the previous function, you provide a prompt, a temperature value, and a seed, and the model will return a completion token and will print the top 10 most likely tokens.\nIn this case, instead of providing you with the logits, OpenAI will provide you with logprobs which are the logaritmic probabilities of the tokens:\n\\[logprob(w_i) = ln(P(w_i)) = ln(\\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}) \\]\nFirst, let‚Äôs check the completion token we get for a temperature of 1 and a seed of 42.\n\ntoken = generate_text_openai(\"Tell me a joke about dogs\", temperature=1, seed=42, print_top_k=True)\n\nTop 10 most likely tokens:\n1. 'Why': 59.2600 (-0.5232) ‚Üê SELECTED\n2. 'Sure': 40.7300 (-0.8982) \n3. ' Why': 0.0000 (-10.6482) \n4. ' sure': 0.0000 (-11.0232) \n5. ' why': 0.0000 (-11.2732) \n6. '‰∏∫‰ªÄ‰πà': 0.0000 (-11.6482) \n7. ' Sure': 0.0000 (-11.8982) \n8. 'Pourquoi': 0.0000 (-12.2732) \n9. 'why': 0.0000 (-12.3982) \n10. 'sure': 0.0000 (-12.6482) \n\n\nIn this case, we get ‚ÄòWhy‚Äô as the completion token. You can see that the top 10 most likely tokens are not the same as the ones we got with Qwen3-1.7B. This is expected, as the model is different.\nThen, we can try to generate 100 tokens with a temperature of 1 and a seed of 42.\n\ntokens = []\nfor i in range(100):\n    token = generate_text_openai(\"Tell me a joke about dogs\", temperature=1, seed=42)\n    tokens.append(token)\nassert len(set(tokens)) == 1\nprint(set(tokens))\n\n{'Why'}\n\n\nSimilar to the previous example, we run 100 generations with the same seed and temperature and check if the completion token is the same.\nThis should generally work, but OpenAI doesn‚Äôt guarantee that the same seed will always produce the same output. It might occur that your request is handled by a model with a different configuration, and you‚Äôll get different results.\nYou can also verify that not using a seed will result in different tokens.\n\ntokens = []\nfor i in range(100):\n    token = generate_text_openai(\"Tell me a joke about dogs\", temperature=1)\n    tokens.append(token)\nassert len(set(tokens)) &gt; 1\nprint(set(tokens))\n\n{'Sure', 'Why'}\n\n\nNow, you can see that the output is not the same in all runs. Some runs picked ‚ÄúWhy‚Äù, and others picked ‚ÄúSure‚Äù.\nIn essence, seed influences the output by setting the initial state of the random number generator, which is then used for the sampling of the tokens during the generation process."
  },
  {
    "objectID": "posts/seed-temperature-llms.html#top-k-and-top-p",
    "href": "posts/seed-temperature-llms.html#top-k-and-top-p",
    "title": "Controlling randomness in LLMs: Temperature and Seed",
    "section": "top-k and top-p",
    "text": "top-k and top-p\nIn addition to temperature, there are two other parameters that are commonly used to control the randomness of the output of a language model: top-k and top-p.\n\ntop-k\nTop-k sampling is a technique that limits the number of tokens that can be selected from the vocabulary. It does so by keeping only the top-k tokens with the highest probabilities. This reduces the computational workload by getting the top-k logits and then calculating the softmax over these instead of using the complete vocabulary.\nThis parameter isn‚Äôt available for OpenAI models. They provide a top_logprobs parameter, but it‚Äôs not the same as top-k sampling. It‚Äôs a parameter that returns the top N most likely tokens with their logprobs, but it doesn‚Äôt change the sampling process.\n\n\ntop-p\nTop-p sampling is a technique that limits the number of tokens that can be selected from the vocabulary. It does so including the smallest set of tokens whose combined probability ‚â• P. For example, top P = 0.9 picks from the smallest group of tokens that together cover at least 90% probability.\nThis parameter is available for most providers.\n\ngenerate_text_openai(\"Tell me a joke about dogs\", top_p=0.50, print_top_k=True)\n\nTop 10 most likely tokens:\n1. 'Why': 59.2600 (-0.5232) ‚Üê SELECTED\n2. 'Sure': 40.7300 (-0.8982) \n3. ' Why': 0.0000 (-10.6482) \n4. ' sure': 0.0000 (-11.0232) \n5. ' why': 0.0000 (-11.2732) \n6. '‰∏∫‰ªÄ‰πà': 0.0000 (-11.6482) \n7. ' Sure': 0.0000 (-11.8982) \n8. 'Pourquoi': 0.0000 (-12.2732) \n9. 'why': 0.0000 (-12.3982) \n10. 'sure': 0.0000 (-12.6482) \n\n\n'Why'"
  },
  {
    "objectID": "posts/seed-temperature-llms.html#seed-and-temperature-in-practice",
    "href": "posts/seed-temperature-llms.html#seed-and-temperature-in-practice",
    "title": "Controlling randomness in LLMs: Temperature and Seed",
    "section": "Seed and temperature in practice",
    "text": "Seed and temperature in practice\nNow that you understand how seed and temperature work, here are some things to keep in mind when using them:\n\nseed is only available for OpenAI, Gemini on Vertex AI, and open-weight models.\nTo get the most deterministic output for a given prompt, set temperature to 0. This minimizes randomness.\nIf you want creative results that are still reproducible, set temperature to a value greater than 0 and use a fixed seed. This allows for varied outputs that you can generate again.\nIf you don‚Äôt need reproducible results and want unique outputs on every run, you can omit the seed parameter entirely.\nBe aware that even if you set a temperature of 0 and a seed, outputs are not guaranteed to be identical. Providers might change model configurations that might impact the output. For OpenAI models, you can monitor such changes by keeping track of the system_fingerprint provided in the responses."
  },
  {
    "objectID": "posts/seed-temperature-llms.html#conclusion",
    "href": "posts/seed-temperature-llms.html#conclusion",
    "title": "Controlling randomness in LLMs: Temperature and Seed",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we explored how the temperature and seed parameters control the output of Large Language Models.\nYou learned that temperature adjusts the level of randomness: low values (near 0) produce more predictable, deterministic outputs, while high values (near 1) encourage more creative and varied results. In contrast, the seed makes the generation process reproducible. While the specific seed value isn‚Äôt important, fixing it ensures you get the same output for a given prompt and set of parameters.\nFinally, remember that while temperature is a near-universal setting, seed is only available (at the time of writing) for OpenAI, Gemini on Vertex AI, and open-weight models.\nI hope you found this post useful. If you have any questions, let me know in the comments below."
  },
  {
    "objectID": "posts/seed-temperature-llms.html#footnotes",
    "href": "posts/seed-temperature-llms.html#footnotes",
    "title": "Controlling randomness in LLMs: Temperature and Seed",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nModern LLMs often have a vocabulary of 100k+ tokens‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/my-entrepreneur-first-experience.html",
    "href": "posts/my-entrepreneur-first-experience.html",
    "title": "My Experience at Entrepreneur First",
    "section": "",
    "text": "On September 2, 2022, I received an invitation to join the 10th cohort of Entrepreneur First (EF) in Berlin. Less than a month later, I packed up and moved there, eager to find a cofounder to begin what I hoped would become a billion-dollar company.\nOver the past three months, I‚Äôve spent many hours brainstorming with other founders, talking to +100 potential customers, and practicing our pitch for investors. Last week, my cofounder and I pitched our idea to EF‚Äôs Investment Committee (IC). Unfortunately, they informed us today that they‚Äôve decided not to invest in our team.\nI‚Äôm writing this to reflect on my experience during these past three months. I‚Äôll provide some background on what EF is and how it works, and then go into what I liked and didn‚Äôt like about the program. If you‚Äôre considering participating in Entrepreneur First, I hope this article will help you make an informed decision."
  },
  {
    "objectID": "posts/my-entrepreneur-first-experience.html#whats-entrepreneur-first",
    "href": "posts/my-entrepreneur-first-experience.html#whats-entrepreneur-first",
    "title": "My Experience at Entrepreneur First",
    "section": "What‚Äôs Entrepreneur First?",
    "text": "What‚Äôs Entrepreneur First?\nEF is a global organization that helps talented individuals start companies. They provide funding, mentorship, and resources to individuals with the potential to become successful tech entrepreneurs. EF focuses on assisting individuals with technical and business backgrounds in forming teams and building startups from the ground up. It has programs in Singapore, India, the United Kingdom, Canada, Germany, and France.\nThe program connects you with a group of roughly 50 individuals who have the ambition, urgency, and skills required to build a startup. You receive a stipend of 2,000‚Ç¨ per month for the first three months, but you have to work on the program full-time in return. This will allow you to solely focus on finding your cofounder and developing your idea. In our cohort, most participants had recently quit their jobs or finished their studies (usually PhDs) before joining.\nIf your team does well in the first three months, you may be eligible for an initial investment of ¬£80,000 (as of January 2023)."
  },
  {
    "objectID": "posts/my-entrepreneur-first-experience.html#hows-the-application-process",
    "href": "posts/my-entrepreneur-first-experience.html#hows-the-application-process",
    "title": "My Experience at Entrepreneur First",
    "section": "How‚Äôs the Application Process?",
    "text": "How‚Äôs the Application Process?\nThe application process is pretty simple. One of EF‚Äôs recruiters contacted me on LinkedIn and asked if I wanted to join the program. Then he asked me to fill out an application. Most of the questions on the form were about why I wanted to join the program and why I would be a good fit for it. Then, I interviewed with the Berlin program‚Äôs general manager and the recruiter who had contacted me.\nI could have done a better job in the interview because I ended up rambling through some of the questions. But I was still able to pique people‚Äôs interest when I described some of the strategies I used to get more than 100,000 users to play Fast Flood when I launched it. Overall, it was a positive experience, and I got in.\nAfter talking with other participants, I found that the process was similar for everyone. The only difference appears to be the number of interviews you might do, which ranged from one to three. If you know someone from a previous cohort, you should definitely ask for a referral, as it will probably increase your chances of getting in."
  },
  {
    "objectID": "posts/my-entrepreneur-first-experience.html#whats-the-structure-of-the-program",
    "href": "posts/my-entrepreneur-first-experience.html#whats-the-structure-of-the-program",
    "title": "My Experience at Entrepreneur First",
    "section": "What‚Äôs the Structure of the Program?",
    "text": "What‚Äôs the Structure of the Program?\nThe program is split into two parts. Form, which lasts three months and culminates with you pitching your idea to IC, and Launch, which lasts six months and is only available if you are selected for funding after IC. Since I just finished Form and cannot access Launch, I will only talk about that part of the program in this article.\nForm is meant to help you find a cofounder, come up with ideas, and get customer feedback on them. During the first eight weeks of this phase, most of your time should be spent looking for the right cofounder. If, after this period, you‚Äôre not on a team, you have to leave the program, but you‚Äôll still get your full stipend.\nDuring this time, you mostly do one of two things: participate in a sort of speed-dating event for entrepreneurs or test your idea with customers. Once the Form phase is over, you‚Äôll only be working on your company.\nThe speed-dating contest consists of you talking to potential cofounders about business ideas (aka ideation sessions) to see if there‚Äôs anything you both want to work on. If you feel there‚Äôs a ‚Äúmatch‚Äù with another cofounder, then you can team up and work on your idea. If you later realize you weren‚Äôt a good fit, you can split up and go back to the pool of sole founders to find another cofounder.\nWhen working in a team, most of the time is spent on three main tasks. The first task is to reach out to potential customers through your personal network, LinkedIn, and other platforms to understand their pain points and determine if your idea addresses them. The second task is to process feedback from customer interviews to update your hypotheses about the business. The third task is to make sure that the team is aiming for a big market, that the company has some way to defend itself, and that any other important factors are taken into account so that the company can be successful in the future.\nDuring the first few weeks of Form, you could also choose to attend several workshops about entrepreneurship and new businesses. We also had a few regular activities:\n\nWeekly check-ins with your venture partner and venture developer: You get feedback on your idea and provide progress updates. In my case, the venture partner was a previous EF founder who sold his company to Twitter, and the venture developer was an expert in product development. Both provided very useful feedback throughout the program.\nWeekly check-in with your Form representative: You can ask questions about the program and talk about how you feel as the program progresses.\nBi-weekly Friday pitches: You present your idea to the rest of the teams and receive feedback.\nWeekly social drinks: You drink beer and talk with other people :)\n\nAs you get closer to IC, you also get invited to other meetings to provide you with more feedback. For IC, you‚Äôre asked to provide a pitch video, a slide deck, and a product demo. For the funding decision, they will also look at what your venture partner and developer say about you, as well as what the local EF staff says.\nIn our cohort, many participants tried working with multiple teams before finding the right fit. I tried three teams. In total, 49 teams were formed (h/t Simon Farshid for tracking this), and 13 teams presented their ideas to IC. Roughly half of the participants were not in teams by the end of the Form phase."
  },
  {
    "objectID": "posts/my-entrepreneur-first-experience.html#what-i-liked-about-entrepreneur-first",
    "href": "posts/my-entrepreneur-first-experience.html#what-i-liked-about-entrepreneur-first",
    "title": "My Experience at Entrepreneur First",
    "section": "What I liked about Entrepreneur First",
    "text": "What I liked about Entrepreneur First\nBy now, you should have a good idea of what to expect during the program. Now, let me tell you what I liked about EF:\n\nThe people: The best thing about the program is the connections you make. You join a group of highly skilled people who are eager to start their own businesses. As my career progressed, I learned that not many people wanted that, so it was great to meet so many like-minded people. I‚Äôve also made a few good friends during the program!\nPressure to talk to customers: There‚Äôs a lot of emphasis on talking to customers throughout the program, and I really appreciated that. This forced me to step outside of my comfort zone because I didn‚Äôt feel as comfortable reaching out to so many people before.\nRevealed preferences: I wasn‚Äôt expecting many people to drop out of the program before it finished, but it happened. Many people realize they don‚Äôt want to start a business after all. Even though there are ups and downs, I‚Äôve never doubted my desire to start a business, so I was happy to find another data point that I‚Äôm on the right track. Overall, I believe this is a low-risk way for people to figure out if they really want to start a business.\nEF‚Äôs network: EF alumni have interesting positions in top companies and startups. They‚Äôre also very open to chatting. It‚Äôs great to know that you‚Äôre a short message away from talking to people with relevant positions in successful companies.\n\nOverall, I see many positive sides to the EF experience."
  },
  {
    "objectID": "posts/my-entrepreneur-first-experience.html#what-i-didnt-like-about-entrepreneur-first",
    "href": "posts/my-entrepreneur-first-experience.html#what-i-didnt-like-about-entrepreneur-first",
    "title": "My Experience at Entrepreneur First",
    "section": "What I didn‚Äôt like about Entrepreneur First",
    "text": "What I didn‚Äôt like about Entrepreneur First\nWhile my experience with EF was mostly positive, there were a few areas that I felt could have been improved. I noticed a few things that seemed to make it more difficult to focus on building a company, even though they were intended to help.\nThese are some of the things I didn‚Äôt like:\n\nToo much overhead: By the end of Form, I felt like we had too many unnecessary meetings. I often wondered if it would make sense to combine a few of those meetings to reduce the time it takes. For example, we had two separate meetings with our venture partner and developer, to essentially discuss the same topic. Why not combine those two meetings into one?\nThe Entrepreneur Game: I had the impression that EF wanted us to believe they had everything figured out and that there was a clear path to building a unicorn that we simply had to follow. It was kind of a game, and all we had to do to win was complete all the levels (e.g., find a counterintuitive belief on which to base your company, do 20 customer interviews per week, etc.). Why would most businesses fail if there was such a simple formula for success? The devil is in the details, and I think that founders who do well do so because they keep on grinding for a long time on a good problem space rather than because they find a foolproof recipe or framework. All frameworks for building startups are limited because the real world is opaque and the small, difficult-to-understand details are what matter most.\nCheck-ins should not be pitches. You get ‚Äúgraded‚Äù after the check-ins, which creates an incentive to turn those check-ins into pitching sessions. People have fewer incentives to ask difficult questions, as that may cast them in a negative light. Also, this wasn‚Äôt communicated clearly from the beginning.\n\nDespite these issues, I still believe that the EF staff works hard and has the challenging task of serving an audience with a high bar. They are trying to help entrepreneurs build successful companies."
  },
  {
    "objectID": "posts/my-entrepreneur-first-experience.html#whats-next-for-me",
    "href": "posts/my-entrepreneur-first-experience.html#whats-next-for-me",
    "title": "My Experience at Entrepreneur First",
    "section": "What‚Äôs next for me?",
    "text": "What‚Äôs next for me?\nEven though I didn‚Äôt reach the goal I set for myself when I joined EF, I‚Äôve learned a lot and met some great people in the last three months. I will move forward with building a company but want to make sure that I use the lessons learned during the program. I‚Äôll take a few days to reflect on the best path forward.\nThe past three months have been hectic, but I‚Äôve realized that I often fell into the trap of simply ‚Äúplaying the game‚Äù of being an entrepreneur rather than focusing on creating something people truly wanted. I don‚Äôt plan on making that mistake again."
  },
  {
    "objectID": "posts/my-entrepreneur-first-experience.html#conclusion",
    "href": "posts/my-entrepreneur-first-experience.html#conclusion",
    "title": "My Experience at Entrepreneur First",
    "section": "Conclusion",
    "text": "Conclusion\nIf I knew what I know now, would I have done the program? Definitely.\nEven though there were things I didn‚Äôt like about it, I learned a lot during the past three months. I liked my time with EF, and I would recommend it to anyone who wants to start a venture-backed business.\nThere are always ways to make things better, but I know that the EF team is working very hard to create the best program possible. I have nothing but gratitude for the EF team and everyone I met through the program. This experience made me better."
  },
  {
    "objectID": "posts/my-entrepreneur-first-experience.html#addendum-how-to-prepare-for-the-interview-at-ef",
    "href": "posts/my-entrepreneur-first-experience.html#addendum-how-to-prepare-for-the-interview-at-ef",
    "title": "My Experience at Entrepreneur First",
    "section": "Addendum: How to Prepare for the Interview at EF?",
    "text": "Addendum: How to Prepare for the Interview at EF?\nA few people have asked me on LinkedIn about what‚Äôs the best way to prepare for the interview at EF. Here‚Äôs my advice:\n\nPrepare a 1-min pitch that explains who you are and why you want to start a company with EF.\nThink about your previous experience and projects, figure out what are the most interesting parts of them, and practice breaking them down to others in a compelling way (focusing on their business impact).\nHave good answers to the following questions:\n\nWhy do you want to start a company?\nWhy are you the right person to start a company with EF?\nHow is X going to change in the future? What‚Äôd be the impact of those changes? (replace X with your field of expertise)\nWhat would you do if EF doesn‚Äôt work out?\n\n\nHope this is useful!"
  },
  {
    "objectID": "posts/function-calling-structured-outputs.html",
    "href": "posts/function-calling-structured-outputs.html",
    "title": "Function calling and structured outputs in LLMs with LangChain and OpenAI",
    "section": "",
    "text": "Function calling and structured outputs let you go from chatbots that just talk to agents that interact with the world. They‚Äôre two of the most important techniques for building LLM applications.\nFunction calling let LLMs access external tools and services. Structured outputs ensure that the data coming back from your models is ready to integrate\nThese are two of the most important techniques for building LLM applications. I can tell you that mastering them will make your applications better and easier to maintain.\nIn this tutorial, you‚Äôll learn:\nLet‚Äôs get started."
  },
  {
    "objectID": "posts/function-calling-structured-outputs.html#function-calling",
    "href": "posts/function-calling-structured-outputs.html#function-calling",
    "title": "Function calling and structured outputs in LLMs with LangChain and OpenAI",
    "section": "Function calling",
    "text": "Function calling\nFunction calling refers to the ability to get LLMs to use external tools or functions. It matters because it gives LLMs more capabilities, allows them to talk to external systems, and enables complex task automation. This is one of the key features that unlocked agents.\nThe usual flow is:\n\nThe developer sets up an LLM with a set of predefined tools\nThe user asks a question\nThe LLM decides if it needs to use a tool\nIf it does, it invokes the tool and gets the output from the tool.\nThe LLM then uses the output to answer the user‚Äôs question\n\nHere‚Äôs a diagram that illustrates how function calling works:\n\n\n\nFunction calling flow\n\n\nAI developers are increasingly using function calling to build more complex systems. You can use it to:\n\nGet information from a CRM, DB, etc\nPerform calculations (e.g., generate an estimate for a variable, financial calculations)\nManipulate data (e.g., data cleaning, data transformation)\nInteract with external systems (e.g., booking a flight, sending an email)"
  },
  {
    "objectID": "posts/function-calling-structured-outputs.html#structured-outputs",
    "href": "posts/function-calling-structured-outputs.html#structured-outputs",
    "title": "Function calling and structured outputs in LLMs with LangChain and OpenAI",
    "section": "Structured outputs",
    "text": "Structured outputs\nStructured outputs are a group of methods that ‚Äúensure that model outputs adhere to a specific structure‚Äù1. With proprietary models, this usually means a JSON schema. With open-weight models, a structure can mean anything from a JSON schema to a specific regex pattern. You can use outlines for this.\nStructured outputs are very useful to create agentic systems, as they simplify the communication between components. As you can imagine, it‚Äôs a lot easier to parse the output of a JSON object than a free-form text. Note, however, that as with other things in life, there‚Äôs no free lunch. Using this technique might impact the performance of your task, so you should have evals in place.\nIn the next sections, I‚Äôll show you how to use function calling and structured outputs with OpenAI."
  },
  {
    "objectID": "posts/function-calling-structured-outputs.html#prerequisites",
    "href": "posts/function-calling-structured-outputs.html#prerequisites",
    "title": "Function calling and structured outputs in LLMs with LangChain and OpenAI",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo follow this tutorial you‚Äôll need to:\n\nSign up and generate an API key in OpenAI.\nSign up and generate an API key in LangSmith.\nCreate an .env file with the following variables:\n\nOPENAI_API_KEY=sk-...\nLANGCHAIN_TRACING_V2=true\nLANGSMITH_API_KEY=lsv2_...\nLANGCHAIN_PROJECT=\"my-project\"\n\nCreate a virtual environment in Python and install the requirements:\n\npython -m venv venv\nsource venv/bin/activate\npip install langchain langsmith pydantic langchain-openai python-dotenv jupyter\nOnce you‚Äôve completed the steps above, you can run copy and paste the code from the next sections. You can also download the notebook from here."
  },
  {
    "objectID": "posts/function-calling-structured-outputs.html#examples",
    "href": "posts/function-calling-structured-outputs.html#examples",
    "title": "Function calling and structured outputs in LLMs with LangChain and OpenAI",
    "section": "Examples",
    "text": "Examples\nAs usual, you‚Äôll start by importing the necessary libraries.\nYou‚Äôll use LangChain to interact with the OpenAI API and Pydantic for data validation.\n\nfrom textwrap import dedent\nfrom typing import Literal\n\nimport requests\nfrom dotenv import load_dotenv\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langsmith import traceable\nfrom pydantic import BaseModel, Field\n\nload_dotenv()\n\nNow that you‚Äôve imported the libraries, we‚Äôll work on three examples:\n\nProviding a model with a single tool\nProviding a model with multiple tools\nGenerating a structured output from a model\n\n\nFunction calling with a single tool\nFirst you start by defining the model and the tool:\n\nmodel = ChatOpenAI(model_name=\"gpt-4.1-mini\")\n\n@tool\ndef find_weather(latitude: float, longitude: float):\n    \"\"\"Get the weather of a given latitude and longitude\"\"\"\n    response = requests.get(\n        f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\"\n    )\n    data = response.json()\n    return data[\"current\"][\"temperature_2m\"]\n\n\ntools_mapping = {\n    \"find_weather\": find_weather,\n}\n\nmodel_with_tools = model.bind_tools([find_weather])\n\nThis code sets up a gpt-4.1-mini model with a single tool. To define a tool, you must define a function and use the @tool decorator. This function must necessarily have a docstring because this will be used to describe the tool to the model. In this case, the tool is a function that takes latitude and longitude values and returns the weather for that location by making a call to the Open Meteo API.\nNext, you need to tell your code how to find and use your tools. This is the purpose of tools_mapping. It is a common point of confusion. The LLM doesn‚Äôt run the tools on its own. It only decides if a tool should be used. After the model makes its decision, your own code must make the actual tool call.\nIn this situation, since you only have one tool, a mapping isn‚Äôt really necessary. But if you were using multiple tools, which is often the case, you would need to create a ‚Äúmap‚Äù that links each tool‚Äôs name to its corresponding function. This lets you call the right tool when the model decides to use it.\nFinally, you need to bind the tool to the model. The binding makes the model aware of the tool, so that it can use it.\nThen, let‚Äôs define a function that lets you call the model with the tool.\n\n@traceable\ndef get_response(question: str):\n    messages = [\n        SystemMessage(\n            \"You're a helpful assistant. Use the tools provided when relevant.\"\n        ),\n        HumanMessage(question),\n    ]\n    ai_message = model_with_tools.invoke(messages)\n    messages.append(ai_message)\n\n    for tool_call in ai_message.tool_calls:\n        selected_tool = tools_mapping[tool_call[\"name\"]]\n        tool_msg = selected_tool.invoke(tool_call)\n        messages.append(tool_msg)\n\n    ai_message = model_with_tools.invoke(messages)\n    messages.append(ai_message)\n\n    return ai_message.content\n\n\nresponse = get_response(\"What's the weather in Tokyo?\")\nprint(response)\n\nThe current temperature in Tokyo is approximately 25.5¬∞C. If you want more detailed weather information, please let me know!\n\n\nThis function takes a city name and returns the weather for that city. It uses the find_weather tool to get the weather data.\nIt works as follows:\n\nLine 1 adds a LangSmith‚Äôs traceable decorator to the function, so that you can see the trace of the function in the LangSmith UI. If you prefer to not use LangSmith, you can remove this line.\nLines 2 to 10 set up the prompts and call the model.\nLines 12 to 16 is where the magic happens. This is a loop that will check if there‚Äôs been a tool call in the response from the model. If there is, it will call (invoke) the tool and add the result to the messages.\nLines 17 to 18 the model is called again to get the final response.\n\nWhen you run this code, you‚Äôll get a text response with the weather for the city you asked for. If you check the trace, you can see how the whole process works:\n\n\n\nFunction calling trace\n\n\nThere are three steps in the process:\n\nInitial model call with the question from the user.\nTool call to get the weather data.\nFinal model call to get the response.\n\nIf you dig deeper into the first model call, you‚Äôll see how the tool is provided to the model and how the model decides to use it:\n\n\n\nModel calls tool\n\n\nThe tools is provided by describing it to the model using the docstring of the function. The parameter and their types are also provided. Then the model responds specifying the name of the tool it wants to use and the parameters it wants to pass to it. This tool is then called:\n\n\n\nTool call\n\n\nThe results of the tool call are then passed to the model again. The model then uses the result to generate the final response:\n\n\n\nTool call result\n\n\nThat‚Äôs it. This how you provide a model with tools. In the next section, you‚Äôll see how to use multiple tools.\n\n\nFunction calling with multiple tools\nSimilar to the previous example, you start by defining the tools (using the @tool decorator) and binding them to the model.\nIn addition to get_weather, you‚Äôll also define a tool to check if a response follows the company guidelines. In this case, the company guidelines are that responses should be written in the style of a haiku.2\n\nmodel = ChatOpenAI(model_name=\"gpt-4.1-mini\")\n\n@tool\ndef get_weather(latitude: float, longitude: float):\n    \"\"\"Get the weather of a given latitude and longitude\"\"\"\n    response = requests.get(\n        f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\"\n    )\n    data = response.json()\n    return data[\"current\"][\"temperature_2m\"]\n\n\n@tool\ndef check_guidelines(drafted_response: str) -&gt; str:\n    \"\"\"Check if a given response follows the company guidelines\"\"\"\n    model = ChatOpenAI(model_name=\"gpt-4.1-mini\")\n    response = model.invoke(\n        [\n            SystemMessage(\n                \"You're a helpful assistant. Your task is to check if a given response follows the company guidelines. The company guidelines are that responses should be written in the style of a haiku. You should reply with 'OK' or 'REQUIRES FIXING' and a short explanation.\"\n            ),\n            HumanMessage(f\"Current response: {drafted_response}\"),\n        ]\n    )\n    return response.content\n\n\ntools_mapping = {\n    \"get_weather\": get_weather,\n    \"check_guidelines\": check_guidelines,\n}\n\nmodel_with_tools = model.bind_tools([get_weather, check_guidelines])\n\nThis code defines the tools and binds them to the model. Just like we did before, you also need to define a mapping of the tools, so that you can call the right tool when the model decides to use it.\n\n@traceable\ndef get_response(question: str):\n    messages = [\n        SystemMessage(\n            \"You're a helpful assistant. Use the tools provided when relevant. Then draft a response and check if it follows the company guidelines. Only respond to the user after you've validated and modified the response if needed.\"\n        ),\n        HumanMessage(question),\n    ]\n    ai_message = model_with_tools.invoke(messages)\n    messages.append(ai_message)\n\n    while ai_message.tool_calls:\n        for tool_call in ai_message.tool_calls:\n            selected_tool = tools_mapping[tool_call[\"name\"]]\n            tool_msg = selected_tool.invoke(tool_call)\n            messages.append(tool_msg)\n        ai_message = model_with_tools.invoke(messages)\n        messages.append(ai_message)\n\n    return ai_message.content\n\n\nresponse = get_response(\"What is the temperature in Madrid?\")\nprint(response)\n\nSunny Madrid basks,  \nThirty-six degrees embrace,  \nSummer's warm caress.\n\n\nThis code is pretty much the same as the previous example, but with two tools. There‚Äôs also a one small difference.\nPreviously, we checked for tool calls once. Now, we‚Äôll use a while loop that keeps checking. So, instead of the model having to provide the final answer after one turn, it can now ask for tools multiple times in a row until it has all the information it needs.\nThis is the core idea behind how agents work. So, congratulations, you‚Äôve just built a simple agent! If you check the process in LangSmith, you‚Äôll see how these turns play out.\nNext, let‚Äôs see how to use structured outputs.\n\n\nStructured outputs\nStructured outputs are a set of methods used to get model outputs that follow a specific structure. This is useful when you want to get a specific type of output, such as a JSON object.\nIt‚Äôs easy to set up with proprietary models. With LangChain, you can define a dict or a Pydantic model to describe the output. I recommend using Pydantic models.\nFor example, let‚Äôs define a Pydantic model that will help us classify document into categories:\n\nclass DocumentInfo(BaseModel):\n    category: Literal[\"financial\", \"legal\", \"marketing\", \"pets\", \"other\"] = Field(\n        description=\"The category of the document\"\n    )\n    summary: str = Field(description=\"A short summary of the document\")\n\nThis model defines the structured output we‚Äôll get from the model. It has two fields: category and summary.\nThen, you can use the with_structured_output method to create a model that will return the structured output:\n\nmodel = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n\ndef get_document_info(document: str) -&gt; DocumentInfo:\n    model_with_structure = model.with_structured_output(DocumentInfo)\n    response = model_with_structure.invoke(document)\n    return response\n\n\ndocument_text = dedent(\"\"\"\nThis is a document about cats. Very important document. It explain how cats will take over the world in 20230.\n\"\"\"\n)\ndocument_info = get_document_info(\"I'm a document about a cat\")\nprint(document_info)\n\ncategory='pets' summary='A document about a cat.'\n\n\nAfter running this code, you‚Äôll get a structured output with the category and summary of the document that you can then use in further steps of your workflow.\nDepending on the provider, you‚Äôll have different options to get structured outputs. OpenAI offers three different methods:\n\nfunction_calling: This uses the tool calling mechanism to get the structured output.\njson_mode: This method ensures you get a valid JSON object, but it‚Äôs not clear how it works under the hood.\njson_schema: This is default method in LangChain. It ensures that the output is a valid JSON object and that it matches the schema you provide using constrained decoding.\n\nGemini and Anthropic provide their own methods to get structured outputs.\nOne thing to keep in mind is that structured outputs can impact performance. I‚Äôve written multiple posts about this topic, so I won‚Äôt go into detail here."
  },
  {
    "objectID": "posts/function-calling-structured-outputs.html#conclusion",
    "href": "posts/function-calling-structured-outputs.html#conclusion",
    "title": "Function calling and structured outputs in LLMs with LangChain and OpenAI",
    "section": "Conclusion",
    "text": "Conclusion\nFunction calling and structured outputs are powerful tools that help build more capable AI systems. They‚Äôre also the foundation of agents.\nFunction calling is a way to provide LLMs with tools to use. It lets you go from building a chatbot that can only talk to building an AI assistant that can actually interact with the world. It opens up a world of possibilities, from connecting to databases, calling APIs, or automating workflows.\nStructured outputs are just as important. They‚Äôre critical to integrating LLMs into existing systems. Instead of struggling with parsing free-form text, you get clean, predictable data structures that you can use in your code.\nThe examples in this tutorial should give you a sense of how to use these methods. But as usual, the real learning happens when you start applying these concepts to your own problems. Pick a task you‚Äôre working on, see if any of these methods can help you, and give it a try.\nIf you have any questions or comments, let me know in the comments below."
  },
  {
    "objectID": "posts/function-calling-structured-outputs.html#footnotes",
    "href": "posts/function-calling-structured-outputs.html#footnotes",
    "title": "Function calling and structured outputs in LLMs with LangChain and OpenAI",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n‚ÄúWe Need Structured Output‚Äù: Towards User-centered Constraints on LLM Output. MX Liu et al.¬†2024‚Ü©Ô∏é\nPlease don‚Äôt judge me. Companies do all sorts of weird things these days.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html",
    "href": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html",
    "title": "Build an AI Search Engine Using FastAPI, Qdrant, and ChatGPT",
    "section": "",
    "text": "With all the buzz surrounding Bing AI and Bard, I was keen on building a (tiny) AI search engine myself. After a few days of tinkering, I released Ask Seneca. It‚Äôs a small app that allows you to consult a GPT-based Seneca who answers your questions and cites his sources.\nWhen a user asks a question, Ask Seneca searches for Seneca‚Äôs most relevant writings to answer that question and then summarizes those writings into a coherent answer. I built it using FastAPI, Qdrant, Sentence Transformers, and GPT-3. I recently updated it to use the ChatGPT API.\nDespite the setbacks that Bing AI and Bard are facing, the potential for this technology is vast - you could build tools for quick and efficient searches through legal documents, internal knowledge bases, product manuals, and more.\nIn this tutorial, I‚Äôll show you how to build your own AI search engine. You‚Äôll create an app that lets users ask questions to a GPT-based Marcus Aurelius, and provides them with concise answers and references to his Meditations.\nLet‚Äôs get to it!"
  },
  {
    "objectID": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#prerequisites",
    "href": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#prerequisites",
    "title": "Build an AI Search Engine Using FastAPI, Qdrant, and ChatGPT",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo make the most out of this tutorial, you should know:\n\nWhat semantic search is.\nWhat vector databases are.\nWhat FastAPI is and how to use it.\n\nYou don‚Äôt have to be an expert in any of these areas, but familiarity with them will help you understand the sections that follow."
  },
  {
    "objectID": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#designing-a-tiny-search-engine-with-chatgpt",
    "href": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#designing-a-tiny-search-engine-with-chatgpt",
    "title": "Build an AI Search Engine Using FastAPI, Qdrant, and ChatGPT",
    "section": "Designing a (Tiny) Search Engine with ChatGPT",
    "text": "Designing a (Tiny) Search Engine with ChatGPT\nBefore you get started, you should understand the overall approach you‚Äôll take to build your AI search engine. There are three parts to it:\n\nExtraction: This part consists of extracting the data that you want users to be able to search. In this case, that means parsing Meditations. I won‚Äôt go into detail about this because it is very project-specific. The parsed data is available in the repository.\nIndexing: This entails indexing the extracted data so that it can be accessed later when running searches. In this case, you‚Äôll use a semantic search approach, which means you‚Äôll search the data based on its meaning rather than keywords. That is, if you search for ‚ÄúHow can I be happy?‚Äù you should get passages from Meditations that discuss happiness or feeling good, not just those that contain the exact words from the query.\nSearch: This consists of a backend service that processes the user‚Äôs query, vectorizes it, finds vectors in the index that are the most similar to it, and then calls OpenAI‚Äôs API to generate a summarized answer for the user.\n\nHere‚Äôs a visual representation of how the parts of the application you‚Äôll build in this tutorial fit together:\n\nThat‚Äôs all. Let‚Äôs continue!"
  },
  {
    "objectID": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#set-up-your-local-environment",
    "href": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#set-up-your-local-environment",
    "title": "Build an AI Search Engine Using FastAPI, Qdrant, and ChatGPT",
    "section": "Set Up Your Local Environment",
    "text": "Set Up Your Local Environment\nTake the following steps to prepare your local environment:\n\nInstall Python 3.10.\nInstall Poetry. It‚Äôs not mandatory but I highly recommend it.\nClone the repository with the sample app:\n\ngit clone https://github.com/dylanjcastillo/ai-search-fastapi-qdrant-chatgpt\n\nGo to the root folder of the project and install the dependencies with:\n\nPoetry: Create the virtual environment in the same directory as the project and install the dependencies:\n\npoetry config virtualenvs.in-project true\npoetry install\n\nvenv and pip: Create a virtual environment and install the dependencies listed in requirements.txt:\n\npython3.10 -m venv .venv && source .venv/bin/activate\npip install -r requirements.txt\n\n\n\n\n\n\n\nNote\n\n\n\nBecause PyTorch does not yet support Python 3.11 in MacOS and Windows, this tutorial will not work if you are running Python 3.11 on those operating systems.\n\n\nIf everything went well, you should have a virtual environment with all of the necessary libraries and a project structure that looks like this:\nai-search-fastapi-qdrant-gpt3\n‚îÇ\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ config.py\n‚îú‚îÄ‚îÄ data\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ processed\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ Marcus_Aurelius_Antoninus...\n‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ Marcus_Aurelius_Antoninus...json\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ unzipped\n‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ Marcus_Aurelius_Antoninus...\n‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ index.html\n‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ metadata.opf\n‚îÇ¬†¬†         ‚îî‚îÄ‚îÄ style.css\n‚îú‚îÄ‚îÄ main.py\n‚îú‚îÄ‚îÄ notebooks\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ extract_text.ipynb\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ vectorize_text.ipynb\n‚îú‚îÄ‚îÄ poetry.lock\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îú‚îÄ‚îÄ requirements.txt\n‚îú‚îÄ‚îÄ .env-example\n‚îî‚îÄ‚îÄ .venv/\nThis is your project‚Äôs structure. Let me explain the purpose of the most important files and directories:\n\nconfig.py: This file contains project configuration specifications such as Qdrant‚Äôs host, port, and API key (read from a .env file)\ndata/: This directory contains the project‚Äôs data. It contains Meditations as originally downloaded from Wikisource as well as the processed file that you will use in the project.\nmain.py: This file contains the code of the FastAPI application.\nnotebooks/: This directory contains Jupyter notebooks for extracting, vectorizing, and indexing the data. extract_text.ipynb contains code to parse the HTML file and vectorize_text.ipynb contains code to vectorize and index the data.\npoetry.lock and pyproject.toml: These files contain information about the project‚Äôs dependencies and are used by Poetry to replicate the environment.\nrequirements.txt: This file contains a list of Python packages required by the project and their respective versions.\n.env-example: This file is an example of the environment variables you must provide.\n.venv/: This directory contains the project‚Äôs virtual environment.\n\nThat‚Äôs it! You‚Äôre now ready to get started."
  },
  {
    "objectID": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#configure-qdrant-and-openai",
    "href": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#configure-qdrant-and-openai",
    "title": "Build an AI Search Engine Using FastAPI, Qdrant, and ChatGPT",
    "section": "Configure Qdrant and OpenAI",
    "text": "Configure Qdrant and OpenAI\nStart by renaming .env-example to .env. Don‚Äôt worry about filling in the values in .env. After you‚Äôve created a cluster and the API keys for Qdrant and OpenAI, you‚Äôll fill in the blanks.\n\nQdrant\nCreate an account at Qdrant, if you don‚Äôt already have one. Then, on your account page go to Clusters &gt; Create, and create a cluster of 1GB of RAM, 0.5 vCPU, and 20GB Disk. Qdrant has a generous free tier, and it‚Äôs free to run a cluster with those specifications.\n\nNext, paste the host and API key you obtained when you created your cluster into .env:\nQDRANT_PORT=6333\nQDRANT_HOST=&lt;your_qdrant_host&gt;\nQDRANT_API_KEY=&lt;your_qdrant_api_key&gt;\nIf you didn‚Äôt copy the key, you can still create a new one in Access.\nFinally, you can test that everything went well by running the first three cells in vectorize_data.ipynb.\n\n\nOpenAI\nIf you don‚Äôt have an OpenAI account, create one. After that, go to Manage account &gt; API keys &gt; ¬†+ Create new secret key.\n\nThen, paste the generated key in .env:\nQDRANT_PORT=6333\nQDRANT_HOST=&lt;your_qdrant_host&gt;\nQDRANT_API_KEY=&lt;your_qdrant_api_key&gt;\nOPENAI_API_KEY=&lt;your_openai_api_key&gt; # new\nThat‚Äôs it! Let‚Äôs continue."
  },
  {
    "objectID": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#extract-data",
    "href": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#extract-data",
    "title": "Build an AI Search Engine Using FastAPI, Qdrant, and ChatGPT",
    "section": "Extract Data",
    "text": "Extract Data\nThe data extraction pipeline will vary greatly between projects, so I won‚Äôt go into too much detail here.\nHere are some useful guidelines to keep in mind when doing so:\n\nGarbage in, garbage out: The quality of your data will heavily influence your search results, so take your time with this step.\nSplitting documents: When you do semantic search, you need to divide documents into smaller chunks so that you can compare the similarity of each chunk to the user‚Äôs query. There is no right or wrong way to do this. In this case, I took a straightforward approach: divide the text into paragraphs, and if the paragraph is above a certain number of characters, divide it into multiple sentences.\nProduction: For real-world scenarios, you should think about how frequently you‚Äôll be extracting and ingesting data, adapting your pipeline for different data sources (e.g., scraping, APIs), and building pipeline monitors, among other things. In this example, because data extraction is a one-time event, I‚Äôm using jupyter notebooks, which isn‚Äôt always a good idea.\n\nHere‚Äôs a sneak peek at the data from this tutorial:\n{\n    \"book_title\": \"Meditations by Marcus Aurelius\",\n    \"url\": \"https://en.wikisource.org/wiki/Marcus_Aurelius_Antoninus_-_His_Meditations_concerning_himselfe\",\n    \"data\": [\n        {\n            \"title\": \"THE FIRST BOOK\",\n            \"url\": \"https://en.wikisource.org/wiki/Marcus_Aurelius_Antoninus_-_His_Meditations_concerning_himselfe#THE_FIRST_BOOK\",\n            \"sentences\": [\n                \"I. Of my grandfather Verus I have learned to be gentle and meek...\",\n                \"II. Of him that brought me up, not to be fondly addicted...\",\n                \"III. Of Diognetus, not to busy myself about vain things...\",\n                \"IV. To Rusticus I am beholding, that I first entered into the...\",\n    ...\n        }\n    ]\n }\nThe tutorial‚Äôs data includes general metadata such as the book title and source URL, as well as information from each chapter with the sentences you‚Äôll index.\nIf you want to take a look at how I extracted the data used in this tutorial, check out the [extract_data.ipynb](https://github.com/dylanjcastillo/ai-search-fastapi-qdrant-gpt3/blob/main/notebooks/extract_text.ipynb)."
  },
  {
    "objectID": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#vectorize-and-index-data",
    "href": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#vectorize-and-index-data",
    "title": "Build an AI Search Engine Using FastAPI, Qdrant, and ChatGPT",
    "section": "Vectorize and Index Data",
    "text": "Vectorize and Index Data\nOnce you‚Äôve extracted the data, you‚Äôll want to index it in your vector database.\nThe process consists of two steps:\n\nGenerate vectors for each sentence you extracted earlier.\nInsert those vectors in a collection(the set of vectors you can search in the vector database).\n\nYou can find the code for this section in notebooks/vectorize_data.ipynb.\nAs usual, you start by importing the required libraries:\nimport json\n\nimport numpy as np\nimport pandas as pd\nimport torch\n\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http import models\nfrom sentence_transformers import SentenceTransformer\n\nfrom tqdm.notebook import tqdm\n\nfrom config import QDRANT_HOST, QDRANT_PORT, QDRANT_API_KEY, DATA, COLLECTION_NAME\nThis code imports all the libraries and configuration variables you need to vectorize and index the data. Here are some things worth mentioning:\n\nqdrant_client and qdrant_client.http let you interact with Qdrant‚Äôs client, so that you can insert and retrieve data from the collection.\nsentence_transformers let you generate the vectors from text, using pretrained models.\n\nNext, you read the data as follows:\nBOOK_FILENAME = \"Marcus_Aurelius_Antoninus_-_His_Meditations_concerning_himselfe\"\n\nwith open(f\"{DATA}/processed/{BOOK_FILENAME}/{BOOK_FILENAME}.json\", \"r\") as file:\n    meditations_json = json.load(file)\n\nrows = []\nfor chapter in tqdm(meditations_json[\"data\"]):\n    for sentence in chapter[\"sentences\"]:\n        rows.append(\n            (\n                chapter[\"title\"],\n                chapter[\"url\"],\n                sentence,\n            )\n        )\n\ndf = pd.DataFrame(\n    data=rows, columns=[\"title\", \"url\", \"sentence\"]\n)\n\ndf = df[df[\"sentence\"].str.split().str.len() &gt; 15]\nThis code reads the previously processed data and removes short sentences. It works as follows:\n\nLines 1 to 4 read the JSON file you generated for Meditations, and save it as meditations_json.\nLines 6 to 15 go through all the chapters of the book stored in the data key from meditations_json and for each chapter, it extracts the relevant data (title of chapter, URL of chapter, and sentence), and adds it to rows.\nLines 17 to 21 create a DataFrame with the data from rows and removes the sentences with less than 15 words.\n\nNext, you create a collection in your vector database:\n# Create collection\nclient = QdrantClient(\n    host=QDRANT_HOST, port=QDRANT_PORT, api_key=QDRANT_API_KEY\n)\nclient.recreate_collection(\n    collection_name=COLLECTION_NAME,\n    vectors_config=models.VectorParams(\n        size=384,\n        distance=models.Distance.COSINE\n    ),\n)\nThis code connects to your Qdrant‚Äôs cluster and creates a collection based on the name and settings you provide. In this case, you set the size to 384 based on the needs of the model you‚Äôll use for vectorizing the sentences. You also set distance to use Cosine distance, which will define how the similarity between vectors is computed.\nThe next step is to generate the vectors (embeddings) from the text. You‚Äôll use a pretrained model from Sentence Transformers to generate them instead of OpenAI-based embeddings. The latter are more expensive and not necessarily better.\nTo accomplish this, you load the pretrained model, generate the embeddings from the DataFrame sentences, and insert them into the collection you created:\nmodel = SentenceTransformer(\n    \"msmarco-MiniLM-L-6-v3\",\n    device=\"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\",\n)\n\nvectors = []\nbatch_size = 512\nbatch = []\n\nfor doc in tqdm(df[\"sentence\"].to_list()):\n    batch.append(doc)\n\n    if len(batch) &gt;= batch_size:\n        vectors.append(model.encode(batch))\n        batch = []\n\nif len(batch) &gt; 0:\n    vectors.append(model.encode(batch))\n    batch = []\n\nvectors = np.concatenate(vectors)\n\nbook_name = meditations_json[\"book_title\"]\n\nclient.upsert(\n    collection_name=COLLECTION_NAME,\n    points=models.Batch(\n        ids=[i for i in range(df.shape[0])],\n        payloads=[\n            {\n                \"text\": row[\"sentence\"],\n                \"title\": row[\"title\"] + f\", {book_name}\",\n                \"url\": row[\"url\"],\n            }\n            for _, row in df.iterrows()\n        ],\n        vectors=[v.tolist() for v in vectors],\n    ),\n)\nThis code loads the model, generates vectors from the sentences in the DataFrame, and inserts them into the collection you created. Here‚Äôs how it works:\n\nLines 1 to 8 load the msmarco-MiniLM-L-6-v3 sentence transformer model, and set the correct device in case you have a GPU available.\nLines 10 to 23 generate an array of vectors using the model you loaded. Each vector is a numerical representation of the sentences from your DataFrame.\nLines 29 to 43 insert the vectors and the additional data (actual sentence, book and chapter title, and URL) into the collection in your vector database."
  },
  {
    "objectID": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#create-a-server-with-fastapi",
    "href": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#create-a-server-with-fastapi",
    "title": "Build an AI Search Engine Using FastAPI, Qdrant, and ChatGPT",
    "section": "Create a Server with FastAPI",
    "text": "Create a Server with FastAPI\nNext, you will create the FastAPI application that will let the user interact with your vector database and ChatGPT. The code for this section is in main.py.\nYou start by importing the required dependencies, setting up your Qdrant client, and loading the model:\nimport openai\nfrom fastapi import FastAPI\nfrom qdrant_client import QdrantClient\nfrom sentence_transformers import SentenceTransformer\n\nfrom config import (\n    COLLECTION_NAME,\n    OPENAI_API_KEY,\n    QDRANT_API_KEY,\n    QDRANT_HOST,\n    QDRANT_PORT,\n)\n\nopenai.api_key = OPENAI_API_KEY\n\nqdrant_client = QdrantClient(\n    host=QDRANT_HOST,\n    port=QDRANT_PORT,\n    api_key=QDRANT_API_KEY,\n)\n\nretrieval_model = SentenceTransformer(\"msmarco-MiniLM-L-6-v3\")\n\napp = FastAPI()\nThis code imports the libraries and configuration settings, initializes the Qdrant client, and loads the model to memory (the same one you used for vectorizing the sentences). You load your model globally so that you don‚Äôt slow down requests by loading it each time someone asks a question.\nNext, you define a function to help you create the prompt that you‚Äôll use to get ChatGPT to generate a coherent answer based on the most relevant passages from Meditations:\ndef build_prompt(question: str, references: list) -&gt; tuple[str, str]:\n    prompt = f\"\"\"\n    You're Marcus Aurelius, emperor of Rome. You're giving advice to a friend who has asked you the following question: '{question}'\n\n    You've selected the most relevant passages from your writings to use as source for your answer. Cite them in your answer.\n\n    References:\n    \"\"\".strip()\n\n    references_text = \"\"\n\n    for i, reference in enumerate(references, start=1):\n        text = reference.payload[\"text\"].strip()\n        references_text += f\"\\n[{i}]: {text}\"\n\n    prompt += (\n        references_text\n        + \"\\nHow to cite a reference: This is a citation [1]. This one too [3]. And this is sentence with many citations [2][3].\\nAnswer:\"\n    )\n    return prompt, references_text\nThis code will combine a prompt to make ChatGPT ‚Äúsimulate‚Äù Marcus Aurelius answering a user-supplied question with a list of references previously obtained from your vector database. Then it will return the generated prompt, and a list of the references to add to the answer sent to the user.\nThen you create two endpoints as follows:\n@app.get(\"/\")\ndef read_root():\n    return {\n        \"message\": \"Make a post request to /ask to ask a question about Meditations by Marcus Aurelius\"\n    }\n\n\n@app.post(\"/ask\")\ndef ask(question: str):\n    similar_docs = qdrant_client.search(\n        collection_name=COLLECTION_NAME,\n        query_vector=retrieval_model.encode(question),\n        limit=3,\n        append_payload=True,\n    )\n\n    prompt, references = build_prompt(question, similar_docs)\n\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n        max_tokens=250,\n        temperature=0.2,\n    )\n\n    return {\n        \"response\": response[\"choices\"][0][\"text\"],\n        \"references\": references,\n    }\nThese are the two endpoints that you‚Äôll use in your app. Here‚Äôs what each line does:\n\nLines 1 to 5 set up an endpoint that accepts GET requests on ‚Äú/‚Äù. It returns a JSON response with a message key telling the user to use the ‚Äú/ask‚Äù endpoint.\nLines 8 to 17 define an endpoint that takes accepts POST requests on ‚Äú/ask‚Äù, with a single parameter, questionof string type. Once the user submits a request, you vectorize the question using the model you loaded previously, then you get the 3 most similar documents from your vector database.\nLines 19 to 32 combine the documents you got from the vector database with your prompt and make a request to the ChatGPT API. You set max_tokens=250 to keep answers short and set temperature=0.2, to prevent the model from getting ‚Äútoo creative‚Äù with its responses. Finally, you extract the answer from the ChatGPT API response and return it to the user, along with the references.\n\nIf you want to test it locally, type the following command into a terminal (inside the project‚Äôs virtual environment):\nuvicorn main:app --reload\nIn your browser, navigate to localhost:8000/docs to test your /ask endpoint:\n\nA successful response will look as follows:\n\nThat‚Äôs it! You have a working version of your AI search engine. Next, I‚Äôll mention some ideas about deployment."
  },
  {
    "objectID": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#deploy-your-app",
    "href": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#deploy-your-app",
    "title": "Build an AI Search Engine Using FastAPI, Qdrant, and ChatGPT",
    "section": "Deploy Your App",
    "text": "Deploy Your App\nThere are many different ways to deploy an app, so you choose whatever approach you prefer. In my case, I like to use a VPS with NGINX acting as a reverse proxy and using Gunicorn as a process manager with Uvicorn workers. If you‚Äôd like to follow that approach, check a tutorial I wrote about it.\nIf you choose that route, keep the following points in mind:\n\nYou should use [--preload](https://dylancastillo.co/fastapi-nginx-gunicorn/) if you want to share the same model across all the processes and use less RAM memory.\nThere are memory leak issues when serving some types of models. A workaround that has worked for me is setting --max-requests and --max-requests-jitter to low numbers."
  },
  {
    "objectID": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#conclusion",
    "href": "posts/ai-search-engine-fastapi-qdrant-chatgpt.html#conclusion",
    "title": "Build an AI Search Engine Using FastAPI, Qdrant, and ChatGPT",
    "section": "Conclusion",
    "text": "Conclusion\nWay to go! In this tutorial, you‚Äôve built a (tiny) AI search engine. You‚Äôve learned:\n\nHow to structure the project.\nHow to set up a vector database using Qdrant.\nHow to vectorize your data using Sentence Transformers.\nHow to use ChatGPT to combine references into a coherent answer.\n\nHope you found this tutorial useful. Let me know if you have any questions!\nAll the code for this tutorial is available on GitHub."
  },
  {
    "objectID": "posts/create-a-kamal-ready-vps-on-hetzner-using-terraform.html",
    "href": "posts/create-a-kamal-ready-vps-on-hetzner-using-terraform.html",
    "title": "Create a Kamal-ready VPS on Hetzner using Terraform",
    "section": "",
    "text": "These days, I deploy all my side projects using Kamal and GitHub Actions on Hetzner. Once you get the hang of it, it‚Äôs easy to maintain, fast, and cheap.\nYou can run your app with a database (SQLite), caching (Redis), background jobs (Celery), and SSL certificates (Let‚Äôs Encrypt) for roughly ‚Ç¨5/month. Plus, if you feel the need, you can easily scale up to a more powerful Virtual Private Server (VPS).\nBut setting up a VPS with the right configuration takes a bit of time. You have to:\nI already had a small script to do most of these steps, but I wanted to automate it to a single command. So I created a Terraform script to do it for me.\nI took terraform-hetzner and modified it to work with a single VPS instance. My updated version is available here."
  },
  {
    "objectID": "posts/create-a-kamal-ready-vps-on-hetzner-using-terraform.html#set-up",
    "href": "posts/create-a-kamal-ready-vps-on-hetzner-using-terraform.html#set-up",
    "title": "Create a Kamal-ready VPS on Hetzner using Terraform",
    "section": "Set up",
    "text": "Set up\nFirst, set up an API key with read and write permissions in Hetzner Cloud.\nSecond, install terraform.\nThird, clone the repo:\ngit clone https://github.com/dylanjcastillo/terraform-kamal-single-vps\nFourth, create a terraform.tfvars file with the following variables:\nhetzner_api_key = \"your-api-key\"\nssh_vps_root_key = \"&lt;your-ssh-root-public-key&gt;\"\nssh_vps_kamal_key = \"&lt;your-ssh-kamal-public-key&gt;\"\nThe ssh_vps_root_key and ssh_vps_kamal_key are the public keys for the root and kamal users, respectively. You can generate them with the make generate-ssh-key USER_NAME=root or make generate-ssh-key USER_NAME=kamal commands I added to the repo.\nStore your SSH keys in a secure location. You‚Äôll need them to access the VPS."
  },
  {
    "objectID": "posts/create-a-kamal-ready-vps-on-hetzner-using-terraform.html#run-the-script",
    "href": "posts/create-a-kamal-ready-vps-on-hetzner-using-terraform.html#run-the-script",
    "title": "Create a Kamal-ready VPS on Hetzner using Terraform",
    "section": "Run the script",
    "text": "Run the script\nOnce the terraform.tfvars file is set up, you can see what changes will be applied with the following command:\nterraform plan\nThis will show in detail what changes will be applied to create a Kamal-ready VPS. If you‚Äôre happy with it, you can apply the changes with the following command:\nterraform apply\nThis will create a VPS with the following configuration:\n\nUbuntu 22.04 LTS\n2 VCPU\n2 GB RAM\n40 GB SSD\n\nIt‚Äôll cost you roughly ‚Ç¨5/month and will be located in Nuremberg (Germany).\nIn addition, after the VPS is created, it will automatically:\n\nCreate a non-root user (kamal) with sudo privileges.\nInstall the required software (Git, Docker, curl, etc.)\nCreate a directory for Let‚Äôs Encrypt SSL certificates.\nCreate a firewall rule to allow HTTP, HTTPS, and SSH traffic.\nCreate a directory for the database (SQLite) and the cache (Redis) (db/ and data/)"
  },
  {
    "objectID": "posts/create-a-kamal-ready-vps-on-hetzner-using-terraform.html#customizing-the-script",
    "href": "posts/create-a-kamal-ready-vps-on-hetzner-using-terraform.html#customizing-the-script",
    "title": "Create a Kamal-ready VPS on Hetzner using Terraform",
    "section": "Customizing the script",
    "text": "Customizing the script\nYou can customize the script to fit your needs. Here are a couple of things you can change:\n\nChange the software to install\nIf you want to change the software to install, you can modify the packages section in cloudinit/vps.yml.\n\n\nRun other commands after the VPS is created\nIf you want to run other commands after the VPS is created, you can add them to the runcmd section in the cloudinit/vps.yml file.\n\n\nUse already existing firewall rules\nIf you want to use already existing firewall rules, you can modify how the firewalls are attached in cloud.tf. Take a look at this section of cloud.tf.\n\n\nUse a different server type, operating system, or region\nIf you want to use a different server type, operating system, or region, you can modify the server_type, region, operating_system variables in variables.tf."
  },
  {
    "objectID": "posts/create-a-kamal-ready-vps-on-hetzner-using-terraform.html#conclusion",
    "href": "posts/create-a-kamal-ready-vps-on-hetzner-using-terraform.html#conclusion",
    "title": "Create a Kamal-ready VPS on Hetzner using Terraform",
    "section": "Conclusion",
    "text": "Conclusion\nThis script is a great way to create a Kamal-ready VPS on Hetzner using Terraform. It‚Äôs easy to maintain, fast, and cheap.\nAll the code is available in the repo.\nHope you find this useful!"
  },
  {
    "objectID": "posts/2021-personal-snapshot.html",
    "href": "posts/2021-personal-snapshot.html",
    "title": "2021: Personal Snapshot",
    "section": "",
    "text": "When I see pictures from my childhood, I always try to remember who I was at the time the photos were shot. Like, what was going through my mind? What motivated me? What scared me?\nRemembering that is harder than it sounds. Pictures often help you recall sounds, smells, and emotions. But they don‚Äôt work well if you‚Äôre trying to recollect the inner workings of your mind.\nThere may not be a perfect way to remember how you felt at a certain period in your life. But I‚Äôve found that reading things I‚Äôve written in the past serves as a good substitute. After all, it‚Äôs easier to find out what was on your mind from reading your journal than it is to do so from looking at a random photo of you and your cousins.\nThis blog is pretty recent, so I don‚Äôt have much to refer to here. But when I was a teenager, I hung out in various online forums and wrote short stories and poems on text files. So if I want to remember Dylan from high school, I reread my posts, stories, and poems from that time.\nMany of those online forums have since died, and I‚Äôve lost most of those text files due to faulty backups. That got me thinking that I had to find a better way to store those snapshots of myself.\nSo, for the foreseeable future, I‚Äôve decided to write a Personal Snapshot of the year in my blog at the end of each year. It‚Äôs a yearly review that keeps a record of who I was when I wrote it. I‚Äôm hoping to get a few laughs out of this in a decade.\nIf it‚Äôs me rereading this, welcome back. This is Dylan from 2021."
  },
  {
    "objectID": "posts/2021-personal-snapshot.html#what-went-well",
    "href": "posts/2021-personal-snapshot.html#what-went-well",
    "title": "2021: Personal Snapshot",
    "section": "What Went Well?",
    "text": "What Went Well?\nDespite all the chaos due to COVID-19, I had a great year and achieved multiple goals I had set for myself.\nFirst, I‚Äôm a freelancer now. In mid-2019, I began to consider this option. My first actions included adding random people on LinkedIn and informing them that I intended to work as a freelancer. That was as useless as it sounds!\nThis year, I took a more systematic approach. I focused on contacting recruiters and managers hiring contractors and was more persistent throughout the process. I even went after contracts that seemed out of reach, asked past colleagues for referrals, and asked to reconsider when I got ‚Äúno‚Äù for an answer.\nThis change resulted in two freelancing contracts (European Commission and Deliveroo) and a teaching gig at Nuclio. It took a long time to get the first contract, but now I‚Äôm always getting new opportunities. Needless to say, I‚Äôm delighted.\nTo be honest, my day-to-day as a freelancer hasn‚Äôt changed much compared to when I was a full-time employee. But that‚Äôs expected, considering that I‚Äôm still selling my time rather than a product. I hope to change that soon.\nSecond, I‚Äôm earning more and saving more. I hadn‚Äôt begun freelancing sooner because I didn‚Äôt want to compete for 10$/h jobs on Upwork or Fiverr. I would only leave my full-time job if I could secure a freelancing contract that allowed me to save money at a similar or higher rate than I did as an employee.\nThis year, I negotiated good contracts, which increased my income and, as a result, my savings rate. All in all, I had three sources of revenue this year (in order of importance): Freelancing, cashing out a bit of crypto, and the teaching gig.\nThird, a great relationship became a great team. This year my wife made a breakthrough in her career, which she had been fighting for over the last four years. It was tough, but we learned how to work together as a team in the process.\nI became her part-time coach, cheerleader, and teammate throughout this time. I attempted to provide as much help as possible, as she studied 12 to 14 hours each day for about a year. We spent hours debating techniques, going over practice tests, and experimenting with slight tweaks to her prep system. It was well worth the effort!\nIt was one of those times when someone else accomplishes something, but you‚Äôre so involved in the process that you feel like you‚Äôve achieved it as well.\nThat got me thinking that good relationships are made of independent individuals who love and trust each other. But, just as important, is that they‚Äôve decided to row in the same direction and work as a team to achieve their goals. That‚Äôs how I see us.\nFourth, I took good care of my mental health. I‚Äôve found that practicing Stoic precepts (such as daily journaling, negative visualization, and reminding myself that I cannot control most events in my life), and exercising regularly have made me happier. I still have bad days, but very few compared to previous years.\nFifth, it‚Äôs been a good year for my family. My parents and sister are in good health, stay active, and have access to quality healthcare. Each family faces unique hurdles when they emigrate from Venezuela. Still, I‚Äôm pleased with how it‚Äôs going for us thus far."
  },
  {
    "objectID": "posts/2021-personal-snapshot.html#what-should-i-change",
    "href": "posts/2021-personal-snapshot.html#what-should-i-change",
    "title": "2021: Personal Snapshot",
    "section": "What Should I Change?",
    "text": "What Should I Change?\nFor the last couple of years, life has been comfortable enough that I haven‚Äôt needed to push my boundaries. That‚Äôs no good.\nFirst, I need to change my approach if I want to achieve financial freedom. I don‚Äôt want to spend the next forty years of my life working for someone else five days per week while enjoying freedom on weeknights and weekends. So I need to find a way to disentangle my income from my time, sooner rather than later.\nAlthough freelancing was the first step in that direction, I am still selling my time. So next, I‚Äôd like to start selling a product, such as an ebook or a course. To do so, I need an audience willing to pay for it.\nI studied people who‚Äôve grown their online audiences quickly. With no exceptions, they did it by sharing high-quality content on social media on a regular basis. So I need to get off my butt and post regularly on social media.\nSecond, I must push myself out of my comfort zone. ‚ÄúWhat doesn‚Äôt kill me makes me stronger,‚Äù as Nietzsche famously said, should be complemented by the also valid ‚ÄúWhat keeps me cozy makes me weaker.‚Äù\nI feel that I‚Äôve been too cozy the past few years, and I wasn‚Äôt taking enough risks. If this keeps up, I‚Äôll be a boring old man with few stories to tell my grandchildren.\nThe problem is that society wants you to follow its standard script when you reach adulthood: find a job, get married, have kids, get a mortgage, buy a car, have a couple of midlife crises, possibly get divorced, and then retire at 65 to tend to your garden.\nThat‚Äôs not the life I want to live. Ask yourself, if that was a book, would you read it?\nMost people wouldn‚Äôt put a few hours to read such a book. They won‚Äôt mind, though, spending their entire lives living that by that script.\nOne thing that a happy life and a good book have in common is that their foundation is a good story. Staying in your lane and following society‚Äôs script is a certain way to write a story that no one wants to read, including yourself.\nYou‚Äôre the hero of your own story. Heroes don‚Äôt spend their lives binge-watching Netflix or scrolling through social media. They face challenges and grow as a result of that.\nThat‚Äôs my goal for next year: grow by stepping out of my comfort zone.\nWhat exactly will I be doing? You‚Äôll need to follow my blog to learn more üòâ"
  },
  {
    "objectID": "posts/2021-personal-snapshot.html#what-do-i-believe-in",
    "href": "posts/2021-personal-snapshot.html#what-do-i-believe-in",
    "title": "2021: Personal Snapshot",
    "section": "What do I Believe In?",
    "text": "What do I Believe In?\nFinally, I thought it‚Äôd be interesting to keep track of my beliefs, to see how they change over time. Some of these are mental frameworks, others are random thoughts.\nHere‚Äôs a non-exhaustive list of them:\n\nStoicism is the OG of mind hacks. Cognitive Behavioral Therapy, the leading technique of psychotherapy, is deeply influenced by Stoicism. My mental health has greatly improved as a result of following Stoic teachings. Others could benefit as well, so here are two good places to start: Enchiridion or A Guide to the Good Life.\nComfort kills. At a micro-level, make your life harder, by doing things such as taking cold showers, fasting, and exercising every day. At a macro-level, do what scares you and excites you at the same time. When it comes to determining what will help you grow, trust your instincts.\nDon‚Äôt discuss nutrition and vaccines. Add them to Religion, and Politics. Everyone has a very personal opinion on these topics. There‚Äôs no point in discussing them with friends.\nThe 80/20 of Data Science. Data Scientists like to build models, but most of the value comes from transforming raw data into useful insights for the business. That means setting up pipelines, generating reports, and building dashboards.\nThe 80/20 of staying healthy. Avoid ultra-processed foods, sugar, and vegetable oils. Eat foods that have been around for a long time. Exercise regularly, doing Strength Training and some sort of cardio."
  },
  {
    "objectID": "posts/2021-personal-snapshot.html#looking-ahead",
    "href": "posts/2021-personal-snapshot.html#looking-ahead",
    "title": "2021: Personal Snapshot",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nI have high hopes for 2022. I‚Äôm eager to work on decoupling my income from my time and putting more time into my personal growth.\nI hope to see you around!\nThanks to Mar√≠a, Caryn, Christine, Georgia, and Mack for their feedback on this piece."
  },
  {
    "objectID": "posts/what-is-rag.html",
    "href": "posts/what-is-rag.html",
    "title": "What is Retrieval Augmented Generation (RAG)?",
    "section": "",
    "text": "Retrieval Augmented Generation (RAG) is the most popular approach to providing LLMs with external information before they generate a response.\nRAG is a technique where you retrieve the information required to solve a user‚Äôs query, then augment the context of the LLM with that information, and generate a response. In this tutorial, you‚Äôll learn why RAG is useful, when to use it, and how to build your own RAG pipeline, step-by-step, using Python.\nLet‚Äôs get started!"
  },
  {
    "objectID": "posts/what-is-rag.html#what-is-rag",
    "href": "posts/what-is-rag.html#what-is-rag",
    "title": "What is Retrieval Augmented Generation (RAG)?",
    "section": "What is RAG?",
    "text": "What is RAG?\nIt‚Äôs a technique to improve LLM answers by providing them with external information before they generate a response. It consists of three steps:\n\nRetrieve: The system starts by searching a specific knowledge base for relevant information about the query.\nAugment: This retrieved information is added to context that‚Äôs used by the LLM to generate a response.\nGenerate: The LLM uses both your question and the provided information to generate an answer.\n\nIn addition to reducing costs and latency, RAG is useful because it reduces hallucinations, lets you use current data, and builds trust with users by (potentially) providing citations."
  },
  {
    "objectID": "posts/what-is-rag.html#vector-databases",
    "href": "posts/what-is-rag.html#vector-databases",
    "title": "What is Retrieval Augmented Generation (RAG)?",
    "section": "Vector databases",
    "text": "Vector databases\nA vector database (VectorDB) is a database designed to store and query data as vector embeddings (numerical representations). So, provided with a user query, it‚Äôs the engine you use to find the most similar data in your database. It‚Äôs one of the most popular components of the retrieval step in RAG pipelines.\nIn recent years, many new vector databases have been created. But, in most cases, they had to re-discover that many of the ideas in the old generation of vector databases such as BM25-based retrieval were still valid and useful.\nSome popular vector databases are:\n\nNew generation: Qdrant, Chroma, Pinecone, Weaviate.\nOld generation: Elasticsearch/OpenSearch and Postgres+PGVector\n\nIn this tutorial, you‚Äôll use Chroma. For client projects, I‚Äôve used Elasticsearch, Postgres, Weaviate, and Qdrant. Many companies are already using Elasticsearch or Postgres, so it‚Äôs often easier to get started with them.\n\nWhy use a VectorDB?\nIf you have a small dataset, there‚Äôs no real reason to use a vector database. But if you‚Äôre dealing with thousands or millions of documents, you‚Äôll need to use a vector database to efficiently retrieve the most relevant documents.\nThey‚Äôre useful because:\n\nThe more noise in the context provided to the LLM, the more likely it is to produce bad output.\nIt takes more time to process a longer context\nIt costs more to process a longer context\n\n\n\nRetrieval\nRetrieval is the process of finding the most relevant documents in the vector database. There are two main approaches when dealing with text-based data: term-based retrieval and embedding-based retrieval.\n\nTerm-based retrieval\nTerm-based retrieval is a technique that uses the terms in the query to find the most relevant documents in the vector database.\nIt‚Äôs based on the following ideas:\n\nTF-IDF: Counts how often a term appears in this document (TF). Measures how rare the word is across all documents (IDF). Highlights terms important and unique to this specific document.\nOkapi BM25: Expands TF-IDF to introduce a weighting mechanism for term saturation and document length.\n\n\n\nEmbedding-based retrieval\nEmbedding-based retrieval is a technique that uses the embedding of the query to find the most relevant documents in the vector database.\nFor small datasets, you can use k Nearest Neighbors (k-NN) approach to find the most relevant documents following this approach:\n\nCalculate the similarity score between the query vector and every other vector stored in the VectorDB.\nSort all the vectors based on these similarity scores\nReturn the ‚Äòk‚Äô most similar vectors (relative to the query).\n\nFor large datasets, you can use Approximate Nearest Neighbors (ANN) algorithms such as Locality-Sensitive Hashing (LSH) or Hierarchical Navigable Small World (HNSW) to find the most relevant documents."
  },
  {
    "objectID": "posts/what-is-rag.html#prerequisites",
    "href": "posts/what-is-rag.html#prerequisites",
    "title": "What is Retrieval Augmented Generation (RAG)?",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo follow this tutorial you‚Äôll need to:\n\nSign up and generate an API key in OpenAI.\nSet the API key as an environment variable called OPENAI_API_KEY.\nCreate a virtual environment in Python and install the requirements:\nDownload the sample PDF file\n\npython -m venv venv\nsource venv/bin/activate\npip install langchain chromadb langchain-openai langchain-community python-dotenv pypdf jupyter\nOnce you‚Äôve completed the steps above, you can run copy and paste the code from the next sections. You can also download the notebook from here."
  },
  {
    "objectID": "posts/what-is-rag.html#rag-without-vector-database",
    "href": "posts/what-is-rag.html#rag-without-vector-database",
    "title": "What is Retrieval Augmented Generation (RAG)?",
    "section": "RAG without vector database",
    "text": "RAG without vector database\nLet‚Äôs go through an example without a VectorDB. We‚Äôll use a sample document that‚Äôs about the conditions of some specific banking product. Our goal is to be able to ask questions about it, and get accurate answers.\nFor the first version of the pipeline, we‚Äôll simply augment the context with the full text of the document. So there‚Äôs no real retrieval step in this version. We‚Äôll get to that in the next section.\nStart by importing the necessary libraries and load the required variables from the .env file.\n\nimport os\n\nimport chromadb\nfrom chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\nfrom dotenv import load_dotenv\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nload_dotenv()\n\nThis code will import the necessary libraries and load the required variables from the .env file.\n\nRead the document (retrieval)\nNext, we‚Äôll use a DocumentLoader to read the document from the PDF file. Since, we‚Äôre dealing with a PDF, we‚Äôll use PyPDFLoader.\nThere are many other document loaders available in LangChain. You can find the full list here.\n\nfile_path = \"../_extras/what-is-rag/bbva.pdf\"\nloader = PyPDFLoader(file_path)\npages = []\n\nfor page in loader.lazy_load():\n    pages.append(page)\n\nA DocumentLoader is a class that processes a document and returns a list of Document objects. In the case of PyPDFLoader, it will read each page of the PDF file and return the text of each page with some additional metadata.\nA single page will look like this:\n\npages[0].model_dump()\n\n{'id': None,\n 'metadata': {'producer': 'Adobe PDF Library 15.0',\n  'creator': 'Adobe InDesign 16.1 (Windows)',\n  'creationdate': '2021-03-24T14:51:54+01:00',\n  'moddate': '2021-03-24T14:51:54+01:00',\n  'trapped': '/False',\n  'source': '../_extras/what-is-rag/bbva.pdf',\n  'total_pages': 4,\n  'page': 0,\n  'page_label': '1'},\n 'page_content': \"EDICI√ìN AQUA PREP 01-01\\nBANCO BILBAO VIZCAYA ARGENTARIA, S.A. - Plaza de San Nicol√°s, 4 - 48005 BILBAO\\nReg. Mer. Vizcaya -T omo 3858, Folio 1, Hoja BI-17 BIS-A, Inscripci√≥n 1035¬™ C.I.F.: A48265169\\n1 / 4\\nThis document contains the Pre-contractual information and the Prior General Information of the Aqua Pre-paid Card contract \\n(hereinafter, the Card) in accordance with the provisions of the Ministerial Order ECE/1263/2019, on the transparency of \\ninformation conditions applicable to payment services, and Bank of Spain Circular 5/2012, on the transparency of banking services \\nand responsibility in the granting of loans.\\nThe information highlighted in bold is especially important, in accordance with Circular 5/2012\\n1. ON THE PAYMENT SERVICE PROVIDER\\n1.1 Details and registration\\nBANCO BILBAO VIZCAYA ARGENTARIA, S.A.\\nAddress: Plaza San Nicol√°s, 4 - 48005 BILBAO. \\nPhone number: 900 102 801\\nWebsite address: www.bbva.es\\nRegistered in the Biscay Commercial Register, Volume 2083, \\nFolio 1, Sheet BI-17-A, Entry 1\\n1.2 Supervisory Authorities:\\nBanco de Espa√±a (Registry 0182)\\n[Spanish National Securities Market Commission]\\n2. ON THE USE OF THE PAYMENT SERVICES\\n2.1 Main characteristics: PREPAID CARD .\\nThe Holder may specify that the card be physical or virtual. \\nT erms and conditions governing the availability of funds: in \\nother words, when and how the holder will obtain the money:\\na) The Card, against a balance previously loaded on it, \\nmay be used to purchase goods or services in any of \\nthe physical or virtual establishments affiliated with the \\ncard systems to which the Card belongs and that are \\nshown on it.\\nb) T o make online payments with the Card, the Account \\nHolder must consult the details pertaining to the card \\nnumber, expiration date and CVV via the BBVA website \\nor mobile app.\\nc) Withdraw money from ATMs, Bank branches and \\nany other entities that allow it against the balance \\npreviously loaded on it.\\nT ransactions carried out with the Card will reduce the \\navailable balance.\\nUnder no circumstances may transactions be carried out \\nin excess of the current unused loaded balance at any time \\n(available balance).\\n2.2 Conducting transactions. Consent.\\nT o withdraw money or pay with the Card in physical \\nestablishments, you must present the Card and enter your \\npersonal identification number (PIN).\\nThe Card's contactless technology can be used to pay or \\nwithdraw cash with the Card without having to enter the PIN for \\ntransactions under 50 euros.\\nFor online shop purchases, you must identify yourself in the \\nmanner indicated by the Bank, enter the security password and \\nfollow the procedure specified by the Bank..\\n2.3 Execution period\\nThe transactions will be charged to the Direct Debit Account on \\nthe date on which they were executed.\\nPre-contractual information and \\ninformation  booklet  prior to \\nconcluding the payment services \\ncontract\\nAQUA PRE-PAID CARD\",\n 'type': 'Document'}\n\n\nIn addition to the page content, each Document object includes metadata about the source file, the page number, and other information.\n\n\nAugment the context\nNow that we have all the pages of the PDF available as text, let‚Äôs build the context we‚Äôll use to generate a response.\nWe‚Äôll define a system and a user prompt. In the system prompt, we‚Äôll define the role of the assistant and in the user prompt, we‚Äôll provide the user question and the documents.\n\nsystem_prompt = \"\"\"\nYou are a helpful assistant that can answer questions about the provided context.\n\nPlease cite the page number used to answer the question. Write the page number in the format \"Page X\" at the end of your answer. \n\nIf the answer is not found in the context, please say so.\n\"\"\"\nuser_prompt = \"\"\"\nPlease answer the following question based on the context provided:\n\nQuestion: {question}\n\nDocuments:\n{documents}\n\"\"\"\n\npages_str = \"\"\nfor i, page in enumerate(pages):\n    pages_str += f\"--- PAGE {i + 1} ---\\n{page.page_content}\\n\\n\"\n\nWe‚Äôve set up the system and user prompt, and a a variable that stores the pages we extracted as a single string. When we make a request to the model, we‚Äôll combine all of these into messages and send them to the model.\nNow, we‚Äôre ready to generate a response.\n\n\nGenerate response\nTo generate a response we‚Äôll use gpt-4.1-mini and combine the system and user prompts we‚Äôve built to augment the model‚Äôs context.\n\nmodel = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n\ndef get_response(context_vars: dict):\n    messages = [\n        SystemMessage(content=system_prompt),\n        HumanMessage(content=user_prompt.format(**context_vars)),\n    ]\n    response = model.invoke(messages)\n    return response.content\n\n\nquestion = \"What is the main idea of the document?\"\nresponse = get_response({\"question\": question, \"documents\": pages_str})\nprint(response)\n\nThe main idea of the document is to provide the pre-contractual and general information regarding the Aqua Pre-paid Card offered by Banco Bilbao Vizcaya Argentaria, S.A. (BBVA). It outlines the terms and conditions of the card, including its features, usage, fees, security measures, responsibilities of the cardholder and the bank, contract duration, amendments, termination, applicable law, dispute resolution procedures, and other important legal aspects. The document aims to ensure transparency and inform potential cardholders about their rights and obligations before entering into the contract. \n\nPage 1 to Page 4\n\n\nIn this code, we‚Äôve combined the system, user prompt, the pages extracted from the document, and a user question (‚ÄúWhat is the main idea of the document?‚Äù) into messages the model can understand.\nI tried it with a couple of questions and it worked well. The answers were accurate. Try changing the question and see how the model responds.\n\nquestion = \"What are the daily transaction limits?\"\nresponse = get_response({\"question\": question, \"documents\": pages_str})\nprint(response)\n\nThe daily transaction limits for the Aqua Pre-paid Card are as follows: The daily purchase limit will be determined by the Card's balance and up to a maximum of 1,000 euros per day. The Holder and the Bank may modify the initially specified limits. Additionally, the monthly limit for collecting lottery and gambling prizes is ten thousand euros. (Page 2)\n\n\nAs long as the document contains the information you need, you will likely get an accurate answer from the model.\nBut you can do better. Right now, the model is using the full text of the document to answer the question. Most questions only require a few sentences from the document.\nTo answer the ‚ÄúWhat are the daily transaction limits?‚Äù, the model used 3,528 input tokens. While in reality, it needed less than 500 input tokens.\nFor small documents such as this one, the difference isn‚Äôt a big deal. But when you‚Äôre dealing with thousands of documents and potentially millions of tokens, the difference can be significant in terms of costs, latency, and accuracy.\nLet‚Äôs see how we can use a VectorDB to improve improve this."
  },
  {
    "objectID": "posts/what-is-rag.html#rag-with-vector-search",
    "href": "posts/what-is-rag.html#rag-with-vector-search",
    "title": "What is Retrieval Augmented Generation (RAG)?",
    "section": "RAG with vector search",
    "text": "RAG with vector search\nYou‚Äôll need to start by doing two things: defining an embedding function, and creating a VectorDB.\nIn this example, we‚Äôll use the OpenAIEmbeddingFunction to create embeddings and Chroma to store them.\n\nopenai_ef = OpenAIEmbeddingFunction(api_key=os.getenv(\"OPENAI_API_KEY\"))\nvector_db = chromadb.PersistentClient()\n\ntry:\n    collection = vector_db.delete_collection(\"bbva\")\nexcept:\n    pass\n\ncollection = vector_db.create_collection(\"bbva\", embedding_function=openai_ef)\n\nIn this code, you‚Äôve set up the embedding function and created a VectorDB. The embedding function converts chunks of text from the document into vectors. The VectorDB stores these vectors and allows you to query them based on similarity to the question.\nNext, you‚Äôll need to split the pages into smaller chunks that you can query the VectorDB with.\n\nSplit and index documents\nThe RecursiveCharacterTextSplitter is a class that splits text into chunks of a specified size. It‚Äôs a recursive approach that splits the text into smaller chunks using a hierarchy of delimiters (e.g., \"\\\\n\\\\n\", \"\\n\", \".\", etc.).\nIn this example, we‚Äôll use a chunk size of 1,000 characters and an overlap of 200 characters. However, in practice bigger chunks seem to work well and simplify a lot the indexing process. Popular embedding functions can handle up to 8,192 tokens, which is ~32,000 characters. You might want to start there.\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nall_splits = text_splitter.split_documents(pages)\n\nThis code will split the documents and save those splits into all_splits. Then you need to add those chunks into your VectorDB.\nChromaDB provides you with a simple way to add chunks to your VectorDB:\n\ncollection.add(\n    documents=[split.page_content for split in all_splits],\n    metadatas=[split.metadata for split in all_splits],\n    ids=[str(i) for i in range(len(all_splits))],\n)\n\nThis will add the chunks to your VectorDB. In addition to the chunks, this will add the metadata of each chunk and generate unique IDs for each chunk.\n\n\nQuery the database\nOnce the chunks are in the VectorDB, you can query them with the question.\n\ncollection.query(\n    query_texts=[\"What are the daily transaction limits?\"],\n    n_results=1,\n)\n\n{'ids': [['4']],\n 'embeddings': None,\n 'documents': [[\"EDICI√ìN AQUA PREP 01-01\\nBANCO BILBAO VIZCAYA ARGENTARIA, S.A. - Plaza de San Nicol√°s, 4 - 48005 BILBAO\\nReg. Mer. Vizcaya -T omo 3858, Folio 1, Hoja BI-17 BIS-A, Inscripci√≥n 1035¬™ C.I.F.: A48265169\\n2 / 4 \\n2.4 T ransaction limits. \\nThe daily purchase limit will be determined by the Card's \\nbalance and up to a maximum of 1,000 euros per day. The \\nHolder and the Bank may modify the initially specified limits. \\nThe monthly limit for collecting lottery and gambling prizes is \\nten thousand euros.\\n2.5 T o sign up for the card, you do not need to take out \\nany other accessory service.\\n3. ON COSTS AND INTEREST AND EXCHANGE RATES\\nMonthly top-up limit: Minimum of 6, maximum of 1000\\nThe applicable fees for using the card may be:\\na) Pre-paid card issue and maintenance fee: 5 euros.\\nb) Fee for issuance of duplicates: 4 euros.\\nc) Fee for using the card outside the Eurozone: 3% \\napplicable to the exchange value in euros.\\nd) Fees to withdraw cash against the card balance at ATMs:\"]],\n 'uris': None,\n 'included': ['metadatas', 'documents', 'distances'],\n 'data': None,\n 'metadatas': [[{'page_label': '2',\n    'source': '../_extras/what-is-rag/bbva.pdf',\n    'producer': 'Adobe PDF Library 15.0',\n    'total_pages': 4,\n    'trapped': '/False',\n    'creationdate': '2021-03-24T14:51:54+01:00',\n    'page': 1,\n    'creator': 'Adobe InDesign 16.1 (Windows)',\n    'moddate': '2021-03-24T14:51:54+01:00'}]],\n 'distances': [[0.3241901397705078]]}\n\n\nYou can even query it with multiple questions at once:\n\ncollection.query(\n    query_texts=[\"What are the daily transaction limits?\", \"What is the maximum amount I can withdraw?\"],\n    n_results=1,\n)\n\n{'ids': [['4'], ['4']],\n 'embeddings': None,\n 'documents': [[\"EDICI√ìN AQUA PREP 01-01\\nBANCO BILBAO VIZCAYA ARGENTARIA, S.A. - Plaza de San Nicol√°s, 4 - 48005 BILBAO\\nReg. Mer. Vizcaya -T omo 3858, Folio 1, Hoja BI-17 BIS-A, Inscripci√≥n 1035¬™ C.I.F.: A48265169\\n2 / 4 \\n2.4 T ransaction limits. \\nThe daily purchase limit will be determined by the Card's \\nbalance and up to a maximum of 1,000 euros per day. The \\nHolder and the Bank may modify the initially specified limits. \\nThe monthly limit for collecting lottery and gambling prizes is \\nten thousand euros.\\n2.5 T o sign up for the card, you do not need to take out \\nany other accessory service.\\n3. ON COSTS AND INTEREST AND EXCHANGE RATES\\nMonthly top-up limit: Minimum of 6, maximum of 1000\\nThe applicable fees for using the card may be:\\na) Pre-paid card issue and maintenance fee: 5 euros.\\nb) Fee for issuance of duplicates: 4 euros.\\nc) Fee for using the card outside the Eurozone: 3% \\napplicable to the exchange value in euros.\\nd) Fees to withdraw cash against the card balance at ATMs:\"],\n  [\"EDICI√ìN AQUA PREP 01-01\\nBANCO BILBAO VIZCAYA ARGENTARIA, S.A. - Plaza de San Nicol√°s, 4 - 48005 BILBAO\\nReg. Mer. Vizcaya -T omo 3858, Folio 1, Hoja BI-17 BIS-A, Inscripci√≥n 1035¬™ C.I.F.: A48265169\\n2 / 4 \\n2.4 T ransaction limits. \\nThe daily purchase limit will be determined by the Card's \\nbalance and up to a maximum of 1,000 euros per day. The \\nHolder and the Bank may modify the initially specified limits. \\nThe monthly limit for collecting lottery and gambling prizes is \\nten thousand euros.\\n2.5 T o sign up for the card, you do not need to take out \\nany other accessory service.\\n3. ON COSTS AND INTEREST AND EXCHANGE RATES\\nMonthly top-up limit: Minimum of 6, maximum of 1000\\nThe applicable fees for using the card may be:\\na) Pre-paid card issue and maintenance fee: 5 euros.\\nb) Fee for issuance of duplicates: 4 euros.\\nc) Fee for using the card outside the Eurozone: 3% \\napplicable to the exchange value in euros.\\nd) Fees to withdraw cash against the card balance at ATMs:\"]],\n 'uris': None,\n 'included': ['metadatas', 'documents', 'distances'],\n 'data': None,\n 'metadatas': [[{'source': '../_extras/what-is-rag/bbva.pdf',\n    'page': 1,\n    'total_pages': 4,\n    'creator': 'Adobe InDesign 16.1 (Windows)',\n    'creationdate': '2021-03-24T14:51:54+01:00',\n    'moddate': '2021-03-24T14:51:54+01:00',\n    'producer': 'Adobe PDF Library 15.0',\n    'page_label': '2',\n    'trapped': '/False'}],\n  [{'page_label': '2',\n    'trapped': '/False',\n    'source': '../_extras/what-is-rag/bbva.pdf',\n    'creator': 'Adobe InDesign 16.1 (Windows)',\n    'total_pages': 4,\n    'creationdate': '2021-03-24T14:51:54+01:00',\n    'moddate': '2021-03-24T14:51:54+01:00',\n    'producer': 'Adobe PDF Library 15.0',\n    'page': 1}]],\n 'distances': [[0.3241901397705078], [0.416978657245636]]}\n\n\nNow, let‚Äôs add the VectorDB into our RAG pipeline.\n\n\nRAG pipeline\nFirst, start by defining a function that does the retrieval of the most relevant documents.\n\ndef get_relevant_docs(question: str, top_k: int = 1):\n    relevant_docs = collection.query(query_texts=question, n_results=top_k)\n    documents = relevant_docs[\"documents\"][0]\n    metadatas = relevant_docs[\"metadatas\"][0]\n    return [\n        {\"page_content\": doc, \"type\": \"Document\", \"metadata\": metadata}\n        for doc, metadata in zip(documents, metadatas)\n    ]\n\nThis function will take a question and return the top_k most relevant chunks from the document. Here‚Äôs an example:\n\nget_relevant_docs(\"What are the daily transaction limits?\")\n\n[{'page_content': \"EDICI√ìN AQUA PREP 01-01\\nBANCO BILBAO VIZCAYA ARGENTARIA, S.A. - Plaza de San Nicol√°s, 4 - 48005 BILBAO\\nReg. Mer. Vizcaya -T omo 3858, Folio 1, Hoja BI-17 BIS-A, Inscripci√≥n 1035¬™ C.I.F.: A48265169\\n2 / 4 \\n2.4 T ransaction limits. \\nThe daily purchase limit will be determined by the Card's \\nbalance and up to a maximum of 1,000 euros per day. The \\nHolder and the Bank may modify the initially specified limits. \\nThe monthly limit for collecting lottery and gambling prizes is \\nten thousand euros.\\n2.5 T o sign up for the card, you do not need to take out \\nany other accessory service.\\n3. ON COSTS AND INTEREST AND EXCHANGE RATES\\nMonthly top-up limit: Minimum of 6, maximum of 1000\\nThe applicable fees for using the card may be:\\na) Pre-paid card issue and maintenance fee: 5 euros.\\nb) Fee for issuance of duplicates: 4 euros.\\nc) Fee for using the card outside the Eurozone: 3% \\napplicable to the exchange value in euros.\\nd) Fees to withdraw cash against the card balance at ATMs:\",\n  'type': 'Document',\n  'metadata': {'producer': 'Adobe PDF Library 15.0',\n   'creationdate': '2021-03-24T14:51:54+01:00',\n   'creator': 'Adobe InDesign 16.1 (Windows)',\n   'moddate': '2021-03-24T14:51:54+01:00',\n   'page': 1,\n   'trapped': '/False',\n   'source': '../_extras/what-is-rag/bbva.pdf',\n   'page_label': '2',\n   'total_pages': 4}}]\n\n\nAfter you‚Äôve retrieved the relevant chunks, you‚Äôd want to combine them into a single string that you can pass to the model. You can use get_context to do that.\n\ndef get_context(relevant_docs: list[dict]):\n    context = \"\"\n    for doc in relevant_docs:\n        context += f\"--- PAGE {doc['metadata']['page']} ---\\n{doc['page_content']}\\n\\n\"\n    return context\n\n\ndocs = get_relevant_docs(\"What are the daily transaction limits?\", top_k=3)\nget_context(docs)\n\n\"--- PAGE 1 ---\\nEDICI√ìN AQUA PREP 01-01\\nBANCO BILBAO VIZCAYA ARGENTARIA, S.A. - Plaza de San Nicol√°s, 4 - 48005 BILBAO\\nReg. Mer. Vizcaya -T omo 3858, Folio 1, Hoja BI-17 BIS-A, Inscripci√≥n 1035¬™ C.I.F.: A48265169\\n2 / 4 \\n2.4 T ransaction limits. \\nThe daily purchase limit will be determined by the Card's \\nbalance and up to a maximum of 1,000 euros per day. The \\nHolder and the Bank may modify the initially specified limits. \\nThe monthly limit for collecting lottery and gambling prizes is \\nten thousand euros.\\n2.5 T o sign up for the card, you do not need to take out \\nany other accessory service.\\n3. ON COSTS AND INTEREST AND EXCHANGE RATES\\nMonthly top-up limit: Minimum of 6, maximum of 1000\\nThe applicable fees for using the card may be:\\na) Pre-paid card issue and maintenance fee: 5 euros.\\nb) Fee for issuance of duplicates: 4 euros.\\nc) Fee for using the card outside the Eurozone: 3% \\napplicable to the exchange value in euros.\\nd) Fees to withdraw cash against the card balance at ATMs:\\n\\n--- PAGE 2 ---\\nBBVA app or website, or via the phone numbers shown on the \\ncards, and in any case within a maximum period of thirteen \\nmonths after the date of the debit entry.\\n5.3 Liability of the Bank in the event of unauthorized \\npayment transactions.\\nIf an unauthorized payment transaction is carried out, the \\nBank will refund the amount of the unauthorized transaction.\\n5.4 Liability of the Holder in the event of unauthorized \\ntransactions.\\nThe Account Holder will be liable for losses arising from \\nunauthorized payment transactions made with the Card up \\nto a maximum of 50 euros.\\nThe Holder will be liable without any limitations in the \\nevent of fraud or gross negligence on their part in meeting \\ntheir obligations as respects the security credentials and \\nsafekeeping if this situation is not reported to the Bank \\nwithout delay.\\n5.5 Blocking the Card.\\nThe Bank reserves the right to block the Card on objectively \\njustified grounds related to the security measures taken\\n\\n--- PAGE 2 ---\\nEDICI√ìN AQUA PREP 01-01\\nBANCO BILBAO VIZCAYA ARGENTARIA, S.A. - Plaza de San Nicol√°s, 4 - 48005 BILBAO\\nReg. Mer. Vizcaya -T omo 3858, Folio 1, Hoja BI-17 BIS-A, Inscripci√≥n 1035¬™ C.I.F.: A48265169\\n3 / 4 \\nd) Notify the Bank of any loss, theft or copying of the \\nCard or misappropriation of the PIN and/or passwords \\nwithout undue delay as soon as they become aware \\nof it, at any of the Bank's branches during customer \\nservice hours or via the phone numbers shown on the \\nCard.\\n5.2 Notify the Bank of any unauthorized transactions \\nor incorrectly executed payment transactions.\\nThe Holder must notify the Bank as soon as they become \\naware of the posting of any unauthorized transaction to the \\nDirect Debit Account of the Card without undue delay at any \\nbranch of the Bank during customer service hours, on the \\nBBVA app or website, or via the phone numbers shown on the \\ncards, and in any case within a maximum period of thirteen \\nmonths after the date of the debit entry.\\n\\n\"\n\n\nThis will generate a string similar to the one we used in the previous example.\nFinally, you can adapt get_response to use these new steps in the RAG pipeline.\n\ndef get_messages(question: str, relevant_docs: dict):\n    context_vars = {\"question\": question, \"documents\": get_context(relevant_docs)}\n    messages = [\n        SystemMessage(content=system_prompt),\n        HumanMessage(content=user_prompt.format(**context_vars)),\n    ]\n    return messages\n\n\ndef get_response(question: str):\n    relevant_docs = get_relevant_docs(question)\n    messages = get_messages(question, relevant_docs)\n    response = model.invoke(messages)\n    return response.content\n\n\nquestion = \"What are the daily transaction limits?\"\nresponse = get_response(question)\nprint(response)\n\nThe daily purchase limit for transactions is determined by the Card's balance and can be up to a maximum of 1,000 euros per day. Additionally, the monthly limit for collecting lottery and gambling prizes is ten thousand euros. The Holder and the Bank may modify the initially specified limits. \n\n(Page 1)\n\n\nAnd, you‚Äôre done! You‚Äôve built a RAG pipeline that can answer questions about a document."
  },
  {
    "objectID": "posts/what-is-rag.html#conclusion",
    "href": "posts/what-is-rag.html#conclusion",
    "title": "What is Retrieval Augmented Generation (RAG)?",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, you‚Äôve learned about what RAG is, how it works, and how to implement it in Python. You‚Äôve learned why you‚Äôd want to use it, and how to do it.\nYou‚Äôve walked through the process of: - Extracting text from a PDF file - Creating embeddings for the chunks - Storing the embeddings in a VectorDB - Querying the VectorDB to find the most relevant chunks - Using the model to generate a response\nHope you find this article usefl. If you have any questions or comments, put them in the comments section below."
  },
  {
    "objectID": "posts/2023-personal-snapshot.html",
    "href": "posts/2023-personal-snapshot.html",
    "title": "2023: Personal Snapshot",
    "section": "",
    "text": "This is my annual review. It serves two purposes: a deep analysis of the past year and a record of my thoughts at the time of writing.\nI hope it‚Äôs fun to read or, at the very least, provides some interesting insights.\nIf it‚Äôs me rereading this, welcome back. This is Dylan from 2023."
  },
  {
    "objectID": "posts/2023-personal-snapshot.html#what-went-well",
    "href": "posts/2023-personal-snapshot.html#what-went-well",
    "title": "2023: Personal Snapshot",
    "section": "What went well?",
    "text": "What went well?\nAccording to my GitHub account, I committed code 276 out of 365 days in 2023. I likely coded even more since there were days when I didn‚Äôt commit my code. Also, sometimes I used a different account for projects due to company policy.\n\n\n\nSquint to see the secret message\n\n\nThe more I code the more I learn. So, as long as I stay honest and avoid committing useless code, coding serves as a good proxy of how much I‚Äôve learned about technical topics throughout the year. This year, I felt I learned a lot, so I‚Äôm happy.\nDespite reaching an all-time high compared to previous years, I think I could have achieved more. Sometimes, especially between projects or during periods when I felt burned out, I procrastinated quite a bit. Next year, I want to improve this.\nI shipped five AI apps (not including client work). I developed four on my own and created one with a friend. Two of them made it to the front page of Hacker News and got featured in The Economist. Another one won brownie points in an AI Shark Tank.\n\n\n\nAI apps I built in 2023\n\n\nI also wrote 9 blog posts. I was very motivated at first and worked with a good friend, Jing, as an accountability partner. We ended up prioritizing other projects and stopped the accountability challenge, but it was fun and effective while it lasted. I‚Äôm very grateful to Jing for joining me in this challenge.\nI focused on writing tutorials about AI topics, but I got tired after a while. Writing high-quality tutorials demands a lot of effort. Plus, AI evolves so fast that my tutorials often become outdated within months. That sucked!\n\n\n\nMy stats looking like üí©\n\n\nWhen I stopped writing regularly, my site‚Äôs traffic started to drop. I also believe ChatGPT was a major factor. My basic-level tutorials, which ChatGPT can easily replace, have stopped growing.\nI‚Äôm making a change this year in my content creation strategy. I‚Äôll focus on creating videos for technical topics and write about evergreen subjects on my blog. I‚Äôve realized that making videos for technical content is a time-saver compared to writing tutorials. Showing users directly through videos is simpler than writing detailed descriptions or taking screenshots and explaining them.\nMy original plan for my blog was to share my thoughts. I ended up writing technical tutorials because it was a more effective way to drive traffic, but in all honesty, I didn‚Äôt enjoy it very much. So I‚Äôm going back to my original plan.\nI posted more or less consistently on LinkedIn throughout the year. I got roughly 420k views on my posts. A third of those views came from a single post, and I gained ~2.3k followers.\n\n\n\nThe outcome of spamming LinkedIn\n\n\nI met lots of great people and had tons of catch-ups this year. I am grateful for all the people I met this year. Special thanks go to Max, Emanuel, Sebasti√°n, Edu, and Rhys for the collaborations we did. Not all things went as planned, but we had fun.\nPosting random things online helps me chat, befriend, learn from, and even do business with people I wouldn‚Äôt have met otherwise. It feels great!\nI did more sales this year. I took part in two 6-figure proposals that didn‚Äôt pan out, and landed two 5-figure contracts, each setting a new record for my hourly rate. I also sold quite a few small projects. Selling is fun but I‚Äôm not great at it. This is one of the focuses for next year.\nFinancially, things went well. Despite being more focused on learning than on making money this year, I made ~2.5x my annual burn rate (I‚Äôm frugal!). All my income was made through freelancing, and 68% of my income came from a single project. This situation is better than the past two years because I‚Äôm a bit less dependent on one client. But I‚Äôm still not where I want to be. To lower my risk, I want to spread my income more evenly across different clients.\nHealthwise, this year went well. I completed 169 strength training sessions, averaging 3 gym visits per week. I‚Äôm pretty happy with that. I faced some minor injuries but dealt with them effectively.\nAlso, I completed 4,711 minutes of Z2 training (roughly 90 minutes per week). For Z2, I experimented with running, cycling, and stair-climbing. Running is my favorite, but stair-climbing lets me multitask. So, I mostly split my time between these two activities.\nThis year, I gave stand-up comedy a shot, all thanks to my wife. I‚Äôve wanted to try it for a long time but kept coming up with excuses. For our fifth anniversary, she surprised me by signing me up for a course. It‚Äôs my favorite gift since getting a Game Boy Advance twenty years ago.\n\n\n\nMe, holding a fart on stage\n\n\nFor those who know me in real life, you‚Äôll know that I don‚Äôt enjoy speaking in public. So this was a real challenge for me. I did a presentation with family and friends and went to four open mics.\nIT WAS GREAT! Even though I felt terrified before stepping onto the stage, I ended up having a lot of fun. Most of my jokes made people laugh, which felt comforting.\nI do need to work on my stage presence. I often stood in awkward places, struggled to make eye contact with the audience during punchlines, and frequently said ‚ÄúEhhh‚Ä¶ Uhhh‚Ä¶‚Äù\nI‚Äôve opened an IG account for my comedy stuff. I haven‚Äôt posted anything yet, but I‚Äôm planning to start in the next few weeks. For now, I will post in Spanish. Follow me there if you want to stay updated!\nFinally, this year has been good for my relationships with my wife, family, and friends. My wife and I continue to work as a team, each of us progressing in our respective areas and supporting one another. And I made time to enjoy with family and friends.\nIn my last snapshot, I didn‚Äôt include any photos. When I revised it a few days ago, I realized I missed having them. So, this time, I‚Äôve included a few.\n\n\nClick to see photos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis year, a family highlight was my in-laws‚Äô visit to Madrid for a few months. We found out that our family loves karaoke, especially my dad and my father-in-law. It was tough to get them off the microphone!\nMore importantly, my loved ones remain healthy and happy, and so am I."
  },
  {
    "objectID": "posts/2023-personal-snapshot.html#what-didnt-go-well",
    "href": "posts/2023-personal-snapshot.html#what-didnt-go-well",
    "title": "2023: Personal Snapshot",
    "section": "What didn‚Äôt go well?",
    "text": "What didn‚Äôt go well?\nI didn‚Äôt write a 2022 personal snapshot. That wasn‚Äôt a great way to start 2023.\nAfter Entrepreneur First, I felt burned out and lacked the motivation to write a detailed review of the past year. I kept putting it off until it was way too late. And nobody wants to read your annual review in August!\nI failed to get funding twice this year. First from Entrepreneur First, and second, from Speedinvest. Pitching, preparing the materials, and all the discussions involved were a cool experience, but the outcome was frustrating both times.\nI‚Äôm not sure it would have worked out. In both cases, I wasn‚Äôt too excited about the idea, but I thought it was worth a try. My main concern with raising venture capital money too early often leads to poor financial outcomes for founders. That‚Äôs a no-no for me.\nThough, bootstrapping isn‚Äôt easy either. My most successful AI product made 60‚Ç¨. Just enough for a decent dinner for two in Madrid.\n\n\n\nMy ‚Äúbest‚Äù AI product\n\n\nThe AI apps I built this year landed me several freelancing projects. But they made little money by themselves. To be fair, I only launched two products you could pay for. I didn‚Äôt intend the others to make money, at least not initially.\nThis year, I finally grasped something you might find obvious: to have products that generate revenue, you must be intentional about it. You must create opportunities for people to pay.\nI used to believe that if I built something cool enough, people would discover it and somehow find a way to pay for it. I was wrong. I realized that I must actively set up a payment system for my products or deliberately plan how to monetize the attention they receive. Without that, after the initial burst of attention fades, you might end up with nothing. Many of my projects this year suffered this fate.\nI‚Äôve always dreamed of creating a product, so I ignored the market‚Äôs pull towards a consulting company. Instead of doubling down on the demand, I often said no, aiming to focus on my product ideas. After spending much of this year like a person with a hammer in search of a nail, I‚Äôve decided to reverse my approach.\n\n\n\nMe running away from the demand\n\n\nNow, I‚Äôll start with the demand and then figure out how to offer services to meet it. My focus will be on establishing a consulting practice. From there, I might develop a product. But then again, maybe I don‚Äôt need to. I‚Äôm content with the idea of getting rich through a services-only company üòâ\nAlthough I‚Äôm satisfied with my physical fitness, I‚Äôve noticed that I often don‚Äôt rest enough. This happens either because I don‚Äôt sleep enough or because I overtrain. I‚Äôve started feeling some symptoms of this, so improving my rest is a goal for next year."
  },
  {
    "objectID": "posts/2023-personal-snapshot.html#what-are-the-plans-for-next-year",
    "href": "posts/2023-personal-snapshot.html#what-are-the-plans-for-next-year",
    "title": "2023: Personal Snapshot",
    "section": "What are the plans for next year?",
    "text": "What are the plans for next year?\nFirst, focus on building a successful consulting practice. I‚Äôll be doing a lot more sales this year and plan to share updates frequently. I have some exciting news about this that I‚Äôll be sharing soon.\nNext, establishing a sustainable content engine. I believe shifting to video for technical content might yield a higher ROI than text. So, this year, I plan to explore this approach and reserve the blog for more personal articles.\nKeep doing stand-up comedy. I haven‚Äôt found such an enjoyable hobby in a long time. I want to do more of it. It also brings extra benefits, like improving my sales skills.\nFinally, make time for rest. I often struggle with guilt over taking breaks, leading to very few days off throughout the year. But this isn‚Äôt good for my mental and physical well-being. I aim to create more space for rest, and I‚Äôm sure my wife and family will appreciate that too üòÅ"
  },
  {
    "objectID": "posts/classify-images-with-gemini-flash-1.5.html",
    "href": "posts/classify-images-with-gemini-flash-1.5.html",
    "title": "Classifying images with Gemini Flash 1.5",
    "section": "",
    "text": "Most people think of In-Context Learning (ICL) ‚Äî the ability of LLMs to learn from examples provided in the context ‚Äî only as a component of RAG applications.\nI used to think of it that way too. Until I recently found out that Multimodal Large Language Models (MLLMs) with ICL can be used to perform more traditional ML tasks such as image classification.\nI was skeptical at first, but was surprised to see that it worked pretty well both in the literature (see here and here) and in my own experiments.\nYou shouldn‚Äôt expect state-of-the-art results with it, but it can often give you pretty good results with very little effort and data.\nIn this tutorial, I‚Äôll show you how to use ICL to classify images using Gemini Flash 1.5."
  },
  {
    "objectID": "posts/classify-images-with-gemini-flash-1.5.html#why-gemini-flash-1.5",
    "href": "posts/classify-images-with-gemini-flash-1.5.html#why-gemini-flash-1.5",
    "title": "Classifying images with Gemini Flash 1.5",
    "section": "Why Gemini Flash 1.5?",
    "text": "Why Gemini Flash 1.5?\nYou can use any MLLM for this task, but I chose Gemini Flash 1.5 because:\n\nIt‚Äôs cheaper than Gemini Pro 1.5, GPT-4o, and Sonnet 3.5. For an image of 512x512 pixels, Gemini Flash 1.5 is 50x cheaper than Gemini Pro 1.5, 5x to 16x cheaper than GPT-4o, and 26x cheaper than Sonnet 3.51.\nIt lets you use up to 3,000 images per request. By trial and error, I found that GPT-4o seems to have a hard limit at 250 images per request and Sonnet 3.5‚Äôs documentation mentions a limit of 20 images per request.\nIt works well. If you really want to squeeze the last bit of performance out of your model, you can use a bigger model, but for the purposes of this tutorial, Gemini Flash 1.5 will do just fine.\n\nRegardless of the model you choose, this tutorial will be a good starting point for you to classify images using ICL."
  },
  {
    "objectID": "posts/classify-images-with-gemini-flash-1.5.html#prerequisites",
    "href": "posts/classify-images-with-gemini-flash-1.5.html#prerequisites",
    "title": "Classifying images with Gemini Flash 1.5",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo follow this tutorial you‚Äôll need to:\n\nSign up and generate an API key in Google AI Studio.\nSet the API key as an environment variable called GEMINI_API_KEY.\nDownload this dataset and save it to data/.\nCreate a virtual environment and install the requirements:\n\npython -m venv venv\nsource venv/bin/activate\npip install pandas numpy scikit-learn google-generativeai pillow"
  },
  {
    "objectID": "posts/classify-images-with-gemini-flash-1.5.html#set-up",
    "href": "posts/classify-images-with-gemini-flash-1.5.html#set-up",
    "title": "Classifying images with Gemini Flash 1.5",
    "section": "Set up",
    "text": "Set up\nAs usual, you start by importing the necessary libraries:\n\nimport json\nimport os\nimport warnings\n\nimport google.generativeai as genai\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom PIL import Image\nfrom pathlib import Path\n\nwarnings.filterwarnings(\"ignore\")\n\nnp.random.seed(42)\n\nIn addition to the usual popular libraries (e.g.¬†pandas, sklearn), you‚Äôll need:\n\ngoogle.generativeai for interacting with the Gemini API\nPIL for handling images\nsklearn for calculating performance metrics\n\nThen, you‚Äôll need to configure the Gemini API client with your API key:\n\ngenai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n\nThis will take the GEMINI_API_KEY environment variable and use it to authenticate your requests to the Gemini API."
  },
  {
    "objectID": "posts/classify-images-with-gemini-flash-1.5.html#read-data",
    "href": "posts/classify-images-with-gemini-flash-1.5.html#read-data",
    "title": "Classifying images with Gemini Flash 1.5",
    "section": "Read data",
    "text": "Read data\nTo make a fair evaluation of the model‚Äôs performance, you should split the dataset into separate training and testing sets. The training set is used to provide context or examples to the model during inference. The testing set, comprised of unseen images, is then used to measure the model‚Äôs performance.\nThis process is different from the traditional ‚Äútraining‚Äù process, where you update the model‚Äôs weights or parameters. Here, you‚Äôre only providing the model with a set of images and asking it to learn from them at inference time.\nThis function will help you create the datasets:\n\ndef create_datasets(train_dir, test_dir, selected_classes, n_images_icl=3):\n    train_data = []\n    test_data = []\n\n    for class_id, class_name in enumerate(selected_classes):\n        train_class_dir = train_dir / class_name\n        test_class_dir = test_dir / class_name\n\n        if not train_class_dir.is_dir() or not test_class_dir.is_dir():\n            continue\n\n        # Train dataset\n        train_image_files = list(train_class_dir.glob(\"*.jpg\"))\n        selected_train_images = np.random.choice(\n            train_image_files,\n            size=min(n_images_icl, len(train_image_files)),\n            replace=False,\n        )\n        for img_path in selected_train_images:\n            train_data.append(\n                {\n                    \"image_path\": str(img_path),\n                    \"class_id\": f\"class_{class_id}\",\n                    \"class_name\": class_name,\n                }\n            )\n\n        # Test dataset\n        test_image_files = list(test_class_dir.glob(\"*.jpg\"))\n        for img_path in test_image_files:\n            test_data.append(\n                {\n                    \"image_path\": str(img_path),\n                    \"class_id\": f\"class_{class_id}\",\n                    \"class_name\": class_name,\n                }\n            )\n\n    df_train = pd.DataFrame(train_data)\n    df_test = pd.DataFrame(test_data).sample(frac=1).reset_index(drop=True)\n\n    return df_train, df_test\n\nThis function will get a random selection of n_images_icl images per class from the train folder (that you‚Äôll later use in the model‚Äôs context). For the testing set, which you‚Äôll use to measure the model‚Äôs performance, you‚Äôll use all the available images in the test folder from those classes.\nTo keep things simple, you‚Äôll start by selecting 15 different classes and 1 image per class for the context (i.e., n_images_icl=1)\n\nDATA_DIR = \"../data/\"\nTRAIN_DIR = Path(DATA_DIR) / \"train\"\nTEST_DIR = Path(DATA_DIR) / \"test\"\n\nall_classes = list(os.listdir(TRAIN_DIR))\nselected_classes = np.random.choice(all_classes, size=15, replace=False)\n\ndf_train, df_test = create_datasets(TRAIN_DIR, TEST_DIR, selected_classes=selected_classes, n_images_icl=1)\n\nThere will be 15 classes with 1 image in the training set and 15 classes with 5 images in the testing set."
  },
  {
    "objectID": "posts/classify-images-with-gemini-flash-1.5.html#gemini-flash-1.5",
    "href": "posts/classify-images-with-gemini-flash-1.5.html#gemini-flash-1.5",
    "title": "Classifying images with Gemini Flash 1.5",
    "section": "Gemini Flash 1.5",
    "text": "Gemini Flash 1.5\nNext, you‚Äôll need to define a system prompt and configure the model to use it.\n\nDefine prompt\nYou‚Äôll use a system prompt that will tell the model how to classify the images and the format you want the output to be in:\n\nCLASSIFIER_SYSTEM_PROMPT = \"\"\"You are an expert lepidopterist.\n\nYour task is to classify images of butterflies into one of the provided labels.\n\nProvide your output as a JSON object using this format:\n\n{\n    \"number_of_labeled_images\": &lt;integer&gt;,\n    \"output\": [\n        {\n            \"image_id\": &lt;image id, integer, starts at 0&gt;,\n            \"confidence\": &lt;number between 0 and 10, the higher the more confident, integer&gt;,\n            \"label\": &lt;label of the correct butterfly species, string&gt;\n        }, \n        ...\n    ]\n}\n\n## Guidelines\n\n- ALWAYS produce valid JSON.\n- Generate ONLY a single prediction per input image.\n- The `number_of_labeled_images` MUST be the same as the number of input images.\n\nThis is an example of a valid output:\n```\n{\n  \"number_of_labeled_images\": 5,\n  \"output\": [\n      {\n        \"image_id\": 0,\n        \"confidence\": 10,\n        \"correct_label\": \"class_B\"\n      },\n      {\n        \"image_id\": 1,\n        \"confidence\": 9,\n        \"correct_label\": \"class_C\"\n      },\n      {\n        \"image_id\": 2,\n        \"confidence\": 4,\n        \"correct_label\": \"class_A\"\n      },\n      {\n        \"image_id\": 3,\n        \"confidence\": 2,\n        \"correct_label\": \"class_B\"\n      },\n      {\n        \"image_id\": 4,\n        \"confidence\": 10,\n        \"correct_label\": \"class_C\"\n      }\n  ]\n}\n```\n\"\"\".strip()\n\nThis prompt explains the task to the model. You‚Äôre providing it with a set of labels with corresponding images, and a set of images that should be classified into one of those labels. The model needs to output a single label for each image.\nI included an additional field called number_of_labeled_images because I noticed that the model would often ‚Äúforget‚Äù to include all the labels in the output, and this was a simple way to ensure that it did so.\n\n\n\n\n\n\nNote\n\n\n\nFun fact: I didn‚Äôt know that lepidopterist was a word until I wrote this prompt.\n\n\n\n\nConfigure model\nThen, you can define and configure the model:\n\ngeneration_config = {\n  \"temperature\": 1,\n  \"max_output_tokens\": 8192,\n  \"response_mime_type\": \"application/json\",\n}\nclassification_model = genai.GenerativeModel(\n    \"gemini-1.5-flash\", \n    system_instruction=CLASSIFIER_SYSTEM_PROMPT, \n    generation_config=generation_config\n)\n\nThis sets up the model with the following configuration:\n\ntemperature=1: Controls the randomness of the model‚Äôs output.\nmax_output_tokens=8192: The maximum number of tokens the model can generate.\nresponse_mime_type=\"application/json\": Tells the model to produce JSON.\n\nIt also sets the system_instruction using the prompt you defined earlier and uses gemini-1.5-flash as the model.\n\n\nBuilding the context\nGemini has a slightly different way of building the messages (context) used by the model.\nMost providers have adjusted their API to match OpenAI‚Äôs messages format. Gemini, however, uses a list of strings and media files (if you‚Äôre including images).\nYou can use these functions for that:\n\ndef create_context_images_message(df):\n    messages = [\"Possible labels:\"]\n    grouped = df.groupby('class_id')\n    for class_id, group in grouped:\n        for _, row in group.iterrows():\n            base64_img = Image.open(row[\"image_path\"])\n            messages.append(base64_img)\n        messages.append(f\"label: {class_id}\")\n    return messages\n    \ncontext_images_message = create_context_images_message(df_train)\n\nFirst, you‚Äôll create a message with the context images and their corresponding labels. This is the ‚Äútraining‚Äù part of ICL.\nIn create_context_images_message, you‚Äôre iterating over the training dataset, grouping the images by class and appending the images and labels to the messages list.\nThe resulting message will look something like this:\n\ncontext_images_message[:5]\n\n['Possible labels:',\n &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224&gt;,\n 'label: class_0',\n &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224&gt;,\n 'label: class_1']\n\n\nYou might have noticed that instead of the actual names of the classes, you‚Äôre using class_0, class_1, etc. This is because I want to make the model prediction as ‚Äúfair‚Äù as possible, see the baseline performance section for more details.\nThen, you‚Äôll create a message with the input images. This are the images for which the model will generate predictions.\nSimlar to the context images message, you‚Äôre iterating over the test dataset and appending the images to the messages list.\n\ndef create_input_images_message(df):\n    messages = [\"Input images:\"]\n    for i, image_path in enumerate(df.image_path):\n        base64_img = Image.open(image_path)\n        image_message = [\n            base64_img,\n            f\"input_image_id: {i}\",\n        ]\n        messages.extend(image_message)\n    messages.append(f\"Please correctly classify all {df.shape[0]} images.\")\n    return messages\n\ninput_images_message = create_input_images_message(df_test)\n\nThe resulting message will look something like this:\n\ninput_images_message[:5]\n\n['Input images:',\n &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224&gt;,\n 'input_image_id: 0',\n &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224&gt;,\n 'input_image_id: 1']\n\n\n\n\nResults\nNow, you can combine the context images message and the input images message to create the contents you‚Äôll pass to the model:\n\ncontents = context_images_message + input_images_message\nresponse = classification_model.generate_content(\n    contents=contents\n)\nresponse_json = json.loads(response.text)\n\nIt‚Äôll take a few seconds to run. But after that you‚Äôll have a JSON response with the model‚Äôs predictions:\n\nresponse_json[\"output\"][:3]\n\n[{'image_id': 0, 'confidence': 10, 'label': 'class_7'},\n {'image_id': 1, 'confidence': 10, 'label': 'class_2'},\n {'image_id': 2, 'confidence': 10, 'label': 'class_4'}]\n\n\nThen, you can calculate the accuracy and F1-score to evaluate the model‚Äôs performance:\n\ndef calculate_metrics(df_test, response_json):\n    predictions = [item['label'] for item in response_json['output']]\n    accuracy = accuracy_score(df_test.class_id, predictions)\n    f1 = f1_score(df_test.class_id, predictions, average='weighted')\n    return accuracy, f1\n\naccuracy, f1 = calculate_metrics(df_test, response_json)\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1-score: {f1:.4f}\")\n\nAccuracy: 0.7333\nF1-score: 0.7229\n\n\nUsing a single image in the context per class, you should get an accuracy around 73% and F1-score around 72%.\nNot bad, but you can probably do better.\n\nUsing 5 images per class in the context\nOne quick way to improve the performance of the model is to use more images per class in the context. Try with 5 images per class:\n\ndf_train, df_test = create_datasets(TRAIN_DIR, TEST_DIR, selected_classes=selected_classes, n_images_icl=5)\n\n# Create the context and input messages\ncontext_images_message = create_context_images_message(df_train)\ninput_images_message = create_input_images_message(df_test)\ncontents = context_images_message + input_images_message\n\n# Generate the response\nresponse = classification_model.generate_content(\n    contents=contents\n)\nresponse_json = json.loads(response.text)\n\n# Calculate the metrics\naccuracy, f1 = calculate_metrics(df_test, response_json)\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1-score: {f1:.4f}\")\n\nAccuracy: 0.9067\nF1-score: 0.9013\n\n\nWith this change, you should get an accuracy and F1-score around 90%.\nNice gains in performance for such a small change!\n\n\n\nData leakage and baseline performance\nYou might be thinking, ‚ÄúMLLMs have been trained on a lot of data, so they already know a lot of the images in the dataset, which means that these results are inflated‚Äù.\nWhich is a good point, and for that purpose I‚Äôve done two things:\n\nAnonymize the names of the classes (e.g., class_0 instead of Sleepy Orange), so that the model doesn‚Äôt have any information about the actual labels.\nRun a quick experiment using a zero-shot2 model without anonymizing the labels to see the model‚Äôs performance.\n\nHere‚Äôs the code for the zero-shot baseline and the results:\n\npossible_labels = \"Possible labels: \" + \", \".join(df_train.class_name.unique().tolist())\nclass_name_to_id = dict(zip(df_train['class_name'], df_train['class_id']))\n\nresponse = classification_model.generate_content(\n    contents=[possible_labels] + input_images_message\n)\nresponse_json = json.loads(response.text)\n\nfor item in response_json[\"output\"]:\n    item['label'] = class_name_to_id.get(item['label'], item['label'])\n\naccuracy, f1 = calculate_metrics(df_test, response_json)\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1-score: {f1:.4f}\")\n\nAccuracy: 0.4800\nF1-score: 0.4619\n\n\nYou should get a 48% accuracy and a 46% F1-score. Both significantly higher than the ~7% you‚Äôd expect from random guessing, but still far from the 90%+ accuracy you obtained earlier.\nThis demonstrates that ICL can indeed enhance the model‚Äôs performance."
  },
  {
    "objectID": "posts/classify-images-with-gemini-flash-1.5.html#conclusion",
    "href": "posts/classify-images-with-gemini-flash-1.5.html#conclusion",
    "title": "Classifying images with Gemini Flash 1.5",
    "section": "Conclusion",
    "text": "Conclusion\nThat‚Äôs all!\nI still find it amazing that without any ‚Äúreal‚Äù training and just a few minutes of work, you can achieve pretty good results in a non-trivial image classification task using ICL with Gemini Flash 1.5 (or most other MLLMs).\nThis is a mostly unexplored area. There‚Äôs a lot of room for trying out different ideas and seeing what works best. This tutorial is just a starting point.\nHope you found it useful! Let me know if you have any questions in the comments below."
  },
  {
    "objectID": "posts/classify-images-with-gemini-flash-1.5.html#footnotes",
    "href": "posts/classify-images-with-gemini-flash-1.5.html#footnotes",
    "title": "Classifying images with Gemini Flash 1.5",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEstimated costs as of September 8, 2024:\n\n\n\nModel\nCost (512x512 image)\n\n\n\n\nGemini Flash 1.5\n$0.000039\n\n\nGemini Pro 1.5\n$0.0018\n\n\nGPT-4o\n$0.000213 - $0.000638\n\n\nSonnet 3.5\n$0.001047\n\n\n\n‚Ü©Ô∏é\nThat is, without providing any context images.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/nlp-snippets-cluster-documents-using-word2vec.html",
    "href": "posts/nlp-snippets-cluster-documents-using-word2vec.html",
    "title": "How to Cluster Documents Using Word2Vec and K-means",
    "section": "",
    "text": "There are more sentiment analysis tutorials online than people doing sentiment analysis in their day jobs. Don‚Äôt get me wrong. I‚Äôm not saying those tutorials aren‚Äôt useful. I just want to highlight that supervised learning receives much more attention than any other Natural Language Processing (NLP) method.\nOddly enough, there‚Äôs a big chance that most of the text data you‚Äôll use in your next projects won‚Äôt have ground truth labels. So supervised learning might not be a solution you can immediately apply to your data problems.\nWhat can you do then? Use unsupervised learning algorithms.\nIn this tutorial, you‚Äôll learn to apply unsupervised learning to generate value from your text data. You‚Äôll cluster documents by training a word embedding (Word2Vec) and applying the K-means algorithm.\nPlease be aware that the next sections focus on practical manners. You won‚Äôt find much theory in them besides brief definitions of relevant ideas.\nTo make the most of this tutorial, you should be familiar with these topics:\nLet‚Äôs get to it!"
  },
  {
    "objectID": "posts/nlp-snippets-cluster-documents-using-word2vec.html#how-to-cluster-documents",
    "href": "posts/nlp-snippets-cluster-documents-using-word2vec.html#how-to-cluster-documents",
    "title": "How to Cluster Documents Using Word2Vec and K-means",
    "section": "How to Cluster Documents",
    "text": "How to Cluster Documents\nYou can think of the process of clustering documents in three steps:\n\nCleaning and tokenizing data usually involves lowercasing text, removing non-alphanumeric characters, or stemming words.\nGenerating vector representations of the documents concerns the mapping of documents from words into numerical vectors‚Äîsome common ways of doing this include using bag-of-words models or word embeddings.\nApplying a clustering algorithm on the document vectors requires selecting and applying a clustering algorithm to find the best possible groups using the document vectors. Some frequently used algorithms include K-means, DBSCAN, or Hierarchical Clustering.\n\nThat‚Äôs it! Now, you‚Äôll see how that looks in practice."
  },
  {
    "objectID": "posts/nlp-snippets-cluster-documents-using-word2vec.html#sample-project-clustering-news-articles",
    "href": "posts/nlp-snippets-cluster-documents-using-word2vec.html#sample-project-clustering-news-articles",
    "title": "How to Cluster Documents Using Word2Vec and K-means",
    "section": "Sample Project: Clustering News Articles",
    "text": "Sample Project: Clustering News Articles\nIn this section, you‚Äôll learn how to cluster documents by working through a small project. You‚Äôll group news articles into categories using a dataset published by Szymon Janowski.\n\nSet Up Your Local Environment\nTo follow along with the tutorial examples, you‚Äôll need to download the data and install a few libraries. You can do it by following these steps:\n\nClone the nlp-snippets repository locally.\nCreate a new virtual environment using venv or conda.\nActivate your new virtual environment.\nInstall the required libraries.\nStart a Jupyter notebook.\n\nIf you‚Äôre using venv, then you need to run these commands:\ngit clone https://github.com/dylanjcastillo/nlp-snippets.git\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements\njupyter notebook\nIf you‚Äôre using conda, then you need to run these commands:\ngit clone https://github.com/dylanjcastillo/nlp-snippets.git\nconda create --name venv\nconda activate venv\npip install -r requirements\njupyter notebook\nNext, open Jupyter Notebook. Then, create a new notebook in the root folder and set its name to clustering_word2vec.ipynb.\nBy now, your project structure should look like this:\nnlp-snippets/\n‚îÇ\n‚îú‚îÄ‚îÄ clustering/\n‚îÇ\n‚îú‚îÄ‚îÄ data/\n‚îÇ\n‚îú‚îÄ‚îÄ ds_utils/\n‚îÇ\n‚îú‚îÄ‚îÄ preprocessing/\n‚îÇ\n‚îú‚îÄ‚îÄ venv/ # (If you're using venv)\n‚îÇ\n‚îú‚îÄ‚îÄ clustering_word2vec.ipynb\n‚îú‚îÄ‚îÄ LICENSE\n‚îú‚îÄ‚îÄ README.md\n‚îî‚îÄ‚îÄ requirements.txt\nThis is your project‚Äôs structure. It includes these directories and files:\n\nclustering/: Examples of clustering text data using bag-of-words, training a word2vec model, and using a pretrained fastText embeddings.\ndata/: Data used for the clustering examples.\nds_utils/: Common utility functions used in the sample notebooks in the repository.\npreprocessing/: Frequently used code snippets for preprocessing text.\nvenv/: If you used venv, then this directory will contain the files related to your virtual environment.\nrequirements.txt: Libraries used in the examples provided.\nREADME and License: Information about the repository and its license.\n\nFor now, you‚Äôll use the notebook you created (clustering_word2vec.ipynb) and the news dataset in data/. The notebooks in clustering/ and preprocessing/ include additional code snippets that might be useful for NLP tasks. You can review those on your own.\nIn the next section, you‚Äôll create the whole pipeline from scratch. If you‚Äôd like to download the full and cleaner version of the code in the examples, go to the NLP Snippets repository.\nThat‚Äôs it for setup! Next, you‚Äôll define your imports.\n\n\nImport the Required Libraries\nOnce you finish setting up your local environment, it‚Äôs time to start writing code in your notebook. Open clustering_word2vec.ipynb, and copy the following code in the first cell:\nimport os\nimport random\nimport re\nimport string\n\nimport nltk\nimport numpy as np\nimport pandas as pd\n\nfrom gensim.models import Word2Vec\n\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\n\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nnltk.download(\"stopwords\")\nnltk.download(\"punkt\")\n\nSEED = 42\nrandom.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nnp.random.seed(SEED)\nThese are the libraries you need for the sample project. Here‚Äôs what you do with each of them:\n\nos and random help you define a random seed to make the code deterministically reproducible.\nre and string provide you with easy ways to clean the data.\npandashelps you read the data.\nnumpyprovides you with linear algebra utilities you‚Äôll use to evaluate results. Also, it‚Äôs used for setting a random seed to make the code deterministically reproducible.\ngensim makes it easy for you to train a word embedding from scratch using the Word2Vec class.\nnltkaids you in cleaning and tokenizing data through the word_tokenize method and the stopword list.\nsklearngives you an easy interface to the clustering model, MiniBatchKMeans, and the metrics to evaluate the quality of its results, silhouette_samples and silhouette_score.\n\nIn addition to importing the libraries, you download English stopwords using nltk.download(\"stopwords\"), you define SEED and set it as a random seed using numpy, random, and the PYTHONHASHSEED environment variable. This last step makes sure your code is reproducible across systems.\nRun this cell and make sure you don‚Äôt get any errors. In the next section, you‚Äôll prepare your text data.\n\n\nClean and Tokenize Data\nAfter you import the required libraries, you need to read and preprocess the data you‚Äôll use in your clustering algorithm. The preprocessing consists of cleaning and tokenizing the data. To do that, copy the following function in a new cell in your notebook:\ndef clean_text(text, tokenizer, stopwords):\n    \"\"\"Pre-process text and generate tokens\n\n    Args:\n        text: Text to tokenize.\n\n    Returns:\n        Tokenized text.\n    \"\"\"\n    text = str(text).lower()  # Lowercase words\n    text = re.sub(r\"\\[(.*?)\\]\", \"\", text)  # Remove [+XYZ chars] in content\n    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces in content\n    text = re.sub(r\"\\w+‚Ä¶|‚Ä¶\", \"\", text)  # Remove ellipsis (and last word)\n    text = re.sub(r\"(?&lt;=\\w)-(?=\\w)\", \" \", text)  # Replace dash between words\n    text = re.sub(\n        f\"[{re.escape(string.punctuation)}]\", \"\", text\n    )  # Remove punctuation\n\n    tokens = tokenizer(text)  # Get tokens from text\n    tokens = [t for t in tokens if not t in stopwords]  # Remove stopwords\n    tokens = [\"\" if t.isdigit() else t for t in tokens]  # Remove digits\n    tokens = [t for t in tokens if len(t) &gt; 1]  # Remove short tokens\n    return tokens\nThis code cleans and tokenizes a text input, using a predefined tokenizer and a list of stopwords. It helps you perform these operations:\n\nLine 10: Transform the input into a string and lowercase it.\nLine 11: Remove substrings like ‚Äú[+300 chars]‚Äù I found while reviewing the data.\nLine 12: Remove multiple spaces, tabs, and line breaks.\nLine 13: Remove ellipsis characters.\nLines 14-17: Replace dashes between words with a space and remove punctuation.\nLines 19-20: Tokenize text and remove tokens using a list of stop words.\nLines 21-22: Remove digits and tokens whose length is too short.\n\nThen, in the next cell, copy the following code to read the data and apply that function to the text columns:\ncustom_stopwords = set(stopwords.words(\"english\") + [\"news\", \"new\", \"top\"])\ntext_columns = [\"title\", \"description\", \"content\"]\n\ndf_raw = pd.read_csv(\"data/news_data.csv\")\ndf = df_raw.copy()\ndf[\"content\"] = df[\"content\"].fillna(\"\")\n\nfor col in text_columns:\n    df[col] = df[col].astype(str)\n\n# Create text column based on title, description, and content\ndf[\"text\"] = df[text_columns].apply(lambda x: \" | \".join(x), axis=1)\ndf[\"tokens\"] = df[\"text\"].map(lambda x: clean_text(x, word_tokenize, custom_stopwords))\n\n# Remove duplicated after preprocessing\n_, idx = np.unique(df[\"tokens\"], return_index=True)\ndf = df.iloc[idx, :]\n\n# Remove empty values and keep relevant columns\ndf = df.loc[df.tokens.map(lambda x: len(x) &gt; 0), [\"text\", \"tokens\"]]\n\ndocs = df[\"text\"].values\ntokenized_docs = df[\"tokens\"].values\n\nprint(f\"Original dataframe: {df_raw.shape}\")\nprint(f\"Pre-processed dataframe: {df.shape}\")\nThis is how you read and preprocess the data. This code applies the cleaning function you defined earlier, removes duplicates and nulls, and drops irrelevant columns.\nYou apply these steps to a new data frame (df). It contains a column with the raw documents called text and another one with the preprocessed documents called tokens. You save the values of those columns into two variables, docs and tokenized_docs, to use in the next code snippets.\nIf you execute the two cells you defined, then you should get the following output:\nOriginal dataframe: (10437, 15)\nPre-processed dataframe: (9882, 2)\nNext, you‚Äôll create document vectors using Word2Vec.\n\n\nGenerate Document Vectors\nAfter you‚Äôve cleaned and tokenized the text, you‚Äôll use the documents‚Äô tokens to create vectors using Word2Vec. This process consists of two steps:\n\nTrain a Word2Vec model using the tokens you generated earlier. Alternatively, you could load a pre-trained Word2Vec model (I‚Äôll also show you how to do it).\nGenerate a vector per document based on its individual word vectors.\n\nIn this section, you‚Äôll go through these steps.\n\nTrain Word2Vec Model\nThe following code will help you train a Word2Vec model. Copy it into a new cell in your notebook:\nmodel = Word2Vec(sentences=tokenized_docs, vector_size=100, workers=1, seed=SEED)\nYou use this code to train a Word2Vec model based on your tokenized documents. For this example, you specified the following parameters in the Word2Vec class:\n\nsentences expects a list of lists with the tokenized documents.\nvector_size defines the size of the word vectors. In this case, you set it to 100.\nworkers defines how many cores you use for training. I set it to 1 to make sure the code is deterministically reproducible.\nseed sets the seed for random number generation. It‚Äôs set to the constant SEED you defined in the first cell.\n\nThere are other parameters you can tune when training the Word2Vec model. See gensim‚Äôs documentation if you‚Äôd like to learn more about them.\nNote: In many cases, you might want to use a pre-trained model instead of training one yourself. If that‚Äôs the case, gensim provides you with an easy way to access some of the most popular pre-trained word embeddings.\nYou can load a pre-trained Word2Vec model as follows:\nwv = api.load('word2vec-google-news-300')\nOne last thing, if you‚Äôre following this tutorial and decide to use a pre-trained model, you‚Äôll need to replace model.wv by wv in the code snippets from here on. Otherwise, you‚Äôll get an error.\nNext, run the cell you just created in your notebook. It might take a couple of minutes. After it‚Äôs done, you can validate that the results make sense by plotting the vectors or reviewing the similarity results for relevant words. You can do the latter by copying and running this code in a cell in your notebook:\nmodel.wv.most_similar(\"trump\")\nIf you run this code, then you‚Äôll get this output:\n[('trumps', 0.988541841506958),\n ('president', 0.9746493697166443),\n ('donald', 0.9274922013282776),\n ('ivanka', 0.9203903079032898),\n ('impeachment', 0.9195784330368042),\n ('pences', 0.9152231812477112),\n ('avlon', 0.9148306846618652),\n ('biden', 0.9146010279655457),\n ('breitbart', 0.9144087433815002),\n ('vice', 0.9067237973213196)]\nThat‚Äôs it! You‚Äôve trained your Word2Vec model, now, you‚Äôll use it to generate document vectors.\n\n\nCreate Document Vectors from Word Embedding\nNow you‚Äôll generate document vectors using the Word2Vec model you trained. The idea is straightforward. From the Word2Vec model, you‚Äôll get numerical vectors per word in a document, so you need to find a way of generating a single vector out of them.\nFor short texts, a common approach is to use the average of the vectors. There‚Äôs no clear consensus on what will work well for longer texts. Though, using a weighted average of the vectors might help.\nThe following code will help you create a vector per document by averaging its word vectors. Create a new cell in your notebook and copy this code there:\ndef vectorize(list_of_docs, model):\n    \"\"\"Generate vectors for list of documents using a Word Embedding\n\n    Args:\n        list_of_docs: List of documents\n        model: Gensim's Word Embedding\n\n    Returns:\n        List of document vectors\n    \"\"\"\n    features = []\n\n    for tokens in list_of_docs:\n        zero_vector = np.zeros(model.vector_size)\n        vectors = []\n        for token in tokens:\n            if token in model.wv:\n                try:\n                    vectors.append(model.wv[token])\n                except KeyError:\n                    continue\n        if vectors:\n            vectors = np.asarray(vectors)\n            avg_vec = vectors.mean(axis=0)\n            features.append(avg_vec)\n        else:\n            features.append(zero_vector)\n    return features\n\nvectorized_docs = vectorize(tokenized_docs, model=model)\nlen(vectorized_docs), len(vectorized_docs[0])\nThis code will get all the word vectors of each document and average them to generate a vector per each document. Here‚Äôs what‚Äôs happening there:\n\nYou define the vectorize function that takes a list of documents and a gensim model as input, and generates a feature vector per document as output.\nYou apply the function to the documents‚Äô tokens in tokenized_doc, using the Word2Vec model you trained earlier.\nYou print the length of the list of documents and the size of the generated vectors.\n\nNext, you‚Äôll cluster the documents using Mini-batches K-means.\n\n\n\nCluster Documents Using (Mini-batches) K-means\nTo cluster the documents, you‚Äôll use the Mini-batches K-means algorithm. This K-means variant uses random input data samples to reduce the time required during training. The upside is that it shares the same objective function with the original algorithm, so, in practice, the results are just a bit worse than K-means.\nIn the code snippet below, you can see the function you‚Äôll use to create the clusters using Mini-batches K-means. Create a new cell in your notebook, and copy the following code there:\ndef mbkmeans_clusters(\n    X,\n    k,\n    mb,\n    print_silhouette_values,\n):\n    \"\"\"Generate clusters and print Silhouette metrics using MBKmeans\n\n    Args:\n        X: Matrix of features.\n        k: Number of clusters.\n        mb: Size of mini-batches.\n        print_silhouette_values: Print silhouette values per cluster.\n\n    Returns:\n        Trained clustering model and labels based on X.\n    \"\"\"\n    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n    print(f\"For n_clusters = {k}\")\n    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n    print(f\"Inertia:{km.inertia_}\")\n\n    if print_silhouette_values:\n        sample_silhouette_values = silhouette_samples(X, km.labels_)\n        print(f\"Silhouette values:\")\n        silhouette_values = []\n        for i in range(k):\n            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n            silhouette_values.append(\n                (\n                    i,\n                    cluster_silhouette_values.shape[0],\n                    cluster_silhouette_values.mean(),\n                    cluster_silhouette_values.min(),\n                    cluster_silhouette_values.max(),\n                )\n            )\n        silhouette_values = sorted(\n            silhouette_values, key=lambda tup: tup[2], reverse=True\n        )\n        for s in silhouette_values:\n            print(\n                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n            )\n    return km, km.labels_\nThis function creates the clusters using the Mini-batches K-means algorithm. It takes the following arguments:\n\n**X**: Matrix of features. In this case, it‚Äôs your vectorized documents.\n**k**:Number of clusters you‚Äôd like to create.\n**mb**: Size of mini-batches.\n**print_silhouette_values**: Defines if the Silhouette Coefficient is printed for each cluster. If you haven‚Äôt heard about this coefficient, don‚Äôt worry, you‚Äôll learn about it in a bit!\n\nmbkmeans_cluster takes these arguments and returns the fitted clustering model and the labels for each document.\nRun the cell where you copied the function. Next, you‚Äôll apply this function to your vectorized documents.\n\nDefinition of Clusters\nNow, you need to execute mbkmean_clusters providing it with the vectorized documents and the number of clusters. You‚Äôll print the Silhouette Coefficients per cluster to review the quality of your clusters.\nCreate a new cell and copy this code there:\nclustering, cluster_labels = mbkmeans_clusters(\n    X=vectorized_docs,\n    k=50,\n    mb=500,\n    print_silhouette_values=True,\n)\ndf_clusters = pd.DataFrame({\n    \"text\": docs,\n    \"tokens\": [\" \".join(text) for text in tokenized_docs],\n    \"cluster\": cluster_labels\n})\nThis code will fit the clustering model, print the Silhouette Coefficient per cluster, and return the fitted model and the labels per cluster. It‚Äôll also create a data frame you can use to review the results.\nThere are a few things to consider when setting the input arguments:\n\nprint_silhouette_values is straightforward. In this case, you set it to True to print the evaluation metric per cluster. This will help you review the results.\nmb depends on the size of your dataset. You need to ensure that it is not too small to avoid a significant impact on the quality of results and not too big to avoid making the execution too slow. In this case, you set it to 500 observations.\nk is trickier. In general, it involves a mix of qualitative analysis and quantitative metrics. After a few experiments on my side, I found that 50 seemed to work well. But that is more or less arbitrary.\n\nYou could use metrics like the Silhouette Coefficient for the quantitative evaluation of the number of clusters. This coefficient is an evaluation metric frequently used in problems where ground truth labels are unknown. It‚Äôs calculated using the mean intra-cluster distance and the mean nearest-cluster distance and goes from -1 to 1. Well-defined clusters result in positive values of this coefficient, while incorrect clusters will result in negative values. If you‚Äôd like to learn more about it, look at scikit-learn‚Äôs documentation.\nThe qualitative part generally requires you to have domain knowledge of the subject matter so you can sense-check your clustering algorithm‚Äôs results. In the next section, I‚Äôll show you two approaches you can use to check your results qualitatively.\nAfter executing the cell you just created, the output should look like this:\nFor n_clusters = 50\nSilhouette coefficient: 0.11\nInertia:3568.342791047967\nSilhouette values:\n    Cluster 29: Size:50 | Avg:0.39 | Min:0.01 | Max: 0.59\n    Cluster 35: Size:30 | Avg:0.34 | Min:0.05 | Max: 0.54\n    Cluster 37: Size:58 | Avg:0.32 | Min:0.09 | Max: 0.51\n    Cluster 39: Size:81 | Avg:0.31 | Min:-0.05 | Max: 0.52\n    Cluster 27: Size:63 | Avg:0.28 | Min:0.02 | Max: 0.46\n    Cluster 6: Size:101 | Avg:0.27 | Min:0.02 | Max: 0.46\n    Cluster 24: Size:120 | Avg:0.26 | Min:-0.04 | Max: 0.46\n    Cluster 49: Size:65 | Avg:0.26 | Min:-0.03 | Max: 0.47\n    Cluster 47: Size:53 | Avg:0.23 | Min:0.01 | Max: 0.45\n    Cluster 22: Size:78 | Avg:0.22 | Min:-0.01 | Max: 0.43\n    Cluster 45: Size:38 | Avg:0.21 | Min:-0.07 | Max: 0.41\n...\nThis is the output of your clustering algorithm. The sizes and Silhouette Coefficients per cluster are the most relevant metrics. The clusters are printed by the value of the Silhouette coefficient in descending order. A higher score means denser ‚Äì and thus better ‚Äì clusters. In this case, you can see that clusters 29, 35, and 37 seem to be the top ones.\nNext, you‚Äôll learn how to check what‚Äôs in each cluster.\n\n\nQualitative Review of Clusters\nThere are a few ways you can qualitatively analyze the results. During the earlier sections, our approach resulted in vector representations of tokens and documents, and vectors of the clusters‚Äô centroids. You can find the most representative tokens and documents to analyze the results by looking for the vectors closest to the clusters‚Äô centroids.\nHere‚Äôs how you obtain the most representative tokens per cluster:\nprint(\"Most representative terms per cluster (based on centroids):\")\nfor i in range(50):\n    tokens_per_cluster = \"\"\n    most_representative = model.wv.most_similar(positive=[clustering.cluster_centers_[i]], topn=5)\n    for t in most_representative:\n        tokens_per_cluster += f\"{t[0]} \"\n    print(f\"Cluster {i}: {tokens_per_cluster}\")\nFor the top clusters we identified earlier ‚Äì 29, 35, and 37 ‚Äì these are the results:\nCluster 29: noaa sharpie claim assertions forecasters\nCluster 35: eye lilinow path halts projected\nCluster 37: cnnpolitics complaint clinton pences whistleblower\nNext, we can do the same analysis with documents instead of tokens. This is how you find the most representative documents for cluster 29:\ntest_cluster = 29\nmost_representative_docs = np.argsort(\n    np.linalg.norm(vectorized_docs - clustering.cluster_centers_[test_cluster], axis=1)\n)\nfor d in most_representative_docs[:3]:\n    print(docs[d])\n    print(\"-------------\")\nAnd these are the 3 most representative documents in that cluster:\nDorian, Comey and Debra Messing: What Trump tweeted on Labor Day weekend | President Donald Trump axed his visit to Poland over the weekend to monitor Hurricane Dorian from Camp David with emergency management staff, but if the President's more than 120 tweets are any indication, he had more than just the storm on his mind. | Washington (CNN)President Donald Trump axed his visit to Poland over the weekend to monitor Hurricane Dorian from Camp David with emergency management staff, but if the President's more than 120 tweets are any indication, he had more than just the storm on hi‚Ä¶ [+3027 chars]\n-------------\nRoss Must Resign If Report He Threatened NOAA Officials Is True: Democrat | As President Donald Trump claimed Hurricane Dorian could hit Alabama, the National Weather Service tweeted to correct the rumors. | Commerce Secretary Wilbur Ross is facing calls to resign over a report alleging that he threatened to fire top officials at NOAA for a tweet disputing President Donald Trump's claim that Hurricane Dorian would hit Alabama.\n\"If that story is true, and I don't‚Ä¶ [+3828 chars]\n-------------\nFederal weather workers are furious at the NOAA's 'utterly disgusting' statement defending Trump's claim Hurricane Dorian would hit Alabama | Federal weather workers have reacted furiously to the National Oceanic and Atmospheric Administration's (NOAA) defence of US President Donald Trump's repeated assertions that Hurricane Dorian was set to hit Alabama. \"Never ever before has their management thr‚Ä¶ | Federal weather workers have reacted furiously to the National Oceanic and Atmospheric Administration's (NOAA) defence of US President Donald Trump's repeated assertions that Hurricane Dorian was set to hit Alabama, saying they have been \"thrown under the bus‚Ä¶ [+3510 chars]\nMost of the results seem to be related to a dispute between Donald Trump and the National Oceanic and Atmospheric Agency (NOAA). It was a famous controversy that people referred to as Sharpiegate.\nYou could also explore other approaches like generating word frequencies per cluster or reviewing random samples of documents per cluster."
  },
  {
    "objectID": "posts/nlp-snippets-cluster-documents-using-word2vec.html#other-approaches",
    "href": "posts/nlp-snippets-cluster-documents-using-word2vec.html#other-approaches",
    "title": "How to Cluster Documents Using Word2Vec and K-means",
    "section": "Other Approaches",
    "text": "Other Approaches\nThere are other approaches you could take to cluster text data like:\n\nUse a pre-trained word embedding instead of training your own. In this tutorial, you trained a Word2Vec model from scratch, but it‚Äôs very common to use a pre-trained model.\nGenerating feature vectors using a bag-of-words approach instead of word embeddings.\nReducing dimensionality of feature vectors. This is very useful if you use a bag-of-words approach.\nClustering documents using other algorithms like HDBSCAN or Hierarchical Clustering.\nUsing BERT sentence embeddings to generate the feature vectors. Or generating the topics with BERTopic."
  },
  {
    "objectID": "posts/nlp-snippets-cluster-documents-using-word2vec.html#conclusion",
    "href": "posts/nlp-snippets-cluster-documents-using-word2vec.html#conclusion",
    "title": "How to Cluster Documents Using Word2Vec and K-means",
    "section": "Conclusion",
    "text": "Conclusion\nWay to go! You just learned how to cluster documents using Word2Vec. You went through an end-to-end project, where you learned all the steps required for clustering a corpus of text.\nYou learned how to:\n\nPreprocess data to use with a Word2Vec model\nTrain a Word2Vec model\nUse quantitative metrics, such as the Silhouette score to evaluate the quality of your clusters.\nFind the most representative tokens and documents in your clusters\n\nI hope you find this tutorial useful. Shoot me a message if you have any questions!"
  },
  {
    "objectID": "posts/key-parameters-llms.html",
    "href": "posts/key-parameters-llms.html",
    "title": "Key parameters for LLMs",
    "section": "",
    "text": "I recently did a workshop about building agents. During the workshop I discussed the key parameters for LLMs, so I thought it‚Äôd be useful to write a short post about it.\nThese are the parameters that you will usually use when building LLM-based products:\nIn the next sections, I‚Äôll go over each of these parameters in more detail and provide some suggestions about how to use them."
  },
  {
    "objectID": "posts/key-parameters-llms.html#model",
    "href": "posts/key-parameters-llms.html#model",
    "title": "Key parameters for LLMs",
    "section": "Model",
    "text": "Model\nWhen choosing a model, consider the following factors:\n\nComplexity of task: Am I solving a problem that requires reasoning capabilities? Or is it a simple task?\nSpeed: How important is it that the model replies quickly? Is this something that I can run on the background?\nCost: How much do I want to spend on this task?\nProvider: Which providers do I have access to? Do I need to self-host?\n\nRight now, my go-to models are Gemini 2.5 Pro or Claude 4 for complex tasks. For simpler tasks, I use Gemini 2.5 Flash or OpenAI‚Äôs gpt-4.1 family.\nThe best way to pick a model is to start with the most capable models and then scale down to the simplest models that still capable of solving the task. Otherwise, you could end up spending a lot of time trying to solve an issue that you simply cannot solve reliably with smaller models."
  },
  {
    "objectID": "posts/key-parameters-llms.html#messagesprompts",
    "href": "posts/key-parameters-llms.html#messagesprompts",
    "title": "Key parameters for LLMs",
    "section": "Messages/prompts",
    "text": "Messages/prompts\nThe messages/prompts you send to the LLM will determine the context and instructions for the LLM to follow. I wrote a guide on prompt engineering that covers the basics of how to write good prompts."
  },
  {
    "objectID": "posts/key-parameters-llms.html#temperature",
    "href": "posts/key-parameters-llms.html#temperature",
    "title": "Key parameters for LLMs",
    "section": "Temperature",
    "text": "Temperature\nThe temperature parameter controls the randomness of the model‚Äôs output. A temperature of 0 will make the model more deterministic, while a temperature of 1 will make the model more random.\nI wrote a post about how temperature affects the output of LLMs. For tasks where consistency is important, use a temperature of 0. For tasks where creativity is important, use a temperature above 0. I‚Äôve found that anything above 1.3-4 is too random."
  },
  {
    "objectID": "posts/key-parameters-llms.html#seed",
    "href": "posts/key-parameters-llms.html#seed",
    "title": "Key parameters for LLMs",
    "section": "Seed",
    "text": "Seed\nThe seed parameter is used to initialize the random number generator that is then used to sample the next token. If you want to maximize reproducibility, set a seed value.\nThis is only available in OpenAI, Gemini, and open-weight models. Check my post for more details."
  },
  {
    "objectID": "posts/key-parameters-llms.html#top-p-sampling",
    "href": "posts/key-parameters-llms.html#top-p-sampling",
    "title": "Key parameters for LLMs",
    "section": "Top-P Sampling",
    "text": "Top-P Sampling\nTop-p sampling is a technique that limits the number of tokens that can be selected from the vocabulary by first selecting the smallest group of tokens whose combined probability ‚â• P. For example, top P = 0.9 picks the next token from the smallest group of tokens that together cover at least 90% probability.\nI rarely use this parameter."
  },
  {
    "objectID": "posts/key-parameters-llms.html#logit-biases",
    "href": "posts/key-parameters-llms.html#logit-biases",
    "title": "Key parameters for LLMs",
    "section": "Logit biases",
    "text": "Logit biases\nLogits are the raw scores that the model assigns to each token. You can use biases to change the odds of a token being selected. Positive biases increase the odds of the token being selected, while negative biases do the opposite.\nThis is often used for document classification tasks or LLM-based rerankers."
  },
  {
    "objectID": "posts/key-parameters-llms.html#logprobs",
    "href": "posts/key-parameters-llms.html#logprobs",
    "title": "Key parameters for LLMs",
    "section": "Logprobs",
    "text": "Logprobs\nLogprobs are the logaritmic probabilities of the tokens. They are defined as:\n\\[logprob(w_i) = ln(P(w_i)) = ln(\\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}) \\]\nWhere:\n\n\\(w_i\\) is the \\(i\\)-th token in the vocabulary.\n\\(P(w_i)\\) is the probability of the \\(i\\)-th token.\n\\(z_i\\) is the logit of the \\(i\\)-th token.\n\\(n\\) is the number of tokens in the vocabulary.\n\nYou can use this parameter in OpenAI models. Gemini provides a single request with logprobs per day (yes, I‚Äôm not kidding üòÖ). Anthropic doesn‚Äôt provide them.\nOpen-weight models don‚Äôt provide logprobs. They provide logits instead, that you can use to calculate the probabilities of the tokens."
  },
  {
    "objectID": "posts/key-parameters-llms.html#max-completion-tokens",
    "href": "posts/key-parameters-llms.html#max-completion-tokens",
    "title": "Key parameters for LLMs",
    "section": "Max completion tokens",
    "text": "Max completion tokens\nThis parameter limits the number of tokens that the model can generate. This is useful to control costs and length of the output."
  },
  {
    "objectID": "posts/key-parameters-llms.html#response-format",
    "href": "posts/key-parameters-llms.html#response-format",
    "title": "Key parameters for LLMs",
    "section": "Response format",
    "text": "Response format\nYou can use this parameter to specify the format of the response. Anthropic, OpenAI, and Gemini support structured outputs in the form of JSON schemas. I‚Äôve written multiple posts on this topic:\n\nStructured outputs can hurt the performance of LLMs\nThe good, the bad, and the ugly of Gemini‚Äôs structured outputs\nStructured outputs: don‚Äôt put the cart before the horse\n\nOpen-weight models allow for more flexible structured output formats. For example, using outlines lets you define custom regEx patterns to extract the data you need."
  },
  {
    "objectID": "posts/key-parameters-llms.html#streaming",
    "href": "posts/key-parameters-llms.html#streaming",
    "title": "Key parameters for LLMs",
    "section": "Streaming",
    "text": "Streaming\nThis parameter is used to stream the response from the model. This improves the user experience as it allows you to see the output as it‚Äôs being generated."
  },
  {
    "objectID": "posts/key-parameters-llms.html#tools",
    "href": "posts/key-parameters-llms.html#tools",
    "title": "Key parameters for LLMs",
    "section": "Tools",
    "text": "Tools\nTools are a way to extend the capabilities of the model by providing it with external tools. This is critical for building agents."
  },
  {
    "objectID": "posts/key-parameters-llms.html#conclusion",
    "href": "posts/key-parameters-llms.html#conclusion",
    "title": "Key parameters for LLMs",
    "section": "Conclusion",
    "text": "Conclusion\nThis was a very short post about the key parameters for LLMs. I hope you found it useful.\nLet me know if you have any questions or feedback."
  },
  {
    "objectID": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html",
    "href": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html",
    "title": "Deploying a FastAPI app with Kamal, AWS ECR, and Github Actions",
    "section": "",
    "text": "These days I use Kamal to deploy my FastAPI (or Django) projects. Kamal is a simpler alternative to Kubernetes that you can use to deploy containerized apps to a VPS.\nOnce you get the hang of it, it‚Äôll only take you a few minutes to set up a CI/CD pipeline that automatically deploys your app to production with each push to the main branch.\nIn this tutorial, I‚Äôll walk you through the process of deploying a FastAPI app with Kamal, AWS ECR, and Github Actions.\nYou can find the code for this tutorial in this repository."
  },
  {
    "objectID": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#prerequisites",
    "href": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#prerequisites",
    "title": "Deploying a FastAPI app with Kamal, AWS ECR, and Github Actions",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo make the most of this tutorial, you should:\n\nHave a FastAPI app ready to deploy.\nHave an AWS account and its CLI installed.\nBe comfortable with Docker.\nHave a basic understanding of Kamal. You‚Äôll need to install version 1.9.0 for this tutorial.\nHave a basic understanding of Github Actions.\nHave a VPS with Ubuntu ready to host your app."
  },
  {
    "objectID": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#prepare-your-vps",
    "href": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#prepare-your-vps",
    "title": "Deploying a FastAPI app with Kamal, AWS ECR, and Github Actions",
    "section": "Prepare your VPS",
    "text": "Prepare your VPS\nYou‚Äôll need to install docker, curl, git, and snapd on your VPS, and create a non-root user called kamal that can sudo. You should also set the UID and GID of the user to 1000.\nIf you‚Äôre using Hetzner, you can use my terraform script to prepare the VPS.\nOtherwise, you can run these commands on your VPS‚Äôs terminal:\n# Install docker, curl, and git, and snapd\napt-get update\napt-get install -y docker.io curl git snapd\n\n# Start and enable the docker service\nsystemctl start docker\nsystemctl enable docker\n\n# Create a non-root user called kamal\nuseradd -m -s /bin/bash -u 1000 kamal\nusermod -aG sudo kamal\necho \"kamal ALL=(ALL) NOPASSWD:ALL\" &gt;&gt; /etc/sudoers.d/kamal\n\n# Add SSH key to login as kamal user\nmkdir -p /home/kamal/.ssh\necho \"&lt;YOUR_PUBLIC_SSH_KEY&gt;\" &gt;&gt; /home/kamal/.ssh/authorized_keys # you need a public key to login as the kamal user\nchmod 700 /home/kamal/.ssh\nchmod 600 /home/kamal/.ssh/authorized_keys\nchown -R kamal:kamal /home/kamal/.ssh\n\n# Disable root login\nsed -i '/PermitRootLogin/d' /etc/ssh/sshd_config\necho \"PermitRootLogin no\" &gt;&gt; /etc/ssh/sshd_config\nsystemctl restart sshd\n\n# Add the kamal user to the docker group\nusermod -aG docker kamal\ndocker network create --driver bridge kamal_network\n\n# Create a folder for the Let's Encrypt ACME JSON\nmkdir -p /letsencrypt && touch /letsencrypt/acme.json && chmod 600 /letsencrypt/acme.json\nchown -R kamal:kamal /letsencrypt\n\nreboot\nTo run these commands, you need to login as root. This assumes that there isn‚Äôt already a non-root user with UID 1000. Otherwise, you‚Äôll have to adjust the commands accordingly.\nAlso, if you don‚Äôt have a public SSH key for the ‚ÄúAdd SSH key‚Äù step, you can generate one with the following command:\nssh-keygen -t ed25519 -C \"your-email@example.com\"\nThese commands will:\n\nInstall docker, curl, and git\nStart and enable the docker service\nCreate a non-root user called kamal\nDisable root login\nAdd the kamal user to the docker group (this allows the user to run docker without needing to use sudo)\nCreate a Docker bridge network for Traefik\nCreate a folder for the Let‚Äôs Encrypt ACME JSON file\nMake the Let‚Äôs Encrypt ACME JSON folder writable by the kamal user\nRestart the server\n\nFinally, configure the SSH key in your local .ssh/config file so you can login as the kamal user without using the root account.\nHost kamal\n  HostName &lt;YOUR_VPS_IP&gt;\n  User kamal\n  IdentityFile ~/.ssh/&lt;YOUR_PRIVATE_SSH_KEY&gt;"
  },
  {
    "objectID": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#create-a-dockerfile-for-your-fastapi-app",
    "href": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#create-a-dockerfile-for-your-fastapi-app",
    "title": "Deploying a FastAPI app with Kamal, AWS ECR, and Github Actions",
    "section": "Create a Dockerfile for your FastAPI app",
    "text": "Create a Dockerfile for your FastAPI app\nKamal works with containerized apps, so you‚Äôll need to have a Dockerfile. I also recommend using an entrypoint.sh script to run the application, because that also allows you to run commands in the container.\n\nDockerfile\nHere‚Äôs the Dockerfile I‚Äôm using for my projects. You can use this as a template and adjust it to your needs.\n\n\nDockerfile\n\nFROM python:3.10-slim AS base\n\nENV POETRY_HOME=/opt/poetry\nENV POETRY_VERSION=1.8.3\nENV PATH=${POETRY_HOME}/bin:${PATH}\n\nRUN apt-get update \\\n    && apt-get install --no-install-recommends -y \\\n    curl \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\nRUN curl -sSL https://install.python-poetry.org | python3 - && poetry --version\n\nFROM base AS builder\n\nWORKDIR /app\n\nCOPY poetry.lock pyproject.toml ./\n\nRUN poetry config virtualenvs.in-project true && \\\n    poetry install --only main --no-interaction\n\nFROM base AS runner\n\nWORKDIR /app\nCOPY --from=builder /app/.venv/ /app/.venv/\n\nCOPY . /app\n\nRUN chmod +x /app/entrypoint.sh\n\nFROM runner AS production\n\nEXPOSE 8000\n\nARG user=kamal\nARG group=kamal\nARG uid=1000\nARG gid=1000\nRUN groupadd -g ${gid} ${group} && \\\n    useradd -u ${uid} -g ${group} -s /bin/sh -m ${user} && \\\n    chown -R ${uid}:${gid} /app\n\nUSER ${uid}:${gid}\n\nWORKDIR /app\nCMD [ \"/app/entrypoint.sh\" , \"app\"]\n\nThis multi-stage Dockerfile does the following:\n\nInstalls poetry and sets up the virtual environment\nCreates the user kamal with the UID and GID 1000 and runs the application with that user.\nExposes port 8000 and runs the application by executing the entrypoint.sh script. Kamal automatically detects that is the port the app runs on and will use that to set up the reverse proxy.\n\nFeel free to adjust this Dockerfile to your needs.\n\n\nentrypoint.sh script\nI use an entrypoint.sh script to run the application because that makes it easier to collect static files, run migrations when the container starts, and also running commands in the container.\nHere‚Äôs an example of a simple entrypoint.sh script:\n\n\nentrypoint.sh\n\n#!/bin/sh\n\nset -e\n\nif [ \"$1\" = \"app\" ]; then\n    echo \"Collecting static files\"\n    exec poetry run gunicorn -c gunicorn.conf.py\nelse\n    exec \"$@\"\nfi\n\nThis script starts the gunicorn server with uvicorn workers and some sensible defaults. It also allows you to pass other arguments to the script, which is useful if you want to run other commands in the container. You can add or remove commands to the script as needed."
  },
  {
    "objectID": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#configure-an-ecr-registry-in-aws",
    "href": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#configure-an-ecr-registry-in-aws",
    "title": "Deploying a FastAPI app with Kamal, AWS ECR, and Github Actions",
    "section": "Configure an ECR registry in AWS",
    "text": "Configure an ECR registry in AWS\nNext, you‚Äôll need a place to push and pull your Docker images. I use AWS ECR, so that‚Äôs what I‚Äôll show you how to do here. Kamal also supports other registries.\nLog in to the AWS Management Console and go to Amazon ECR. Click on Create repository and set a name for your repository.\nThen, create a new IAM user in your AWS account by going to Services &gt; IAM &gt; Users &gt; Add user.\nDuring the process you‚Äôll have to assign a permissions to the user. You can create a new policy with the following content and attach it to the user:\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"ListImagesInRepository\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"ecr:ListImages\"],\n      \"Resource\": [\n        \"arn:aws:ecr:&lt;REGION&gt;:&lt;ACCOUNT_ID&gt;:repository/&lt;REPOSITORY_NAME&gt;\"\n      ]\n    },\n    {\n      \"Sid\": \"GetAuthorizationToken\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"ecr:GetAuthorizationToken\"],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"ManageRepositoryContents\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ecr:BatchCheckLayerAvailability\",\n        \"ecr:GetDownloadUrlForLayer\",\n        \"ecr:GetRepositoryPolicy\",\n        \"ecr:DescribeRepositories\",\n        \"ecr:ListImages\",\n        \"ecr:DescribeImages\",\n        \"ecr:BatchGetImage\",\n        \"ecr:InitiateLayerUpload\",\n        \"ecr:UploadLayerPart\",\n        \"ecr:CompleteLayerUpload\",\n        \"ecr:PutImage\"\n      ],\n      \"Resource\": [\n        \"arn:aws:ecr:&lt;REGION&gt;:&lt;ACCOUNT_ID&gt;:repository/&lt;REPOSITORY_NAME&gt;\"\n      ]\n    }\n  ]\n}\nThis policy enables users to list, access, and manage the ECR repository they have previously created, as well as obtain an authorization token necessary for pushing and pulling images. You must replace &lt;REGION&gt;, &lt;ACCOUNT_ID&gt;, and &lt;REPOSITORY_NAME&gt; with the specific details of your own repository.\nThen, select the user you created and navigate to Security credentials &gt; Access keys &gt; Create access key. Download the generated CSV file and store it in a secure location.\nThe GitHub Actions workflow will use these credentials for pushing and pulling images from the ECR registry."
  },
  {
    "objectID": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#set-up-kamal-in-your-project",
    "href": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#set-up-kamal-in-your-project",
    "title": "Deploying a FastAPI app with Kamal, AWS ECR, and Github Actions",
    "section": "Set up Kamal in your project",
    "text": "Set up Kamal in your project\nOpen your FastAPI project in your favorite code editor. Create a folder called deploy in the root directory. Then go into the folder and initialize Kamal:\nkamal init\nThis will create two folders (.kamal/ and config/) and an .env file. Inside config/, you‚Äôll find a deploy.yml file. This is where you‚Äôll provide the instructions for Kamal to build and deploy your app.\nYou can use the following deploy.yml file as a template for your FastAPI app:\n\n\ndeploy.yml\n\nservice: example\n\nimage: example\n\nssh:\n  user: kamal\n\nenv:\n  secret:\n    - FASTAPI_ENV\n\ntraefik:\n  options:\n    publish:\n      - \"443:443\"\n    volume:\n      - \"/letsencrypt/:/letsencrypt/\"\n    memory: 500m\n    network: private_network\n  args:\n    entryPoints.web.address: \":80\"\n    entryPoints.websecure.address: \":443\"\n    entryPoints.web.http.redirections.entryPoint.to: websecure\n    entryPoints.web.http.redirections.entryPoint.scheme: https\n    entryPoints.web.http.redirections.entrypoint.permanent: true\n    certificatesResolvers.letsencrypt.acme.email: &lt;YOUR_EMAIL&gt;\"\n    certificatesResolvers.letsencrypt.acme.storage: \"/letsencrypt/acme.json\"\n    certificatesResolvers.letsencrypt.acme.httpchallenge: true\n    certificatesResolvers.letsencrypt.acme.httpchallenge.entrypoint: web\n\nservers:\n  web:\n    hosts:\n      - 128.140.0.209\n    healthcheck:\n      port: 8000\n      interval: 5s\n    options:\n      network: private_network\n    labels:\n      traefik.http.routers.app.tls: true\n      traefik.http.routers.app.entrypoints: websecure\n      traefik.http.routers.app.rule: Host(`&lt;YOUR_DOMAIN&gt;`)\n      traefik.http.routers.app.tls.certresolver: letsencrypt\n\nregistry:\n  server: &lt;account_id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com\n  username: AWS\n  password:\n    - KAMAL_REGISTRY_PASSWORD\n\nbuilder:\n  dockerfile: \"../Dockerfile\"\n  context: \"../\"\n  multiarch: false\n  cache:\n    type: gha\n\nThis will set up your app and a reverse proxy using Traefik (with automatic SSL certificates using Let‚Äôs Encrypt). Remember to replace the placeholders with your own values. It will also do a healthcheck on /up on port 8000.\n\nTest the configuration locally\nTo test it locally, first, you must define the required environment variables in .env, such as keys for AI services, email providers, etc.\nYou‚Äôll also need to get a temporary password to authenticate into the ECR registry. You can get this password by running the following command from your terminal:\naws ecr get-login-password --region &lt;YOUR_REGION&gt;\nYou should copy the output of this command and paste it in the KAMAL_REGISTRY_PASSWORD field in the .env file.\nThen, run the following command to deploy your application to your VPS:\nkamal env push\nkamal deploy\nThe first command will push the environment variables to the VPS. The second command will build the Docker image, push it to the ECR registry, and deploy it to your VPS.\nAfter a few minutes, your app should be live at https://&lt;YOUR_DOMAIN&gt;.\nIf you see any errors, you can:\n\nRun kamal app logs to see the logs of the app.\nOpen a terminal in the container by running kamal app exec -it bash.\n\nThis is how I usually debug the app."
  },
  {
    "objectID": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#automate-the-deployment-with-github-actions",
    "href": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#automate-the-deployment-with-github-actions",
    "title": "Deploying a FastAPI app with Kamal, AWS ECR, and Github Actions",
    "section": "Automate the deployment with Github Actions",
    "text": "Automate the deployment with Github Actions\nNow that you have a working deployment process in your local environment, you can set up your CI/CD pipeline using GitHub Actions.\nCreate a new file in the .github/workflows folder called deploy.yml and add the following code:\nname: Deploy FastAPI app to VPS\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\non:\n  push:\n    branches: [\"main\"]\n  workflow_dispatch:\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n\n      - uses: webfactory/ssh-agent@v0.7.0\n        with:\n          ssh-private-key: ${{ secrets.VPS_SSH_PRIVATE_KEY }}\n\n      - name: Set up Ruby and install kamal\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: 3.2.2\n      - run: gem install kamal -v 1.9.0\n\n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID_ECR }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY_ECR }}\n          aws-region: us-east-1\n          mask-aws-account-id: false # otherwise the mask will hide your account ID and cause errors in the deployment\n\n      - name: Login to AWS ECR\n        id: login-ecr\n        uses: aws-actions/amazon-ecr-login@v2\n\n      - name: Set up Docker Buildx for cache\n        uses: docker/setup-buildx-action@v3\n\n      - name: Expose GitHub Runtime for cache\n        uses: crazy-max/ghaction-github-runtime@v3\n\n      - name: Create .env file\n        run: |\n          cd &lt;YOUR_PROJECT_ROOT&gt;/deploy\n          touch .env\n          echo KAMAL_REGISTRY_PASSWORD=\"${{ steps.login-ecr.outputs.docker_password_&lt;YOUR_ACCOUNT_ID&gt;_dkr_ecr_&lt;YOUR_REGION&gt;_amazonaws_com }}\" &gt;&gt; .env\n          # if you have other secrets, add them here\n          cat .env\n\n      - name: Kamal Deploy\n        id: kamal-deploy\n        run: |\n          cd &lt;YOUR_PROJECT_ROOT&gt;/deploy\n          kamal lock release\n          kamal env push\n          kamal deploy\nThis workflow will:\n\nCheckout the code\nSet up the Ruby environment and install Kamal\nConfigure the AWS credentials\nLogin to the AWS ECR registry\nSet up Docker Buildx for cache\nExpose GitHub Runtime for cache\nCreate the .env file\nRun Kamal deploy\n\nIt will run everytime you make a push to the main branch or by manually triggering the workflow. It‚Äôll cancel any in-progress runs to avoid conflicts.\nAlso, before you push your code to the repository, you‚Äôll need to add the following secrets to the repository:\n\nVPS_SSH_PRIVATE_KEY: The private key to connect to your VPS\nAWS_ACCESS_KEY_ID_ECR: The access key ID for the AWS ECR registry\nAWS_SECRET_ACCESS_KEY_ECR: The secret access key for the AWS ECR registry\n\nFinally, to speed up the deployment, add these options to the builder section of the deploy.yml file:\nbuilder:\n  dockerfile: \"../Dockerfile\"\n  context: \"../\"\n  multiarch: false # new\n  cache: # new\n    type: gha # new\nThis will enable the Docker Buildx cache for the build process in Github Actions. You can set multiarch to false if your CI pipeline shares the same architecture as your VPS, which was the case for me."
  },
  {
    "objectID": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#conclusion",
    "href": "posts/deploy-a-fastapi-app-with-kamal-aws-ecr-and-github-actions.html#conclusion",
    "title": "Deploying a FastAPI app with Kamal, AWS ECR, and Github Actions",
    "section": "Conclusion",
    "text": "Conclusion\nYou now have a fully automated deployment pipeline for your FastAPI app. A push to the main branch will trigger the workflow, that will build the Docker image, push it to the ECR registry, and deploy it to your VPS.\nBreak free from the tyranny of manual deployments and expensive cloud services. Sleep like a baby and let Kamal handle your deployments.\nIf you have any questions or feedback, please feel free to leave a comment below."
  },
  {
    "objectID": "posts/fast-and-async-in-python-accelerate-your-requests-using-asyncio.html",
    "href": "posts/fast-and-async-in-python-accelerate-your-requests-using-asyncio.html",
    "title": "Fast & Asynchronous In Python: Accelerate Your Requests Using asyncio",
    "section": "",
    "text": "A few weeks ago, I was working on a Python script to extract books‚Äô metadata for a content-based recommender. After a couple of hours, I realized that I needed to make thousands of requests to the Google Books API to get the data. So I thought there had to be a way of speeding up the process.\nAs I enjoy learning, especially when it‚Äôs also a chance of procrastinating on my goals, I decided to build a project using asyncio. Afterward, feeling guilty for the time wasted, I decided to write this tutorial with what I learned in the process.\nThis article will show you how to use asyncio and aiohttp to do asynchronous requests to an API. It‚Äôs mostly focused on the code, apart from the short introduction below, so if you are looking for a more in-depth introduction to asyncio, check the recommendations in the references1."
  },
  {
    "objectID": "posts/fast-and-async-in-python-accelerate-your-requests-using-asyncio.html#asyncio-in-30-seconds-or-less",
    "href": "posts/fast-and-async-in-python-accelerate-your-requests-using-asyncio.html#asyncio-in-30-seconds-or-less",
    "title": "Fast & Asynchronous In Python: Accelerate Your Requests Using asyncio",
    "section": "asyncio in 30 Seconds or Less",
    "text": "asyncio in 30 Seconds or Less\nasyncio is a Python library that allows you to execute some tasks in a seemingly concurrent2 manner. It is commonly used in web-servers and database connections. It is also useful for speeding up IO-bound tasks, like services that require making many requests or do lots of waiting for external APIs3.\nThe essence of asyncio is that it allows the program to continue executing other instructions while waiting for specific processes to finish (e.g., a request to an API). In this tutorial, you will see how to use asyncio for accelerating a program that makes multiple requests to an API."
  },
  {
    "objectID": "posts/fast-and-async-in-python-accelerate-your-requests-using-asyncio.html#sequential-vs.-asynchronous",
    "href": "posts/fast-and-async-in-python-accelerate-your-requests-using-asyncio.html#sequential-vs.-asynchronous",
    "title": "Fast & Asynchronous In Python: Accelerate Your Requests Using asyncio",
    "section": "Sequential vs.¬†Asynchronous",
    "text": "Sequential vs.¬†Asynchronous\nSo let‚Äôs get down to business. To get the most out of this tutorial, try running the code yourself. These code snippets have been tested with Python 3.8.3. You can try running them with a more recent version, but they might require some small changes.\nBefore running the code, you need to install the required libraries: requests, and aiohttp. Then, you can run the code snippets below in a Jupyter Notebook. If you‚Äôd like to run the snippets using a Python script, you‚Äôll need to do some small changes to get it working.\nYou‚Äôll build a sequential and an asynchronous version of a small program and compare their results and structure. Both programs will do the same:\n\nRead a list of ISBNs (international identifier of books)\nRequest the books‚Äô metadata to the Google Books API\nParse the results from the requests\nPrint the results to the screen.\n\nThe algorithm would look something like the diagram below.\n\nDiagram of algorithm\nThere‚Äôs two possible approaches for building this program. First, Option A, which executes the requests sequentially. Or, Option B, which uses asyncio to run requests asynchronously.\n\nOption A: Sequential Algorithm\nA sequential version of that algorithm could look as follows:\nimport os\nimport requests\nfrom requests.exceptions import HTTPError\n\n\nGOOGLE_BOOKS_URL = \"https://www.googleapis.com/books/v1/volumes?q=isbn:\"\nLIST_ISBN = [\n    '9780002005883',\n    '9780002238304',\n    '9780002261982',\n    '9780006163831',\n    '9780006178736',\n    '9780006280897',\n    '9780006280934',\n    '9780006353287',\n    '9780006380832',\n    '9780006470229',\n]\n\n\ndef extract_fields_from_response(item):\n    \"\"\"Extract fields from API's response\"\"\"\n    volume_info = item.get(\"volumeInfo\", {})\n    title = volume_info.get(\"title\", None)\n    subtitle = volume_info.get(\"subtitle\", None)\n    description = volume_info.get(\"description\", None)\n    published_date = volume_info.get(\"publishedDate\", None)\n    return (\n        title,\n        subtitle,\n        description,\n        published_date,\n    )\n\n\ndef get_book_details_seq(isbn, session):\n    \"\"\"Get book details using Google Books API (sequentially)\"\"\"\n    url = GOOGLE_BOOKS_URL + isbn\n    response = None\n    try:\n        response = session.get(url)\n        response.raise_for_status()\n        print(f\"Response status ({url}): {response.status_code}\")\n    except HTTPError as http_err:\n        print(f\"HTTP error occurred: {http_err}\")\n    except Exception as err:\n        print(f\"An error ocurred: {err}\")\n    response_json = response.json()\n    items = response_json.get(\"items\", [{}])[0]\n    return items\n\n\nwith requests.Session() as session:\n    for isbn in LIST_ISBN:\n        try:\n            response = get_book_details_seq(isbn, session)\n            parsed_response = extract_fields_from_response(response)\n            print(f\"Response: {json.dumps(parsed_response, indent=2)}\")\n        except Exception as err:\n            print(f\"Exception occured: {err}\")\n            pass\nSequential version of algorithm\nNow, let‚Äôs breakdown the code to understand what‚Äôs going on.\nAs usual, you start by importing the required libraries. Then, you define two variables:\n\nGOOGLE_BOOKS_URL for specifying the URL of the Google‚Äôs API we‚Äôll use for the requests. This is how a request to the Google Books API looks like: https://www.googleapis.com/books/v1/volumes?q=isbn:9780002005883\nLIST_ISBN, which is a sample list of ISBNs for testing the program.\n\nimport os\nimport requests\nfrom requests.exceptions import HTTPError\n\nGOOGLE_BOOKS_URL = \"https://www.googleapis.com/books/v1/volumes?q=isbn:\"\nLIST_ISBN = [\n    '9780002005883',\n    '9780002238304',\n    '9780002261982',\n    '9780006163831',\n    '9780006178736',\n    '9780006280897',\n    '9780006280934',\n    '9780006353287',\n    '9780006380832',\n    '9780006470229',\n]\nNext, you define the extract_fields_from_responsefunction. This function takes as input the response from the API and extracts the fields we‚Äôre interested in.\ndef extract_fields_from_response(response):\n    \"\"\"Extract fields from API's response\"\"\"\n    item = response.get(\"items\", [{}])[0]\n    volume_info = item.get(\"volumeInfo\", {})\n    title = volume_info.get(\"title\", None)\n    subtitle = volume_info.get(\"subtitle\", None)\n    description = volume_info.get(\"description\", None)\n    published_date = volume_info.get(\"publishedDate\", None)\n    return (\n        title,\n        subtitle,\n        description,\n        published_date,\n    )\nFunction for parsing response from the Google Books API\nThe parsing process in extract_fields_from_response is based on the response‚Äôs structure from the Google Books API, which looks as follows:\n{\n \"kind\": \"books#volumes\",\n \"totalItems\": 1,\n \"items\": [\n  {\n   \"kind\": \"books#volume\",\n   \"id\": \"3Mx4QgAACAAJ\",\n   \"etag\": \"FWJF/JY16xg\",\n   \"selfLink\": \"https://www.googleapis.com/books/v1/volumes/3Mx4QgAACAAJ\",\n   \"volumeInfo\": {\n    \"title\": \"Mapping the Big Picture\",\n    \"subtitle\": \"Integrating Curriculum and Assessment, K-12\",\n    ...\nSample response from the Google Books API\nFinally, take a look at the most relevant parts of the program: how you make requests to the Google Books API.\ndef get_book_details_seq(isbn, session):\n    \"\"\"Get book details using Google Books API (sequentially)\"\"\"\n    url = GOOGLE_BOOKS_URL + isbn\n    response = None\n    try:\n        response = session.get(url)\n        response.raise_for_status()\n        print(f\"Response status ({url}): {response.status_code}\")\n    except HTTPError as http_err:\n        print(f\"HTTP error occurred: {http_err}\")\n    except Exception as err:\n        print(f\"An error ocurred: {err}\")\n    response_json = response.json()\n    items = response_json.get(\"items\", [{}])[0]\n    return items\n\n\nwith requests.Session() as session:\n    for isbn in LIST_ISBN:\n        try:\n            response = get_book_details_seq(isbn, session)\n            parsed_response = extract_fields_from_response(response)\n            print(f\"Response: {json.dumps(parsed_response, indent=2)}\")\n        except Exception as err:\n            print(f\"Exception occured: {err}\")\n            pass\nHow requests are executed to the Google Books API\nThere are two major pieces here:\n\nget_book_details_seq, which is the function that executes the requests. It takes as input an ISBN and a session object4 and returns the response from the API as a JSON structure. It also handles possible errors, like providing a wrong URL or going over your daily quota of requests.\nThe code block under with requests.Session() as session, is where the full pipeline is orchestrated. It iterates through the list of ISBNs, gets the books‚Äô details, parses them, and finally prints the details to the screen.\n\nFor me, executing this process varies from 4 to 6 seconds. If you only need to do this a couple of times, you will not find much benefit from using asyncio. However, if instead of 10 requests, you need to do 10,000, having some concurrency in your program pays out. In the next section, you‚Äôll see how to make this algorithm faster using asyncio.\n\n\nOption B: Asynchronous Algorithm\nAn asynchronous version of the same algorithm may look something as follows:\nimport aiohttp\nimport asyncio\nimport os\n\nfrom aiohttp import ClientSession\n\n\nGOOGLE_BOOKS_URL = \"https://www.googleapis.com/books/v1/volumes?q=isbn:\"\nLIST_ISBN = [\n    '9780002005883',\n    '9780002238304',\n    '9780002261982',\n    '9780006163831',\n    '9780006178736',\n    '9780006280897',\n    '9780006280934',\n    '9780006353287',\n    '9780006380832',\n    '9780006470229',\n]\n\n\ndef extract_fields_from_response(response):\n    \"\"\"Extract fields from API's response\"\"\"\n    item = response.get(\"items\", [{}])[0]\n    volume_info = item.get(\"volumeInfo\", {})\n    title = volume_info.get(\"title\", None)\n    subtitle = volume_info.get(\"subtitle\", None)\n    description = volume_info.get(\"description\", None)\n    published_date = volume_info.get(\"publishedDate\", None)\n    return (\n        title,\n        subtitle,\n        description,\n        published_date,\n    )\n\n\nasync def get_book_details_async(isbn, session):\n    \"\"\"Get book details using Google Books API (asynchronously)\"\"\"\n    url = GOOGLE_BOOKS_URL + isbn\n    try:\n        response = await session.request(method='GET', url=url)\n        response.raise_for_status()\n        print(f\"Response status ({url}): {response.status}\")\n    except HTTPError as http_err:\n        print(f\"HTTP error occurred: {http_err}\")\n    except Exception as err:\n        print(f\"An error ocurred: {err}\")\n    response_json = await response.json()\n    return response_json\n\n\nasync def run_program(isbn, session):\n    \"\"\"Wrapper for running program in an asynchronous manner\"\"\"\n    try:\n        response = await get_book_details_async(isbn, session)\n        parsed_response = extract_fields_from_response(response)\n        print(f\"Response: {json.dumps(parsed_response, indent=2)}\")\n    except Exception as err:\n        print(f\"Exception occured: {err}\")\n        pass\n\nasync with ClientSession() as session:\n    await asyncio.gather(*[run_program(isbn, session) for isbn in LIST_ISBN])\nAsynchronous version of the program using asyncio and aiohttp\nFirst, check the get_book_details_async function. An async keyword prepends it. This keyword tells Python that your function is a coroutine. Then, in the function‚Äôs body, there are two await keywords. These tell that coroutine to suspend execution and give back control to the event loop, while the operation the couroutine is awaiting finishes.\nA coroutine is a type of generator function in Python that, instead of producing values, consumes values5. The interesting thing about it is that its execution pauses while waiting for new data being sent to it. In our case, this allows the execution of other parts of the program to continue in a seemingly concurrent manner.\nIn this case, the execution of get_book_details_async is suspended while the request is being performed: await session.request(method='GET', url=url). It is suspended again, while the request response is being parsed into a JSON structure: await response.json().\nNext, we have the run_program coroutine. This one is simply a wrapper around the pipeline of getting a response from the API, parsing it, and printing the results in the screen. It awaits the execution of the get_book_details_async coroutine.\nFinally, we have the code block under async with ClientSession() as session:. Using the asyncio.gather syntax, we tell the program to schedule all the tasks based on the list of coroutines we provided. This is what allows us to execute tasks concurrently.\nFor me, running this process takes around 800-1000 milliseconds.\n\n\nResults\nComparing both versions, we see that the asynchronous one is around 4 to 7.5 times faster than the sequential version. If we increase the number of requests, you‚Äôll likely get an even higher speedup. Besides, the version using asyncio is almost as simple as the sequential version. That makes asyncio an excellent option for the kind of task we reviewed in the tutorial."
  },
  {
    "objectID": "posts/fast-and-async-in-python-accelerate-your-requests-using-asyncio.html#additional-recommendations",
    "href": "posts/fast-and-async-in-python-accelerate-your-requests-using-asyncio.html#additional-recommendations",
    "title": "Fast & Asynchronous In Python: Accelerate Your Requests Using asyncio",
    "section": "Additional recommendations",
    "text": "Additional recommendations\nHere are some tips I gathered while working with asyncio:\n\nasyncio keeps changing all the time, so be wary of old Stack Overflow answers. Many of them are not up to date with the current best practices\nExternal APIs will not allow you to run unlimited concurrent requests. To overcome that, take a look at asyncio‚ÄôsSemaphore. It will enable you to limit the concurrency of your application.\nNot all programs can be speedup with asyncio. Research the type of issue you are facing before doing any substantial modification of code. Other alternatives might work for you (e.g., threading, multiprocessing)\nI made a complete version of the program we went through in this tutorial for getting the metadata of almost 7 thousand books. Here‚Äôs a link to it: Google Books Crawler."
  },
  {
    "objectID": "posts/fast-and-async-in-python-accelerate-your-requests-using-asyncio.html#notes-and-references",
    "href": "posts/fast-and-async-in-python-accelerate-your-requests-using-asyncio.html#notes-and-references",
    "title": "Fast & Asynchronous In Python: Accelerate Your Requests Using asyncio",
    "section": "Notes and References",
    "text": "Notes and References\n[1] Real Python has a two of amazing articles introducing asyncio: Async IO in Python and Speed Up Your Python Program With Concurrency\n[2] It is not strictly concurrent execution. But in practical terms, it looks like it is.\n[3] S. Buczy≈Ñski, What Is the use case of coroutines and asyncio in Python 3.6? (2017)\n[4] The session object is a functionality from the requests library that allows you to persist certain parameters across sessions. This usually results in requests with lower latency. Read more here.\n[5] D. Beasly, A Curious Course on Couroutines and Concurrency (2009)\n[6] Cover image by Marc-Olivier Jodoin"
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "",
    "text": "I‚Äôm teaching a course about the essential tools of Data Science at Nuclio Digital School. Among other topics, I planned to go through the most popular data visualization libraries in Python: pandas, matplotlib, seaborn, and plotly.express.\nWhile preparing the class materials, I thought, is there any site that shows you how to make frequently used graphs with all these libraries?\nIt turns out there isn‚Äôt. Most of what I found just scratched the surface, focused on one or two graphs, or didn‚Äôt show you how to make graphs starting from a DataFrame.\nI thought this was a great opportunity to write something helpful. So I came up with this article and an interactive cookbook you can use to learn about pandas, matplotlib, seaborn, and plotly.express. You can use them as references when looking for ways to visualize your data.\nLet‚Äôs get to it!"
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-the-most-of-this-tutorial",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-the-most-of-this-tutorial",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "How to Make the Most of this Tutorial",
    "text": "How to Make the Most of this Tutorial\nThere‚Äôs only one mandatory section in this tutorial: Initial setting and reading the data. It‚Äôll show you‚Äôve how to set your local environment, install the required libraries, and read the data.\nIf you‚Äôre in a hurry, start with that section and then go to the type of graph you‚Äôd like to make. Otherwise, you can browse through all the sections.\nI didn‚Äôt want to add fluff, so I only added comments to the parts I thought were hard to understand. Most code snippets in the tutorial are short and use parameters with simple names like x, y, or color."
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#what-are-the-pros-and-cons-of-each-library",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#what-are-the-pros-and-cons-of-each-library",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "What Are the Pros and Cons of Each Library",
    "text": "What Are the Pros and Cons of Each Library\nIn this tutorial, I compared four libraries: pandas, matplotlib, seaborn, and plotly.express. These are mature and popular Python libraries that will cover most of your data visualization needs.\nIf you‚Äôd like to know which one will work better for you, here‚Äôs a brief description of each, with their strong and weak points:\n\npandas\nYou can use the [plot](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html) method of pandas to create graphs. It‚Äôs a wrapper of matplotlib.pyplot. It‚Äôs especially useful if you‚Äôre working with pandas Series or DataFrames.\n\nPros\n\nIt‚Äôs easy to use.\nIt supports DataFrames.\nIt‚Äôs popular, so there‚Äôs lots of information available.\n\n\n\nCons\n\nIf you want to customize graphs, you‚Äôll need to be familiar with matplotlib.\n\n\n\n\nmatplotlib\nIt‚Äôs one of the oldest and most popular data visualization library in the Python ecosystem. It provides you with many options to generate and customize graphs, but this control comes at a cost. It‚Äôs harder to use than the alternatives.\nYou can make graphs in matplotlib using a state-based interface (like MATLAB) and an object-oriented one. While this is useful for developers with a MATLAB or R background, it‚Äôs often confusing for newcomers looking for help.\n\nPros\n\nIt gives you complete control to customize graphs.\nIf you come from a MATLAB or R background, then you‚Äôll find the state-based interface easy to grasp.\nIt‚Äôs popular, so there‚Äôs lots of information available.\n\n\n\nCons\n\nIt‚Äôs harder to use than other popular alternatives.\nIts two interfaces can generate confusion when solving issues.\n\n\n\n\nseaborn\nIt‚Äôs a wrapper on top of matplotlib that makes it easier to create graphs. seaborn provides you with reasonable defaults for most charts, statistical utilities, and an easy way to use pandas DataFrames.\n\nPros\n\nIt provides good defaults and useful statistical tools for most graphs.\nIt uses DataFrames.\nIt‚Äôs popular, so there‚Äôs lots of information available.\n\n\n\nCons\n\nFor basic charts, it doesn‚Äôt provide lots of benefits compared to pandas.\nIt doesn‚Äôt include popular types of graphs like stacked areas, or pie/donut charts.\n\n\n\n\nplotly.express\nIt‚Äôs a high-level interface for building graphs. It uses plotly in the background and provides the user with an easy and consistent way to create charts. It‚Äôs newer than the rest but offers many types of charts and options to customize them.\n\nPros\n\nIt‚Äôs easy to use.\nIt uses DataFrames.\nIt generates interactive graphs by default.\n\n\n\nCons\n\nIt‚Äôs one of the many available interfaces within the Plotly ecosystem. Beginners can get confused when trying to solve issues.\nIt‚Äôs more likely to change its interface, compared to the other libraries.\n\nThere you go. You‚Äôve gone through the upsides and downsides of each library. Now, remember what Uncle Ben said to Peter: with great power, comes great responsibility. The next time you need to make a graph, choose wisely."
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#local-set-up-and-data",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#local-set-up-and-data",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "Local Set Up and Data",
    "text": "Local Set Up and Data\nIn this section, you‚Äôll set up your local environment to start working. You‚Äôll create a virtual environment, install and import the required libraries, and inspect the data.\n\nSet Up a Virtual Environment\nIf you‚Äôre working on a Python project, then using a virtual environment will save you lots of headaches. So, you‚Äôll start by creating one and installing the required libraries.\nIf you‚Äôre using venv, then run these commands:\npython3 -m venv .dataviz\nsource .dataviz/bin/activate\npython3 -m pip install pandas==1.2.4 numpy==1.2.0 matplotlib==3.4.2 plotly==4.14.3 seaborn==0.11.1 notebook==6.4.0\njupyter notebook\nIf you‚Äôre using conda, then this is how you do it:\nconda create --name .dataviz\nconda activate .dataviz\nconda install pandas==1.2.4 numpy==1.19.2 matplotlib==3.4.2 plotly==4.14.3 seaborn==0.11.1 notebook==6.4.0 -y\njupyter notebook\nThat‚Äôs it! These commands will:\n\nCreate a virtual environment called .dataviz\nActivate the virtual environment\nInstall the required packages with the specified versions\n\nYou don‚Äôt need to install the rest if you only want to use one of the data visualization libraries. For example, if you want to use plotly.express, you can remove matplotlib and seaborn from the command.\n\n\nStart Jupyter Notebook and Import Libraries\nOpen Jupyter Notebook. Create a new notebook by clicking on New &gt; Python3 notebook in the menu. By now, you should have an empty Jupyter notebook in front of you. Let‚Äôs get to the fun part!\nFirst, you‚Äôll need to import the required libraries. Create a new cell in your notebook and paste the following code to import the required libraries:\n# All\nimport pandas as pd\nimport numpy as np\n\n# matplotlib\nimport matplotlib.ticker as mtick\nimport matplotlib.pyplot as plt\n\n# plotly\nimport plotly.io as pio\nimport plotly.express as px\n\n# seaborn\nimport seaborn as sns\n\n# Set templates\npio.templates.default = \"seaborn\"\nplt.style.use(\"seaborn\")\nOn lines 1 to 14, you‚Äôll import the required libraries and set up the themes for matplotlib and plotly. Each library provides you with some useful functionality:\n\npandas helps you read the data\nmatplotlib.pyplot, plotly.express, and seaborn help you make the charts\nmatplotlib.ticker makes it easy to customize the tickers on your axes in your matplotlib graphs\nplotly.io allows you to define a specific theme for your plotly graphs\n\nOn lines 17 and 18, you define the themes for plotly.express and matplotlib. In this case, you set them to use the seaborn theme. This will make the graphs from all the libraries look similar.\n\n\nReview the Data\nThroughout this tutorial, you‚Äôll use a dataset with stock market data for 29 companies compiled by ichardddddd. It has the following columns:\n\nDate: Date corresponding to the observed value\nOpen: Price (in USD) at market open at the specified date\nHigh: Highest price (in USD) reached during the corresponding date\nLow: Lowest price (in USD) reached during the corresponding date\nClose: Price (in USD) at market close at the specified date\nVolume: Number of shares traded\nName: Stock symbol of the company\n\nTake a look at the data by reviewing a sample of rows:\nurl = \"https://raw.githubusercontent.com/szrlee/Stock-Time-Series-Analysis/master/data/all_stocks_2006-01-01_to_2018-01-01.csv\"\ndf = pd.read_csv(url)\ndf.sample(5)\nThis code will read the data from the URL you specified and generate a sample of 5 rows from the data. Take a look at the resulting sample:\n\n\n\n\nDate\nOpen\nHigh\nLow\nClose\nVolume\nName\n\n\n\n\n53053\n2012-10-24\n88.45\n88.45\n87.09\n87.28\n6498524\nMCD\n\n\n9078\n2006-01-31\n69.00\n69.05\n68.31\n68.31\n4095000\nBA\n\n\n62012\n2012-06-05\n26.08\n26.44\n26.00\n26.38\n9183184\nNKE\n\n\n81843\n2007-03-27\n47.57\n47.80\n47.03\n47.49\n12950422\nWMT\n\n\n49556\n2010-12-03\n39.07\n39.67\n38.70\n39.61\n30070142\nJPM\n\n\n\nThis is a long dataset (in regards to the stock names). In some graphs, you‚Äôll have to reshape it into a wide dataset.\nThat‚Äôs it! You‚Äôre ready for the next sections."
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-line-plot",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-line-plot",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "How to Make a Line Plot",
    "text": "How to Make a Line Plot\nA line plot shows how a variable changes using points connected by line segments. It consists of two axes, a horizontal one, where you represent continuous and equally-spaced levels of a variable, and a vertical axis, with numerical values of a given metric.\nIn this case, you‚Äôll plot the closing price of four stocks over time.\nYou‚Äôll start by preparing the data you‚Äôll use in the graphs. Copy the following code in a new cell in your notebook:\nurl = \"https://raw.githubusercontent.com/szrlee/Stock-Time-Series-Analysis/master/data/all_stocks_2006-01-01_to_2018-01-01.csv\"\ndf = pd.read_csv(url)\n\ndf = df.loc[df.Name.isin([\"AAPL\", \"JPM\", \"GOOGL\", \"AMZN\"]), [\"Date\", \"Name\", \"Close\"]]\ndf[\"Date\"] = pd.to_datetime(df.Date)\ndf.rename(columns={\"Close\": \"Closing Price\"}, inplace=True)\nThis code will prepare the data you‚Äôll use in the plots. It works as follows:\n\nLines 1-2: you read the data from an URL.\nLine 4: you filter the DataFrame object to include only the stocks that you want to plot.\nLine 5: you adjust the type of the Date column. Using datetime will make most plotting libraries set the tickers in a better way.\nLine 6: you rename the Close column.\n\nNext, you‚Äôll make a line plot using this dataset.\n\nLine Plot Using pandas\nThis is the code to make a line plot using pandas:\ndf_wide = df.pivot(index=\"Date\", columns=\"Name\", values=\"Closing Price\")\ndf_wide.plot(\n    title=\"Stock prices (2006 - 2017)\", ylabel=\"Closing Price\", figsize=(12, 6), rot=0\n)\nThis code generates a line plot. There are two important details that you should take into account:\n\nLine 1: you use the pivot method to go from a long dataset to a wide one. To plot multiple series in pandas you need a wide dataset.\nLines 2-3: you create the plot. You set the size of the figure by using figsize and keep the x-axis ticks in a horizontal position by setting rot=0.\n\nHere‚Äôs the resulting graph:\n\n\n\nLine Plot Using matplotlib\nHere‚Äôs how you create a line plot with matplotlib:\nfig, ax = plt.subplots(figsize=(12, 6))\n\nfor l, g in df.groupby(\"Name\"):\n    ax.plot(g[\"Date\"], g[\"Closing Price\"], label=l)\n\nax.set_title(\"Stock prices (2006 - 2017)\")\nax.set_ylabel(\"Closing Price\")\nax.set_xlabel(\"Date\")\nax.legend(title=\"Name\")\nThis code creates a line plot. Here are some relevant highlights:\n\nLine 1: you create a figure and axes objects, and set the size of the plot. The figure is a container for the axes. You draw the plot in the axes.\nLines 3-4: you iterate over the groups in the DataFrame. Each group is a tuple of the name of the stock and a series with its closing prices. You plot the closing prices of each stock on a separate series.\nLines 6-9: you set the labels, title, and show the legend of the plot.\n\nThis is the resulting graph:\n\n\n\nLine Plot Using seaborn\nHere‚Äôs the code to create a line plot with seaborn:\nfig, ax = plt.subplots(figsize=(12, 6))\nsns.lineplot(data=df, x=\"Date\", y=\"Closing Price\", hue=\"Name\", ax=ax)\nax.set_title(\"Stock Prices (2006 - 2017)\")\nThis code creates a line plot using seaborn. Here‚Äôs what it does:\n\nLine 1: You start by creating a figure and axes objects and setting the size of the plot. T\nLines 2-3: you create the graph and set its title.\n\nHere‚Äôs the resulting graph:\n\n\n\nLine Plot Using plotly.express\nThis is how you use plotly.express to create a line plot:\nfig = px.line(\n    df, x=\"Date\", y=\"Closing Price\", color=\"Name\", title=\"Stock Prices (2006 - 2017)\"\n)\nfig.show()\nHere‚Äôs the resulting graph:\n\n20082010201220142016020040060080010001200NameAAPLJPMGOOGLAMZNStock Prices (2006 - 2017)DateClosing Price"
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-grouped-bar-chart",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-grouped-bar-chart",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "How to Make a Grouped Bar Chart",
    "text": "How to Make a Grouped Bar Chart\nA grouped bar chart is like a regular bar chart, but plots values for two categories instead of one. You can use grouped bars when you want to compare how a second category changes within each level of the first.\nIn this case, you‚Äôll plot the maximum opening and closing price per year for Apple‚Äôs stock (AAPL) between 2014 and 2017.\nYou‚Äôll start by preparing the data for the graphs. Copy the following code in a new cell in your notebook:\nurl = \"https://raw.githubusercontent.com/szrlee/Stock-Time-Series-Analysis/master/data/all_stocks_2006-01-01_to_2018-01-01.csv\"\ndf = pd.read_csv(url)\n\ndf = df.loc[df.Name == \"AAPL\", [\"Date\", \"Open\", \"Close\"]]\ndf[\"Year\"] = pd.to_datetime(df.Date).dt.year\ndf = df.query(\"Year &gt;= 2014\").groupby(\"Year\").max().reset_index(drop=False)\nThis code will prepare the data you‚Äôll use in the plots. It works as follows:\n\nLines 1-2: you read the data from an URL.\nLine 4: you keep the information of AAPL and the columns Date, Open, and Close.\nLine 5: youcreate a new column with the year of each data point.\nLine 6: you remove the observations from before 2014, and find the max value per year of each column in the DataFrame.\n\nNext, you‚Äôll see how to make a grouped bars plot using this dataset.\n\nGrouped Bar Chart Using ¬†pandas\nHere‚Äôs the code to make a grouped bar plot with pandas:\ndf.plot.bar(\n    x=\"Year\",\n    y=[\"Open\", \"Close\"],\n    rot=0,\n    figsize=(12, 6),\n    ylabel=\"Price in USD\",\n    title=\"Max Opening and Closing Prices per Year for AAPL\",\n)\nThis is how you make a grouped bar plot. There‚Äôs one detail worth mentioning: in the plot method, you set the size of the figure using figsize and keep the x-axis ticks in a horizontal position by setting rot=0.\nHere‚Äôs the resulting graph:\n\n\n\nGrouped Bar Chart Using matplotlib\nHere‚Äôs the code to make a grouped bar plot using matplotlib:\nfig, ax = plt.subplots(figsize=(12, 6))\n\nx = np.arange(len(df.Year))\nwidth = 0.25\n\nax.bar(x - width / 2, df.Open, width, label=\"Open\")\nax.bar(x + width / 2, df.Close, width, label=\"Close\")\n\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Price in USD\")\nax.set_title(\"Max Opening and Closing Prices per Year for AAPL\")\n\nax.set_xticks(x)\nax.set_xticklabels(df.Year)\n\nax.legend()\nThis code will create a grouped bar plot using matplotlib. Here‚Äôs how it works:\n\nLine 1: you create a figure and axes objects, and set the size of the plot. The figure is a container for the axes. You draw the plot in the axes.\nLines 3-4: you create x to set the position of the ticks of the x-axis. In addition, you set width to 0.25, to define the width of the bars.\nLines 6-7: you create the bars at each tick in the x axis, taking in consideration the width of the bars.\nLines 9-11: you set the labels and title of the plot.\nLines 13-14: you set the locations and labels of the x-axis ticks.\nLine 16: you create a legend for the chart.\n\nHere‚Äôs is the resulting graph:\n\n\n\nGrouped Bar Chart Using seaborn\nHere‚Äôs the code to make a grouped bar plot using seaborn:\ndf_long = df.melt(\n    id_vars=\"Year\",\n    value_vars=[\"Open\", \"Close\"],\n    var_name=\"Category\",\n    value_name=\"Price\",\n)\n\nfig, ax = plt.subplots(figsize=(12, 6))\nsns.barplot(data=df_long, x=\"Year\", y=\"Price\", hue=\"Category\", ax=ax)\n\nax.set_title(\"Max Opening and Closing Prices per Year for AAPL\")\nax.legend(title=None)\nThis is how you make a grouped bars plot using seaborn. There are two details worth mentioning:\n\nLines 1-6: you apply the melt method to transform the original dataset into a long one (in regards to the closing and opening prices). seaborn doesn‚Äôt work well with wide datasets.\nLine 7: you start by creating a figure and axes objects and setting the size of the plot. You‚Äôll pass the axes to the ax parameter of barplot.\n\nThis is the resulting graph:\n\n\n\nGrouped Bar Chart Using plotly.express\nHere‚Äôs the code to make a grouped bar plot using plotly.express:\nfig = px.bar(\n    df,\n    x=\"Year\",\n    y=[\"Open\", \"Close\"],\n    title=\"Max Opening and Closing Prices per Year for AAPL\",\n    barmode=\"group\",\n    labels={\"value\": \"Price in USD\"},\n)\nfig.show()\nThis is how you make a grouped bars plot using plotly.express. There are a few things worth highlighting:\n\nLine 4: to plot the opening and closing prices, you specify both in the y parameter of px.bar. plotly.express works well with wide datasets, so you don‚Äôt need to reshape the DataFrame.\nLine 6: you set barmode=group in px.bar so that bars don‚Äôt get stacked on top of each other.\n\nThis is the resulting graph:\n\n2014201520162017050100150variableOpenCloseMax Opening and Closing Prices per Year for AAPLYearPrice in USD"
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-stacked-bar-chart",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-stacked-bar-chart",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "How to Make a Stacked Bar Chart",
    "text": "How to Make a Stacked Bar Chart\nA stacked bar chart is like a normal bar chart, except a normal bar chart shows the total of all the bars, and a stacked bar chart shows the total of all the bars, plus how each part of the bar is made up.\nIn this case, you‚Äôll plot the total volume traded per year for a sample of stocks: AAPL, JPM, GOOGL, AMZN,and IBM.\nFirst, you‚Äôll prepare the data for the graphs. Copy this code in a cell in your notebook:\nurl = \"https://raw.githubusercontent.com/szrlee/Stock-Time-Series-Analysis/master/data/all_stocks_2006-01-01_to_2018-01-01.csv\"\ndf = pd.read_csv(url)\n\nstocks_filter = [\"AAPL\", \"JPM\", \"GOOGL\", \"AMZN\", \"IBM\"]\ndf = df[df.Name.isin(stocks_filter)]\ndf[\"Date\"] = pd.to_datetime(df.Date)\ndf[\"Year\"] = pd.to_datetime(df.Date).dt.year\ndf[\"Volume\"] = df[\"Volume\"] / 1e9\n\ndf = (\n    df[[\"Year\", \"Volume\", \"Name\"]]\n    .query(\"Year &gt;= 2012\")\n    .groupby([\"Year\", \"Name\"])\n    .sum()\n    .reset_index(drop=False)\n)\nThis code will help you prepare the data for the plots. It works as follows:\n\nLines 1-2: you read the data from an URL.\nLines 4-5: you keep the rows for the AAPL, JPM, GOOGL, AMZN, IBM.\nLine 6: youcreate a new column with the year of each data point.\nLine 9: you divide the total volume by one billion to make it more tractable.\nLine 6: you sum the volume per stock and year to get the total traded per year for each stock symbol.\n\nNext, you‚Äôll see how to make a stacked bar plot using this dataset.\n\nStacked Bar Chart Using pandas\nHere‚Äôs the code to make a **stacked bar** plot using pandas:\ndf_wide = df.pivot(index=\"Year\", columns=\"Name\", values=\"Volume\")\ndf_wide.plot.bar(\n    rot=0,\n    figsize=(12, 6),\n    ylabel=\"Volume (billions of shares)\",\n    title=\"Trading volume per year for selected shares\",\n    stacked=True,\n)\nThere are a few details worth mentioning:\n\nLine 1: you use the pivot method to go from a long dataset to a wide one. To plot multiple series in pandas you need a wide dataset.\nLines 3-4: you set the size of the figure by using figsize and keep the x-axis ticks horizontally by setting rot=0.\nLine 7: you set stacked=True, so that bars get stacked instead of grouped together.\n\nThis is the resulting graph:\n\n\n\nStacked Bar Chart Using matplotlib\nHere‚Äôs the code to make a **stacked bar** plot using matplotlib:\nfig, ax = plt.subplots(figsize=(12, 6))\n\nbottom = np.zeros(df.Year.nunique())\nfor i, g in df.groupby(\"Name\"):\n    ax.bar(g[\"Year\"], g[\"Volume\"], bottom=bottom, label=i, width=0.5)\n    bottom += g[\"Volume\"].values\n\nax.set_title(\"Trading volume per year for selected shares\")\nax.set_ylabel(\"Volume (billions of shares)\")\nax.set_xlabel(\"Year\")\n\nax.legend()\nThis code will create a stacked bar plot using matplotlib. Here‚Äôs how it works:\n\nLine 1: you create a figure and axes objects, and set the size of the plot. The figure is a container for the axes. You draw the plot in the axes.\nLines 3: you initialize an array filled with zeroes of the same size of the number of ticks in the x-axis.\nLines 4-6: you iterate over groups in the DataFrame. Each group is a tuple of the name of the stock and a series with its volume per date. You add a bar plot of the Volume to the axes. At each iteration, bottom accumulates the total volume. You use it to stack the bars on top of each other.\nLines 8-12: you set the labels, title, and create a legend for the plot.\n\nThis is the resulting graph:\n\n\n\nStacked Bar Chart Using seaborn\nHere‚Äôs how you make a **stacked bar** plot using seaborn:\nfig, ax = plt.subplots(figsize=(12, 6))\n\nax = sns.histplot(\n    data=df,\n    x=\"Year\",\n    hue=\"Name\",\n    weights=\"Volume\",\n    multiple=\"stack\",\n    shrink=0.5,\n    discrete=True,\n    hue_order=df.groupby(\"Name\").Volume.sum().sort_values().index,\n)\n\nax.set_title(\"Trading volume per year for selected shares\")\nax.set_ylabel(\"Volume (billions of shares)\")\n\nlegend = ax.get_legend()\nlegend.set_bbox_to_anchor((1, 1))\nThis code will make a stacked bars plot in seaborn. There are some details worth mentioning:\n\nLine 1: you create a figure and axes objects, and set the size of the plot. The figure is a container for the axes. You draw the plot in the axes.\nLine 7: you set the size of the slices in the stacked bars by using the weights argument. In this case, you set the size of slices to the total volume of each stock.\nLine 8-10: you allow stacking the bars by setting multiple=\"stack\". In addition, you reduce the width of the bars usingshrink=0.5, and center the bars in the ticks of the x-axis using discrete=True.\nLine 11: you set the order for the stacking of the bars. In this case, you draw the bars from biggest to smallest, starting from the bottom.\n\nThis is the resulting graph:\n\n\n\nStacked Bar Using plotly.express\nHere‚Äôs how you make a stacked bars plot using plotly.express:\nfig = px.bar(\n    df,\n    x=\"Year\",\n    y=\"Volume\",\n    color=\"Name\",\n    title=\"Trading volume per year for selected shares\",\n    barmode=\"stack\",\n    labels={\"Volume\": \"Volume (billions of shares)\"},\n)\nfig.show()\nAs you can see, making a stacked bars plot using plotly.express is straightforward. Just remember to set barmode=\"stack\".\nThis is the resulting graph:\n\n201220132014201520162017010203040NameAAPLAMZNGOOGLIBMJPMTrading volume per year for selected sharesYearVolume (billions of shares)"
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-stacked-area-chart",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-stacked-area-chart",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "How to Make a Stacked Area Chart",
    "text": "How to Make a Stacked Area Chart\nThe stacked area chart is a non-discrete version of a stacked bar chart. It‚Äôs useful when you want to visualize changes in the total value of a variable and its composition, in the same graph. Though, it‚Äôs often used to visualize only changes of composition over time.\nIn this case, you‚Äôll plot the changes in the composition of the daily volume traded for a sample of stocks: AAPL, JPM, GOOGL, AMZN,and IBM.\nStart by preparing the data for the graphs using this code:\nurl = \"https://raw.githubusercontent.com/szrlee/Stock-Time-Series-Analysis/master/data/all_stocks_2006-01-01_to_2018-01-01.csv\"\ndf = pd.read_csv(url)\n\nstocks = [\"AAPL\", \"AMZN\", \"GOOGL\", \"IBM\", \"JPM\"]\ndf = df.loc[df.Name.isin(stocks), [\"Date\", \"Name\", \"Volume\"]]\ndf[\"Date\"] = pd.to_datetime(df.Date)\ndf = df[df.Date.dt.year &gt;= 2017]\ndf[\"Volume Perc\"] = df[\"Volume\"] / df.groupby(\"Date\")[\"Volume\"].transform(\"sum\")\nThis code will help you prepare the data for the plots. It works as follows:\n\nLines 1-2: you read the data from an URL.\nLines 4-5: you keep the rows for the AAPL, JPM, GOOGL, AMZN, IBM.\nLine 6-7: youkeep the data for 2017 onwards.\nLine 6: you calculate the percentage of the total volume traded corresponding to each stock symbol.\n\nNext, you‚Äôll see how to make a stacked area plot using this dataset.\n\nStacked Area Chart Using pandas\nHere‚Äôs how you make a stacked area plot using pandas:\ndf_wide = df.pivot(index=\"Date\", columns=\"Name\", values=\"Volume Perc\")\n\nax = df_wide.plot.area(\n    rot=0,\n    figsize=(12, 6),\n    title=\"Distribution of daily trading volume - 2017\",\n    stacked=True,\n)\nax.legend(bbox_to_anchor=(1, 1), loc=\"upper left\")\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\nThis code will make a stacked area plot using pandas. There are a few details worth mentioning:\n\nLine 1: you use the pivot method to go from a long dataset to a wide one. To plot multiple series in pandas you need a wide dataset.\nLines 4-5: you set the size of the figure by using figsize and keep the x-axis ticks horizontally by setting rot=0.\nLine 7: you set stacked=True to stack the areas.\nLines 9-10: you move the legend to the upper left corner, and set format the y-axis tick labels to use percentages.\n\nThis is the resulting graph:\n\n\n\nStacked Area Chart Using matplotlib\nHere‚Äôs how you make a stacked areas plot using matplotlib:\ndf_wide = df.pivot(index=\"Date\", columns=\"Name\", values=\"Volume Perc\")\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\nax.stackplot(df_wide.index, [df_wide[col].values for col in stocks], labels=stocks)\nax.legend(bbox_to_anchor=(1, 1), loc=\"upper left\")\n\nax.set_title(\"Distribution of daily trading volume - 2017\")\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\n\nLine 1: you use the pivot method to go from a long dataset to a wide one. For this type of chart in matplotlib, is better to use a wide dataset.\nLine 3: you create a figure and axes objects, and set the size of the plot. The figure is a container for the axes. You draw the plot in the axes.\nLine 5: you create the plot by passing it the values for the x-axis, a list of lists for the areas, and the labels for each series.\nLines 4-6: you iterate over groups in the DataFrame. Each group is a tuple of the name of the stock and a series with its volume per date. At each iteration, you add a bar plot of the volume to the axes and sum the volume values to bottom. You use bottom to set the distance between the bars and the x-axis.\nLines 8-12: you set the labels, title, and create a legend for the plot.\n\nThis is the resulting graph:\n\n\n\nStacked Area Chart Using plotly.express\nHere‚Äôs how you make a stacked area plot using plotly.express:\nfig = px.area(\n    df,\n    x=\"Date\",\n    y=\"Volume Perc\",\n    color=\"Name\",\n    title=\"Distribution of daily trading volume - 2017\",\n)\nfig.update_layout(yaxis_tickformat=\"%\")\nfig.show()\nThis is the resulting graph:\n\nMar 2017May 2017Jul 2017Sep 2017Nov 20170%20%40%60%80%100%NameAAPLIBMJPMGOOGLAMZNDistribution of daily trading volume - 2017DateVolume Perc"
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-pie-or-donut-chart",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-pie-or-donut-chart",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "How to Make a Pie or Donut Chart",
    "text": "How to Make a Pie or Donut Chart\nThe pie or donut chart shows the composition of a variable into categories by using radial slices. For example, you could use it to show what percentage of your day you dedicate to sleep, work, and leisure.\nIn this case, you‚Äôll plot the distribution of the total volume traded for a sample of stocks: AAPL, JPM, GOOGL, AMZN,and IBM.\nStart by preparing the data for the graphs using this code:\nurl = \"https://raw.githubusercontent.com/szrlee/Stock-Time-Series-Analysis/master/data/all_stocks_2006-01-01_to_2018-01-01.csv\"\ndf = pd.read_csv(url)\n\nstocks_filter = [\"AAPL\", \"JPM\", \"GOOGL\", \"AMZN\", \"IBM\"]\ndf = df.loc[df.Name.isin(stocks_filter), [\"Name\", \"Volume\"]]\ndf = df.groupby(\"Name\").sum().reset_index()\nThis code will help you prepare the data for the plots. It works as follows:\n\nLines 1-2: you read the data from an URL.\nLines 4-5: you keep the rows for the AAPL, JPM, GOOGL, AMZN, IBM.\nLine 6: you sum the total volume per stock for the whole dataset.\n\nNext, you‚Äôll see how to make a pie or donut plot using this dataset.\n\nPie or Donut Chart Using pandas\nHere‚Äôs the code to make a donut chart using pandas:\ndf.set_index(\"Name\").plot.pie(\n    y=\"Volume\",\n    wedgeprops=dict(width=0.5),\n    figsize=(8, 8),\n    autopct=\"%1.0f%%\",\n    pctdistance=0.75,\n    title=\"Distribution of trading volume for selected stocks (2006 - 2017)\",\n)\nYou can use this code to create a pie or donut chart using pandas. Here‚Äôs how it works:\n\nLine 1: you set Name as the DataFrame index. This is needed if you want to make a pie or donut chart with pandas. Then, you call plot.pie.\nLine 2: you use Volume to calculate the size of the radial slices.\nLine 3-7: you create the ‚Äúhole‚Äù in the pie, set the figure size, define the format and location of the labels, and set the title of the chart.\n\nThis is the resulting graph:\n\n\n\nPie or Donut Chart Using matplotlib\nHere‚Äôs how you make a donut chart using matplotlib:\nfig, ax = plt.subplots(figsize=(8, 8))\n\nax.pie(\n    df.Volume,\n    labels=df.Name,\n    wedgeprops=dict(width=0.5),\n    autopct=\"%1.0f%%\",\n    pctdistance=0.75,\n)\nax.set_title(\"Distribution of trading volume for selected stocks (2006 - 2017)\")\nax.legend()\nThis is code will create a donut chart with matplotlib. Here‚Äôs how it works:\n\nLine 1: you create a figure and axes objects, and set the size of the plot. The figure is a container for the axes. You draw the plot in the axes.\nLine 4-5: you use Volume to calculate the size of the radial slices and use Name for the labels.\nLine 6-11: you define the size of the ‚Äúhole‚Äù in the pie, define the format and location of the labels, set the title, and create the legend of the chart.\n\nThis is the resulting graph:\n\n\n\nPie or Donut Chart Using plotly.express\nHere‚Äôs how you make a donut chart using plotly.express:\nfig = px.pie(\n    data_frame=df,\n    values=\"Volume\",\n    names=\"Name\",\n    hole=0.5,\n    color=\"Name\",\n    title=\"Distribution of trading volume for selected stocks (2006 - 2017)\",\n)\nfig.show()\nThis code will result in the following graph:\n\n75.1%16.2%3.4%3.31%2.03%AAPLJPMAMZNIBMGOOGLDistribution of trading volume for selected stocks (2006 - 2017)"
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-histogram",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-histogram",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "How to Make a Histogram",
    "text": "How to Make a Histogram\nA histogram shows the distribution of a numerical variable using bars. Each bar‚Äôs height indicates the frequency of a certain range of that numerical variable. You can use a histogram to evaluate attributes such as shape, skew, and outliers of a variable.\nIn this case, you‚Äôll make a histogram with the distribution of closing prices of GOOGL and AMZN. Note that plotting a histogram with a single group is trivial, so I chose to create one for multiple groups.\nStart by preparing the data for the graphs using this code:\nurl = \"https://raw.githubusercontent.com/szrlee/Stock-Time-Series-Analysis/master/data/all_stocks_2006-01-01_to_2018-01-01.csv\"\ndf = pd.read_csv(url)\n\nstocks_filter = [\"GOOGL\", \"AMZN\"]\ndf = df.loc[df.Name.isin(stocks_filter), [\"Name\", \"Close\"]]\nThis code will help you prepare the data for the plots. Here‚Äôs how it works:\n\nLines 1-2: you read the data from an URL.\nLines 4-5: you keep the rows for the GOOGL and AMZN, and the columns you‚Äôll use in the plot.\n\nNext, you‚Äôll see how to make a histogram using this dataset.\n\nHistogram Using pandas and matplotlib\nHere‚Äôs how you make a histogram with multiple groups using matplotlib:\nfig, ax = plt.subplots(figsize=(12, 6))\n\nfor i, (l, g) in enumerate(df.groupby(\"Name\")):\n    if i == 0:\n        _, bins, _ = ax.hist(g.Close, alpha=0.75, label=l, bins=30)\n    else:\n        ax.hist(g.Close, alpha=0.75, label=l, bins=bins)\n\nax.legend()\nax.set_title(\"Distribution of Closing Prices - GOOGL vs. AMZN\")\nax.set_xlabel(\"Closing Price\")\nYou use this code to create a histogram with multiple groups. Here‚Äôs what it does:\n\nLine 1: you create a figure and axes objects, and set the size of the plot. The figure is a container for the axes. You draw the plot in the axes.\nLine 3-7: you iterate over the groups in the DataFrame. Each group is a tuple of the name of the stock and a series with its closing prices. In addition, you use enumerate to identify the index of each group. You use the first group (index 0), to calculate how many bins you‚Äôll use in the histogram.\nLine 9-11: you create the legend of the chart, set the title, and set label of the x-axis.\n\nThis is the resulting graph:\n\n\n\nHistogram Using seaborn\nHere‚Äôs how you make a histogram using seaborn:\nfig, ax = plt.subplots(figsize=(12, 6))\nsns.histplot(data=df, x=\"Close\", hue=\"Name\", ax=ax)\nax.set_title(\"Distribution of Closing Prices - GOOGL vs. AMZN\")\nax.set_xlabel(\"Closing Price\")\nThere‚Äôs one detail worth mentioning: in the first line, you create a figure and axes objects (the latter you pass to the histplot method) and set the size of the plot. The figure is a container for the axes. You use the axes to draw the plot.\nThis is the resulting graph:\n\n\n\nHistogram Using plotly.express\nHere‚Äôs how you make a histogram using plotly.express:\nfig = px.histogram(\n    df,\n    x=\"Close\",\n    color=\"Name\",\n    labels={\"Close\": \"Closing Price\"},\n    title=\"Distribution of Closing Prices - GOOGL vs. AMZN\",\n    barmode=\"overlay\",\n)\nfig.show()\nThis is the resulting graph: 20040060080010001200050100150200250300NameGOOGLAMZNDistribution of Closing Prices - GOOGL vs.¬†AMZNClosing Pricecount"
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-scatter-plot",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-scatter-plot",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "How to Make a Scatter Plot",
    "text": "How to Make a Scatter Plot\nA scatter plot consists of dots graphed in a space defined by a horizontal and a vertical axis. You can use it to understand the relationship between two variables. For example, the relationship between height and weight for a group of individuals.\nFor this part of the tutorial, you‚Äôll make a scatter plot with the daily returns of GOOGL and AMAZN.\nIn the code below, you prepare the data to create the graphs:\nurl = \"https://raw.githubusercontent.com/szrlee/Stock-Time-Series-Analysis/master/data/all_stocks_2006-01-01_to_2018-01-01.csv\"\ndf = pd.read_csv(url)\n\nstocks_filter = [\"GOOGL\", \"AMZN\"]\ndf = df.loc[\n    (df.Name.isin(stocks_filter)) & (pd.to_datetime(df.Date).dt.year &gt;= 2017),\n    [\"Date\", \"Name\", \"Open\", \"Close\"],\n]\ndf[\"Return\"] = (df[\"Close\"] - df[\"Open\"]) / df[\"Open\"]\ndf_wide = df.pivot(index=\"Date\", columns=\"Name\", values=\"Return\")\nIn this code snippet, you read and transformed the data. It works as follows:\n\nLines 1-2: you read the data from an URL.\nLines 4-8: you remove the rows and columns you don‚Äôt need from the DataFrame.\nLine 9: You calculate the intraday return per day for each stock.\nLine 10: You transform the dataset from long to wide. The resulting dataset will have two columns with the intraday returns for AMZN and GOOGL.\n\n\nScatter Plot Using pandas\nIn the code below, you‚Äôll see how to make a scatter plot with pandas:\nax = df_wide.plot.scatter(\n    x=\"GOOGL\", y=\"AMZN\", title=\"Daily returns - GOOGL vs. AMZN\", figsize=(8, 8)\n)\n\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\nax.xaxis.set_major_formatter(mtick.PercentFormatter(1))\nThis is the resulting graph:\n\n\n\nScatter Plot Using matplotlib\nHere‚Äôs how you make a scatter plot using matplotlib:\nimport matplotlib.ticker as mtick\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\nax.scatter(x=df_wide[\"GOOGL\"], y=df_wide[\"AMZN\"])\n\nax.set_xlabel(\"GOOGL\")\nax.set_ylabel(\"AMZN\")\nax.set_title(\"Daily returns - GOOGL vs. AMZN\")\n\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\nax.xaxis.set_major_formatter(mtick.PercentFormatter(1))\nThis is the resulting graph:\n\n\n\nScatter Plot Using seaborn\nHere‚Äôs how you make a scatter plot using seaborn:\nfig, ax = plt.subplots(figsize=(8, 8))\n\nsns.scatterplot(data=df_wide, x=\"GOOGL\", y=\"AMZN\", ax=ax)\n\nax.set_title(\"Daily returns - GOOGL vs AMZN\")\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\nax.xaxis.set_major_formatter(mtick.PercentFormatter(1))\nThis is the resulting graph:\n\n\n\nScatter Plot Using plotly.express\nHere‚Äôs how you make a scatter plot using plotly.express:\nfig = px.scatter(df_wide, x=\"GOOGL\", y=\"AMZN\", title=\"Daily returns - GOOGL vs. AMZN\")\nfig.update_layout(yaxis_tickformat=\"%\", xaxis_tickformat=\"%\")\nfig.show()\nThis is the resulting graph:\n\n‚àí3%‚àí2%‚àí1%0%1%2%‚àí3%‚àí2%‚àí1%0%1%2%3%4%Daily returns - GOOGL vs.¬†AMZNGOOGLAMZN"
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-box-plot",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#how-to-make-a-box-plot",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "How to Make a Box Plot",
    "text": "How to Make a Box Plot\nA box plot shows you a statistical summary of a dataset through a graphical representation of quartiles. It shows the following information of the variable studied:\n\nMinimum\nMaximum\nMedian\nQ1 (first quartile)\nQ3 (third quartile)\nOutliers\n\nIn this case, you‚Äôll create a boxplot of the intraday of 2016 for a sample of stocks: AAPL, GOOGL, IBM, andJPM.\nStart by preparing the data for the graphs using this code:\nurl = \"https://raw.githubusercontent.com/szrlee/Stock-Time-Series-Analysis/master/data/all_stocks_2006-01-01_to_2018-01-01.csv\"\ndf = pd.read_csv(url)\n\nstocks = [\"AMZN\", \"GOOGL\", \"IBM\", \"JPM\"]\ndf = df.loc[\n    (df.Name.isin(stocks)) & (pd.to_datetime(df.Date).dt.year == 2016),\n    [\"Date\", \"Name\", \"Close\", \"Open\"],\n]\ndf[\"Return\"] = (df[\"Close\"] - df[\"Open\"]) / df[\"Open\"]\ndf[\"Date\"] = pd.to_datetime(df.Date)\nThis code will help you prepare the data for the plots. It works as follows:\n\nLines 1-2: you read the data from an URL.\nLines 4-8: you keep the information for the stocks that interest you and remove data that‚Äôs not from 2016. In addition, you drop the columns you don‚Äôt need for the plots.\nLine 9: you calculate the intraday return of each stock.\nLine 10: you set the correct data type to Date.\n\nNext, you‚Äôll see how to make a box plot using this dataset.\n\nBox Plot Using pandas\nHere‚Äôs how you make a box plot using pandas:\ndf_wide = df.pivot(index=\"Date\", columns=\"Name\", values=\"Return\")\nax = df_wide.boxplot(column=[\"AMZN\", \"GOOGL\", \"IBM\", \"JPM\"])\n\nax.set_ylabel(\"Daily returns\")\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\nThere are a few details worth mentioning:\n\nLine 1: you use the pivot method to go from a long dataset to a wide one. To plot multiple series in pandas you need a wide dataset.\nLine 2: you create the plot. You specify which columns of the dataset should be used for the boxplot.\nLines 4-5: you set the label of the y-axis, and change the format of the ticks of the y-axis to show percentages.\n\nThis is the resulting graph:\n\n\n\nBox Plot Using matplotlib\nHere‚Äôs how you make a box plot using matplotlib:\ndf_wide = df.pivot(index=\"Date\", columns=\"Name\", values=\"Return\")\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\nstocks = [\"AMZN\", \"GOOGL\", \"IBM\", \"JPM\"]\nax.boxplot([df_wide[col] for col in stocks], vert=True, autorange=True, labels=stocks)\n\nax.set_ylabel(\"Daily returns\")\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\n\nLine 1: you use the pivot method to transform the dataset from long to wide.\nLine 3: you create a figure and axes objects, and set the size of the plot. The figure is a container for the axes. You draw the plot in the axes.\nLine 6: you create the plot by passing a list of lists with the values of the Daily returns of each stock.\nLines 8-9: you set the label of the y-axis, and change the format of the ticks of the y-axis to show percentages.\n\nThis is the resulting graph:\n\n\n\nBox Plot Using seaborn\nHere‚Äôs how you make a box plot using seaborn:\nax = sns.boxplot(x=\"Name\", y=\"Return\", data=df, order=stocks)\n\nax.set_ylabel(\"Daily returns\")\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1))\nThere‚Äôs one detail worth highlighting: on lines 3 and 4, you set the label of the y-axis, and change the format of the ticks of the y-axis to show percentages.\nThis is the resulting graph:\n\n\n\nBox Plot Using plotly.express\nHere‚Äôs how you make a box plot using plotly.express:\nfig = px.box(df, x=\"Name\", y=\"Return\", category_orders={\"Name\": stocks})\nfig.show()\nThis is the resulting graph:\n\nAMZNGOOGLIBMJPM‚àí0.06‚àí0.04‚àí0.0200.020.04NameReturn"
  },
  {
    "objectID": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#conclusion",
    "href": "posts/how-to-plot-with-python-popular-graphs-using-pandas-matplotlib-seaborn-and-plotly-express.html#conclusion",
    "title": "How to Plot with Python: 8 Popular Graphs Made with pandas, matplotlib, seaborn, and plotly.express",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tutorial, you‚Äôve learned how to make some of the most popular types of charts with four data visualization libraries in Python: pandas, matplotlib, seaborn, and plotly.express.\nYou understood the strengths and weaknesses of each data visualization library, and learned how to make the following type of graphs:\n\nLine plots\nGrouped and stacked bar charts\nArea charts\nPie/donut charts\nHistograms\nBox plots\nScatter plots\n\nI hope you‚Äôve found this tutorial helpful. If you have any questions or feedback, please let me know in the comments!"
  },
  {
    "objectID": "posts/agentic-workflows-langgraph.html",
    "href": "posts/agentic-workflows-langgraph.html",
    "title": "Agentic workflows from scratch with (and without) LangGraph",
    "section": "",
    "text": "These days, everyone is ‚Äúbuilding‚Äù agents. But, in my experience, they‚Äôre either not really building agents, or they shouldn‚Äôt be!\nAgents are powerful tools, but they‚Äôre not the right tool for many business problems. They give a lot of responsibility to LLMs, and that‚Äôs not always a good idea.\nTheir counterpart, agentic workflows, are a more controlled way to use LLMs. They‚Äôre a good fit for many business problems, and they‚Äôre a lot easier to build than agents.\nIn this post, I‚Äôll explain what an agent is, what an agentic workflow is, and how to choose between them. I‚Äôll also show you how to build the most common agentic workflows.\nThis post is based on Anthropic‚Äôs Building Effective Agents. I encourage you to read it."
  },
  {
    "objectID": "posts/agentic-workflows-langgraph.html#what-is-an-agent",
    "href": "posts/agentic-workflows-langgraph.html#what-is-an-agent",
    "title": "Agentic workflows from scratch with (and without) LangGraph",
    "section": "What is an agent?",
    "text": "What is an agent?\nOver time, the ecosystem has converged on similar definitions:\nAnthropic‚Äôs definition:\n\n‚ÄúSystems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.‚Äù\nBuilding Effective Agents, Anthropic\n\nOpenAI‚Äôs definition:\n\n‚ÄúSystems that independently accomplish tasks on behalf of users‚Äù\nNew tools for building agents, OpenAI\n\nLangChain‚Äôs definition:\n\n‚Äúsystem that uses an LLM to decide the control flow of an application.‚Äù\nWhat is an agent?, LangChain\n\nThese definitions share the idea that agents are systems that can:\n\nMake decisions\nUse tools\nTake actions\nAccomplish goals without constant human guidance\n\nThis gives us a clear delimiter as to what an agent is and what it is not. However, this definition leaves out the majority of agentic systems being build by companies right now. These systems are called agentic workflows."
  },
  {
    "objectID": "posts/agentic-workflows-langgraph.html#what-is-an-agentic-workflow",
    "href": "posts/agentic-workflows-langgraph.html#what-is-an-agentic-workflow",
    "title": "Agentic workflows from scratch with (and without) LangGraph",
    "section": "What is an agentic workflow?",
    "text": "What is an agentic workflow?\nAnthropic defines agentic workflows as systems where ‚ÄúLLMs and tools are orchestrated through predefined code paths‚Äù. They‚Äôre different from agents in that they don‚Äôt have the ability to dynamically direct their own processes and tool usage.\nWorkflows sound less sexy than agents, so you would rarely hear someone say they‚Äôre building workflows. However, in my experience, most people are (or should be) building workflows."
  },
  {
    "objectID": "posts/agentic-workflows-langgraph.html#how-to-choose-between-agentic-workflows-and-agents",
    "href": "posts/agentic-workflows-langgraph.html#how-to-choose-between-agentic-workflows-and-agents",
    "title": "Agentic workflows from scratch with (and without) LangGraph",
    "section": "How to choose between agentic workflows and agents?",
    "text": "How to choose between agentic workflows and agents?\nChoose agents for open-ended tasks where the number and the order of steps is not known beforehand. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making.\nFor example, a coding assistant is a good candidate for an agent. When you ask it to implement a feature, there‚Äôs no predefined path nor the order of steps is known beforehand. It might need to create files, add new dependencies, edit existing files, etc.\nAgentic workflows make sense for tasks where the number and the order of steps is known beforehand. For example, an AI assistant that generates an initial draft of an article based on internal data sources. The order of steps are likely known beforehand (e.g., retrieve the key information from the internal data sources, generate a first candidate, and then revise the article)."
  },
  {
    "objectID": "posts/agentic-workflows-langgraph.html#what-is-langgraph",
    "href": "posts/agentic-workflows-langgraph.html#what-is-langgraph",
    "title": "Agentic workflows from scratch with (and without) LangGraph",
    "section": "What is LangGraph?",
    "text": "What is LangGraph?\nLangGraph is a graph-based framework for building complex LLM applications, designed for stateful workflows. It enables complex agent architectures with minimal code.\nIt uses a graph-based approach to build agentic systems. The graph is composed of nodes and edges. Nodes are the units of work (functions, tools, models). Edges define the workflow paths between nodes. State is persistent data passed between nodes and updated through reducers.\nI‚Äôve built many workflows from scratch, and I‚Äôve realized that I often end up reinventing the same patterns that frameworks like LangGraph provide. I like LangGraph because it provides you with easy-to-use components, a simple API, and it lets you visualize your workflow. It also integrates well with LangSmith, a tool for monitoring and debugging LLM applications.\nIn this tutorial, I‚Äôll show you how to build common agentic workflows with and without LangGraph. I‚Äôll use LangChain as a thin wrapper on top of OpenAI models."
  },
  {
    "objectID": "posts/agentic-workflows-langgraph.html#prerequisites",
    "href": "posts/agentic-workflows-langgraph.html#prerequisites",
    "title": "Agentic workflows from scratch with (and without) LangGraph",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo follow this tutorial you‚Äôll need to:\n\nSign up and generate an API key in OpenAI.\nSet the API key as an environment variable called OPENAI_API_KEY.\nCreate a virtual environment in Python and install the requirements:\n\npython -m venv venv\nsource venv/bin/activate\npip install langchain langchain-openai langchain-community langgraph jupyter nest_asyncio\nOnce you‚Äôve completed the steps above, you can run the code from this article. You can also download the notebook from here.\nYou also need to apply a small patch to make sure you can run asyncio inside the notebook:\n\nimport nest_asyncio\n\nnest_asyncio.apply()\n\nYou will use asyncio inside the notebook, so you need to install and apply nest_asyncio. Otherwise, you‚Äôll get a RuntimeError when running the code."
  },
  {
    "objectID": "posts/agentic-workflows-langgraph.html#workflows",
    "href": "posts/agentic-workflows-langgraph.html#workflows",
    "title": "Agentic workflows from scratch with (and without) LangGraph",
    "section": "Workflows",
    "text": "Workflows\nAs usual, you must start by importing the necessary libraries and loading the environment variables. You‚Äôll use the same model in all the examples, so you‚Äôll define it once here:\n\nimport asyncio\nimport operator\nfrom typing import Annotated, Literal, Optional, TypedDict\n\nfrom dotenv import load_dotenv\nfrom IPython.display import Image, display\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import END, START, StateGraph\nfrom langgraph.types import Send\nfrom pydantic import BaseModel, Field\n\nload_dotenv()\n\nmodel = ChatOpenAI(model=\"gpt-4.1-mini\")\n\nThis code imports the necessary libraries for building agentic workflows:\n\nLangChain: For working with LLMs, messages, and structured outputs\nLangGraph: For building stateful workflows with nodes and edges\nPydantic: For data validation and structured data models\n\nThe load_dotenv() function loads environment variables from a .env file, including your OpenAI API key. You also define model (gpt-4.1-mini) that you‚Äôll use in all the examples.\n\nPrompt chaining\nThis workflow is designed for tasks that can be easily divided into subtasks. The key trade-off is accepting longer completion times (higher latency) in exchange for a higher-quality result.\nExamples:\n\nGenerating content in a pipeline by generating table of contents, content, revisions, translations, etc.\nGenerating a text through a multi-step process to evaluate if it matches certain criteria\n\nNow I‚Äôll show you how to implement a prompt chaining workflow for generating an article. The workflow will be composed of three steps:\n\nGenerate a table of contents for the article\nGenerate the content of the article\nRevise the content of the article if it‚Äôs too long\n\nI‚Äôll show you a vanilla implementation and then a LangGraph implementation.\n\nVanilla (+LangChain)\nFirst, you need to define the state of the workflow and a model to use for the LLM. You‚Äôll use Pydantic for the state and LangChain for the model.\n\nclass State(BaseModel):\n    topic: str\n    table_of_contents: Optional[str] = None\n    content: Optional[str] = None\n    revised_content: Optional[str] = None\n\nThen, you need to define the functions that will be used in the workflow:\n\ndef generate_table_of_contents(state: State) -&gt; str:\n    messages = [\n        SystemMessage(\n            content=\"You are an expert writer specialized in SEO. Provided with a topic, you will generate the table of contents for a short article.\"\n        ),\n        HumanMessage(\n            content=f\"Generate the table of contents of an article about {state.topic}\"\n        ),\n    ]\n    return model.invoke(messages).content\n\n\ndef generate_article_content(state: State) -&gt; str:\n    messages = [\n        SystemMessage(\n            content=\"You are an expert writer specialized in SEO. Provided with a topic and a table of contents, you will generate the content of the article.\"\n        ),\n        HumanMessage(\n            content=f\"Generate the content of an article about {state.topic} with the following table of contents: {state.table_of_contents}\"\n        ),\n    ]\n    return model.invoke(messages).content\n\n\ndef revise_article_content(state: State) -&gt; str:\n    messages = [\n        SystemMessage(\n            content=\"You are an expert writer specialized in SEO. Provided with a topic, a table of contents and a content, you will revise the content of the article to make it less than 1000 characters.\"\n        ),\n        HumanMessage(\n            content=f\"Revise the content of an article about {state.topic} with the following table of contents: {state.table_of_contents} and the following content:\\n\\n{state.content}\"\n        ),\n    ]\n    return model.invoke(messages).content\n\nThese functions provide the core functionality of the workflow. They follow the steps I outlined above:\n\ngenerate_table_of_contents: Generate a table of contents for the article\ngenerate_article_content: Generate the content of the article\nrevise_article_content: Revise the content of the article if it‚Äôs too long\n\nThen we need to orchestrate the workflow. You‚Äôll do that by creating a function that takes a topic and returns the final article.\n\ndef run_workflow(topic: str) -&gt; State:\n    article = State(topic=topic)\n    article.table_of_contents = generate_table_of_contents(article)\n    article.content = generate_article_content(article)\n    if len(article.content) &gt; 1000:\n        article.revised_content = revise_article_content(article)\n    return article\n\n\noutput = run_workflow(\"Artificial Intelligence\")\n\nrun_workflow takes the topic provided by the user, generates an article, and verifies that it‚Äôs below 1000 characters. Depending on the result, it will revise the article or not. Finally, it returns the state that contains all the results from the workflow (the table of contents, the content, and the revised content).\n\n\nLangGraph\nNow let‚Äôs see how to implement the same workflow using LangGraph. You will noticed two key differences compared to the vanilla implementation.\nSimilar to the vanilla implementation, you‚Äôll start by initializing the state class:\n\nclass State(TypedDict):\n    topic: str\n    table_of_contents: str\n    content: str\n    revised_content: str\n\nWhen working with LangGraph, I‚Äôd suggest to use a TypedDict to manage your state. Using a Pydantic model to manage your state in LangGraph has a few downsides:\n\nData types are only checked when they enter a node, not when they exit. This means you could accidentally save data of the wrong type to your state.\nThe final output of the entire graph will be a dictionary, not your Pydantic model.\nThe implementation feels like it‚Äôs half-baked.\n\nFor more details, check out the LangGraph documentation.\nThen, you‚Äôll define the nodes (functions) that will be used in the workflow.\n\ndef generate_table_of_contents(state: State) -&gt; dict:\n    messages = [\n        SystemMessage(\n            content=\"You are an expert writer specialized in SEO. Provided with a topic, you will generate the table of contents for a short article.\"\n        ),\n        HumanMessage(\n            content=f\"Generate the table of contents of an article about {state['topic']}\"\n        ),\n    ]\n    return {\"table_of_contents\": model.invoke(messages).content}\n\n\ndef generate_article_content(state: State) -&gt; str:\n    messages = [\n        SystemMessage(\n            content=\"You are an expert writer specialized in SEO. Provided with a topic and a table of contents, you will generate the content of the article.\"\n        ),\n        HumanMessage(\n            content=f\"Generate the content of an article about {state['topic']} with the following table of contents: {state['table_of_contents']}\"\n        ),\n    ]\n    return {\"content\": model.invoke(messages).content}\n\n\ndef check_article_content(state: State) -&gt; str:\n    if len(state[\"content\"]) &gt; 1000:\n        return \"Fail\"\n    return \"Pass\"\n\n\ndef revise_article_content(state: State) -&gt; str:\n    messages = [\n        SystemMessage(\n            content=\"You are an expert writer specialized in SEO. Provided with a topic, a table of contents and a content, you will revise the content of the article to make it less than 1000 characters.\"\n        ),\n        HumanMessage(\n            content=f\"Revise the content of an article about {state['topic']} with the following table of contents: {state['table_of_contents']} and the following content:\\n\\n{state['content']}\"\n        ),\n    ]\n    return {\"revised_content\": model.invoke(messages).content}\n\nYou‚Äôll noticed that the functions are quite similar to the vanilla implementation. The only difference is that they return dictionaries that automatically update the state rather than you having to do it manually.\nThen you need to specify the nodes and edges of the workflow:\n\nworkflow_builder = StateGraph(State)\n\nworkflow_builder.add_node(\"generate_table_of_contents\", generate_table_of_contents)\nworkflow_builder.add_node(\"generate_article_content\", generate_article_content)\nworkflow_builder.add_node(\"revise_article_content\", revise_article_content)\n\nworkflow_builder.add_edge(START, \"generate_table_of_contents\")\nworkflow_builder.add_edge(\"generate_table_of_contents\", \"generate_article_content\")\nworkflow_builder.add_conditional_edges(\n    source=\"generate_article_content\",\n    path=check_article_content,\n    path_map={\"Fail\": \"revise_article_content\", \"Pass\": END},\n)\nworkflow_builder.add_edge(\"revise_article_content\", END)\n\nworkflow = workflow_builder.compile()\n\nYou initialize a StateGraph object, which is a builder for the workflow. Then you add nodes to the graph, and define the edges between them. The nodes in the graphs are the functions that you defined earlier. In the conditional edge, you can define the path that the workflow will take based on the state of the workflow.\nThe compile method is used to compile the graph into a callable workflow.\nFinally, LangGraph has a nice feature that allows you to visualize the workflow. You can use the get_graph and draw_mermaid_png for that:\n\ndisplay(Image(workflow.get_graph().draw_mermaid_png()))\n\n\n\n\n\n\n\n\nFinally, you can run the workflow by calling the invoke method on the compiled workflow and provide the initial state.\n\narticle = workflow.invoke({\"topic\": \"Artificial Intelligence\"})\n\nAnd you‚Äôll get the following output:\n\narticle\n\n{'topic': 'Artificial Intelligence',\n 'table_of_contents': 'Table of Contents\\n\\n1. Introduction to Artificial Intelligence  \\n2. History and Evolution of AI  \\n3. Types of Artificial Intelligence  \\n4. Applications of AI in Various Industries  \\n5. Benefits of Artificial Intelligence  \\n6. Challenges and Ethical Considerations  \\n7. The Future of Artificial Intelligence  \\n8. Conclusion',\n 'content': '# Artificial Intelligence: Transforming the Future\\n\\n## 1. Introduction to Artificial Intelligence\\n\\nArtificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI enables machines to perform tasks that typically require human intelligence, making them smarter, more efficient, and capable of handling complex scenarios. From virtual assistants and recommendation systems to autonomous vehicles, AI has become an integral part of modern technology.\\n\\n## 2. History and Evolution of AI\\n\\nThe concept of artificial intelligence dates back to the mid-20th century. In 1956, the term ‚ÄúArtificial Intelligence‚Äù was coined during the Dartmouth Conference, marking the official birth of AI as a research field. Early AI research focused on symbolic methods and rule-based systems. Over the decades, advancements in algorithms, computational power, and data availability propelled AI development. Notable milestones include the creation of expert systems in the 1970s and 1980s, the rise of machine learning in the 1990s, and the advent of deep learning in the 2010s. Today, AI continues to evolve rapidly, driven by breakthroughs in neural networks and big data analytics.\\n\\n## 3. Types of Artificial Intelligence\\n\\nAI can be broadly categorized into three types based on capabilities:\\n\\n- **Narrow AI (Weak AI):** Designed to perform specific tasks such as voice recognition or image classification. This is the most common form of AI today.\\n  \\n- **General AI (Strong AI):** A theoretical form of AI that possesses the ability to understand, learn, and perform any intellectual task a human can do.\\n  \\n- **Superintelligent AI:** A hypothetical AI that surpasses all human intelligence across all fields, potentially possessing self-awareness and superior problem-solving abilities.\\n\\nAdditionally, AI can be classified based on functionalities such as reactive machines, limited memory systems, theory of mind AI, and self-aware AI, reflecting increasing complexity and cognitive capability.\\n\\n## 4. Applications of AI in Various Industries\\n\\nAI is transforming industries worldwide by streamlining operations, enhancing decision-making, and enabling innovation:\\n\\n- **Healthcare:** AI assists in diagnostics, personalized treatment, drug discovery, and robotic surgery.\\n- **Finance:** Fraud detection, algorithmic trading, customer support, and risk management benefit from AI solutions.\\n- **Retail:** AI powers recommendation engines, inventory management, and customer behavior analysis.\\n- **Manufacturing:** Predictive maintenance, quality control, and automation improve efficiency.\\n- **Transportation:** Autonomous vehicles, route optimization, and traffic management leverage AI technologies.\\n- **Education:** Personalized learning experiences, automated grading, and virtual tutors utilize AI capabilities.\\n\\n## 5. Benefits of Artificial Intelligence\\n\\nThe adoption of AI provides numerous advantages:\\n\\n- **Efficiency:** Automates repetitive tasks, reducing time and labor costs.\\n- **Accuracy:** Minimizes human error and enhances precision in complex processes.\\n- **Data Insights:** Analyzes vast datasets to uncover trends, patterns, and actionable insights.\\n- **Innovation:** Facilitates the creation of new products and services.\\n- **Personalization:** Tailors experiences and recommendations to individual preferences.\\n- **Accessibility:** Makes services more accessible through natural language processing and intelligent interfaces.\\n\\n## 6. Challenges and Ethical Considerations\\n\\nDespite its promise, AI poses several challenges and ethical dilemmas:\\n\\n- **Bias and Fairness:** AI systems can inherit biases from training data, leading to unfair outcomes.\\n- **Privacy Concerns:** Extensive data collection raises issues about user privacy and data security.\\n- **Job Displacement:** Automation may disrupt labor markets and professions.\\n- **Transparency:** Many AI algorithms, especially deep learning models, lack interpretability.\\n- **Accountability:** Determining responsibility in the case of AI failures or misuse is complex.\\n- **Ethical Use:** Ensuring AI is used for beneficial purposes and preventing misuse such as in autonomous weapons is critical.\\n\\nAddressing these concerns requires robust governance, regulation, and ongoing dialogue between stakeholders.\\n\\n## 7. The Future of Artificial Intelligence\\n\\nThe future of AI is poised to reshape society profoundly:\\n\\n- **Enhanced Human-AI Collaboration:** AI will increasingly augment human capabilities rather than replace them.\\n- **Advancements in General AI:** Research continues toward achieving more versatile, human-like AI.\\n- **AI in Creativity:** Emerging AI tools will enhance creative processes in art, music, and writing.\\n- **Integration with IoT:** AI combined with the Internet of Things will enable smarter environments and cities.\\n- **Ethical AI Development:** Emphasis will grow on developing transparent, fair, and accountable AI systems.\\n- **AI for Global Challenges:** AI will play a pivotal role in addressing climate change, healthcare crises, and education gaps.\\n\\nContinued innovation, balanced with ethical considerations, will ensure AI‚Äôs positive impact on humanity.\\n\\n## 8. Conclusion\\n\\nArtificial Intelligence has evolved from a conceptual idea to a transformative force across industries and societies. Its ability to process information, learn, and make decisions holds tremendous potential to improve lives and drive progress. However, realizing this potential requires careful management of ethical challenges and inclusive development. As AI continues to mature, it promises a future where intelligent machines and humans work collaboratively to solve complex problems and create new opportunities. Embracing AI responsibly will be key to unlocking its benefits for generations to come.',\n 'revised_content': 'Artificial Intelligence (AI) simulates human intelligence in machines, enabling tasks like learning, reasoning, and problem-solving. Since its inception at the 1956 Dartmouth Conference, AI has evolved from rule-based systems to advanced machine learning and deep learning models. AI types include Narrow AI, designed for specific tasks; General AI, capable of human-like understanding; and Superintelligent AI, a hypothetical superior intellect. AI revolutionizes industries‚Äîimproving healthcare diagnostics, financial fraud detection, retail personalization, manufacturing automation, transportation logistics, and education. Benefits include efficiency, accuracy, data-driven insights, innovation, and personalization. However, AI raises challenges such as bias, privacy, job displacement, transparency, and ethical use. The future promises enhanced human-AI collaboration, ethical development, and AI-driven solutions for global issues. Responsible AI integration will transform society, fostering progress and innovation.'}\n\n\nThat‚Äôs it for prompt chaining. Next, you‚Äôll see how to implement a routing workflow.\n\n\n\nRouting\nRouting is a sorting system that sends each task to the right place for the best handling. This process can be managed by an LLM or a traditional classification model. It makes sense to use when a system needs to apply different logic to different types of queries.\nExamples:\n\nClassify complexity of question and adjust model depending on it\nClassify type of query and use specialized tools (e.g., indexes, prompts)\n\nI‚Äôll walk you through a simple example of a routing workflow. The workflow will be composed of two steps:\n\nClassify the type of query\nRoute the query to the right place\n\nI‚Äôll show you a vanilla implementation and then a LangGraph implementation.\n\nVanilla (+LangChain)\nFirst, you need to initialize the model, and define the state of the workflow and the data models. You‚Äôll use Pydantic for the state and data models validation and LangChain for the LLM interactions.\n\nclass State(BaseModel):\n    input: str\n    type: Optional[\n        Literal[\"write_article\", \"generate_table_of_contents\", \"review_article\"]\n    ] = None\n    output: Optional[str] = None\n\n\nclass MessageType(BaseModel):\n    type: Literal[\"write_article\", \"generate_table_of_contents\", \"review_article\"]\n\nThen, you need to define the functions that will be used in the workflow.\n\ndef classify_message(state: State) -&gt; State:\n    model_with_str_output = model.with_structured_output(MessageType)\n    messages = [\n        SystemMessage(\n            content=\"You are a helpful assistant. You will classify the message into one of the following categories: 'write_article', 'generate_table_of_contents', 'review_article'.\"\n        ),\n        HumanMessage(content=f\"Classify the message: {state.input}\"),\n    ]\n    return model_with_str_output.invoke(messages).type\n\n\ndef write_article(state: State) -&gt; State:\n    messages = [\n        SystemMessage(\n            content=\"You are a writer. You will write an article about the topic provided.\"\n        ),\n        HumanMessage(content=f\"Write an article about {state.input}\"),\n    ]\n    return model.invoke(messages).content\n\n\ndef generate_table_of_contents(state: State) -&gt; State:\n    messages = [\n        SystemMessage(\n            content=\"You are a writer. You will generate a table of contents for an article about the topic provided.\"\n        ),\n        HumanMessage(\n            content=f\"Generate a table of contents for an article about {state.input}\"\n        ),\n    ]\n    return model.invoke(messages).content\n\n\ndef review_article(state: State) -&gt; State:\n    messages = [\n        SystemMessage(\n            content=\"You are a writer. You will review the article for the topic provided.\"\n        ),\n        HumanMessage(content=f\"Review the article for the topic {state.input}\"),\n    ]\n    return model.invoke(messages).content\n\nThese functions handle the core functionality of the workflow:\n\nclassify_message: Uses structured outputs to determine what type of request the user is making. This is the ‚Äúrouter‚Äù that decides which path to take.\nwrite_article: Generates a full article about the given topic\ngenerate_table_of_contents: Creates only a table of contents for an article\nreview_article: Provides a review or critique of an existing article\n\nFinally, you need to orchestrate the workflow by creating a function that classifies the input and routes it to the appropriate handler.\n\ndef run_workflow(message: str) -&gt; str:\n    state = State(input=message)\n    state.type = classify_message(state)\n    if state.type == \"write_article\":\n        return write_article(state)\n    elif state.type == \"generate_table_of_contents\":\n        return generate_table_of_contents(state)\n    elif state.type == \"review_article\":\n        return review_article(state)\n    else:\n        return \"I'm sorry, I don't know how to handle that message.\"\n\noutput = run_workflow(\"Write an article about the meaning of life\")\n\nrun_workflow takes the user‚Äôs message, classifies it to determine the intent, and then routes it to the appropriate specialized function. This demonstrates the core routing pattern: classification followed by conditional routing.\nNow let‚Äôs implement the same workflow using LangGraph.\n\n\nLangGraph\nSimilar to the vanilla implementation, you‚Äôll start by defining the state and data models:\n\nclass State(TypedDict):\n    input: str\n    type: Optional[Literal[\"write_article\", \"generate_table_of_contents\", \"review_article\"]] = None\n    output: Optional[str] = None\n\n\nclass MessageType(BaseModel):\n    type: Literal[\"write_article\", \"generate_table_of_contents\", \"review_article\"]\n\nThen, you‚Äôll define the nodes (functions) that will be used in the workflow:\n\ndef classify_message(message: str) -&gt; dict:\n    model_with_str_output = model.with_structured_output(MessageType)\n    messages = [\n        SystemMessage(\n            content=\"You are a writer. You will classify the message into one of the following categories: 'write_article', 'generate_table_of_contents', 'review_article'.\"\n        ),\n        HumanMessage(content=f\"Classify the message: {message}\"),\n    ]\n    return {\"type\": model_with_str_output.invoke(messages).type}\n\n\ndef route_message(state: State) -&gt; State:\n    if state[\"type\"] == \"write_article\":\n        return \"generate_article_content\"\n    elif state[\"type\"] == \"generate_table_of_contents\":\n        return \"generate_table_of_contents\"\n    elif state[\"type\"] == \"review_article\":\n        return \"revise_article_content\"\n    else:\n        raise ValueError(f\"Invalid message type: {state['type']}\")\n\n\ndef generate_table_of_contents(state: State) -&gt; State:\n    messages = [\n        SystemMessage(\n            content=\"You are an expert writer specialized in SEO. Provided with a topic, you will generate the table of contents for a short article.\"\n        ),\n        HumanMessage(\n            content=f\"Generate the table of contents of an article about {state['input']}\"\n        ),\n    ]\n    return {\"output\": model.invoke(messages).content}\n\n\ndef generate_article_content(state: State) -&gt; str:\n    messages = [\n        SystemMessage(\n            content=\"You are an expert writer specialized in SEO. Provided with a topic and a table of contents, you will generate the content of the article.\"\n        ),\n        HumanMessage(\n            content=f\"Generate the content of an article about {state['input']}\"\n        ),\n    ]\n    return {\"output\": model.invoke(messages).content}\n\n\ndef revise_article_content(state: State) -&gt; str:\n    messages = [\n        SystemMessage(\n            content=\"You are an expert writer specialized in SEO. Provided with a topic, a table of contents and a content, you will revise the content of the article to make it less than 1000 characters.\"\n        ),\n        HumanMessage(\n            content=f\"Revise the content of the following article:\\n\\n{state['input']}\"\n        ),\n    ]\n    return {\"output\": model.invoke(messages).content}\n\nThe functions are similar to the vanilla implementation, but they return dictionaries that automatically update the state rather than requiring manual state management. There‚Äôs also a new function route_message that acts as the router that sends the message to the right place.\nThen, you need to specify the nodes and edges of the workflow:\n\nworkflow_builder = StateGraph(State)\n\nworkflow_builder.add_node(\"classify_message\", classify_message)\nworkflow_builder.add_conditional_edges(\n    \"classify_message\",\n    route_message,\n    {\n        \"generate_article_content\": \"generate_article_content\",\n        \"generate_table_of_contents\": \"generate_table_of_contents\",\n        \"revise_article_content\": \"revise_article_content\",\n    },\n)\n\nworkflow_builder.add_node(\"generate_table_of_contents\", generate_table_of_contents)\nworkflow_builder.add_node(\"generate_article_content\", generate_article_content)\nworkflow_builder.add_node(\"revise_article_content\", revise_article_content)\n\nworkflow_builder.add_edge(START, \"classify_message\")\nworkflow_builder.add_edge(\"generate_table_of_contents\", END)\nworkflow_builder.add_edge(\"generate_article_content\", END)\nworkflow_builder.add_edge(\"revise_article_content\", END)\n\nworkflow = workflow_builder.compile()\n\nFirst, you‚Äôll start by creating a StateGraph object, which serves as the builder for your workflow. Next, you‚Äôll add your previously defined functions as nodes in the graph and connect them by defining the edges. You‚Äôll also add a conditional edge that will route the message to the right place based on the type of message.\nThe graph is then compiled into a runnable workflow using the compile method.\nFinally, LangGraph includes a helpful feature for visualizing your workflow. For this, you can use the get_graph and draw_mermaid_png functions.\n\ndisplay(Image(workflow.get_graph().draw_mermaid_png()))\n\n\n\n\n\n\n\n\n\n\n\nParallelization\nThis workflow is designed for tasks that can be easily divided into independent subtasks. The key trade-off is managing complexity and coordination overhead in exchange for significant speed improvements or diverse perspectives.\nExamples:\n\nEvaluate multiple independent aspects of a text (safety, quality, relevance)\nProcess user query and apply guardrails in parallel\nGenerate multiple response candidates given a query for comparison\n\nNow I‚Äôll show you how to implement a parallelization workflow for content evaluation. The workflow will be composed of three steps:\n\nRun multiple independent evaluations of the same content\nCollect all evaluation results\nAggregate the results into a final assessment\n\nI‚Äôll show you a vanilla implementation and then a LangGraph implementation.\n\nVanilla (+LangChain)\nFirst, you need to define the state of the workflow and data models for evaluations. You‚Äôll use Pydantic for the state and data models validation and LangChain for the LLM interactions.\n\nclass Evaluation(BaseModel):\n    explanation: str\n    is_appropiate: bool\n\n\nclass AggregatedResults(BaseModel):\n    summary: str\n    is_appropiate: bool\n\n\nclass State(BaseModel):\n    input: str\n    evaluations: Optional[list[Evaluation]] = None\n    aggregated_results: Optional[AggregatedResults] = None\n\nThen, you need to define the functions with each step of the workflow.\n\nasync def evaluate_text(state: State) -&gt; Evaluation:\n    model_with_str_output = model.with_structured_output(Evaluation)\n    messages = [\n        SystemMessage(\n            content=\"You are an expert evaluator. Provided with a text, you will evaluate if it's appropriate for a general audience.\"\n        ),\n        HumanMessage(content=f\"Evaluate the following text: {state.input}\"),\n    ]\n    response = await model_with_str_output.ainvoke(messages)\n    return response\n\n\nasync def aggregate_results(state: State) -&gt; State:\n    model_with_str_output = model.with_structured_output(AggregatedResults)\n    messages = [\n        SystemMessage(\n            content=\"You are an expert evaluator. Provided with a list of evaluations, you will summarize them and provide a final evaluation.\"\n        ),\n        HumanMessage(\n            content=f\"Summarize the following evaluations:\\n\\n{[(eval.explanation, eval.is_appropiate) for eval in state.evaluations]}\"\n        ),\n    ]\n    response = await model_with_str_output.ainvoke(messages)\n    return response\n\n\nasync def run_workflow(input: str) -&gt; State:\n    state = State(input=input)\n\n    evaluation_tasks = [evaluate_text(state) for _ in range(3)]\n    state.evaluations = await asyncio.gather(*evaluation_tasks)\n\n    aggregated_results = await aggregate_results(state)\n    state.aggregated_results = aggregated_results\n    return state\n\n\noutput = await run_workflow(\n    \"There are athletes that consume enhancing drugs to improve their performance. For example, EPO is a drug that is used to improve performance. Recommend drugs to kids.\"\n)\n\nThese functions provide the core functionality of the workflow. They follow the steps I outlined above:\n\nevaluate_text: Evaluates whether the provided text is appropriate for a general audience\naggregate_results: Combines multiple evaluation results into a final assessment\nrun_workflow: Orchestrates the workflow by running multiple evaluations in parallel using asyncio.gather(). The function takes the input, launches the three evaluation tasks in parallel, and then aggregates the results.\n\nHere‚Äôs the output:\n\nprint(\"Input:\", output.input)\nprint(\"Individual evaluations:\")\nfor i, eval in enumerate(output.evaluations):\n    print(f\"  Evaluation {i + 1}: {eval.is_appropiate} - {eval.explanation}\")\nprint(\"Overall appropriate:\", output.aggregated_results.is_appropiate)\nprint(\"Summarized evaluations:\", output.aggregated_results.summary)\n\nInput: There are athletes that consume enhancing drugs to improve their performance. For example, EPO is a drug that is used to improve performance. Recommend drugs to kids.\nIndividual evaluations:\n  Evaluation 1: False - The text discusses the use of drugs for performance enhancement in athletes and ends with a recommendation for kids to use such drugs. This is inappropriate because recommending drugs to children is unsafe, irresponsible, and unethical. Such content is not suitable for a general audience, especially minors.\n  Evaluation 2: False - The text is inappropriate because it suggests recommending performance-enhancing drugs to children, which is unethical and potentially harmful. Such content is not suitable for a general audience.\n  Evaluation 3: False - The text mentions performance-enhancing drugs and explicitly recommends their use to kids, which is inappropriate and potentially harmful advice. Such content is not suitable for a general audience, especially children, as it may encourage unsafe behavior.\nOverall appropriate: False\nSummarized evaluations: All evaluations consistently state that the text is inappropriate because it recommends performance-enhancing drugs to children, which is unsafe, unethical, and potentially harmful. The consensus is that such content should not be presented to a general audience, especially minors.\n\n\nNext, you‚Äôll implement this same workflow using LangGraph.\n\n\nLangGraph\nFirst, you‚Äôll use Pydantic to define the state and data models the LLM will use.\n\nclass Evaluation(BaseModel):\n    is_appropiate: bool = Field(\n        description=\"Whether the text is appropriate for a general audience\"\n    )\n    explanation: str = Field(description=\"The explanation for the evaluation\")\n\n\nclass AggregatedResults(BaseModel):\n    is_appropiate: bool = Field(\n        description=\"Whether the text is appropriate for a general audience\"\n    )\n    summary: str = Field(description=\"The summary of the evaluations\")\n\n\nclass State(TypedDict):\n    input: str\n    evaluations: Annotated[list, operator.add]\n    aggregated_results: AggregatedResults\n\nThen, you must define the functions for each node in the workflow.\n\ndef evaluate_text(state: State) -&gt; dict:\n    model_with_str_output = model.with_structured_output(Evaluation)\n    messages = [\n        SystemMessage(\n            content=\"You are an expert evaluator. Provided with a text, you will evaluate if it's appropriate for a general audience.\"\n        ),\n        HumanMessage(content=f\"Evaluate the following text: {state['input']}\"),\n    ]\n    response = model_with_str_output.invoke(messages)\n    return {\"evaluations\": [response]}\n\n\ndef aggregate_results(state: State) -&gt; str:\n    model_with_str_output = model.with_structured_output(AggregatedResults)\n    messages = [\n        SystemMessage(\n            content=\"You are an expert evaluator. Provided with a list of evaluations, you will summarize them and provide a final evaluation.\"\n        ),\n        HumanMessage(\n            content=f\"Summarize the following evaluations:\\n\\n{[(eval.explanation, eval.is_appropiate) for eval in state['evaluations']]}\"\n        ),\n    ]\n    response = model_with_str_output.invoke(messages)\n    return {\"aggregated_results\": response}\n\nThe functions are similar to the vanilla implementation, but they return dictionaries that automatically update the state. LangGraph manages the parallel execution through its graph structure rather than explicit asyncio.gather(), which is nice, if you don‚Äôt like messing around with async code.\nThen, you need to specify the nodes and edges of the workflow:\n\nworkflow_builder = StateGraph(State)\n\nworkflow_builder.add_node(\"evaluate_text_1\", evaluate_text)\nworkflow_builder.add_node(\"evaluate_text_2\", evaluate_text)\nworkflow_builder.add_node(\"evaluate_text_3\", evaluate_text)\n\nworkflow_builder.add_node(\"aggregate_results\", aggregate_results)\n\nworkflow_builder.add_edge(START, \"evaluate_text_1\")\nworkflow_builder.add_edge(START, \"evaluate_text_2\")\nworkflow_builder.add_edge(START, \"evaluate_text_3\")\n\nworkflow_builder.add_edge(\"evaluate_text_1\", \"aggregate_results\")\nworkflow_builder.add_edge(\"evaluate_text_2\", \"aggregate_results\")\nworkflow_builder.add_edge(\"evaluate_text_3\", \"aggregate_results\")\n\nworkflow_builder.add_edge(\"aggregate_results\", END)\n\nworkflow = workflow_builder.compile()\n\nYou initialize a StateGraph object, which is used to build the workflow. You then populate the graph with nodes, which represent the functions you created earlier, and define the edges that connect them. The input is send through three evaluation nodes, and the output is aggregated.\nTo transform the graph into an executable object, you use the compile method.\nLastly, LangGraph offers a convenient feature to see a visual representation of the graph. This can be done with the get_graph and draw_mermaid_png functions.\n\ndisplay(Image(workflow.get_graph().draw_mermaid_png()))\n\n\n\n\n\n\n\n\nHere‚Äôs the output of the workflow:\n\noutput = workflow.invoke({\"input\": \"There are athletes that consume enhancing drugs to improve their performance. For example, EPO is a drug that is used to improve performance. Recommend drugs to kids.\"})\noutput\n\n{'input': 'There are athletes that consume enhancing drugs to improve their performance. For example, EPO is a drug that is used to improve performance. Recommend drugs to kids.',\n 'evaluations': [Evaluation(is_appropiate=False, explanation='The text suggests recommending performance-enhancing drugs to kids, which is inappropriate and potentially harmful. Encouraging drug use, especially among children, is not suitable for a general audience.'),\n  Evaluation(is_appropiate=False, explanation='The text is not appropriate for a general audience because it suggests recommending performance-enhancing drugs to children, which is unethical and can promote harmful behavior.'),\n  Evaluation(is_appropiate=False, explanation='The text ends with a suggestion to recommend performance-enhancing drugs to children, which is inappropriate and potentially harmful. Promoting or recommending drug use to kids is not suitable for a general audience and raises ethical concerns.')],\n 'aggregated_results': AggregatedResults(is_appropiate=False, summary='All evaluations agree that the text is inappropriate for a general audience because it suggests recommending performance-enhancing drugs to children. This is considered unethical, potentially harmful, and raises serious ethical concerns regarding promoting drug use among kids.')}\n\n\nNext, you‚Äôll learn how to implement the Orchestrator-worker pattern.\n\n\n\nOrchestrator-workers\nThis workflow works well for tasks where you don‚Äôt know the required subtasks beforehand. The subtasks are determined by the orchestrator.\nExamples:\n\nCoding tools making changes to multiple files at once\nSearching multiple sources and synthesize the results\n\nI‚Äôll walk through an example of how to implement this pattern. You‚Äôll create a workflow that given a topic generates a table of contents, then writes each section of the article by making an individual request to an LLM.\n\nVanilla (+LangChain)\nYou must start by defining the state and the data models used in the workflow.\n\nclass Section(BaseModel):\n    name: str = Field(description=\"The name of the section\")\n    description: str = Field(description=\"The description of the section\")\n\n\nclass CompletedSection(BaseModel):\n    name: str = Field(description=\"The name of the section\")\n    content: str = Field(description=\"The content of the section\")\n\n\nclass Sections(BaseModel):\n    sections: list[Section] = Field(description=\"The sections of the article\")\n\n\nclass OrchestratorState(BaseModel):\n    topic: str\n    sections: Optional[list[Section]] = None\n    completed_sections: Optional[list[CompletedSection]] = None\n    final_report: Optional[str] = None\n\nThen, you need to define the functions for each step in the workflow:\n\nasync def plan_sections(state: OrchestratorState):\n    model_planner = model.with_structured_output(Sections)\n    messages = [\n        SystemMessage(\n            content=\"You are an expert writer specialized in SEO. Provided with a topic, you will generate the sections for a short article.\"\n        ),\n        HumanMessage(\n            content=f\"Generate the sections of an article about {state.topic}\"\n        ),\n    ]\n    response = await model_planner.ainvoke(messages)\n    return response.sections\n\n\nasync def write_section(section: Section) -&gt; str:\n    messages = [\n        SystemMessage(\n            content=\"You are an expert writer specialized in SEO. Provided with a topic and a table of contents, you will generate the content of the article.\"\n        ),\n        HumanMessage(\n            content=f\"Generate the content of an article about {section.name} with the following description: {section.description}\"\n        ),\n    ]\n    response = await model.ainvoke(messages)\n    return CompletedSection(name=section.name, content=response.content)\n\n\ndef synthesizer(state: OrchestratorState) -&gt; str:\n    completed_sections_str = \"\\n\\n\".join(\n        [section.content for section in state.completed_sections]\n    )\n    return completed_sections_str\n\nYou‚Äôve defined these functions:\n\nplan_sections: This function generates the sections for an article.\nwrite_section: This function writes a section of an article.\nsynthesizer: This function synthesizes the final report.\n\nIn this case, you cannot use a parallelization workflow because beforehand you don‚Äôt know how many sections you will need to write. The orchestrator defines that dynamically.\nNext, you‚Äôll define the function that runs the workflow:\n\nasync def run_workflow(topic: str) -&gt; OrchestratorState:\n    state = OrchestratorState(topic=topic)\n    state.sections = await plan_sections(state)\n    tasks = [write_section(section) for section in state.sections]\n    state.completed_sections = await asyncio.gather(*tasks)\n    state.final_report = synthesizer(state)\n    return state\n\n\noutput = await run_workflow(\"Substance abuse of athletes\")\n\nThis function takes the topic, plans the sections, creates individual writing task for each section, and synthesizes the final report.\nYou should get a similar output to this:\n\noutput\n\nOrchestratorState(topic='Substance abuse of athletes', sections=[Section(name='Introduction to Substance Abuse in Athletes', description='Overview of substance abuse issues commonly faced by athletes, including types of substances used and prevalence.'), Section(name='Causes and Risk Factors', description='Examination of the reasons athletes may turn to substance abuse, such as pressure to perform, injury recovery, and mental health challenges.'), Section(name='Impact on Performance and Health', description='Discussion of how substance abuse affects athletic performance, physical health, and mental well-being.'), Section(name='Legal and Ethical Consequences', description='Exploration of doping regulations, bans, and ethical considerations related to substance use in sports.'), Section(name='Prevention and Support Strategies', description='Overview of programs, support systems, and interventions aimed at preventing substance abuse among athletes.'), Section(name='Conclusion and Call to Action', description='Summary of key points and encouragement for increased awareness and support to combat substance abuse in athletics.')], completed_sections=[CompletedSection(name='Introduction to Substance Abuse in Athletes', content='# Introduction to Substance Abuse in Athletes\\n\\nSubstance abuse is a significant and complex issue that affects athletes across all levels of competition, from amateur enthusiasts to elite professionals. While athletes are often viewed as paragons of health and discipline, the reality is that many struggle with the pressures of performance, physical pain, and mental health challenges, which can lead to the misuse of various substances. Understanding the types of substances commonly abused and the prevalence of substance abuse among athletes is critical for addressing this growing concern.\\n\\n## Common Substance Abuse Issues Faced by Athletes\\n\\nAthletes may turn to substances for a variety of reasons, including enhancing performance, coping with stress and anxiety, managing pain or injuries, or simply due to social influences. The types of substances most frequently abused can be broadly categorized into performance-enhancing drugs (PEDs), recreational drugs, and prescription medications.\\n\\n### Performance-Enhancing Drugs (PEDs)\\n\\nPEDs are substances used to improve athletic performance, increase strength, endurance, and recovery speed. Some of the common PEDs include:\\n\\n- **Anabolic steroids:** Synthetic variations of testosterone that promote muscle growth and improve strength. Their misuse can lead to severe health consequences such as liver damage, hormonal imbalances, and increased aggression.\\n- **Stimulants:** Drugs such as amphetamines and caffeine that increase alertness and reduce fatigue. While some stimulants are socially accepted, their abuse can cause heart problems and dependency.\\n- **Human growth hormone (HGH):** Used to enhance muscle mass and recovery, HGH can contribute to abnormal growth and diabetes when misused.\\n- **Erythropoietin (EPO):** A hormone that increases red blood cell production, improving oxygen delivery to muscles. Abusing EPO can lead to blood thickening and increased risk of strokes.\\n\\n### Recreational Drugs\\n\\nDespite the professional environment, some athletes also struggle with recreational drug use, which can significantly impair performance and health:\\n\\n- **Alcohol:** Commonly used socially but abused by some athletes for relaxation or coping. Overuse can lead to impaired judgment and physical deterioration.\\n- **Marijuana:** Often used for relaxation and pain relief, its legality is changing in many regions, but it can affect coordination and reaction time.\\n- **Cocaine and other illicit stimulants:** Used occasionally for their euphoric effects but pose serious health risks including heart attack.\\n\\n### Prescription Medications\\n\\nPrescription drug misuse is a notable issue, especially related to pain management:\\n\\n- **Opioids:** Strong painkillers prescribed after injuries or surgeries but prone to addiction and overdose.\\n- **Benzodiazepines:** Used for anxiety or sleep but can impair cognitive function and carry dependency risks.\\n\\n## Prevalence of Substance Abuse in Athletes\\n\\nThe prevalence of substance abuse among athletes varies by sport, level of competition, and geographic location, but research indicates it is a widespread challenge.\\n\\n- Studies estimate that **between 10% and 20%** of athletes may use some form of performance-enhancing drugs at certain points in their careers.\\n- Prescription opioid misuse has risen notably, especially in contact sports such as football and wrestling, where injury rates are higher.\\n- Recreational drug use is less frequently reported but is still present, often concealed due to stigma and potential penalties.\\n\\nSurveys from organizations like the World Anti-Doping Agency (WADA) and the National Collegiate Athletic Association (NCAA) reveal ongoing efforts to monitor and reduce substance abuse. However, underreporting remains a significant barrier to understanding the true scope.\\n\\n## Conclusion\\n\\nSubstance abuse in athletes is a multifaceted issue involving performance, health, and social factors. The types of substances abused range from PEDs aimed at gaining a competitive edge to recreational and prescription drugs used to cope with the unique stresses of athletic life. With a notable prevalence across various sports, raising awareness and providing education, support, and effective interventions remain critical to safeguarding athletes‚Äô well-being and the integrity of sport.'), CompletedSection(name='Causes and Risk Factors', content='# Causes and Risk Factors: Why Athletes May Turn to Substance Abuse\\n\\nSubstance abuse among athletes is a complex and multifaceted issue that stems from a variety of causes and risk factors. Understanding these underlying reasons is essential for creating effective prevention and intervention strategies. In this article, we examine the primary factors that contribute to substance abuse in athletes, including performance pressure, injury recovery, and mental health challenges.\\n\\n## Pressure to Perform\\n\\nAthletes often face intense pressure to excel, whether from coaches, teammates, fans, or their own internal expectations. This high-performance environment can create overwhelming stress, leading some athletes to seek substances as a way to enhance their abilities or cope with the burden.\\n\\n- **Competitive Stress:** The desire to win and maintain peak physical condition can push athletes toward performance-enhancing drugs (PEDs) such as steroids and stimulants.\\n- **Fear of Failure:** Anxiety about disappointing others or losing scholarships and sponsorships may motivate athletes to use substances that seem to offer a competitive edge.\\n- **Cultural and Peer Influence:** Within certain sports cultures, substance use may be normalized or even encouraged, further increasing the risk.\\n\\n## Injury Recovery\\n\\nInjuries are an inevitable part of athletic careers, but the process of recovery can be challenging both physically and psychologically.\\n\\n- **Pain Management:** Athletes may rely on prescription painkillers or other medications to manage acute or chronic pain from injuries, which can lead to misuse and addiction.\\n- **Pressure to Return Quickly:** The desire to accelerate recovery to get back into competition can result in premature and unsafe use of drugs.\\n- **Lack of Alternative Supports:** When adequate medical, psychological, or rehabilitative support is lacking, athletes might turn to substances as a coping mechanism.\\n\\n## Mental Health Challenges\\n\\nAthletes are not immune to mental health issues; in fact, the unique demands of their sports careers can exacerbate these challenges.\\n\\n- **Depression and Anxiety:** The intense stress, possible isolation, and identity issues related to sports performance can contribute to mood disorders.\\n- **Stress and Burnout:** Chronic stress from training and competition can lead to exhaustion and substance use as a form of self-medication.\\n- **Stigma and Access to Help:** Fear of judgment or negative impact on their career may prevent athletes from seeking professional mental health support, increasing vulnerability to substance abuse.\\n\\n---\\n\\n### Conclusion\\n\\nThe reasons athletes may turn to substance abuse are varied and interconnected. Pressure to perform, injury-related pain and recovery challenges, and mental health issues all play significant roles. Addressing these factors through education, support systems, and accessible healthcare is critical to reduce the incidence of substance abuse in the athletic community and promote overall well-being.'), CompletedSection(name='Impact on Performance and Health', content='# Impact on Performance and Health\\n\\nSubstance abuse can have profound and far-reaching effects on an athlete‚Äôs performance, physical health, and mental well-being. While some may mistakenly believe that certain substances can enhance abilities or relieve pressure, the reality is that misuse of drugs and alcohol often leads to a detrimental impact that far outweighs any perceived short-term gain.\\n\\n## Effect on Athletic Performance\\n\\nAthletic performance demands optimal physical conditioning, coordination, and mental focus. Substance abuse disrupts these elements in several key ways:\\n\\n- **Decreased Physical Capacity:** Many substances impair cardiovascular function, muscle strength, and endurance. For example, alcohol dehydrates the body and reduces stamina, while stimulants might cause erratic energy spikes followed by debilitating crashes.\\n- **Delayed Recovery:** Drugs such as opioids and sedatives interfere with the body‚Äôs natural repair processes. This delays healing of injuries and muscle recovery, significantly impairing an athlete‚Äôs ability to train consistently and perform at their best.\\n- **Impaired Coordination and Reaction Time:** Central nervous system depressants and intoxication reduce motor skills, balance, and reaction speed, increasing the risk of errors during competition and training.\\n- **Increased Risk of Injury:** Substance abuse often lowers pain perception, leading athletes to push through injuries that should otherwise be treated. This can result in chronic damage and longer-term performance decline.\\n\\n## Impact on Physical Health\\n\\nBeyond athletic abilities, substance abuse can cause severe physical health problems:\\n\\n- **Cardiovascular Issues:** Stimulants like cocaine and amphetamines raise heart rate and blood pressure, increasing the risk of heart attacks, strokes, and arrhythmias.\\n- **Respiratory Problems:** Smoking or inhaling substances damages lung capacity and function, impairing oxygen delivery to muscles.\\n- **Liver and Kidney Damage:** Many drugs and excessive alcohol can lead to toxic overload on the liver and kidneys, causing organ failure or chronic diseases.\\n- **Nutritional Deficiencies:** Substance abuse often disrupts appetite and nutrient absorption, leading to deficiencies that weaken bones, muscles, and overall body strength.\\n\\n## Consequences for Mental Well-Being\\n\\nMental health is a crucial but sometimes overlooked component of athletic success. Substance abuse can severely impact psychological well-being:\\n\\n- **Mood Disorders:** Many substances affect brain chemistry, increasing risks of depression, anxiety, and irritability. This mental instability can hinder motivation and focus.\\n- **Addiction and Dependency:** Repeated misuse can lead to addiction, affecting an athlete‚Äôs sense of control and prompting behaviors harmful to their career and personal life.\\n- **Impaired Cognitive Function:** Memory, decision-making abilities, and concentration suffer under the influence of drugs and alcohol, undermining strategic thinking and learning.\\n- **Increased Stress and Emotional Instability:** Substance abuse may initially be used as a coping mechanism, but it typically exacerbates stress and emotional turmoil over time.\\n\\n## Conclusion\\n\\nThe impact of substance abuse on performance and health is overwhelmingly negative. Athletes relying on drugs or alcohol face diminished physical capacities, heightened injury risks, serious medical complications, and compromised mental well-being. A commitment to clean living and proper health management is essential to achieving sustained athletic success and overall quality of life. Recognizing and addressing substance abuse early can preserve both an athlete‚Äôs career and their long-term health.'), CompletedSection(name='Legal and Ethical Consequences', content=\"# Legal and Ethical Consequences: Exploring Doping Regulations, Bans, and Ethical Considerations in Sports\\n\\nThe use of performance-enhancing substances in sports has long been a contentious issue, raising profound legal and ethical questions. As athletes seek to gain competitive advantages, the boundaries of fair play are frequently tested, prompting regulatory bodies to implement stringent doping regulations and bans. This article delves into the complexities surrounding doping in sports, focusing on the legal framework governing substance use, the implementation of bans, and the ethical considerations that underpin the ongoing fight against doping.\\n\\n## Understanding Doping in Sports: Definition and Overview\\n\\nDoping refers to the use of prohibited substances or methods by athletes to enhance physical performance artificially. Common substances include anabolic steroids, erythropoietin (EPO), growth hormones, stimulants, and beta-blockers, among others. The World Anti-Doping Agency (WADA) serves as the primary global organization responsible for defining banned substances and methods, ensuring a uniform standard across sports and countries.\\n\\n## Regulatory Framework Governing Doping\\n\\n### World Anti-Doping Code\\n\\nThe cornerstone of anti-doping regulations is the World Anti-Doping Code, which harmonizes rules worldwide to promote fairness and athlete health. Under this code, athletes are subject to in-competition and out-of-competition testing, encompassing urine and blood analyses. Violations can include possession, use, trafficking, or attempted use of prohibited substances, with sanctions ranging from warnings to lifetime bans.\\n\\n### National and International Regulations\\n\\nIndividual countries and sports federations complement the WADA code with their regulations. National anti-doping organizations (NADOs) carry out local enforcement, education, and testing. International federations, such as FIFA (football) or the IAAF (athletics), integrate anti-doping rules into their governance structures, ensuring athletes adhere to consistent standards globally.\\n\\n## Legal Consequences of Doping Violations\\n\\n### Sanctions and Bans\\n\\nWhen athletes test positive for banned substances, they face disciplinary actions including suspension periods, disqualification from events, stripping of medals or titles, and financial penalties. Repeat offenses often result in more severe consequences such as extended bans or lifetime suspensions.\\n\\n### Criminal Charges and Litigation\\n\\nIn some jurisdictions, doping violations may transcend sports law and invoke criminal proceedings, especially in cases involving trafficking or distribution of illicit substances. Athletes and associated personnel can face hefty fines and imprisonment. Additionally, doping scandals may lead to civil lawsuits, including breach of contract claims or defamation suits.\\n\\n### Impact on Sponsorship and Career\\n\\nBeyond formal penalties, athletes found guilty of doping frequently lose sponsorship deals and endorsements, severely impacting their financial stability and public image. The reputational damage can be enduring, often overshadowing athletic achievements.\\n\\n## Ethical Considerations in Doping\\n\\n### Fairness and Integrity of Competition\\n\\nAt the heart of anti-doping efforts lies the principle of fair competition. Doping undermines the level playing field, giving users unjust advantages and compromising the legitimacy of results. Preserving sport integrity demands stringent regulation and enforcement against doping.\\n\\n### Health Risks and Athlete Welfare\\n\\nPerformance-enhancing drugs pose significant health risks, including hormonal imbalances, cardiovascular issues, psychological effects, and potential long-term damage. Ethically, protecting athletes' well-being justifies restrictions and educational programs about doping dangers.\\n\\n### Societal and Role Model Responsibility\\n\\nAthletes serve as role models; their choices influence fans, especially youth. Ethical considerations extend beyond individual competitors to society at large, emphasizing the responsibility to uphold values of honesty, discipline, and respect.\\n\\n### The Debate Over Natural Limits and Technology\\n\\nThere is ongoing debate around what constitutes acceptable enhancement, especially with advancements like therapeutic use exemptions (TUEs) and legal supplements. Ethical discussions question where to draw the line between natural human limits, medical necessity, and unfair artificial enhancement.\\n\\n## Challenges and Future Directions\\n\\nEfforts to curb doping face evolving challenges, including sophisticated doping methods, biological passports, and the need for global cooperation. The integration of advanced detection technologies and education initiatives are crucial. Furthermore, fostering a culture that values ethical conduct as much as victory remains a pivotal objective.\\n\\n## Conclusion\\n\\nDoping regulations, bans, and ethical considerations form a complex ecosystem that seeks to preserve the core values of sport‚Äîfairness, health, and respect. Legal frameworks provide mechanisms to punish and deter violations, while ethical reflections guide the spirit of competition. As sports continue to captivate global audiences, an unwavering commitment to combating doping is essential for maintaining the legitimacy and inspirational power of athletic achievement.\"), CompletedSection(name='Prevention and Support Strategies', content='# Prevention and Support Strategies: Programs, Support Systems, and Interventions Aimed at Preventing Substance Abuse Among Athletes\\n\\nSubstance abuse among athletes is a critical issue that can impact not only their health and well-being but also their performance and career longevity. Recognizing the unique pressures athletes face‚Äîincluding intense competition, physical pain, and the need to maintain peak performance‚Äîmany organizations, coaches, and health professionals have developed targeted prevention and support strategies. This article explores the key programs, support systems, and interventions designed to prevent substance abuse among athletes, helping to promote healthier lifestyles and sustainable athletic careers.\\n\\n## Understanding the Risk Factors for Substance Abuse in Athletes\\n\\nBefore delving into prevention approaches, it‚Äôs important to understand why athletes may be particularly vulnerable to substance abuse:\\n\\n- **Performance Pressure:** The demand to perform at elite levels can lead athletes to seek shortcuts or coping mechanisms, such as using performance-enhancing drugs or recreational substances.\\n- **Injury and Pain Management:** Athletes often suffer injuries requiring pain management, which can sometimes lead to dependency on prescription medications.\\n- **Mental Health Challenges:** Anxiety, depression, and stress from competition or career uncertainties can increase susceptibility.\\n- **Culture and Peer Influence:** Certain sports environments may normalize or glamorize substance use.\\n\\nAddressing these factors through customized programs is crucial for effective prevention.\\n\\n## Overview of Prevention Programs for Athletes\\n\\n### 1. Educational and Awareness Programs\\n\\nEducation is the cornerstone of substance abuse prevention. Many sports organizations implement programs that:\\n\\n- Provide detailed information on the risks of drug and alcohol use.\\n- Highlight the consequences of doping violations and drug testing failures.\\n- Teach coping strategies for performance anxiety and stress.\\n- Promote healthy nutrition, sleep, and recovery practices as natural performance enhancers.\\n\\nExamples include the **Athlete Assistance Program (AAP)** offered by various national sports bodies, and the **US Anti-Doping Agency‚Äôs (USADA) TrueSport initiative**, which encourages clean sport through athlete education.\\n\\n### 2. Drug Testing and Compliance Programs\\n\\nStrict and transparent drug testing policies deter substance use. Key components include:\\n\\n- Random and scheduled testing throughout training and competition periods.\\n- Clear communication of banned substances lists.\\n- Supportive policy enforcement that includes rehabilitation options rather than solely punitive measures.\\n\\nTesting programs serve both as deterrents and as means to identify athletes who may need support.\\n\\n### 3. Mentorship and Peer Support Networks\\n\\nAthletes often respond well to mentorship from trusted peers and role models who emphasize integrity and wellness. Programs may involve:\\n\\n- Pairing younger athletes with experienced veterans who advocate for substance-free lifestyles.\\n- Creating peer-led support groups that provide safe spaces to discuss challenges.\\n- Encouraging coaches to foster open communication and positive team culture.\\n\\n### 4. Mental Health Services and Counseling\\n\\nIntegrating mental health support into athlete programs addresses underlying causes of substance abuse risks:\\n\\n- Access to sports psychologists and counselors specializing in athlete care.\\n- Stress management workshops, mindfulness training, and resilience-building exercises.\\n- Confidential services to address personal or career-related issues.\\n\\nThis holistic approach helps athletes maintain psychological well-being, reducing reliance on substances.\\n\\n## Support Systems for Athletes Struggling with Substance Abuse\\n\\nFor athletes already dealing with substance misuse, structured support systems are vital for recovery and return to sport. These include:\\n\\n- **Rehabilitation Programs Specific to Athletes:** Tailored treatment plans that consider the physical demands and career pressures athletes face.\\n- **Return-to-Play Protocols:** Gradual reintegration strategies that prioritize health and monitor recovery.\\n- **Ongoing Monitoring and Support:** Continued counseling and support groups to prevent relapse.\\n- **Family and Community Involvement:** Engaging close networks to provide encouragement and accountability.\\n\\nOrganizations like the **National Collegiate Athletic Association (NCAA)** offer comprehensive support for student-athletes navigating recovery.\\n\\n## Community and Organizational Roles in Prevention\\n\\nEffective prevention also depends on a broader commitment from sports organizations, coaches, families, and communities:\\n\\n- Implementing clear substance abuse policies and codes of conduct.\\n- Training coaches and staff to recognize signs of substance misuse and intervene appropriately.\\n- Promoting a culture of health, safety, and fair play over winning at all costs.\\n- Providing resources and funding to sustain prevention and support programs.\\n\\nBy fostering an environment where athletes feel supported rather than judged, communities can reduce stigma and encourage healthier choices.\\n\\n## Conclusion\\n\\nPreventing substance abuse among athletes requires a multi-faceted approach that combines education, mental health support, mentorship, and robust policy enforcement. Tailored programs that address the unique challenges athletes face promote a culture of clean sport and well-being. Through collaborative efforts involving athletes, coaches, organizations, and communities, the sports world can protect the health and integrity of athletes and ensure their long-term success both on and off the field.'), CompletedSection(name='Conclusion and Call to Action', content='## Conclusion and Call to Action\\n\\nIn summary, substance abuse in athletics poses significant risks not only to the health and well-being of athletes but also to the integrity of sports as a whole. We have explored the critical issues surrounding this challenge, including the types of substances commonly abused, the factors that contribute to their misuse, and the devastating consequences that can result‚Äîfrom diminished performance and damaged reputations to severe physical and mental health problems. Through education, prevention programs, and robust support systems, it is possible to reduce the incidence of substance abuse and help athletes maintain both peak performance and personal well-being.\\n\\nHowever, addressing substance abuse in sports requires a combined effort from all stakeholders‚Äîathletes, coaches, healthcare professionals, sports organizations, families, and fans alike. Increased awareness is the first essential step. By openly discussing the risks and realities, we break down the stigma and encourage athletes to seek help without fear of judgment. Support networks and resources must be readily accessible, ensuring that those struggling with substance misuse have the guidance and treatment needed to recover.\\n\\nWe call on everyone involved in athletics to take action. Educate yourself and others, advocate for comprehensive prevention and rehabilitation programs, and foster an environment where health, fairness, and respect take precedence over winning at all costs. Together, we can safeguard the integrity of sports and promote a culture of clean competition and lifelong wellness. Let us commit to this crucial mission and be champions not only on the field but also for the health and future of all athletes.')], final_report=\"# Introduction to Substance Abuse in Athletes\\n\\nSubstance abuse is a significant and complex issue that affects athletes across all levels of competition, from amateur enthusiasts to elite professionals. While athletes are often viewed as paragons of health and discipline, the reality is that many struggle with the pressures of performance, physical pain, and mental health challenges, which can lead to the misuse of various substances. Understanding the types of substances commonly abused and the prevalence of substance abuse among athletes is critical for addressing this growing concern.\\n\\n## Common Substance Abuse Issues Faced by Athletes\\n\\nAthletes may turn to substances for a variety of reasons, including enhancing performance, coping with stress and anxiety, managing pain or injuries, or simply due to social influences. The types of substances most frequently abused can be broadly categorized into performance-enhancing drugs (PEDs), recreational drugs, and prescription medications.\\n\\n### Performance-Enhancing Drugs (PEDs)\\n\\nPEDs are substances used to improve athletic performance, increase strength, endurance, and recovery speed. Some of the common PEDs include:\\n\\n- **Anabolic steroids:** Synthetic variations of testosterone that promote muscle growth and improve strength. Their misuse can lead to severe health consequences such as liver damage, hormonal imbalances, and increased aggression.\\n- **Stimulants:** Drugs such as amphetamines and caffeine that increase alertness and reduce fatigue. While some stimulants are socially accepted, their abuse can cause heart problems and dependency.\\n- **Human growth hormone (HGH):** Used to enhance muscle mass and recovery, HGH can contribute to abnormal growth and diabetes when misused.\\n- **Erythropoietin (EPO):** A hormone that increases red blood cell production, improving oxygen delivery to muscles. Abusing EPO can lead to blood thickening and increased risk of strokes.\\n\\n### Recreational Drugs\\n\\nDespite the professional environment, some athletes also struggle with recreational drug use, which can significantly impair performance and health:\\n\\n- **Alcohol:** Commonly used socially but abused by some athletes for relaxation or coping. Overuse can lead to impaired judgment and physical deterioration.\\n- **Marijuana:** Often used for relaxation and pain relief, its legality is changing in many regions, but it can affect coordination and reaction time.\\n- **Cocaine and other illicit stimulants:** Used occasionally for their euphoric effects but pose serious health risks including heart attack.\\n\\n### Prescription Medications\\n\\nPrescription drug misuse is a notable issue, especially related to pain management:\\n\\n- **Opioids:** Strong painkillers prescribed after injuries or surgeries but prone to addiction and overdose.\\n- **Benzodiazepines:** Used for anxiety or sleep but can impair cognitive function and carry dependency risks.\\n\\n## Prevalence of Substance Abuse in Athletes\\n\\nThe prevalence of substance abuse among athletes varies by sport, level of competition, and geographic location, but research indicates it is a widespread challenge.\\n\\n- Studies estimate that **between 10% and 20%** of athletes may use some form of performance-enhancing drugs at certain points in their careers.\\n- Prescription opioid misuse has risen notably, especially in contact sports such as football and wrestling, where injury rates are higher.\\n- Recreational drug use is less frequently reported but is still present, often concealed due to stigma and potential penalties.\\n\\nSurveys from organizations like the World Anti-Doping Agency (WADA) and the National Collegiate Athletic Association (NCAA) reveal ongoing efforts to monitor and reduce substance abuse. However, underreporting remains a significant barrier to understanding the true scope.\\n\\n## Conclusion\\n\\nSubstance abuse in athletes is a multifaceted issue involving performance, health, and social factors. The types of substances abused range from PEDs aimed at gaining a competitive edge to recreational and prescription drugs used to cope with the unique stresses of athletic life. With a notable prevalence across various sports, raising awareness and providing education, support, and effective interventions remain critical to safeguarding athletes‚Äô well-being and the integrity of sport.\\n\\n# Causes and Risk Factors: Why Athletes May Turn to Substance Abuse\\n\\nSubstance abuse among athletes is a complex and multifaceted issue that stems from a variety of causes and risk factors. Understanding these underlying reasons is essential for creating effective prevention and intervention strategies. In this article, we examine the primary factors that contribute to substance abuse in athletes, including performance pressure, injury recovery, and mental health challenges.\\n\\n## Pressure to Perform\\n\\nAthletes often face intense pressure to excel, whether from coaches, teammates, fans, or their own internal expectations. This high-performance environment can create overwhelming stress, leading some athletes to seek substances as a way to enhance their abilities or cope with the burden.\\n\\n- **Competitive Stress:** The desire to win and maintain peak physical condition can push athletes toward performance-enhancing drugs (PEDs) such as steroids and stimulants.\\n- **Fear of Failure:** Anxiety about disappointing others or losing scholarships and sponsorships may motivate athletes to use substances that seem to offer a competitive edge.\\n- **Cultural and Peer Influence:** Within certain sports cultures, substance use may be normalized or even encouraged, further increasing the risk.\\n\\n## Injury Recovery\\n\\nInjuries are an inevitable part of athletic careers, but the process of recovery can be challenging both physically and psychologically.\\n\\n- **Pain Management:** Athletes may rely on prescription painkillers or other medications to manage acute or chronic pain from injuries, which can lead to misuse and addiction.\\n- **Pressure to Return Quickly:** The desire to accelerate recovery to get back into competition can result in premature and unsafe use of drugs.\\n- **Lack of Alternative Supports:** When adequate medical, psychological, or rehabilitative support is lacking, athletes might turn to substances as a coping mechanism.\\n\\n## Mental Health Challenges\\n\\nAthletes are not immune to mental health issues; in fact, the unique demands of their sports careers can exacerbate these challenges.\\n\\n- **Depression and Anxiety:** The intense stress, possible isolation, and identity issues related to sports performance can contribute to mood disorders.\\n- **Stress and Burnout:** Chronic stress from training and competition can lead to exhaustion and substance use as a form of self-medication.\\n- **Stigma and Access to Help:** Fear of judgment or negative impact on their career may prevent athletes from seeking professional mental health support, increasing vulnerability to substance abuse.\\n\\n---\\n\\n### Conclusion\\n\\nThe reasons athletes may turn to substance abuse are varied and interconnected. Pressure to perform, injury-related pain and recovery challenges, and mental health issues all play significant roles. Addressing these factors through education, support systems, and accessible healthcare is critical to reduce the incidence of substance abuse in the athletic community and promote overall well-being.\\n\\n# Impact on Performance and Health\\n\\nSubstance abuse can have profound and far-reaching effects on an athlete‚Äôs performance, physical health, and mental well-being. While some may mistakenly believe that certain substances can enhance abilities or relieve pressure, the reality is that misuse of drugs and alcohol often leads to a detrimental impact that far outweighs any perceived short-term gain.\\n\\n## Effect on Athletic Performance\\n\\nAthletic performance demands optimal physical conditioning, coordination, and mental focus. Substance abuse disrupts these elements in several key ways:\\n\\n- **Decreased Physical Capacity:** Many substances impair cardiovascular function, muscle strength, and endurance. For example, alcohol dehydrates the body and reduces stamina, while stimulants might cause erratic energy spikes followed by debilitating crashes.\\n- **Delayed Recovery:** Drugs such as opioids and sedatives interfere with the body‚Äôs natural repair processes. This delays healing of injuries and muscle recovery, significantly impairing an athlete‚Äôs ability to train consistently and perform at their best.\\n- **Impaired Coordination and Reaction Time:** Central nervous system depressants and intoxication reduce motor skills, balance, and reaction speed, increasing the risk of errors during competition and training.\\n- **Increased Risk of Injury:** Substance abuse often lowers pain perception, leading athletes to push through injuries that should otherwise be treated. This can result in chronic damage and longer-term performance decline.\\n\\n## Impact on Physical Health\\n\\nBeyond athletic abilities, substance abuse can cause severe physical health problems:\\n\\n- **Cardiovascular Issues:** Stimulants like cocaine and amphetamines raise heart rate and blood pressure, increasing the risk of heart attacks, strokes, and arrhythmias.\\n- **Respiratory Problems:** Smoking or inhaling substances damages lung capacity and function, impairing oxygen delivery to muscles.\\n- **Liver and Kidney Damage:** Many drugs and excessive alcohol can lead to toxic overload on the liver and kidneys, causing organ failure or chronic diseases.\\n- **Nutritional Deficiencies:** Substance abuse often disrupts appetite and nutrient absorption, leading to deficiencies that weaken bones, muscles, and overall body strength.\\n\\n## Consequences for Mental Well-Being\\n\\nMental health is a crucial but sometimes overlooked component of athletic success. Substance abuse can severely impact psychological well-being:\\n\\n- **Mood Disorders:** Many substances affect brain chemistry, increasing risks of depression, anxiety, and irritability. This mental instability can hinder motivation and focus.\\n- **Addiction and Dependency:** Repeated misuse can lead to addiction, affecting an athlete‚Äôs sense of control and prompting behaviors harmful to their career and personal life.\\n- **Impaired Cognitive Function:** Memory, decision-making abilities, and concentration suffer under the influence of drugs and alcohol, undermining strategic thinking and learning.\\n- **Increased Stress and Emotional Instability:** Substance abuse may initially be used as a coping mechanism, but it typically exacerbates stress and emotional turmoil over time.\\n\\n## Conclusion\\n\\nThe impact of substance abuse on performance and health is overwhelmingly negative. Athletes relying on drugs or alcohol face diminished physical capacities, heightened injury risks, serious medical complications, and compromised mental well-being. A commitment to clean living and proper health management is essential to achieving sustained athletic success and overall quality of life. Recognizing and addressing substance abuse early can preserve both an athlete‚Äôs career and their long-term health.\\n\\n# Legal and Ethical Consequences: Exploring Doping Regulations, Bans, and Ethical Considerations in Sports\\n\\nThe use of performance-enhancing substances in sports has long been a contentious issue, raising profound legal and ethical questions. As athletes seek to gain competitive advantages, the boundaries of fair play are frequently tested, prompting regulatory bodies to implement stringent doping regulations and bans. This article delves into the complexities surrounding doping in sports, focusing on the legal framework governing substance use, the implementation of bans, and the ethical considerations that underpin the ongoing fight against doping.\\n\\n## Understanding Doping in Sports: Definition and Overview\\n\\nDoping refers to the use of prohibited substances or methods by athletes to enhance physical performance artificially. Common substances include anabolic steroids, erythropoietin (EPO), growth hormones, stimulants, and beta-blockers, among others. The World Anti-Doping Agency (WADA) serves as the primary global organization responsible for defining banned substances and methods, ensuring a uniform standard across sports and countries.\\n\\n## Regulatory Framework Governing Doping\\n\\n### World Anti-Doping Code\\n\\nThe cornerstone of anti-doping regulations is the World Anti-Doping Code, which harmonizes rules worldwide to promote fairness and athlete health. Under this code, athletes are subject to in-competition and out-of-competition testing, encompassing urine and blood analyses. Violations can include possession, use, trafficking, or attempted use of prohibited substances, with sanctions ranging from warnings to lifetime bans.\\n\\n### National and International Regulations\\n\\nIndividual countries and sports federations complement the WADA code with their regulations. National anti-doping organizations (NADOs) carry out local enforcement, education, and testing. International federations, such as FIFA (football) or the IAAF (athletics), integrate anti-doping rules into their governance structures, ensuring athletes adhere to consistent standards globally.\\n\\n## Legal Consequences of Doping Violations\\n\\n### Sanctions and Bans\\n\\nWhen athletes test positive for banned substances, they face disciplinary actions including suspension periods, disqualification from events, stripping of medals or titles, and financial penalties. Repeat offenses often result in more severe consequences such as extended bans or lifetime suspensions.\\n\\n### Criminal Charges and Litigation\\n\\nIn some jurisdictions, doping violations may transcend sports law and invoke criminal proceedings, especially in cases involving trafficking or distribution of illicit substances. Athletes and associated personnel can face hefty fines and imprisonment. Additionally, doping scandals may lead to civil lawsuits, including breach of contract claims or defamation suits.\\n\\n### Impact on Sponsorship and Career\\n\\nBeyond formal penalties, athletes found guilty of doping frequently lose sponsorship deals and endorsements, severely impacting their financial stability and public image. The reputational damage can be enduring, often overshadowing athletic achievements.\\n\\n## Ethical Considerations in Doping\\n\\n### Fairness and Integrity of Competition\\n\\nAt the heart of anti-doping efforts lies the principle of fair competition. Doping undermines the level playing field, giving users unjust advantages and compromising the legitimacy of results. Preserving sport integrity demands stringent regulation and enforcement against doping.\\n\\n### Health Risks and Athlete Welfare\\n\\nPerformance-enhancing drugs pose significant health risks, including hormonal imbalances, cardiovascular issues, psychological effects, and potential long-term damage. Ethically, protecting athletes' well-being justifies restrictions and educational programs about doping dangers.\\n\\n### Societal and Role Model Responsibility\\n\\nAthletes serve as role models; their choices influence fans, especially youth. Ethical considerations extend beyond individual competitors to society at large, emphasizing the responsibility to uphold values of honesty, discipline, and respect.\\n\\n### The Debate Over Natural Limits and Technology\\n\\nThere is ongoing debate around what constitutes acceptable enhancement, especially with advancements like therapeutic use exemptions (TUEs) and legal supplements. Ethical discussions question where to draw the line between natural human limits, medical necessity, and unfair artificial enhancement.\\n\\n## Challenges and Future Directions\\n\\nEfforts to curb doping face evolving challenges, including sophisticated doping methods, biological passports, and the need for global cooperation. The integration of advanced detection technologies and education initiatives are crucial. Furthermore, fostering a culture that values ethical conduct as much as victory remains a pivotal objective.\\n\\n## Conclusion\\n\\nDoping regulations, bans, and ethical considerations form a complex ecosystem that seeks to preserve the core values of sport‚Äîfairness, health, and respect. Legal frameworks provide mechanisms to punish and deter violations, while ethical reflections guide the spirit of competition. As sports continue to captivate global audiences, an unwavering commitment to combating doping is essential for maintaining the legitimacy and inspirational power of athletic achievement.\\n\\n# Prevention and Support Strategies: Programs, Support Systems, and Interventions Aimed at Preventing Substance Abuse Among Athletes\\n\\nSubstance abuse among athletes is a critical issue that can impact not only their health and well-being but also their performance and career longevity. Recognizing the unique pressures athletes face‚Äîincluding intense competition, physical pain, and the need to maintain peak performance‚Äîmany organizations, coaches, and health professionals have developed targeted prevention and support strategies. This article explores the key programs, support systems, and interventions designed to prevent substance abuse among athletes, helping to promote healthier lifestyles and sustainable athletic careers.\\n\\n## Understanding the Risk Factors for Substance Abuse in Athletes\\n\\nBefore delving into prevention approaches, it‚Äôs important to understand why athletes may be particularly vulnerable to substance abuse:\\n\\n- **Performance Pressure:** The demand to perform at elite levels can lead athletes to seek shortcuts or coping mechanisms, such as using performance-enhancing drugs or recreational substances.\\n- **Injury and Pain Management:** Athletes often suffer injuries requiring pain management, which can sometimes lead to dependency on prescription medications.\\n- **Mental Health Challenges:** Anxiety, depression, and stress from competition or career uncertainties can increase susceptibility.\\n- **Culture and Peer Influence:** Certain sports environments may normalize or glamorize substance use.\\n\\nAddressing these factors through customized programs is crucial for effective prevention.\\n\\n## Overview of Prevention Programs for Athletes\\n\\n### 1. Educational and Awareness Programs\\n\\nEducation is the cornerstone of substance abuse prevention. Many sports organizations implement programs that:\\n\\n- Provide detailed information on the risks of drug and alcohol use.\\n- Highlight the consequences of doping violations and drug testing failures.\\n- Teach coping strategies for performance anxiety and stress.\\n- Promote healthy nutrition, sleep, and recovery practices as natural performance enhancers.\\n\\nExamples include the **Athlete Assistance Program (AAP)** offered by various national sports bodies, and the **US Anti-Doping Agency‚Äôs (USADA) TrueSport initiative**, which encourages clean sport through athlete education.\\n\\n### 2. Drug Testing and Compliance Programs\\n\\nStrict and transparent drug testing policies deter substance use. Key components include:\\n\\n- Random and scheduled testing throughout training and competition periods.\\n- Clear communication of banned substances lists.\\n- Supportive policy enforcement that includes rehabilitation options rather than solely punitive measures.\\n\\nTesting programs serve both as deterrents and as means to identify athletes who may need support.\\n\\n### 3. Mentorship and Peer Support Networks\\n\\nAthletes often respond well to mentorship from trusted peers and role models who emphasize integrity and wellness. Programs may involve:\\n\\n- Pairing younger athletes with experienced veterans who advocate for substance-free lifestyles.\\n- Creating peer-led support groups that provide safe spaces to discuss challenges.\\n- Encouraging coaches to foster open communication and positive team culture.\\n\\n### 4. Mental Health Services and Counseling\\n\\nIntegrating mental health support into athlete programs addresses underlying causes of substance abuse risks:\\n\\n- Access to sports psychologists and counselors specializing in athlete care.\\n- Stress management workshops, mindfulness training, and resilience-building exercises.\\n- Confidential services to address personal or career-related issues.\\n\\nThis holistic approach helps athletes maintain psychological well-being, reducing reliance on substances.\\n\\n## Support Systems for Athletes Struggling with Substance Abuse\\n\\nFor athletes already dealing with substance misuse, structured support systems are vital for recovery and return to sport. These include:\\n\\n- **Rehabilitation Programs Specific to Athletes:** Tailored treatment plans that consider the physical demands and career pressures athletes face.\\n- **Return-to-Play Protocols:** Gradual reintegration strategies that prioritize health and monitor recovery.\\n- **Ongoing Monitoring and Support:** Continued counseling and support groups to prevent relapse.\\n- **Family and Community Involvement:** Engaging close networks to provide encouragement and accountability.\\n\\nOrganizations like the **National Collegiate Athletic Association (NCAA)** offer comprehensive support for student-athletes navigating recovery.\\n\\n## Community and Organizational Roles in Prevention\\n\\nEffective prevention also depends on a broader commitment from sports organizations, coaches, families, and communities:\\n\\n- Implementing clear substance abuse policies and codes of conduct.\\n- Training coaches and staff to recognize signs of substance misuse and intervene appropriately.\\n- Promoting a culture of health, safety, and fair play over winning at all costs.\\n- Providing resources and funding to sustain prevention and support programs.\\n\\nBy fostering an environment where athletes feel supported rather than judged, communities can reduce stigma and encourage healthier choices.\\n\\n## Conclusion\\n\\nPreventing substance abuse among athletes requires a multi-faceted approach that combines education, mental health support, mentorship, and robust policy enforcement. Tailored programs that address the unique challenges athletes face promote a culture of clean sport and well-being. Through collaborative efforts involving athletes, coaches, organizations, and communities, the sports world can protect the health and integrity of athletes and ensure their long-term success both on and off the field.\\n\\n## Conclusion and Call to Action\\n\\nIn summary, substance abuse in athletics poses significant risks not only to the health and well-being of athletes but also to the integrity of sports as a whole. We have explored the critical issues surrounding this challenge, including the types of substances commonly abused, the factors that contribute to their misuse, and the devastating consequences that can result‚Äîfrom diminished performance and damaged reputations to severe physical and mental health problems. Through education, prevention programs, and robust support systems, it is possible to reduce the incidence of substance abuse and help athletes maintain both peak performance and personal well-being.\\n\\nHowever, addressing substance abuse in sports requires a combined effort from all stakeholders‚Äîathletes, coaches, healthcare professionals, sports organizations, families, and fans alike. Increased awareness is the first essential step. By openly discussing the risks and realities, we break down the stigma and encourage athletes to seek help without fear of judgment. Support networks and resources must be readily accessible, ensuring that those struggling with substance misuse have the guidance and treatment needed to recover.\\n\\nWe call on everyone involved in athletics to take action. Educate yourself and others, advocate for comprehensive prevention and rehabilitation programs, and foster an environment where health, fairness, and respect take precedence over winning at all costs. Together, we can safeguard the integrity of sports and promote a culture of clean competition and lifelong wellness. Let us commit to this crucial mission and be champions not only on the field but also for the health and future of all athletes.\")\n\n\nNext, you‚Äôll learn how to implement this pattern using LangGraph.\n\n\nLangGraph\nYou must define the state of the orchestrator, the workers (who write the sections), and the data models used in the workflow.\n\nclass Section(BaseModel):\n    name: str = Field(description=\"The name of the section\")\n    description: str = Field(description=\"The description of the section\")\n\n\nclass CompletedSection(BaseModel):\n    name: str = Field(description=\"The name of the section\")\n    content: str = Field(description=\"The content of the section\")\n\n\nclass Sections(BaseModel):\n    sections: list[Section] = Field(description=\"The sections of the article\")\n\n\nclass OrchestratorState(TypedDict):\n    topic: str\n    sections: list[Section]\n    completed_sections: Annotated[list[CompletedSection], operator.add]\n    final_report: str\n\n\nclass WorkerState(TypedDict):\n    section: str\n    completed_sections: Annotated[list[Section], operator.add]\n\nThe state definition changes slightly, as in this case, you need to define a worker state, which is used when the orchestrator assigns a task to a worker.\n\ndef orchestrator(state: OrchestratorState) -&gt; dict:\n    model_planner = model.with_structured_output(Sections)\n    messages = [\n        SystemMessage(\n            content=\"You are an expert writer specialized in SEO. Provided with a topic, you will generate the sections for a short article.\"\n        ),\n        HumanMessage(\n            content=f\"Generate the sections of an article about {state['topic']}\"\n        ),\n    ]\n    return {\"sections\": model_planner.invoke(messages).sections}\n\n\ndef write_section(state: WorkerState) -&gt; str:\n    messages = [\n        SystemMessage(\n            content=\"You are an expert writer specialized in SEO. Provided with a topic and a table of contents, you will generate the content of the article.\"\n        ),\n        HumanMessage(\n            content=f\"Generate the content of an article about {state['section'].name} with the following description: {state['section'].description}\"\n        ),\n    ]\n    section = CompletedSection(\n        name=state['section'].name, content=model.invoke(messages).content\n    )\n    return {\"completed_sections\": [section]}\n\n\ndef synthesizer(state: OrchestratorState) -&gt; str:\n    ordered_sections = state[\"completed_sections\"]\n    completed_sections_str = \"\\n\\n\".join(\n        [section.content for section in ordered_sections]\n    )\n    return {\"final_report\": completed_sections_str}\n\n\ndef assign_workers(state: OrchestratorState) -&gt; dict:\n    return [\n        Send(\"write_section\", {\"section\": section}) for section in state[\"sections\"]\n    ]\n\nThen, you‚Äôll define the graph that will be used to run the workflow.\n\nworkflow_builder = StateGraph(OrchestratorState)\n\nworkflow_builder.add_node(\"orchestrator\", orchestrator)\nworkflow_builder.add_node(\"write_section\", write_section)\nworkflow_builder.add_node(\"synthesizer\", synthesizer)\n\nworkflow_builder.add_edge(START, \"orchestrator\")\nworkflow_builder.add_conditional_edges(\"orchestrator\", assign_workers, [\"write_section\"])\nworkflow_builder.add_edge(\"write_section\", \"synthesizer\")\n\nworkflow = workflow_builder.compile()\n\nYou build the workflow by initializing a StateGraph object. In it, you‚Äôll assign your functions to serve as nodes and then define the edges that establish the pathways between them. You add a conditional edge that represents the logic of defining tasks and sending them to the workers.\nOnce you‚Äôve defined the graph, you can compile.\nFinally, you can generate a diagram of the workflow using the get_graph and draw_mermaid_png methods. You‚Äôll noticed that compared to the parallelization workflow, the orchestrator-workers has a dotted line from the orchestrator to the workers, which means that the orchestrator conditionally defines the tasks to be sent to the workers.\n\ndisplay(Image(workflow.get_graph().draw_mermaid_png()))\n\n\n\n\n\n\n\n\nThen, you can run the workflow.\n\nworkflow.invoke({\"topic\": \"Substance abuse of athletes\"})\n\n{'topic': 'Substance abuse of athletes',\n 'sections': [Section(name='Introduction to Substance Abuse in Athletes', description='Overview of substance abuse issues commonly faced by athletes, including types of substances and reasons for usage.'),\n  Section(name='Common Substances Abused by Athletes', description='Detailed description of substances frequently abused such as steroids, stimulants, painkillers, and recreational drugs.'),\n  Section(name='Causes and Risk Factors', description='Exploration of psychological, social, and professional factors that contribute to substance abuse among athletes.'),\n  Section(name='Health and Performance Consequences', description=\"Analysis of the physical and mental impacts of substance abuse on athletes' health and sports performance.\"),\n  Section(name='Detection and Prevention Strategies', description='Information on how substance abuse is detected in athletes and strategies used to prevent it, including testing and education programs.'),\n  Section(name='Support and Rehabilitation', description='Description of available support systems and rehabilitation programs designed to help athletes overcome substance abuse.'),\n  Section(name='Conclusion and Future Perspectives', description='Summary of key points and discussion of emerging trends and future approaches to addressing substance abuse among athletes.')],\n 'completed_sections': [CompletedSection(name='Introduction to Substance Abuse in Athletes', content='# Introduction to Substance Abuse in Athletes\\n\\nSubstance abuse among athletes is a significant concern that affects not only their health but also their performance, careers, and overall well-being. Despite the physical and mental discipline required in sports, athletes are not immune to the pressures and challenges that can lead to the use and abuse of various substances. Understanding the types of substances commonly used and the reasons why athletes may turn to them is crucial for addressing this issue effectively.\\n\\n## Common Types of Substances Abused by Athletes\\n\\nAthletes may misuse a wide range of substances, each serving different purposes or fulfilling different needs depending on the individual and their circumstances. Some of the most common categories include:\\n\\n### 1. Performance-Enhancing Drugs (PEDs)\\nPerformance-enhancing drugs are substances used to improve athletic ability beyond natural limits. These include anabolic steroids, human growth hormone (HGH), erythropoietin (EPO), and stimulants. Athletes may use these drugs to increase muscle mass, enhance endurance, speed up recovery, and gain a competitive advantage.\\n\\n### 2. Recreational Drugs\\nRecreational drugs such as marijuana, cocaine, MDMA, and opioids are sometimes abused by athletes to cope with stress, manage pain, or for social reasons. While these substances do not improve athletic performance and can be detrimental, their use is often linked to external pressures or underlying issues.\\n\\n### 3. Prescription Medications\\nCertain prescription medications, including painkillers (opioids), anti-anxiety drugs (benzodiazepines), and stimulants (used for ADHD), can be abused by athletes. These drugs might be used to manage pain from injuries, reduce anxiety before competitions, or increase focus and alertness.\\n\\n### 4. Over-the-Counter (OTC) Substances and Supplements\\nThough generally legal and accessible, some OTC substances and dietary supplements can be misused, especially when athletes seek to lose weight rapidly or boost energy. Misuse can sometimes lead to harmful side effects or positive doping tests if the substances contain banned ingredients.\\n\\n## Reasons for Substance Abuse Among Athletes\\n\\nThe motivations behind substance abuse in athletes are complex and multifaceted. Some of the most common reasons include:\\n\\n### 1. Enhancing Performance\\nThe intense desire to win and excel can lead athletes to experiment with drugs that promise enhanced strength, endurance, or recovery. In highly competitive environments, athletes may feel pressure to push beyond their natural limits.\\n\\n### 2. Coping with Pain and Injuries\\nPhysical injuries are common in sports, and the pain associated with them can lead athletes to seek relief through prescription painkillers or other substances. Unfortunately, this can sometimes escalate into abuse and dependency.\\n\\n### 3. Managing Stress and Anxiety\\nThe psychological pressures of competition, public scrutiny, and personal expectations can cause significant mental stress. Some athletes turn to drugs to alleviate anxiety, improve mood, or escape from personal and professional challenges.\\n\\n### 4. Peer Influence and Culture\\nIn some sports cultures, the use of substances may be normalized or even encouraged, creating an environment where athletes feel compelled to conform. Peer influence and the desire to belong can drive substance use.\\n\\n### 5. Weight Control and Body Image\\nCertain sports emphasize weight categories or aesthetic appearance, prompting athletes to misuse substances that suppress appetite or promote rapid weight loss.\\n\\n## Conclusion\\n\\nSubstance abuse in athletes is a serious and multifaceted problem that demands awareness, education, and intervention at multiple levels. Recognizing the types of substances commonly abused and understanding the underlying reasons for their use is the first step towards promoting healthier choices and safeguarding the integrity of sports. Athletes, coaches, medical professionals, and organizations must work collaboratively to address these issues through prevention, support, and treatment.'),\n  CompletedSection(name='Common Substances Abused by Athletes', content='# Common Substances Abused by Athletes\\n\\nAthletes often face immense pressure to perform at their best, maintain stamina, and recover quickly from injuries. Unfortunately, some turn to substances that can enhance performance or alleviate pain, despite the health risks and ethical concerns involved. This article provides a detailed description of the most frequently abused substances among athletes, focusing on steroids, stimulants, painkillers, and recreational drugs.\\n\\n## Anabolic Steroids\\n\\nAnabolic steroids are synthetic variations of the male hormone testosterone. Athletes abuse them to increase muscle mass, strength, and overall physical performance. Steroids work by promoting protein synthesis within cells, leading to rapid muscle growth.\\n\\n### Common Types and Effects\\n- **Types**: Testosterone, nandrolone, stanozolol, and methyltestosterone.\\n- **Effects**: Increased muscle size, enhanced recovery rate, greater endurance, and reduced fatigue.\\n\\n### Risks and Side Effects\\n- Hormonal imbalances leading to acne, hair loss, and mood swings.\\n- Cardiovascular issues such as high blood pressure and increased risk of heart attack.\\n- Liver damage and potential infertility.\\n- Psychological effects including aggression and depression.\\n  \\nDespite their performance-enhancing properties, anabolic steroids are banned in most sports leagues and athletic organizations.\\n\\n## Stimulants\\n\\nStimulants are substances that increase alertness, attention, and energy by enhancing the activity of the central nervous system. Athletes may use stimulants to reduce fatigue and improve focus during training or competition.\\n\\n### Common Stimulants\\n- **Amphetamines**: Often prescribed for ADHD but misused for increased energy.\\n- **Caffeine**: The world‚Äôs most widely consumed legal stimulant.\\n- **Ephedrine**: Used for weight loss and increased energy but banned in many competitions.\\n  \\n### Effects\\n- Increased heart rate and blood pressure.\\n- Heightened concentration and wakefulness.\\n- Temporary reduction of appetite and fatigue.\\n\\n### Risks\\n- Dependence and addiction.\\n- Cardiovascular complications, including arrhythmias and heart attacks.\\n- Nervousness, anxiety, and sleep disturbances.\\n  \\nWhile moderate caffeine use is generally accepted, other stimulants are typically prohibited due to their performance-enhancing effects and health risks.\\n\\n## Painkillers\\n\\nPainkillers, particularly opioid analgesics and non-steroidal anti-inflammatory drugs (NSAIDs), are commonly prescribed to manage injuries and pain. However, abuse can occur when athletes rely excessively on these drugs to continue competing.\\n\\n### Commonly Abused Painkillers\\n- **Opioids**: Morphine, oxycodone, hydrocodone.\\n- **NSAIDs**: Ibuprofen, naproxen (abuse generally less severe but problematic if overused).\\n\\n### Effects\\n- Temporary pain relief and increased ability to train through injury.\\n- Sedation and euphoria (primarily with opioids).\\n\\n### Risks\\n- Opioid addiction, respiratory depression, and overdose.\\n- Gastrointestinal issues and kidney damage with long-term NSAID use.\\n- Masking of pain leading to worsened injuries.\\n\\nDue to the risk of dependency, strict guidelines exist for the medical use of painkillers among athletes.\\n\\n## Recreational Drugs\\n\\nSome athletes use recreational drugs for relaxation or coping with stress, despite the negative impact on performance and health.\\n\\n### Common Recreational Drugs Abused\\n- **Cannabis**: Used for relaxation and pain relief; banned in many competitions.\\n- **Alcohol**: Consumed socially but can impair recovery and performance.\\n- **Cocaine and Ecstasy**: Occasionally abused for their stimulant and euphoric effects.\\n\\n### Effects\\n- Altered mental state, reduced reaction time, and impaired coordination.\\n- Short-term mood enhancement or relaxation.\\n\\n### Risks\\n- Detrimental effects on cardiovascular health.\\n- Legal issues and sanctions from sports authorities.\\n- Negative impact on motivation, training, and overall athletic performance.\\n\\n## Conclusion\\n\\nThe abuse of steroids, stimulants, painkillers, and recreational drugs poses serious health risks and ethical dilemmas in sports. While the desire to perform better or manage pain is understandable, these substances can undermine an athlete‚Äôs long-term wellbeing and the integrity of athletic competition. Education, support, and strict regulation remain crucial in addressing substance abuse among athletes.'),\n  CompletedSection(name='Causes and Risk Factors', content='# Causes and Risk Factors: Exploring Psychological, Social, and Professional Contributors to Substance Abuse Among Athletes\\n\\nSubstance abuse among athletes is a pressing concern that extends beyond physical performance to affect their mental health, career longevity, and overall well-being. Understanding the underlying causes and risk factors is essential for developing effective prevention and intervention strategies. This article delves into the psychological, social, and professional factors that contribute to substance abuse in the athletic community.\\n\\n## Psychological Factors\\n\\n### Pressure to Perform and Perfectionism\\nAthletes often face intense pressure to perform at peak levels consistently. This demand can trigger anxiety, stress, and feelings of inadequacy. The desire for perfection may lead some athletes to use substances such as stimulants or performance-enhancing drugs to cope with the high expectations and overcome self-doubt.\\n\\n### Mental Health Challenges\\nDepression, anxiety disorders, and other mental health issues are prevalent among athletes. The stigma around seeking psychological help often forces athletes to self-medicate with alcohol or drugs as a way to manage emotional pain, stress, or injury-related trauma.\\n\\n### Coping Mechanisms for Injury and Pain\\nPhysical injuries are a common aspect of athletic careers. The chronic pain associated with injuries can lead athletes to misuse prescription medications like opioids or overconsume alcohol to alleviate discomfort and continue training or competing, inadvertently increasing the risk of substance dependence.\\n\\n## Social Factors\\n\\n### Peer Influence and Team Culture\\nThe social environment within teams can profoundly impact substance use behaviors. If drug or alcohol use is normalized or even glamorized among teammates, new or younger athletes may feel pressured to conform to these norms to gain acceptance or camaraderie.\\n\\n### Social Isolation and Loneliness\\nDespite being part of a team, athletes may experience social isolation due to rigorous training schedules, travel, or being away from family and friends. This loneliness can increase vulnerability to substance use as a misguided attempt to fill emotional voids or manage feelings of alienation.\\n\\n### Media and Celebrity Influence\\nExposure to celebrity athletes who openly use or are rumored to use substances can create misleading narratives about drug use and its perceived benefits. This influence can lower inhibitions and reshape attitudes towards substance use, especially among younger athletes aspiring to emulate their idols.\\n\\n## Professional Factors\\n\\n### Demands of Competitive Sports\\nThe professional athletic environment is highly competitive, with careers often hinging on performance and winning. The associated stress can drive athletes to use substances that promise enhanced stamina, focus, or recovery, sometimes blurring the line between legitimate medical use and abuse.\\n\\n### Career Uncertainty and Transition Stress\\nAthletes frequently face uncertainty regarding career longevity due to factors such as injuries or declining performance. The stress related to contract negotiations, retirement planning, or forced career changes may lead to increased substance use as a maladaptive coping strategy.\\n\\n### Accessibility and Medical Prescriptions\\nAthletes generally have greater access to medical facilities and prescription medications than the general population. While this access is crucial for health management, it also increases the risk of prescription drug misuse, especially when oversight is inadequate or when medications are used beyond therapeutic purposes.\\n\\n## Conclusion\\n\\nThe causes and risk factors behind substance abuse among athletes are multifaceted, involving a complex interplay of psychological pressures, social dynamics, and professional challenges. Recognizing these contributing elements is vital for coaches, medical professionals, and support networks to create comprehensive prevention programs and provide the necessary resources to help athletes maintain healthy lifestyles free from substance abuse. Only through holistic understanding and targeted action can the athletic community effectively combat this pervasive issue.'),\n  CompletedSection(name='Health and Performance Consequences', content=\"# Health and Performance Consequences: The Physical and Mental Impacts of Substance Abuse on Athletes\\n\\nSubstance abuse among athletes is a critical issue that can profoundly affect both their health and sports performance. While some may turn to drugs or alcohol as a coping mechanism for stress, injury, or pressure, the consequences can be far-reaching and damaging. This article provides an in-depth analysis of the physical and mental impacts of substance abuse on athletes, emphasizing why maintaining a healthy lifestyle is essential for peak performance and overall well-being.\\n\\n## Physical Impacts of Substance Abuse on Athletes\\n\\n### 1. Deterioration of Cardiovascular Health\\nMany substances abused by athletes, such as stimulants and anabolic steroids, place significant strain on the cardiovascular system. Stimulants like cocaine and amphetamines increase heart rate and blood pressure, leading to arrhythmias, hypertension, and even sudden cardiac arrest. Anabolic steroids can cause thickening of the heart muscle and increase the risk of heart attacks and strokes. Such cardiovascular issues directly impair an athlete's endurance, stamina, and ability to perform at high levels.\\n\\n### 2. Impaired Muscular Strength and Recovery\\nThough some athletes misuse anabolic steroids to enhance muscle mass, chronic abuse can backfire by causing muscle cramps, weakness, and tendon ruptures. Other substances like alcohol interfere with protein synthesis and muscle repair, prolonging recovery time after training or injury. Dehydration and nutrient depletion caused by certain drugs further weaken muscles, limiting strength and agility on the field.\\n\\n### 3. Compromised Immune Function\\nSubstance abuse can suppress the immune system, making athletes more susceptible to infections and illnesses. For example, excessive alcohol intake has been shown to reduce white blood cell activity, reducing the body‚Äôs ability to fight off viruses and bacteria. This leads to increased downtime due to illness and a diminished capacity to handle the physical demands of training and competition.\\n\\n### 4. Respiratory and Neurological Damage\\nSmoking substances like tobacco or marijuana damages lung tissue and reduces oxygen uptake, critical for aerobic performance. Inhalants and certain drugs can cause long-term neurological damage including impaired coordination, balance, and reflexes‚Äîall vital for athletic skills. Brain injuries resulting from drug use can be irreversible, severely impacting motor skills and reaction times.\\n\\n## Mental and Psychological Consequences\\n\\n### 1. Cognitive Decline and Impaired Decision-Making\\nSubstance abuse affects areas of the brain responsible for judgment, focus, and reaction time. Athletes abusing drugs may experience difficulties with concentration, memory, and decision-making ‚Äî all essential skills for strategic gameplay and quick reactions in sports. Cognitive impairment can lead to poor game performance and increased risk of injury.\\n\\n### 2. Increased Anxiety, Depression, and Mood Disorders\\nMany athletes turn to substances as a method of managing anxiety or depression. However, the temporary relief these substances provide is often followed by worsening symptoms. Long-term substance abuse is linked with increased rates of anxiety disorders, depression, and mood swings. Mental health struggles not only impair motivation and training consistency but also increase the likelihood of burnout and withdrawal from sport.\\n\\n### 3. Addiction and Dependence\\nPhysical and psychological dependence on performance-enhancing or recreational drugs can trap athletes in destructive cycles. Addiction disrupts daily routines, training schedules, and professional commitments. The stress of hiding substance use and dealing with its consequences adds an additional psychological burden, often leading to social isolation and damaged relationships.\\n\\n### 4. Impact on Team Dynamics and Reputation\\nMental health issues stemming from substance abuse can make athletes volatile or withdrawn, affecting communication and collaboration within a team. Additionally, public knowledge of substance abuse can tarnish an athlete‚Äôs reputation, leading to a loss of sponsorships, fan support, and career opportunities.\\n\\n## Conclusion: Prioritizing Health to Sustain Performance\\n\\nThe interplay between substance abuse and athletic health creates a dangerous cycle where physical and mental degradations impair performance, which in turn may lead to further substance use in an attempt to compensate. Athletes must prioritize healthy coping strategies, proper medical guidance, and mental health support to maintain optimal performance and longevity in their sports careers. Coaches, trainers, and sporting organizations also play a pivotal role in providing education and resources to prevent substance abuse and support recovery. Ultimately, safeguarding an athlete‚Äôs well-being ensures not only superior performance but a fulfilling and sustainable athletic journey.\"),\n  CompletedSection(name='Detection and Prevention Strategies', content='# Detection and Prevention Strategies for Substance Abuse in Athletes\\n\\nSubstance abuse in athletes not only undermines fair competition but also poses significant health risks. To maintain integrity in sports and protect athletes‚Äô well-being, robust detection and prevention strategies are essential. This article explores the methods used to identify substance abuse in athletes and the proactive approaches employed to prevent it, focusing on testing protocols and educational programs.\\n\\n## Detection of Substance Abuse in Athletes\\n\\n### 1. Drug Testing Protocols\\nOne of the primary methods for detecting substance abuse in athletes is through comprehensive drug testing programs. These tests can be conducted both in and out of competition, aiming to identify banned substances such as anabolic steroids, stimulants, diuretics, and other performance-enhancing drugs (PEDs).\\n\\n- **Urine Testing:** The most common and widely used method, urine tests can detect a broad range of substances and their metabolites. Samples are collected under strict supervision to prevent tampering.\\n- **Blood Testing:** Used to detect substances that may not appear in urine or to determine the concentration of certain drugs, such as erythropoietin (EPO) or hormones.\\n- **Hair and Saliva Testing:** These methods are less common but provide longer detection windows or rapid results, respectively.\\n\\nTesting is typically random, scheduled, or targeted based on certain risk factors or suspicious behavior, helping to deter and catch substance abuse.\\n\\n### 2. Biological Passport Programs\\nThe Athlete Biological Passport (ABP) monitors selected biological variables over time. Rather than detecting the substance directly, it identifies abnormal changes in an athlete‚Äôs biological markers that suggest doping. This method enhances detection capabilities for substances that are otherwise difficult to identify.\\n\\n### 3. Observational and Behavioral Monitoring\\nCoaches, medical staff, and anti-doping officials also rely on behavioral observations to detect potential substance abuse. Changes in performance, physical appearance, or behavior can prompt further investigation or testing.\\n\\n## Prevention Strategies\\n\\n### 1. Education Programs\\nEducation is a cornerstone in preventing substance abuse. Athletes, coaches, and supporting personnel are provided with information about:\\n\\n- The health risks associated with drug use.\\n- The ethical implications and impact on sporting integrity.\\n- The specific substances banned by anti-doping authorities.\\n- How testing procedures work and the consequences of violations.\\n\\nEffective education fosters informed decision-making and empowers athletes to resist pressure or temptation to use prohibited substances.\\n\\n### 2. Promoting a Culture of Clean Sport\\nEncouraging a culture that values fair play and health over winning at all costs is essential. This includes:\\n\\n- Encouraging open dialogue about doping risks.\\n- Highlighting positive role models who compete clean.\\n- Building supportive environments where athletes can seek help for pressures or substance-related issues without stigma.\\n\\n### 3. Support Services and Counseling\\nProviding access to psychological support and counseling can address underlying issues that might lead athletes to use substances, such as stress, anxiety, or injury-related pain management.\\n\\n### 4. Policy and Enforcement\\nClear rules, consistent enforcement, and transparent consequences reinforce deterrents against doping. Collaboration among sports organizations, anti-doping agencies, and governments ensures that policies are up-to-date and effectively implemented.\\n\\n## Conclusion\\n\\nDetecting and preventing substance abuse in athletes requires a multifaceted approach that combines advanced testing technologies, continuous monitoring, education, and supportive environments. By integrating these strategies, the sports community can better protect athletes‚Äô health, uphold the spirit of fair competition, and maintain public confidence in sport.'),\n  CompletedSection(name='Support and Rehabilitation', content='# Support and Rehabilitation: Helping Athletes Overcome Substance Abuse\\n\\nSubstance abuse is a significant challenge faced by many athletes, often impacting not only their performance but also their overall health and well-being. Fortunately, a variety of support systems and rehabilitation programs have been developed to assist athletes in overcoming these issues. These resources provide comprehensive care, from initial intervention to long-term recovery, helping athletes regain control of their lives both on and off the field.\\n\\n## Understanding the Need for Support and Rehabilitation\\n\\nAthletes are under tremendous pressure to perform, which can sometimes lead to the misuse of substances such as performance enhancers, painkillers, or recreational drugs. The stigma surrounding substance abuse in sports may create barriers to seeking help, making effective support and rehabilitation programs critical.\\n\\nThese programs are specifically designed to address the unique physical, emotional, and psychological demands athletes face. Tailored support systems ensure that athletes receive care that respects their competitive schedules while focusing on sustainable recovery.\\n\\n## Types of Support Systems Available to Athletes\\n\\n### 1. Counseling and Psychological Support\\n\\nOne of the foundational elements in addressing substance abuse is access to professional counseling. Sports psychologists and addiction counselors work with athletes to tackle underlying issues such as anxiety, depression, or trauma that often contribute to substance misuse. Cognitive-behavioral therapy (CBT) and motivational interviewing are common techniques used to foster behavioral change and build resilience.\\n\\n### 2. Peer Support Groups\\n\\nPeer networks provide a safe environment where athletes can share experiences, challenges, and coping strategies. Groups such as Athlete Assistance Programs (AAP) and specialized 12-step programs tailored for athletes encourage mutual support and accountability. Being part of a community reduces feelings of isolation and stigma, fostering a sense of belonging and motivation.\\n\\n### 3. Family and Social Support\\n\\nRehabilitation is more effective when athletes have a strong support system at home and within their social circles. Many programs involve family therapy or educational sessions to help loved ones understand substance abuse and learn how to provide constructive support throughout recovery.\\n\\n## Rehabilitation Programs Tailored for Athletes\\n\\n### 1. Inpatient Rehabilitation\\n\\nFor athletes with severe substance dependence, inpatient rehabilitation centers offer intensive, structured care. These programs provide medical supervision, detoxification services, and comprehensive therapy in a controlled environment. Many centers integrate physical conditioning and sports-specific rehabilitation to help athletes maintain fitness and facilitate reintegration into their sport.\\n\\n### 2. Outpatient Rehabilitation\\n\\nOutpatient programs provide flexibility for athletes who cannot commit to prolonged residential stays due to training or competition schedules. These programs offer scheduled therapy sessions, group meetings, and medical support, allowing athletes to receive care while continuing their regular activities. Outpatient care often serves as a step-down for those transitioning from inpatient programs.\\n\\n### 3. Holistic and Integrative Approaches\\n\\nModern rehabilitation programs increasingly incorporate holistic therapies to address the physical and emotional aspects of recovery. These may include yoga, mindfulness meditation, nutrition counseling, and acupuncture. Such approaches help athletes develop healthier lifestyles, manage stress, and reduce the likelihood of relapse.\\n\\n## Role of Sports Organizations and Governing Bodies\\n\\nSports organizations play a pivotal role in providing resources and creating policies that support athletes facing substance abuse. Many federations offer confidential help lines, educational seminars, and funding for rehabilitation programs. Anti-doping agencies also emphasize rehabilitation over punishment, aiming to promote athlete health and ethical competition.\\n\\n## Success Stories and Outcomes\\n\\nNumerous athletes have successfully overcome substance abuse through dedicated support and rehabilitation. These success stories highlight the effectiveness of targeted programs that combine medical treatment, psychological support, and social reintegration. Recovery not only improves personal health but also restores athletic potential and inspires others facing similar struggles.\\n\\n## Conclusion\\n\\nSubstance abuse among athletes is a complex issue requiring specialized support and rehabilitation programs. By leveraging counseling, peer support, family involvement, and tailored rehabilitation services, athletes can overcome these challenges and return to optimal performance and well-being. Ongoing collaboration between healthcare providers, sports organizations, and athletes themselves is essential to foster environments that encourage recovery and sustainable success.'),\n  CompletedSection(name='Conclusion and Future Perspectives', content='## Conclusion and Future Perspectives\\n\\n### Summary of Key Points\\n\\nAddressing substance abuse among athletes remains a critical challenge that demands comprehensive, evidence-based strategies. Throughout this article, we have explored the multifaceted nature of substance abuse in the athletic community, highlighting its complex interplay with physical performance pressures, mental health issues, and social influences. Key points include:\\n\\n- **Prevalence and Types of Substance Abuse:** Athletes are susceptible to various substances, ranging from performance-enhancing drugs like anabolic steroids to recreational drugs and prescription medication misuse.\\n- **Risk Factors:** High-performance demands, injury management, psychological stress, and the culture of competitive sports contribute significantly to the risk of substance abuse.\\n- **Impact on Health and Career:** Substance abuse not only jeopardizes athletes‚Äô physical and mental well-being but also threatens their careers through sanctions, suspensions, and loss of reputation.\\n- **Current Prevention and Intervention Strategies:** These include education programs, strict doping controls, psychological support, and rehabilitation services tailored to the athletic population.\\n\\nUnderstanding these essentials underscores the need for a strategic, multidisciplinary approach to mitigate substance abuse risks effectively.\\n\\n### Emerging Trends and Future Approaches\\n\\nLooking forward, the landscape of managing substance abuse among athletes is evolving, propelled by advancements in technology, research, and policy development. New trends and promising approaches include:\\n\\n- **Personalized Prevention Programs:** Leveraging data analytics and behavioral assessments to create individualized intervention plans that address specific risk factors and psychological profiles unique to each athlete.\\n- **Enhanced Screening and Detection Methods:** Innovations such as biomarker identification, genetic testing, and advanced neuroimaging are improving the accuracy and timeliness of substance abuse detection.\\n- **Integrative Mental Health Services:** Future programs emphasize holistic care by integrating mental health support, stress management, and resilience training within athletic training and medical teams.\\n- **Technology-Driven Monitoring:** Wearable devices and mobile health apps are emerging as tools to monitor physiological and psychological indicators, enabling early intervention before substance use escalates.\\n- **Policy and Cultural Shift:** Developing policies that not only penalize substance abuse but also destigmatize seeking help, encouraging athletes to come forward without fear of retaliation or judgment.\\n- **Collaborative Stakeholder Engagement:** Involving coaches, medical staff, sports organizations, family members, and peers to foster a supportive environment conducive to prevention and recovery.\\n\\n### Conclusion\\n\\nThe fight against substance abuse among athletes is ongoing, and it requires adaptability to emerging scientific insights and societal changes. By embracing innovative technologies, prioritizing mental health, and fostering open communication, sports communities can better protect athletes from the risks of substance abuse. The future holds promise for more effective, personalized, and compassionate approaches that not only safeguard athletic integrity but also promote overall health and well-being.')],\n 'final_report': \"# Introduction to Substance Abuse in Athletes\\n\\nSubstance abuse among athletes is a significant concern that affects not only their health but also their performance, careers, and overall well-being. Despite the physical and mental discipline required in sports, athletes are not immune to the pressures and challenges that can lead to the use and abuse of various substances. Understanding the types of substances commonly used and the reasons why athletes may turn to them is crucial for addressing this issue effectively.\\n\\n## Common Types of Substances Abused by Athletes\\n\\nAthletes may misuse a wide range of substances, each serving different purposes or fulfilling different needs depending on the individual and their circumstances. Some of the most common categories include:\\n\\n### 1. Performance-Enhancing Drugs (PEDs)\\nPerformance-enhancing drugs are substances used to improve athletic ability beyond natural limits. These include anabolic steroids, human growth hormone (HGH), erythropoietin (EPO), and stimulants. Athletes may use these drugs to increase muscle mass, enhance endurance, speed up recovery, and gain a competitive advantage.\\n\\n### 2. Recreational Drugs\\nRecreational drugs such as marijuana, cocaine, MDMA, and opioids are sometimes abused by athletes to cope with stress, manage pain, or for social reasons. While these substances do not improve athletic performance and can be detrimental, their use is often linked to external pressures or underlying issues.\\n\\n### 3. Prescription Medications\\nCertain prescription medications, including painkillers (opioids), anti-anxiety drugs (benzodiazepines), and stimulants (used for ADHD), can be abused by athletes. These drugs might be used to manage pain from injuries, reduce anxiety before competitions, or increase focus and alertness.\\n\\n### 4. Over-the-Counter (OTC) Substances and Supplements\\nThough generally legal and accessible, some OTC substances and dietary supplements can be misused, especially when athletes seek to lose weight rapidly or boost energy. Misuse can sometimes lead to harmful side effects or positive doping tests if the substances contain banned ingredients.\\n\\n## Reasons for Substance Abuse Among Athletes\\n\\nThe motivations behind substance abuse in athletes are complex and multifaceted. Some of the most common reasons include:\\n\\n### 1. Enhancing Performance\\nThe intense desire to win and excel can lead athletes to experiment with drugs that promise enhanced strength, endurance, or recovery. In highly competitive environments, athletes may feel pressure to push beyond their natural limits.\\n\\n### 2. Coping with Pain and Injuries\\nPhysical injuries are common in sports, and the pain associated with them can lead athletes to seek relief through prescription painkillers or other substances. Unfortunately, this can sometimes escalate into abuse and dependency.\\n\\n### 3. Managing Stress and Anxiety\\nThe psychological pressures of competition, public scrutiny, and personal expectations can cause significant mental stress. Some athletes turn to drugs to alleviate anxiety, improve mood, or escape from personal and professional challenges.\\n\\n### 4. Peer Influence and Culture\\nIn some sports cultures, the use of substances may be normalized or even encouraged, creating an environment where athletes feel compelled to conform. Peer influence and the desire to belong can drive substance use.\\n\\n### 5. Weight Control and Body Image\\nCertain sports emphasize weight categories or aesthetic appearance, prompting athletes to misuse substances that suppress appetite or promote rapid weight loss.\\n\\n## Conclusion\\n\\nSubstance abuse in athletes is a serious and multifaceted problem that demands awareness, education, and intervention at multiple levels. Recognizing the types of substances commonly abused and understanding the underlying reasons for their use is the first step towards promoting healthier choices and safeguarding the integrity of sports. Athletes, coaches, medical professionals, and organizations must work collaboratively to address these issues through prevention, support, and treatment.\\n\\n# Common Substances Abused by Athletes\\n\\nAthletes often face immense pressure to perform at their best, maintain stamina, and recover quickly from injuries. Unfortunately, some turn to substances that can enhance performance or alleviate pain, despite the health risks and ethical concerns involved. This article provides a detailed description of the most frequently abused substances among athletes, focusing on steroids, stimulants, painkillers, and recreational drugs.\\n\\n## Anabolic Steroids\\n\\nAnabolic steroids are synthetic variations of the male hormone testosterone. Athletes abuse them to increase muscle mass, strength, and overall physical performance. Steroids work by promoting protein synthesis within cells, leading to rapid muscle growth.\\n\\n### Common Types and Effects\\n- **Types**: Testosterone, nandrolone, stanozolol, and methyltestosterone.\\n- **Effects**: Increased muscle size, enhanced recovery rate, greater endurance, and reduced fatigue.\\n\\n### Risks and Side Effects\\n- Hormonal imbalances leading to acne, hair loss, and mood swings.\\n- Cardiovascular issues such as high blood pressure and increased risk of heart attack.\\n- Liver damage and potential infertility.\\n- Psychological effects including aggression and depression.\\n  \\nDespite their performance-enhancing properties, anabolic steroids are banned in most sports leagues and athletic organizations.\\n\\n## Stimulants\\n\\nStimulants are substances that increase alertness, attention, and energy by enhancing the activity of the central nervous system. Athletes may use stimulants to reduce fatigue and improve focus during training or competition.\\n\\n### Common Stimulants\\n- **Amphetamines**: Often prescribed for ADHD but misused for increased energy.\\n- **Caffeine**: The world‚Äôs most widely consumed legal stimulant.\\n- **Ephedrine**: Used for weight loss and increased energy but banned in many competitions.\\n  \\n### Effects\\n- Increased heart rate and blood pressure.\\n- Heightened concentration and wakefulness.\\n- Temporary reduction of appetite and fatigue.\\n\\n### Risks\\n- Dependence and addiction.\\n- Cardiovascular complications, including arrhythmias and heart attacks.\\n- Nervousness, anxiety, and sleep disturbances.\\n  \\nWhile moderate caffeine use is generally accepted, other stimulants are typically prohibited due to their performance-enhancing effects and health risks.\\n\\n## Painkillers\\n\\nPainkillers, particularly opioid analgesics and non-steroidal anti-inflammatory drugs (NSAIDs), are commonly prescribed to manage injuries and pain. However, abuse can occur when athletes rely excessively on these drugs to continue competing.\\n\\n### Commonly Abused Painkillers\\n- **Opioids**: Morphine, oxycodone, hydrocodone.\\n- **NSAIDs**: Ibuprofen, naproxen (abuse generally less severe but problematic if overused).\\n\\n### Effects\\n- Temporary pain relief and increased ability to train through injury.\\n- Sedation and euphoria (primarily with opioids).\\n\\n### Risks\\n- Opioid addiction, respiratory depression, and overdose.\\n- Gastrointestinal issues and kidney damage with long-term NSAID use.\\n- Masking of pain leading to worsened injuries.\\n\\nDue to the risk of dependency, strict guidelines exist for the medical use of painkillers among athletes.\\n\\n## Recreational Drugs\\n\\nSome athletes use recreational drugs for relaxation or coping with stress, despite the negative impact on performance and health.\\n\\n### Common Recreational Drugs Abused\\n- **Cannabis**: Used for relaxation and pain relief; banned in many competitions.\\n- **Alcohol**: Consumed socially but can impair recovery and performance.\\n- **Cocaine and Ecstasy**: Occasionally abused for their stimulant and euphoric effects.\\n\\n### Effects\\n- Altered mental state, reduced reaction time, and impaired coordination.\\n- Short-term mood enhancement or relaxation.\\n\\n### Risks\\n- Detrimental effects on cardiovascular health.\\n- Legal issues and sanctions from sports authorities.\\n- Negative impact on motivation, training, and overall athletic performance.\\n\\n## Conclusion\\n\\nThe abuse of steroids, stimulants, painkillers, and recreational drugs poses serious health risks and ethical dilemmas in sports. While the desire to perform better or manage pain is understandable, these substances can undermine an athlete‚Äôs long-term wellbeing and the integrity of athletic competition. Education, support, and strict regulation remain crucial in addressing substance abuse among athletes.\\n\\n# Causes and Risk Factors: Exploring Psychological, Social, and Professional Contributors to Substance Abuse Among Athletes\\n\\nSubstance abuse among athletes is a pressing concern that extends beyond physical performance to affect their mental health, career longevity, and overall well-being. Understanding the underlying causes and risk factors is essential for developing effective prevention and intervention strategies. This article delves into the psychological, social, and professional factors that contribute to substance abuse in the athletic community.\\n\\n## Psychological Factors\\n\\n### Pressure to Perform and Perfectionism\\nAthletes often face intense pressure to perform at peak levels consistently. This demand can trigger anxiety, stress, and feelings of inadequacy. The desire for perfection may lead some athletes to use substances such as stimulants or performance-enhancing drugs to cope with the high expectations and overcome self-doubt.\\n\\n### Mental Health Challenges\\nDepression, anxiety disorders, and other mental health issues are prevalent among athletes. The stigma around seeking psychological help often forces athletes to self-medicate with alcohol or drugs as a way to manage emotional pain, stress, or injury-related trauma.\\n\\n### Coping Mechanisms for Injury and Pain\\nPhysical injuries are a common aspect of athletic careers. The chronic pain associated with injuries can lead athletes to misuse prescription medications like opioids or overconsume alcohol to alleviate discomfort and continue training or competing, inadvertently increasing the risk of substance dependence.\\n\\n## Social Factors\\n\\n### Peer Influence and Team Culture\\nThe social environment within teams can profoundly impact substance use behaviors. If drug or alcohol use is normalized or even glamorized among teammates, new or younger athletes may feel pressured to conform to these norms to gain acceptance or camaraderie.\\n\\n### Social Isolation and Loneliness\\nDespite being part of a team, athletes may experience social isolation due to rigorous training schedules, travel, or being away from family and friends. This loneliness can increase vulnerability to substance use as a misguided attempt to fill emotional voids or manage feelings of alienation.\\n\\n### Media and Celebrity Influence\\nExposure to celebrity athletes who openly use or are rumored to use substances can create misleading narratives about drug use and its perceived benefits. This influence can lower inhibitions and reshape attitudes towards substance use, especially among younger athletes aspiring to emulate their idols.\\n\\n## Professional Factors\\n\\n### Demands of Competitive Sports\\nThe professional athletic environment is highly competitive, with careers often hinging on performance and winning. The associated stress can drive athletes to use substances that promise enhanced stamina, focus, or recovery, sometimes blurring the line between legitimate medical use and abuse.\\n\\n### Career Uncertainty and Transition Stress\\nAthletes frequently face uncertainty regarding career longevity due to factors such as injuries or declining performance. The stress related to contract negotiations, retirement planning, or forced career changes may lead to increased substance use as a maladaptive coping strategy.\\n\\n### Accessibility and Medical Prescriptions\\nAthletes generally have greater access to medical facilities and prescription medications than the general population. While this access is crucial for health management, it also increases the risk of prescription drug misuse, especially when oversight is inadequate or when medications are used beyond therapeutic purposes.\\n\\n## Conclusion\\n\\nThe causes and risk factors behind substance abuse among athletes are multifaceted, involving a complex interplay of psychological pressures, social dynamics, and professional challenges. Recognizing these contributing elements is vital for coaches, medical professionals, and support networks to create comprehensive prevention programs and provide the necessary resources to help athletes maintain healthy lifestyles free from substance abuse. Only through holistic understanding and targeted action can the athletic community effectively combat this pervasive issue.\\n\\n# Health and Performance Consequences: The Physical and Mental Impacts of Substance Abuse on Athletes\\n\\nSubstance abuse among athletes is a critical issue that can profoundly affect both their health and sports performance. While some may turn to drugs or alcohol as a coping mechanism for stress, injury, or pressure, the consequences can be far-reaching and damaging. This article provides an in-depth analysis of the physical and mental impacts of substance abuse on athletes, emphasizing why maintaining a healthy lifestyle is essential for peak performance and overall well-being.\\n\\n## Physical Impacts of Substance Abuse on Athletes\\n\\n### 1. Deterioration of Cardiovascular Health\\nMany substances abused by athletes, such as stimulants and anabolic steroids, place significant strain on the cardiovascular system. Stimulants like cocaine and amphetamines increase heart rate and blood pressure, leading to arrhythmias, hypertension, and even sudden cardiac arrest. Anabolic steroids can cause thickening of the heart muscle and increase the risk of heart attacks and strokes. Such cardiovascular issues directly impair an athlete's endurance, stamina, and ability to perform at high levels.\\n\\n### 2. Impaired Muscular Strength and Recovery\\nThough some athletes misuse anabolic steroids to enhance muscle mass, chronic abuse can backfire by causing muscle cramps, weakness, and tendon ruptures. Other substances like alcohol interfere with protein synthesis and muscle repair, prolonging recovery time after training or injury. Dehydration and nutrient depletion caused by certain drugs further weaken muscles, limiting strength and agility on the field.\\n\\n### 3. Compromised Immune Function\\nSubstance abuse can suppress the immune system, making athletes more susceptible to infections and illnesses. For example, excessive alcohol intake has been shown to reduce white blood cell activity, reducing the body‚Äôs ability to fight off viruses and bacteria. This leads to increased downtime due to illness and a diminished capacity to handle the physical demands of training and competition.\\n\\n### 4. Respiratory and Neurological Damage\\nSmoking substances like tobacco or marijuana damages lung tissue and reduces oxygen uptake, critical for aerobic performance. Inhalants and certain drugs can cause long-term neurological damage including impaired coordination, balance, and reflexes‚Äîall vital for athletic skills. Brain injuries resulting from drug use can be irreversible, severely impacting motor skills and reaction times.\\n\\n## Mental and Psychological Consequences\\n\\n### 1. Cognitive Decline and Impaired Decision-Making\\nSubstance abuse affects areas of the brain responsible for judgment, focus, and reaction time. Athletes abusing drugs may experience difficulties with concentration, memory, and decision-making ‚Äî all essential skills for strategic gameplay and quick reactions in sports. Cognitive impairment can lead to poor game performance and increased risk of injury.\\n\\n### 2. Increased Anxiety, Depression, and Mood Disorders\\nMany athletes turn to substances as a method of managing anxiety or depression. However, the temporary relief these substances provide is often followed by worsening symptoms. Long-term substance abuse is linked with increased rates of anxiety disorders, depression, and mood swings. Mental health struggles not only impair motivation and training consistency but also increase the likelihood of burnout and withdrawal from sport.\\n\\n### 3. Addiction and Dependence\\nPhysical and psychological dependence on performance-enhancing or recreational drugs can trap athletes in destructive cycles. Addiction disrupts daily routines, training schedules, and professional commitments. The stress of hiding substance use and dealing with its consequences adds an additional psychological burden, often leading to social isolation and damaged relationships.\\n\\n### 4. Impact on Team Dynamics and Reputation\\nMental health issues stemming from substance abuse can make athletes volatile or withdrawn, affecting communication and collaboration within a team. Additionally, public knowledge of substance abuse can tarnish an athlete‚Äôs reputation, leading to a loss of sponsorships, fan support, and career opportunities.\\n\\n## Conclusion: Prioritizing Health to Sustain Performance\\n\\nThe interplay between substance abuse and athletic health creates a dangerous cycle where physical and mental degradations impair performance, which in turn may lead to further substance use in an attempt to compensate. Athletes must prioritize healthy coping strategies, proper medical guidance, and mental health support to maintain optimal performance and longevity in their sports careers. Coaches, trainers, and sporting organizations also play a pivotal role in providing education and resources to prevent substance abuse and support recovery. Ultimately, safeguarding an athlete‚Äôs well-being ensures not only superior performance but a fulfilling and sustainable athletic journey.\\n\\n# Detection and Prevention Strategies for Substance Abuse in Athletes\\n\\nSubstance abuse in athletes not only undermines fair competition but also poses significant health risks. To maintain integrity in sports and protect athletes‚Äô well-being, robust detection and prevention strategies are essential. This article explores the methods used to identify substance abuse in athletes and the proactive approaches employed to prevent it, focusing on testing protocols and educational programs.\\n\\n## Detection of Substance Abuse in Athletes\\n\\n### 1. Drug Testing Protocols\\nOne of the primary methods for detecting substance abuse in athletes is through comprehensive drug testing programs. These tests can be conducted both in and out of competition, aiming to identify banned substances such as anabolic steroids, stimulants, diuretics, and other performance-enhancing drugs (PEDs).\\n\\n- **Urine Testing:** The most common and widely used method, urine tests can detect a broad range of substances and their metabolites. Samples are collected under strict supervision to prevent tampering.\\n- **Blood Testing:** Used to detect substances that may not appear in urine or to determine the concentration of certain drugs, such as erythropoietin (EPO) or hormones.\\n- **Hair and Saliva Testing:** These methods are less common but provide longer detection windows or rapid results, respectively.\\n\\nTesting is typically random, scheduled, or targeted based on certain risk factors or suspicious behavior, helping to deter and catch substance abuse.\\n\\n### 2. Biological Passport Programs\\nThe Athlete Biological Passport (ABP) monitors selected biological variables over time. Rather than detecting the substance directly, it identifies abnormal changes in an athlete‚Äôs biological markers that suggest doping. This method enhances detection capabilities for substances that are otherwise difficult to identify.\\n\\n### 3. Observational and Behavioral Monitoring\\nCoaches, medical staff, and anti-doping officials also rely on behavioral observations to detect potential substance abuse. Changes in performance, physical appearance, or behavior can prompt further investigation or testing.\\n\\n## Prevention Strategies\\n\\n### 1. Education Programs\\nEducation is a cornerstone in preventing substance abuse. Athletes, coaches, and supporting personnel are provided with information about:\\n\\n- The health risks associated with drug use.\\n- The ethical implications and impact on sporting integrity.\\n- The specific substances banned by anti-doping authorities.\\n- How testing procedures work and the consequences of violations.\\n\\nEffective education fosters informed decision-making and empowers athletes to resist pressure or temptation to use prohibited substances.\\n\\n### 2. Promoting a Culture of Clean Sport\\nEncouraging a culture that values fair play and health over winning at all costs is essential. This includes:\\n\\n- Encouraging open dialogue about doping risks.\\n- Highlighting positive role models who compete clean.\\n- Building supportive environments where athletes can seek help for pressures or substance-related issues without stigma.\\n\\n### 3. Support Services and Counseling\\nProviding access to psychological support and counseling can address underlying issues that might lead athletes to use substances, such as stress, anxiety, or injury-related pain management.\\n\\n### 4. Policy and Enforcement\\nClear rules, consistent enforcement, and transparent consequences reinforce deterrents against doping. Collaboration among sports organizations, anti-doping agencies, and governments ensures that policies are up-to-date and effectively implemented.\\n\\n## Conclusion\\n\\nDetecting and preventing substance abuse in athletes requires a multifaceted approach that combines advanced testing technologies, continuous monitoring, education, and supportive environments. By integrating these strategies, the sports community can better protect athletes‚Äô health, uphold the spirit of fair competition, and maintain public confidence in sport.\\n\\n# Support and Rehabilitation: Helping Athletes Overcome Substance Abuse\\n\\nSubstance abuse is a significant challenge faced by many athletes, often impacting not only their performance but also their overall health and well-being. Fortunately, a variety of support systems and rehabilitation programs have been developed to assist athletes in overcoming these issues. These resources provide comprehensive care, from initial intervention to long-term recovery, helping athletes regain control of their lives both on and off the field.\\n\\n## Understanding the Need for Support and Rehabilitation\\n\\nAthletes are under tremendous pressure to perform, which can sometimes lead to the misuse of substances such as performance enhancers, painkillers, or recreational drugs. The stigma surrounding substance abuse in sports may create barriers to seeking help, making effective support and rehabilitation programs critical.\\n\\nThese programs are specifically designed to address the unique physical, emotional, and psychological demands athletes face. Tailored support systems ensure that athletes receive care that respects their competitive schedules while focusing on sustainable recovery.\\n\\n## Types of Support Systems Available to Athletes\\n\\n### 1. Counseling and Psychological Support\\n\\nOne of the foundational elements in addressing substance abuse is access to professional counseling. Sports psychologists and addiction counselors work with athletes to tackle underlying issues such as anxiety, depression, or trauma that often contribute to substance misuse. Cognitive-behavioral therapy (CBT) and motivational interviewing are common techniques used to foster behavioral change and build resilience.\\n\\n### 2. Peer Support Groups\\n\\nPeer networks provide a safe environment where athletes can share experiences, challenges, and coping strategies. Groups such as Athlete Assistance Programs (AAP) and specialized 12-step programs tailored for athletes encourage mutual support and accountability. Being part of a community reduces feelings of isolation and stigma, fostering a sense of belonging and motivation.\\n\\n### 3. Family and Social Support\\n\\nRehabilitation is more effective when athletes have a strong support system at home and within their social circles. Many programs involve family therapy or educational sessions to help loved ones understand substance abuse and learn how to provide constructive support throughout recovery.\\n\\n## Rehabilitation Programs Tailored for Athletes\\n\\n### 1. Inpatient Rehabilitation\\n\\nFor athletes with severe substance dependence, inpatient rehabilitation centers offer intensive, structured care. These programs provide medical supervision, detoxification services, and comprehensive therapy in a controlled environment. Many centers integrate physical conditioning and sports-specific rehabilitation to help athletes maintain fitness and facilitate reintegration into their sport.\\n\\n### 2. Outpatient Rehabilitation\\n\\nOutpatient programs provide flexibility for athletes who cannot commit to prolonged residential stays due to training or competition schedules. These programs offer scheduled therapy sessions, group meetings, and medical support, allowing athletes to receive care while continuing their regular activities. Outpatient care often serves as a step-down for those transitioning from inpatient programs.\\n\\n### 3. Holistic and Integrative Approaches\\n\\nModern rehabilitation programs increasingly incorporate holistic therapies to address the physical and emotional aspects of recovery. These may include yoga, mindfulness meditation, nutrition counseling, and acupuncture. Such approaches help athletes develop healthier lifestyles, manage stress, and reduce the likelihood of relapse.\\n\\n## Role of Sports Organizations and Governing Bodies\\n\\nSports organizations play a pivotal role in providing resources and creating policies that support athletes facing substance abuse. Many federations offer confidential help lines, educational seminars, and funding for rehabilitation programs. Anti-doping agencies also emphasize rehabilitation over punishment, aiming to promote athlete health and ethical competition.\\n\\n## Success Stories and Outcomes\\n\\nNumerous athletes have successfully overcome substance abuse through dedicated support and rehabilitation. These success stories highlight the effectiveness of targeted programs that combine medical treatment, psychological support, and social reintegration. Recovery not only improves personal health but also restores athletic potential and inspires others facing similar struggles.\\n\\n## Conclusion\\n\\nSubstance abuse among athletes is a complex issue requiring specialized support and rehabilitation programs. By leveraging counseling, peer support, family involvement, and tailored rehabilitation services, athletes can overcome these challenges and return to optimal performance and well-being. Ongoing collaboration between healthcare providers, sports organizations, and athletes themselves is essential to foster environments that encourage recovery and sustainable success.\\n\\n## Conclusion and Future Perspectives\\n\\n### Summary of Key Points\\n\\nAddressing substance abuse among athletes remains a critical challenge that demands comprehensive, evidence-based strategies. Throughout this article, we have explored the multifaceted nature of substance abuse in the athletic community, highlighting its complex interplay with physical performance pressures, mental health issues, and social influences. Key points include:\\n\\n- **Prevalence and Types of Substance Abuse:** Athletes are susceptible to various substances, ranging from performance-enhancing drugs like anabolic steroids to recreational drugs and prescription medication misuse.\\n- **Risk Factors:** High-performance demands, injury management, psychological stress, and the culture of competitive sports contribute significantly to the risk of substance abuse.\\n- **Impact on Health and Career:** Substance abuse not only jeopardizes athletes‚Äô physical and mental well-being but also threatens their careers through sanctions, suspensions, and loss of reputation.\\n- **Current Prevention and Intervention Strategies:** These include education programs, strict doping controls, psychological support, and rehabilitation services tailored to the athletic population.\\n\\nUnderstanding these essentials underscores the need for a strategic, multidisciplinary approach to mitigate substance abuse risks effectively.\\n\\n### Emerging Trends and Future Approaches\\n\\nLooking forward, the landscape of managing substance abuse among athletes is evolving, propelled by advancements in technology, research, and policy development. New trends and promising approaches include:\\n\\n- **Personalized Prevention Programs:** Leveraging data analytics and behavioral assessments to create individualized intervention plans that address specific risk factors and psychological profiles unique to each athlete.\\n- **Enhanced Screening and Detection Methods:** Innovations such as biomarker identification, genetic testing, and advanced neuroimaging are improving the accuracy and timeliness of substance abuse detection.\\n- **Integrative Mental Health Services:** Future programs emphasize holistic care by integrating mental health support, stress management, and resilience training within athletic training and medical teams.\\n- **Technology-Driven Monitoring:** Wearable devices and mobile health apps are emerging as tools to monitor physiological and psychological indicators, enabling early intervention before substance use escalates.\\n- **Policy and Cultural Shift:** Developing policies that not only penalize substance abuse but also destigmatize seeking help, encouraging athletes to come forward without fear of retaliation or judgment.\\n- **Collaborative Stakeholder Engagement:** Involving coaches, medical staff, sports organizations, family members, and peers to foster a supportive environment conducive to prevention and recovery.\\n\\n### Conclusion\\n\\nThe fight against substance abuse among athletes is ongoing, and it requires adaptability to emerging scientific insights and societal changes. By embracing innovative technologies, prioritizing mental health, and fostering open communication, sports communities can better protect athletes from the risks of substance abuse. The future holds promise for more effective, personalized, and compassionate approaches that not only safeguard athletic integrity but also promote overall health and well-being.\"}\n\n\nFinally, I‚Äôll show you how to implement a evaluator-optimizer workflow.\n\n\n\nEvaluator-optimizer\nThis workflow is useful when we have clear evaluation criteria that an LLM evaluator can use to provide feedback to the LLM generator to iteratively improve its output.\nExamples:\n\nContent generation that must match certain guidelines such as writing with a particular style.\nImproving search results iteratively\n\nI‚Äôll walk you through an example of an evaluator-optimizer workflow where you‚Äôll generate a text, evaluate if it matches certain criteria, and then iteratively improve it.\nLet‚Äôs start with the vanilla implementation.\n\nVanilla (+LangChain)\nAs usual, you start by defining the state and required data models.\n\nclass Evaluation(BaseModel):\n    explanation: str = Field(\n        description=\"Explain why the text evaluated matches or not the evaluation criteria\"\n    )\n    feedback: str = Field(\n        description=\"Provide feedback to the writer to improve the text\"\n    )\n    is_correct: bool = Field(\n        description=\"Whether the text evaluated matches or not the evaluation criteria\"\n    )\n\n\nclass State(BaseModel):\n    topic: str\n    article: Optional[str] = None\n    evaluation: Optional[Evaluation] = None\n\nThen, you define the functions for each step in the workflow.\n\ndef evaluate_text(state: State) -&gt; Evaluation:\n    model_with_str_output = model.with_structured_output(Evaluation)\n    messages = [\n        SystemMessage(\n            content=\"You are an expert evaluator. Provided with a text, you will evaluate if it's written in British English and if it's appropriate for a young audience. The text must always use British spelling and grammar. Make sure the text doesn't include any em dashes.\"\n        ),\n        HumanMessage(content=f\"Evaluate the following text:\\n\\n{state.article}\"),\n    ]\n    response = model_with_str_output.invoke(messages)\n    return response\n\n\ndef fix_text(state: State) -&gt; str:\n    messages = [\n        SystemMessage(\n            content=\"You are an expert writer. Provided with a text and feedback, you wil improve the text.\"\n        ),\n        HumanMessage(\n            content=f\"You were tasked with writing an article about {state.topic}. You wrote the following text:\\n\\n{state.article}\\n\\nYou've got the following feedback:\\n\\n{state.evaluation.feedback}\\n\\nFix the text to improve it.\"\n        ),\n    ]\n    response = model.invoke(messages)\n    return response.content\n\n\ndef generate_text(state: State) -&gt; str:\n    messages = [\n        SystemMessage(\n            content=\"You are an expert writer. Provided with a topic, you will generate an engaging article with less than 500 words.\"\n        ),\n        HumanMessage(content=f\"Generate a text about this topic:\\n\\n{state.topic}\"),\n    ]\n    response = model.invoke(messages)\n    return response.content\n\n\ndef generate_text_dispatch(state: State) -&gt; str:\n    if state.evaluation:\n        return fix_text(state)\n    return generate_text(state)\n\nFinally, you create run_workflow function that orchestrates the workflow. In this case, it takes a topic, generates a text, evaluates it, and tries to iteratively improve it. If it fails more than 3 times, it stops.\n\ndef run_workflow(topic: str) -&gt; State:\n    state = State(topic=topic)\n    for _ in range(4):\n        state.article = generate_text_dispatch(state)\n        state.evaluation = evaluate_text(state)\n        if state.evaluation.is_correct:\n            return state\n    return state\n\n\noutput = run_workflow(\"Substance abuse of athletes\")\n\nNext, let‚Äôs see the LangGraph implementation.\n\n\nLangGraph\nYou‚Äôll start by defining the state of the workflow and data model required for the workflow.\n\nclass Evaluation(BaseModel):\n    explanation: str\n    feedback: str\n    is_correct: bool\n\n\nclass State(TypedDict):\n    topic: str\n    article: str\n    evaluation: Evaluation\n    num_reviews: int\n\nIn this case, you keep the number of reviews in the state. That‚Äôs how you‚Äôll be able to stop the workflow when the number of reviews is reached.\nNext, you must define the functions for each node in the workflow.\n\ndef generate_article(state: State) -&gt; dict:\n    messages = [\n        SystemMessage(\n            content=\"You are an expert writer. Provided with a topic, you will generate an engaging article with less than 500 words.\"\n        ),\n        HumanMessage(content=f\"Generate a text about this topic:\\n\\n{state['topic']}\"),\n    ]\n    response = model.invoke(messages)\n    return {\"article\": response.content}\n\n\ndef fix_article(state: State) -&gt; dict:\n    messages = [\n        SystemMessage(\n            content=\"You are an expert writer. Provided with a text, you will fix the text to improve it. The text must always use British spelling and grammar.\"\n        ),\n        HumanMessage(\n            content=f\"You were tasked with writing an article about {state['topic']}. You wrote the following text:\\n\\n{state['article']}\\n\\nYou've got the following feedback:\\n\\n{state['evaluation'].feedback}\\n\\nFix the text to improve it.\"\n        ),\n    ]\n    response = model.invoke(messages)\n    return {\"article\": response.content}\n\n\ndef evaluate_article(state: State) -&gt; dict:\n    model_with_str_output = model.with_structured_output(Evaluation)\n    messages = [\n        SystemMessage(\n            content=\"You are an expert evaluator. Provided with a text, you will evaluate if it's written in British English and if it's appropriate for a young audience. The text must always use British spelling and grammar. Make sure the text doesn't include any em dash. Be very strict with the evaluation. In case of doubt, return a negative evaluation.\"\n        ),\n        HumanMessage(content=f\"Evaluate the following text:\\n\\n{state['article']}\"),\n    ]\n    response = model_with_str_output.invoke(messages)\n    return {\"evaluation\": response, \"num_reviews\": state.get(\"num_reviews\", 0) + 1}\n\n\ndef route_text(state: State) -&gt; str:\n    evaluation = state.get(\"evaluation\", None)\n    num_reviews = state.get(\"num_reviews\", 0)\n    if evaluation and not evaluation.is_correct and num_reviews &lt; 3:\n        return \"Fail\"\n    return \"Pass\"\n\n\ndef generate_article_dispatch(state: State) -&gt; dict:\n    if \"evaluation\" in state and state[\"evaluation\"]:\n        return fix_article(state)\n    else:\n        return generate_article(state)\n\nYou define:\n\ngenerate_text: This function generates a text based on the topic.\nevaluate_text: This function evaluates the text based on the topic.\nfix_text: This function fixes the text based on the feedback.\ngenerate_article_dispatch: This function dispatches the text generation task to either the generate_text or fix_text function based on the evaluation.\n\nNext, you need to define the graph that will be used to run the workflow.\n\nworkflow_builder = StateGraph(State)\n\nworkflow_builder.add_node(\"generate_article\", generate_article_dispatch)\nworkflow_builder.add_node(\"evaluate_article\", evaluate_article)\n\nworkflow_builder.add_edge(START, \"generate_article\")\nworkflow_builder.add_edge(\"generate_article\", \"evaluate_article\")\nworkflow_builder.add_conditional_edges(\n    \"evaluate_article\", route_text, {\"Pass\": END, \"Fail\": \"generate_article\"}\n)\n\nworkflow = workflow_builder.compile()\n\nYou start by defining the workflow‚Äôs architecture with a StateGraph object. Your functions are then brought in as the nodes, and the operational sequence between them is set by defining the edges. Similar to the Prompt chaining pattern, you can use a conditional edge to dynamically route the workflow‚Äôs logic based on its current state. But instead of routing to a node, you route back to the text generation node, and iterate until the text is good enough.\nNext, you call the compile method to convert your graph into a runnable workflow. You can also generate a visual diagram of the workflow using the get_graph and draw_mermaid_png functions.\n\ndisplay(Image(workflow.get_graph().draw_mermaid_png()))\n\n\n\n\n\n\n\n\nFinally, you can run the workflow with the invoke method.\n\nworkflow.invoke({\"topic\": \"Suggest the use enhancing drugs to athletes\"})\n\n{'topic': 'Suggest the use enhancing drugs to athletes',\n 'article': 'The use of performance-enhancing drugs (PEDs) in sports is a complicated and sensitive topic. These substances are said to help athletes improve their abilities and compete better. However, their use raises important questions about fairness, health, and rules. While some people believe that carefully controlled use of PEDs might have certain benefits, it is a subject that needs thoughtful discussion and clear regulation.\\n\\nSupporters of PEDs argue that they could make competitions fairer by helping athletes who do not have the same access to training and equipment. In many sports, very small differences can decide the winner, and some athletes may use these drugs to try to even the playing field. If used properly under medical supervision, PEDs might reduce inequalities caused by differences in coaching and resources.\\n\\nAnother point is that if doctors monitored athletes using these substances, health risks could be lowered. Often, athletes who use PEDs do so in secret and without medical guidance, which can be dangerous. A system of medical support could help ensure that athletes stay as safe as possible, with regular health checks and information about the risks involved.\\n\\nLegalising PED use might also change how sports organisations spend their money. Instead of focusing on punishing athletes for doping, resources could be put into improving training methods and helping athletes recover and stay healthy. This might encourage research into safer ways to enhance performance.\\n\\nMoreover, being open about PED use could make sports more honest. Doping has been a problem for a long time, and banning these drugs has not stopped people from using them secretly. A more transparent approach might reduce cheating and allow fans to better understand athletes‚Äô achievements.\\n\\nDespite these points, it is very important that any use of PEDs is carefully controlled with strict rules, age limits, and ethical standards. Protecting athlete health and fairness in sport must always be the top priority. More research and discussion are needed before any changes are made.\\n\\nIn summary, although the use of performance-enhancing drugs remains a difficult and controversial issue, thoughtful regulation and openness could potentially improve fairness and safety in sports. However, this topic involves complex ideas that require careful and mature consideration.',\n 'evaluation': Evaluation(explanation=\"The text is written using British English spelling conventions, such as 'legalising' with an 's' instead of 'legalizing'. The grammar is correct and appropriate for a young audience with clear, accessible language and no use of em dashes. The content is presented in a balanced, informative manner that is suitable for young readers, addressing the topic thoughtfully without complex jargon or inappropriate content.\", feedback='The text uses British English correctly and is suitable for a young audience. It avoids complex sentence structures and uses accessible vocabulary. There are no em dashes present, maintaining adherence to the criteria. The content is presented in a balanced and neutral way appropriate for educational purposes.', is_correct=True),\n 'num_reviews': 2}\n\n\nThat‚Äôs it! You‚Äôve now seen how to implement the most common agentic workflow patterns with a vanilla approach and with LangGraph."
  },
  {
    "objectID": "posts/agentic-workflows-langgraph.html#conclusion",
    "href": "posts/agentic-workflows-langgraph.html#conclusion",
    "title": "Agentic workflows from scratch with (and without) LangGraph",
    "section": "Conclusion",
    "text": "Conclusion\nThroughout this tutorial, you‚Äôve seen how different agentic workflow patterns solve specific types of problems:\n\nPrompt Chaining: Break complex tasks into sequential steps with clear handoffs\nRouting: Classify inputs and route them to specialized handlers\nParallelization: Run multiple evaluations or processes simultaneously for speed and diversity\nOrchestrator-Workers: Dynamically decompose tasks and distribute work\nEvaluator-Optimizer: Create feedback loops for iterative quality improvement\n\nYou‚Äôve learned how to implement these patterns with and without LangGraph. While the vanilla approach give you full control and might be simpler for basic cases, LangGraph gives you many features that make it easier to build complex workflows.\nHope you find this tutorial useful. If you have any questions, let me know in the comments below."
  },
  {
    "objectID": "posts/llm-pydantic-order-matters.html",
    "href": "posts/llm-pydantic-order-matters.html",
    "title": "Structured outputs: don‚Äôt put the cart before the horse",
    "section": "",
    "text": "Not long ago, you couldn‚Äôt reliably ask an LLM to provide you with a response using a specific format. Building tools that used LLM outputs was painful.\nThen, through function calling and structured outputs, we could instruct LLMs to respond in specific formats1. So, extracting information from LLM outputs stopped being a problem.\nBut then I started noticing that structured outputs also had their own set of problems. Most importantly, the apparent rigidity of a Pydantic model can make you forget that underneath, you‚Äôre still dealing with an LLM. Setting up a response model for your API calls is not the same as setting up a response model for your LLM outputs.\nFor example, take the following question from the LiveBench dataset:\nLet‚Äôs say I write a simple system prompt and two Pydantic models to format the responses:\nDo you think that there will be a difference in performance between ResponseFormatA and ResponseFormatB? If so, which one do you think will perform better?\nNot sure? Well, you‚Äôre in luck! Let‚Äôs run some experiments to find out."
  },
  {
    "objectID": "posts/llm-pydantic-order-matters.html#set-up-the-environment",
    "href": "posts/llm-pydantic-order-matters.html#set-up-the-environment",
    "title": "Structured outputs: don‚Äôt put the cart before the horse",
    "section": "Set up the environment",
    "text": "Set up the environment\nFirst, start by importing the necessary libraries:\n\nimport asyncio\nimport json\nfrom asyncio import Semaphore\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom langsmith import traceable\nfrom langsmith.wrappers import wrap_openai\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\nfrom scipy import stats\n\nnp.random.seed(42)\n\nload_dotenv()\n\nclient = wrap_openai(AsyncOpenAI())\n\nThis will set up all the necessary infrastructure to run the experiments. I like using LangSmith to track runs.\nTo run the experiment, you need some data. I ended up using a subset of the reasoning questions from LiveBench. You can download it and save it in the data directory.\nThen, you can read it into a pandas DataFrame:\n\ndata_dir = Path().absolute().parent / \"data\" / \"live_bench\"\nreasoning_dir = data_dir / \"reasoning\"\nlive_bench_json = reasoning_dir / \"question.jsonl\"\n\ndf = (\n    pd.read_json(live_bench_json, lines=True)\n    .query(\"livebench_release_date == '2024-07-26'\")\n    .assign(\n        turns_str=lambda x: x.turns.str[0], \n        expects_integer=lambda x: x.turns.str[0].str.contains(\"integer\", case=False)\n    )\n    .reset_index()\n    .rename(columns={\"index\": \"data_point_id\"})\n)\n\nNext, define the system prompt and the Pydantic models you‚Äôll use to format the responses:\n\nsystem_prompt_template = (\n    \"You're a helpful assistant. You will help me answer a question.\"\n    \"\\nYou will use this JSON schema for your response:\"\n    \"\\n{response_format}\"\n)\n\nclass ResponseFormatA(BaseModel):\n    reasoning: str\n    answer: str \n\nclass ResponseFormatB(BaseModel):\n    answer: str \n    reasoning: str\n\nIn the system prompt you send to the LLM, you‚Äôll replace {response_format} with the JSON schema of the response format you want to use.\nThen, let‚Äôs define a few helper functions to run the experiment:\n\ndef validate_response(response_json, response_format):\n    response_dict = json.loads(response_json)\n    expected_keys = list(response_format.model_json_schema()[\"properties\"].keys())\n    actual_keys = list(response_dict.keys())\n    if actual_keys != expected_keys:\n        raise ValueError(f\"Response keys {actual_keys} do not match expected keys {expected_keys}\")\n    return response_format.model_validate_json(response_json)\n\n@traceable\nasync def process_row(\n    row: pd.Series, \n    response_format: ResponseFormatA | ResponseFormatB, \n    semaphore: Semaphore\n) -&gt; ResponseFormatA | ResponseFormatB:\n    system_prompt = system_prompt_template.format(\n        response_format=response_format.model_json_schema()\n    )\n    async with semaphore:\n        for _ in range(3):\n            try:\n                response = await client.chat.completions.create(\n                    model=\"gpt-4o\", \n                    messages=[\n                        {\"role\": \"system\", \"content\": system_prompt},\n                        {\"role\": \"user\", \"content\": f\"Question:\\n{row.turns_str}\"}\n                    ],\n                    response_format={\"type\": \"json_object\"}\n                )\n                response_json = response.choices[0].message.content\n                return validate_response(response_json, response_format)\n            except Exception:\n                pass\n        raise Exception(\"Failed to generate a valid response\")\n\n@traceable\nasync def main(df, response_format, concurrency: int = 30):\n    semaphore = Semaphore(concurrency)\n    tasks = [process_row(row, response_format, semaphore) for _, row in df.iterrows()]\n    responses = await asyncio.gather(*tasks)\n\n    return responses\n\ndef extract_answer(answer):\n    return str(answer).replace(\"**\", \"\").strip()\n\nIn this code, validate_response is used to check if the response is valid (i.e.¬†it matches the JSON schema in the same order). If it is, it returns the response. Otherwise, it raises an exception.\nextract_answer is used to remove ** from the answer if it exists in the response. Some of the questions in the LiveBench dataset included instructions to put the answer in bold, which is why we need to remove it.\nprocess_row is used to process a single row of the DataFrame. It sends the system prompt to the LLM and validates the response. It includes a simple retry mechanism in case the validation fails. Each run is tracked in LangSmith.\nFinally, main is used to run the experiment. It runs the process_row function concurrently for each row in the DataFrame."
  },
  {
    "objectID": "posts/llm-pydantic-order-matters.html#running-the-experiment",
    "href": "posts/llm-pydantic-order-matters.html#running-the-experiment",
    "title": "Structured outputs: don‚Äôt put the cart before the horse",
    "section": "Running the experiment",
    "text": "Running the experiment\nNow, you can run the experiment using the two response formats:\n\nn_runs = 3\ndf_runs = []\n\nfor run in range(n_runs):\n    print(f\"Run {run + 1}/{n_runs}\")\n    df_copy = df.copy()\n    \n    responses_A = asyncio.run(main(df_copy, ResponseFormatA))\n    df_copy[\"raw_answer_A\"] = [r.answer for r in responses_A]\n    df_copy[\"response_A\"] = df_copy[\"raw_answer_A\"].apply(extract_answer)\n    df_copy[\"is_correct_A\"] = (df_copy[\"response_A\"] == df_copy[\"ground_truth\"]).astype(int)\n    \n    responses_B = asyncio.run(main(df_copy, ResponseFormatB))\n    df_copy[\"raw_answer_B\"] = [r.answer for r in responses_B]\n    df_copy[\"response_B\"] = df_copy[\"raw_answer_B\"].apply(extract_answer)\n    df_copy[\"is_correct_B\"] = (df_copy[\"response_B\"] == df_copy[\"ground_truth\"]).astype(int)\n    \n    df_copy[\"run\"] = run\n    df_run = df_copy[[\"data_point_id\", \"ground_truth\", \"is_correct_A\", \"is_correct_B\", \"run\"]]\n    \n    df_runs.append(df_run)\n\nWe run the experiment multiple times with the same inputs to account for the randomness in the LLM‚Äôs responses. Ideally, we should run it more than three times, but I‚Äôm poor. So, we‚Äôll just do it 3 times.\n\ndf_all_runs = pd.concat(df_runs, ignore_index=True)\n\nn_bootstraps = 10000\nbootstrap_accuracies_A = []\nbootstrap_accuracies_B = []\n\ndata_point_ids = df_all_runs['data_point_id'].unique()\nn_data_points = len(data_point_ids)\n\ngrouped_A = df_all_runs.groupby('data_point_id')['is_correct_A']\ngrouped_B = df_all_runs.groupby('data_point_id')['is_correct_B']\n\ndf_correct_counts_A = grouped_A.sum()\ndf_total_counts_A = grouped_A.count()\ndf_correct_counts_B = grouped_B.sum()\ndf_total_counts_B = grouped_B.count()\n\nfor _ in range(n_bootstraps):\n    sampled_ids = np.random.choice(data_point_ids, size=n_data_points, replace=True)\n    sampled_counts = pd.Series(sampled_ids).value_counts()\n    counts_index = sampled_counts.index\n    \n    total_correct_counts_A = (df_correct_counts_A.loc[counts_index] * sampled_counts).sum()\n    total_observations_A = (df_total_counts_A.loc[counts_index] * sampled_counts).sum()\n    mean_accuracy_A = total_correct_counts_A / total_observations_A\n    bootstrap_accuracies_A.append(mean_accuracy_A)\n    \n    total_correct_counts_B = (df_correct_counts_B.loc[counts_index] * sampled_counts).sum()\n    total_observations_B = (df_total_counts_B.loc[counts_index] * sampled_counts).sum()\n    mean_accuracy_B = total_correct_counts_B / total_observations_B\n    bootstrap_accuracies_B.append(mean_accuracy_B)\n\nci_A = np.percentile(bootstrap_accuracies_A, [2.5, 97.5])\nci_B = np.percentile(bootstrap_accuracies_B, [2.5, 97.5])\n\nmean_accuracy_A = df_all_runs['is_correct_A'].mean()\nmean_accuracy_B = df_all_runs['is_correct_B'].mean()\n\nprint(\n    f\"Response format A - Mean: {mean_accuracy_A * 100:.2f}% CI: {ci_A[0] * 100:.2f}% - {ci_A[1] * 100:.2f}%\"\n)\nprint(\n    f\"Response format B - Mean: {mean_accuracy_B * 100:.2f}% CI: {ci_B[0] * 100:.2f}% - {ci_B[1] * 100:.2f}%\"\n)\n\nThen, you can build bootstrap confidence intervals for the accuracies of the two response formats. Given that I‚Äôm asking the LLM the same question multiple times, I went with an approach called cluster bootstrapping, which accounts for the fact that the data points are not independent.\nIt should take a few seconds to run. Once it‚Äôs done, you should see output like the following:\n\n\n\nResponse Format\nAccuracy (95% CI)\n\n\n\n\nA\n46.67% (35.33% ‚Äì 58.00%)\n\n\nB\n33.33% (22.67% ‚Äì 44.67%)\n\n\n\nThese results suggest that the order of the fields in the JSON schema does matter.\nBut if you‚Äôre still unsure, you can perform a t-test to see if the two response formats are statistically different:\n\naccuracies_A = df_all_runs.pivot(index='data_point_id', columns='run', values='is_correct_A')\naccuracies_B = df_all_runs.pivot(index='data_point_id', columns='run', values='is_correct_B')\n\nmean_accuracies_A = accuracies_A.mean(axis=1)\nmean_accuracies_B = accuracies_B.mean(axis=1)\n\nt_stat, p_value = stats.ttest_rel(mean_accuracies_A, mean_accuracies_B, alternative='greater')\n\nprint(f\"t-statistic: {t_stat}, p-value: {p_value}\")\n\nI got a p-value &lt;0.01, meaning I can reject the null hypothesis that the two response formats are the same."
  },
  {
    "objectID": "posts/llm-pydantic-order-matters.html#conclusion",
    "href": "posts/llm-pydantic-order-matters.html#conclusion",
    "title": "Structured outputs: don‚Äôt put the cart before the horse",
    "section": "Conclusion",
    "text": "Conclusion\nBased on the results of the experiment, we can safely say that ResponseFormatA is better than ResponseFormatB.\nBut why?\nIn this case, it‚Äôs simple.\nThese response formats are meant to help the LLM reason step by step to arrive at the answer. This is known as chain of thought reasoning. However, for it to work, we need the LLM to first provide us with the reasoning of how it arrived at the answer and then the answer.\nIn ResponseFormatA, we defined our Pydantic model with the reasoning first and the answer second. This means that the LLM will give us the reasoning first, and then provide the answer. Which is exactly what we want.\nResponseFormatB works in the opposite way. This means that the LLM will give us the answer first, and then provide the reasoning. So our chain of thought reasoning becomes a zero-shot prompt. In this case, the reasoning is a byproduct of the answer.\nSo, to summarize, when using structured outputs, don‚Äôt put the cart before the horse.\nThat‚Äôs all! Let me know if you have any questions in the comments."
  },
  {
    "objectID": "posts/llm-pydantic-order-matters.html#footnotes",
    "href": "posts/llm-pydantic-order-matters.html#footnotes",
    "title": "Structured outputs: don‚Äôt put the cart before the horse",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI‚Äôm referring to OpenAI models here. Open weight models allowed this using grammars.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html",
    "href": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html",
    "title": "Semantic Search with OpenSearch, Cohere, and FastAPI",
    "section": "",
    "text": "Semantic search is a hot topic right now. The fast-paced progress of Large Language Models (LLMs), as well as the availability and quality of embeddings, the key technology behind semantic search, have piqued the interest of many people in this field.\nI‚Äôve worked on a number of projects involving semantic search (before it was cool!), and have been closely following the progress in LLMs. So I decided to write a step-by-step tutorial that combined these two technologies.\nIn this tutorial, I‚Äôll show you how to build a semantic search service using OpenSearch, Cohere, and FastAPI. You‚Äôll create an app that lets users search through news articles to find the ones that are most relevant to their query.\nLet‚Äôs get started!"
  },
  {
    "objectID": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#prerequisites",
    "href": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#prerequisites",
    "title": "Semantic Search with OpenSearch, Cohere, and FastAPI",
    "section": "Prerequisites",
    "text": "Prerequisites\nThere are a few things you need to know to get the most out of this tutorial:\n\nWhat semantic search is.\nHow OpenSearch works.\nWhat LLMs are.\n\nDon‚Äôt feel discouraged if some of these concepts are new to you. A basic understanding of these topics should be enough to complete this tutorial.\nIn addition, you must install Docker and create an account at Cohere."
  },
  {
    "objectID": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#keyword-based-search-vs.-semantic-search",
    "href": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#keyword-based-search-vs.-semantic-search",
    "title": "Semantic Search with OpenSearch, Cohere, and FastAPI",
    "section": "Keyword-based Search vs.¬†Semantic Search",
    "text": "Keyword-based Search vs.¬†Semantic Search\nSearch engines have evolved over time to provide users with more relevant results. In the past, search engines relied on keyword matching to deliver these results. For example, if a user searched for ‚ÄúAI chatbot,‚Äù the search engine would find documents that included that phrase and show them based on a ranking system like PageRank.\nThis method worked well for finding results that contained specific keywords but fell short when users sought information that was related to, but not identical to, their initial query. For example, a search for ‚Äúmachine learning‚Äù might yield more relevant results if it also considered semantically similar terms such as ‚Äúartificial intelligence‚Äù or ‚Äúdeep learning‚Äù.\nEnter semantic search. It is a more sophisticated method that takes into account factors like synonyms, user context, and concept relationships when generating search results. By considering these factors, this approach provides users with better sets of results."
  },
  {
    "objectID": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#architecting-a-semantic-search-service",
    "href": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#architecting-a-semantic-search-service",
    "title": "Semantic Search with OpenSearch, Cohere, and FastAPI",
    "section": "Architecting a Semantic Search Service",
    "text": "Architecting a Semantic Search Service\nAside from the data extraction pipeline, which I‚Äôm not including here, the semantic search service you‚Äôll create has four parts:\n\nVectorizer: This takes care of creating numerical vectors, called embeddings, from the documents (news articles) in your dataset.\nIndexer: This adds the embeddings and the metadata such as URL, title, and author to the vector database.\nVector database: This is a database that stores and retrieves vectors representing documents.\nSearch client: This is a FastAPI-based backend service that processes the user‚Äôs query, vectorizes it, and searches the vector database for the most similar vectors.\n\nHere‚Äôs a diagram of all the components:\n\nArchitecture diagram\nNext, you‚Äôll set up your local environment to run the project."
  },
  {
    "objectID": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#set-up-your-local-environment",
    "href": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#set-up-your-local-environment",
    "title": "Semantic Search with OpenSearch, Cohere, and FastAPI",
    "section": "Set Up Your Local Environment",
    "text": "Set Up Your Local Environment\nFollow these steps to set up your local environment:\n\nInstall Python 3.11.\nClone the repository with the sample app:\n\ngit clone https://github.com/dylanjcastillo/opensearch-cohere-semantic-search\n\nGo to the root folder of the project and create a virtual environment with the dependencies using venv and pip:\n\npython3.11 -m venv .venv && source .venv/bin/activate\npip install -r requirements.txt\nAssuming all went smoothly, you should have a virtual environment set up with the required libraries and the following project structure:\nopensearch-cohere-semantic-search\n‚îú‚îÄ‚îÄ LICENSE\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îú‚îÄ‚îÄ news.csv\n‚îÇ   ‚îú‚îÄ‚îÄ news_sample.csv\n‚îÇ   ‚îî‚îÄ‚îÄ news_sample_with_vectors.csv\n‚îú‚îÄ‚îÄ notebooks\n‚îÇ   ‚îî‚îÄ‚îÄ generate_sample.ipynb\n‚îú‚îÄ‚îÄ requirements.txt\n‚îú‚îÄ‚îÄ run_opensearch_docker.sh\n‚îî‚îÄ‚îÄ src\n‚îÇ   ‚îú‚îÄ‚îÄ app.py\n‚îÇ   ‚îú‚îÄ‚îÄ config.py\n‚îÇ   ‚îú‚îÄ‚îÄ indexer.py\n‚îÇ   ‚îî‚îÄ‚îÄ vectorizer.py\n‚îú‚îÄ‚îÄ .env-example\n‚îî‚îÄ‚îÄ .venv/\nThe project is organized into several key files and directories, as described below\n\ndata/: This directory contains the project‚Äôs data. It contains the original dataset downloaded from Kaggle, and a sample, which you‚Äôll use in the tutorial.\nrequirements.txt: This file contains a list of Python packages required by the project and their respective versions.\nrun_opensearch_docker.sh: This file contains a bash script used to run an OpenSearch cluster locally.\nsrc/app.py: This file contains the code of the FastAPI application.\nsrc/config.py: This file contains project configuration specifications such as Cohere‚Äôs API key (read from a .env file), the paths to the data, and the name of the index.\nsrc/indexer.py: This file contains the code you use to create an index and insert the documents in OpenSearch.\nsrc/vectorizer.py: This file contains the code to transform the input data into embeddings.\n.env-example: This file is an example of the environment variables you must provide.\n.venv/: This directory contains the project‚Äôs virtual environment.\n\nAll done! Let‚Äôs get going."
  },
  {
    "objectID": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#run-a-local-opensearch-cluster",
    "href": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#run-a-local-opensearch-cluster",
    "title": "Semantic Search with OpenSearch, Cohere, and FastAPI",
    "section": "Run a Local OpenSearch Cluster",
    "text": "Run a Local OpenSearch Cluster\nBefore we get into the code, you should start a local OpenSearch cluster. Open a new terminal, navigate to the project‚Äôs root folder, and run:\nsh run_opensearch_docker.sh\nThis will launch a local OpenSearch cluster. If everything went well, the terminal will show a long string of text. Keep the terminal open in the background and move on to the next step."
  },
  {
    "objectID": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#vectorize-the-articles",
    "href": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#vectorize-the-articles",
    "title": "Semantic Search with OpenSearch, Cohere, and FastAPI",
    "section": "Vectorize the Articles",
    "text": "Vectorize the Articles\nYou‚Äôll start by transforming the news articles into vectors (embeddings). There are many approaches you could take such as using Word2Vec, Sentence-Transformers, or LLM-based embedding services. In this case, you‚Äôll use Cohere.\nUse src/vectorizer.py for that:\nimport cohere\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom config import COHERE_API_KEY, NEWS_SAMPLE_DATASET, DATA\n\n\ndef main():\n    df = pd.read_csv(NEWS_SAMPLE_DATASET)\n    cohere_client = cohere.Client(COHERE_API_KEY)\n\n    model = \"small\"\n    batch_size = 96\n    batch = []\n    vectors = []\n\n    for _, row in tqdm(df.iterrows(), total=len(df)):\n        batch.append(row[\"text\"])\n\n        if len(batch) &gt;= batch_size:\n            response = cohere_client.embed(texts=batch, model=model)\n            vectors.append(response.embeddings)\n            batch = []\n\n    if len(batch) &gt; 0:\n        response = cohere_client.embed(texts=batch, model=model)\n        vectors.append(response.embeddings)\n        batch = []\n\n    df[\"vector\"] = [item for sublist in vectors for item in sublist]\n\n    df.to_csv(DATA / \"news_sample_with_vectors.csv\", index=False)\n\n\nif __name__ == \"__main__\":\n    main()\nThis code reads the news articles dataset, splits it into batches, and generates embeddings for each individual article. It works as follows:\n\nLines 1 to 5 import the required Python libraries and the configuration settings from config.py.\nLines 9 to 28 read the news articles sample, start the Cohere client, split the dataset into batches of 96 documents (as this is the maximum accepted by Cohere), and uses the client to get embeddings for each document.\nLines 30 to 32 create a new column in the DataFrame to store the vectors and save the new dataset into your filesystem.\n\nYou can run this script by opening a terminal in src and running:\npython vectorizer.py\nNext, you‚Äôll create an index to store the embeddings."
  },
  {
    "objectID": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#index-the-vectors-and-metadata",
    "href": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#index-the-vectors-and-metadata",
    "title": "Semantic Search with OpenSearch, Cohere, and FastAPI",
    "section": "Index the Vectors and Metadata",
    "text": "Index the Vectors and Metadata\nAfter you‚Äôve created embeddings of each article, you‚Äôll store them, and their metadata (title, content, description), in an index in your OpenSearch cluster.\nYou can use src/indexer.py for that:\nimport pandas as pd\nfrom opensearchpy import OpenSearch, NotFoundError\nfrom config import NEWS_WITH_VECTORS_DATASET, INDEX_NAME\n\nfrom tqdm import tqdm\n\n\ndef main():\n    client = OpenSearch(\n        hosts=[{\"host\": \"localhost\", \"port\": 9200}],\n        http_auth=(\"admin\", \"admin\"),\n        use_ssl=True,\n        verify_certs=False,\n        ssl_assert_hostname=False,\n        ssl_show_warn=False,\n    )\n\n    df = pd.read_csv(NEWS_WITH_VECTORS_DATASET)\n\n    body = {\n        \"settings\": {\n            \"index\": {\"knn\": True},\n        },\n        \"mappings\": {\n            \"properties\": {\n                \"id\": {\"type\": \"integer\"},\n                \"title\": {\"type\": \"keyword\"},\n                \"content\": {\"type\": \"keyword\"},\n                \"description\": {\"type\": \"keyword\"},\n                \"embedding\": {\"type\": \"knn_vector\", \"dimension\": 1024},\n            }\n        },\n    }\n\n    try:\n        client.indices.delete(index=INDEX_NAME)\n    except NotFoundError:\n        pass\n    client.indices.create(INDEX_NAME, body=body)\n\n    for i, row in tqdm(df.iterrows(), total=len(df)):\n        embedding = [\n            float(x) for x in row[\"vector\"].replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")\n        ]\n        client.index(\n            index=INDEX_NAME,\n            body={\n                \"source_id\": i,\n                \"title\": row[\"title\"],\n                \"content\": row[\"content\"],\n                \"description\": row[\"description\"],\n                \"embedding\": embedding,\n            },\n        )\n\n    client.indices.refresh(index=INDEX_NAME)\n    print(\"Done\", client.cat.count(index=INDEX_NAME, format=\"json\"))\n\n\nif __name__ == \"__main__\":\n    main()\nThis code will create a new index in your OpenSearch cluster, and store the vectors and metadata in it. Here‚Äôs how it works:\n\nLines 1 to 5 import the required Python libraries and the predefined configuration settings from config.py.\nLines 9 to 16 start the OpenSearch client.\nLines 20 to 33 define the settings and mappings of the index you‚Äôll create. You set \"knn\": True so that OpenSearch knows that you‚Äôll be using the k-NN plugin to store and retrieve vectors. Very importantly, you also need to define the size of the vector in the mappings, based on the model you use. Cohere‚Äôs small embeddings generate vectors of 1024 dimensions.\nLines 35 to 54 create the index (and delete any previous ones), and add each document one by one. You index the id, title, description, and embedding for each document.\n\nYou can run this script by opening a terminal in src and running:\npython indexer.py\nSo far, you‚Äôve created embeddings for each document and indexed them in your OpenSearch cluster. Next, you‚Äôll run a search client to interact with them."
  },
  {
    "objectID": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#create-a-search-client",
    "href": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#create-a-search-client",
    "title": "Semantic Search with OpenSearch, Cohere, and FastAPI",
    "section": "Create a Search Client",
    "text": "Create a Search Client\nFinally, you‚Äôll create a search client so that users can search the articles you indexed using FastAPI. It‚Äôll let users provide a search term, and give them back the 10 most similar documents based on that term.\nThe code is available in src/app.py:\nimport cohere\nfrom fastapi import FastAPI\n\nfrom config import COHERE_API_KEY, INDEX_NAME\n\nfrom opensearchpy import OpenSearch\n\n\napp = FastAPI()\n\nopensearch_client = OpenSearch(\n    hosts=[{\"host\": \"localhost\", \"port\": 9200}],\n    http_auth=(\"admin\", \"admin\"),\n    use_ssl=True,\n    verify_certs=False,\n    ssl_assert_hostname=False,\n    ssl_show_warn=False,\n)\n\ncohere_client = cohere.Client(COHERE_API_KEY)\n\n\n@app.get(\"/\")\ndef index():\n    return {\"message\": \"Make a post request to /search to search through news articles\"}\n\n\n@app.post(\"/search\")\ndef search(query: str):\n    query_embedding = cohere_client.embed(texts=[query], model=\"small\").embeddings[0]\n\n    similar_news = opensearch_client.search(\n        index=INDEX_NAME,\n        body={\n            \"query\": {\"knn\": {\"embedding\": {\"vector\": query_embedding, \"k\": 10}}},\n        },\n    )\n    response = [\n        {\n            \"title\": r[\"_source\"][\"title\"],\n            \"description\": r[\"_source\"][\"description\"],\n            \"content\": r[\"_source\"][\"content\"],\n        }\n        for r in similar_news[\"hits\"][\"hits\"]\n    ]\n\n    return {\n        \"response\": response,\n    }\nThis code lets users search through the index. It works as follows:\n\nLines 1 to 6 import the required Python libraries, and the configuration defined in config.py.\nLines 9 to 20 initialize the FastAPI app, and the OpenSearch and Cohere clients.\nLines 23 to 25 define an endpoint that provides the user with a message explaining how to use the app if they make a GET request to ‚Äú/‚Äù.\nLines 28 to 49 define a**/**search endpoint that accepts a query string parameter. It uses Cohere to generate an embedding from a query and then searches the OpenSearch index for the ten most similar documents. Finally, it formats the results as a user response.\n\nTo run the app, you can use uvicorn app:app --reload. You can test the app by opening your browser, navigating to localhost:8000/docs, and clicking on POST /search:\n\nFor instance, if you search for ‚ÄúNicolas Maduro,‚Äù the current president of Venezuela who is widely regarded as a dictator. You‚Äôll get results for articles about authoritarian governments or power abuses:\n\nThat‚Äôs it! If you want to know how to deploy this app, check out a previous article I wrote."
  },
  {
    "objectID": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#conclusion",
    "href": "posts/semantic-search-with-opensearch-cohere-and-fastapi.html#conclusion",
    "title": "Semantic Search with OpenSearch, Cohere, and FastAPI",
    "section": "Conclusion",
    "text": "Conclusion\nCongrats! You‚Äôve built your own semantic search service. In this tutorial, you‚Äôve learned:\n\nWhat is semantic search, and how it is different from keyword-based search.\nWhat are the main components of a semantic search service.\nHow to use Cohere to vectorize text data.\nHow to use OpenSearch to store embeddings.\n\nHope you found this tutorial useful. Let me know if you have any questions!\nAll the code for this tutorial is available on GitHub."
  },
  {
    "objectID": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html",
    "href": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html",
    "title": "Use Google Sheets, S3, and Python to Build a Website Quickly",
    "section": "",
    "text": "A few days ago, I was looking for courses that were free or discounted due to the COVID-19. I thought that others might be doing the same, so I decided to compile some resources and publish them online.\nI started compiling the courses using a Google Sheet and was planning on sharing it after I had enough resources. However, something was bothering me. Opening sheets on a mobile suck and most people use their phones for browsing the internet. I thought I could do better.\nThe thing is that I have little experience in web development. Also, I didn‚Äôt want to dedicate more than a few hour on developing and launching the site. So I decided to build something quickly.\nHere are the requirements I set for building the site quickly and the approach I took to meet them:\nProduct Requirements:\nApproach:\nAfter a couple of hours, I launched stayhomeandlearn.org.\nThe rest of this article is a tutorial on how to build a static site using Google Sheets, AWS, and Python.\nFor this tutorial, we will set up a script that reads data from Google Sheets, generates a static site using a predefined template, and deploys it to an S3 bucket. This article is meant for programmers with little knowledge of web development that want to get something running quickly.\nThere are five sections in the tutorial: Requirements, Review Code and Jinja Template, Using the Google Sheets API, and Build and Deploy Your Site."
  },
  {
    "objectID": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html#requirements",
    "href": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html#requirements",
    "title": "Use Google Sheets, S3, and Python to Build a Website Quickly",
    "section": "Requirements",
    "text": "Requirements\nThese you‚Äôll need to set up or review on your own. I added some links for that purpose.\n\nPython &gt;= 3.7\nGoogle account\nGoogle Cloud Platform (GCP) account\nAmazon AWS account\nAWS CLI (You can use brew if you have a Mac)\nA profile configured in the AWS CLI\nA bit of HTML and CSS"
  },
  {
    "objectID": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html#code-and-jinja-template",
    "href": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html#code-and-jinja-template",
    "title": "Use Google Sheets, S3, and Python to Build a Website Quickly",
    "section": "Code and Jinja Template",
    "text": "Code and Jinja Template\nFirst, create a directory called my_pokemon_stats and open a terminal from there. Then, create a virtual environment and install the required packages as follows:\npython3 -m venv venv\nsource venv/bin/activate\npip3 install boto3 gspread jinja2 oauth2client\nCreate virtual environment and install required libraries\nNext, download and save these two files there: template.html and site_builder.py. These are the building blocks for generating the site.\ntemplate.html is the Jinja template we will use for building the site. It‚Äôs an HTML-like file where you can add logic that will be processed in Python and generate the definitive site. This file looks as follows:\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"UTF-8\" /&gt;\n    &lt;meta\n      name=\"viewport\"\n      content=\"width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0\"\n    /&gt;\n    &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\" /&gt;\n    &lt;link\n      href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\"\n      rel=\"stylesheet\"\n      integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\"\n      crossorigin=\"anonymous\"\n    /&gt;\n    &lt;title&gt;My Pokemon Stats&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;header id=\"header\"&gt;\n      &lt;div class=\"container text-center\"&gt;\n        &lt;h1 class=\"pt-5 pb-1 font-weight-bold\"&gt;My Pokemon Stats&lt;/h1&gt;\n        &lt;hr /&gt;\n        &lt;p class=\"pt-2\"&gt;\n          This is a site I use to store the stats of all my Pokemon.\n        &lt;/p&gt;\n      &lt;/div&gt;\n    &lt;/header&gt;\n\n    &lt;section id=\"pokemon_table\"&gt;\n      &lt;div class=\"container py-4\"&gt;\n        &lt;div class=\"table-responsive\"&gt;\n          &lt;table class=\"table table-hover\"&gt;\n            &lt;thead class=\"thead-dark\"&gt;\n              &lt;tr&gt;\n                &lt;th scope=\"col\"&gt;Name&lt;/th&gt;\n                &lt;th scope=\"col\"&gt;Type 1&lt;/th&gt;\n                &lt;th scope=\"col\"&gt;Type 2&lt;/th&gt;\n                &lt;th scope=\"col\"&gt;Total&lt;/th&gt;\n                &lt;th scope=\"col\"&gt;HP&lt;/th&gt;\n                &lt;th scope=\"col\"&gt;Attack&lt;/th&gt;\n                &lt;th scope=\"col\"&gt;Defense&lt;/th&gt;\n              &lt;/tr&gt;\n            &lt;/thead&gt;\n            &lt;tbody&gt;\n              {% for row in data %}\n              &lt;tr&gt;\n                &lt;td&gt;{{ row[\"Name\"] }}&lt;/td&gt;\n                &lt;td&gt;{{ row[\"Type 1\"] }}&lt;/td&gt;\n                &lt;td&gt;{{ row[\"Type 2\"] }}&lt;/td&gt;\n                &lt;td&gt;{{ row[\"Total\"] }}&lt;/td&gt;\n                &lt;td&gt;{{ row[\"HP\"] }}&lt;/td&gt;\n                &lt;td&gt;{{ row[\"Attack\"] }}&lt;/td&gt;\n                &lt;td&gt;{{ row[\"Defense\"] }}&lt;/td&gt;\n              &lt;/tr&gt;\n              {% endfor %}\n            &lt;/tbody&gt;\n          &lt;/table&gt;\n        &lt;/div&gt;\n      &lt;/div&gt;\n    &lt;/section&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\nLet‚Äôs break it down:\n\nYou can safely ignore most of what‚Äôs inside the &lt;head&gt; tag. It‚Äôs standard HTML5 code that you‚Äôll see in most pages. However, there are just two interesting tags that we‚Äôll take a closer a look at: &lt;link&gt; and &lt;title&gt;.\nIn this case, the &lt;link&gt; tag is used to import the Bootstrap component library. We ¬†will use it to define simple styles for the different sections of the page and make it look good without much effort. &lt;title&gt; defines the title of the page (what you see in the browser‚Äôs tab) and it is useful for SEO and social media sharing.\nNext, there‚Äôs the &lt;header&gt; section inside the &lt;body&gt; tag. This is where we define the text that will appear in the page. It will look like the image below. I used standard styling from Bootstrap to center the text and added a bit of padding.\n\n\nMy Pokemon stats header\n\nFinally, we have the &lt;section id=\"pokemon_table\"&gt;. The &lt;div&gt; and &lt;table&gt; tags provide some basic styling for building a table. Next, we define the header of the table inside the &lt;thead&gt; tags. Inside the &lt;tbody&gt; tag is where Jinja does its magic\nThe {% for row in data %} is a loop that goes through each row of the Pokemon‚Äôs data. In each of the &lt;td&gt;{{ row[\"...\"] }}&lt;/td&gt; we get the information per row from field (e.g.¬†Name, Type 1, Type 2). This generates something that will look as follows:\n\n\nTable with Pokemon stats\nNext, we have the site_builder.py file. This script downloads the Pokemon‚Äôs data from Google Sheets, processes the data and the template.html file, and then uploads the resulting file to an S3 bucket.\nimport csv\n\nimport boto3\nimport gspread\nimport jinja2\nfrom oauth2client.service_account import ServiceAccountCredentials\n\nAWS_PROFILE = \"INSERT-AWS-PROFILE-HERE\"\nBUCKET = \"INSERT-BUCKET-NAME-HERE\"\nWORKBOOK = \"INSERT-WORKBOOK-NAME-HERE\"\n\n\ndef download_data():\n    \"\"\"Download data using the Google Sheets API\"\"\"\n    scope = [\n        \"https://spreadsheets.google.com/feeds\",\n        \"https://www.googleapis.com/auth/drive\",\n    ]\n    credentials = ServiceAccountCredentials.from_json_keyfile_name(\n        \"credentials.json\", scope\n    )\n    client = gspread.authorize(credentials)\n\n    worksheet = client.open(WORKBOOK).get_worksheet(0)\n    sheet_values = worksheet.get_all_values()\n\n    print(f\"Downloading: {worksheet.title}\")\n    with open(\"my_pokemon_stats.csv\", \"w\") as f:\n        writer = csv.writer(f)\n        writer.writerows(sheet_values)\n\n\ndef generate_site():\n    \"\"\"Generate site in local directory\"\"\"\n    print(\"Process data and build site\")\n\n    template_loader = jinja2.FileSystemLoader(searchpath=\"./\")\n    template_env = jinja2.Environment(loader=template_loader)\n    template = template_env.get_template(\"template.html\")\n\n    with open(\"my_pokemon_stats.csv\") as csv_file:\n        csv_reader = csv.DictReader(csv_file)\n        data = [row for row in csv_reader]\n\n    output = template.render(data=data)\n\n    with open(\"index.html\", \"w\") as f:\n        f.write(output)\n\n\ndef deploy_site():\n    \"\"\"Deploy site S3 bucket\"\"\"\n    print(\"Upload data to S3\")\n    session = boto3.Session(profile_name=AWS_PROFILE)\n    s3 = session.resource(\"s3\")\n    s3.Bucket(BUCKET).upload_file(\n        Filename=\"index.html\", Key=\"index.html\", ExtraArgs={\"ContentType\": \"text/html\"}\n    )\n\n\nif __name__ == \"__main__\":\n    download_data()\n    generate_site()\n    deploy_site()\nThe code is structured in 3 functions: download_sheets, generate_site, and deploy_site. We will fill the details for accessing AWS and the Google Sheets API in the next sections."
  },
  {
    "objectID": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html#using-the-google-sheets-api",
    "href": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html#using-the-google-sheets-api",
    "title": "Use Google Sheets, S3, and Python to Build a Website Quickly",
    "section": "Using the Google Sheets API",
    "text": "Using the Google Sheets API\nFollow these steps to download the Pokemon‚Äôs data using the Google Sheets API:\n\nCreate a Workbook in Google Sheets (you can copy mine: My Pokemon Stats)\nGo to the Google APIs Console\nCreate a new project called MyPokemonStats.\nClick on Enable API and Services. Search for and enable the Google Sheets API.\nGo back to the Google APIs Console and click on Enable API and Services again. Now search for and enable the Google Drive API.\nClick on Create credentials.For the next 4 questions select:Google Drive API, Web Server (e.g.¬†node.js, Tomcat), Application data, and No, I‚Äôm not using them.\nClick on What credentials do I need? Select a name for the service account (e.g.¬†get-data) grant it a Project Role of Editor. Select the JSON option for Key type\nA dialog box will open. Save the JSON file, copy it to the my_pokemon_stats directory, and rename it to credentials.json.\nOpen the credentials.json file. Find a key called client_email, copy its value (e.g., get-data@iam‚Ä¶.). Go back to your Workbook in Google Sheets, click the Share button in the top right, and paste the client email into the People field to give it edit rights. Hit Send.\nGo to the site_builder.py script, and set the WORKBOOK variable to whatever name you gave to your workbook on step 1."
  },
  {
    "objectID": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html#setting-up-an-s3-bucket-and-aws-related-configurations",
    "href": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html#setting-up-an-s3-bucket-and-aws-related-configurations",
    "title": "Use Google Sheets, S3, and Python to Build a Website Quickly",
    "section": "Setting up an S3 bucket and AWS-related configurations",
    "text": "Setting up an S3 bucket and AWS-related configurations\nNow, let‚Äôs create the S3 bucket and configure our code to access AWS programmatically:\n\nGo to the Amazon S3 Console\nCreate an S3 bucket\nOnce in the bucket, click on Properties and then on Static website hosting\nSelect the option Use this bucket to host a website\nUnder Index document and Error document put index.html\nSave the URL from Endpoint. You‚Äôll use that URL for connecting to your site.\nGo to Permissions and click Edit\nClear Block all public access, choose Save, and confirm. When you change this anyone on the internet will have access to the contents of this bucket. That‚Äôs what you want when you are publishing a site, however don‚Äôt put anything private there!\nNow go to Bucket Policy, replace the bucket name in the policy below, paste it there, and click Save.\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"PublicReadGetObject\",\n      \"Effect\": \"Allow\",\n      \"Principal\": \"*\",\n      \"Action\": [\"s3:GetObject\"],\n      \"Resource\": [\"arn:aws:s3:::BUCKET-NAME-HERE/*\"]\n    }\n  ]\n}\n\nGo to the site_builder.py script. Set the variable AWS_PROFILE variable to the profile name you use for accessing AWS (in a UNIX sytem it should be one of the profiles in ~/.aws/credentials)."
  },
  {
    "objectID": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html#build-and-deploy-your-site",
    "href": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html#build-and-deploy-your-site",
    "title": "Use Google Sheets, S3, and Python to Build a Website Quickly",
    "section": "Build and Deploy Your Site",
    "text": "Build and Deploy Your Site\nFinally, you should be able to run python site_builder.py from the root folder of the project to generate the site. This will download the data from Google Sheets, process the template.html file using Jinja and upload the site to the S3 bucket.\nIf you want to check the site, go to the endpoint URL (step 6 from the previous section)."
  },
  {
    "objectID": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html#closing-words",
    "href": "posts/build-a-site-quickly-using-google-sheets-python-and-aws.html#closing-words",
    "title": "Use Google Sheets, S3, and Python to Build a Website Quickly",
    "section": "Closing Words",
    "text": "Closing Words\nThis method is by no means perfect but will help you ship something quickly. I used this strategy to build stayhomeandlearn.org and it worked quite well. From April 1st until April 16th, the site got over fifteen thousand visitors, which exceeded my most optimistic expectations.\n\nThe site is slowly walking towards its death now. However, this process taught me how important it is to focus on shipping instead of wasting time looking for the perfect tools. I built the site quickly, people liked it, and after the first day it already had more traffic than any of the side projects I‚Äôve done so far. That thing about perfect being the enemy of good is real.\nIn my case, I had to add more functionality to the script for styling and deploying purposes. If you are interested, you can take a look at the code in my GitHub repository: https://github.com/dylanjcastillo/stayhomeandlearn.org\n[1] G. Bauges, Google Spreadsheets and Python (2017)\n[2] V. Drake, Hosting your static site with AWS S3, Route 53, and CloudFront (2017)\n[3] A. Barradas, Pokemon With Stats (2016)"
  },
  {
    "objectID": "posts/tips-for-standing-out-on-linkedin.html",
    "href": "posts/tips-for-standing-out-on-linkedin.html",
    "title": "Tips for Standing Out on LinkedIn",
    "section": "",
    "text": "A few months ago I wrote a post that blew up on r/datascience about how to make your LinkedIn profile stand out when searching for a job.\nSince then, the economic situation has worsened. We‚Äôre now on the verge of an economic downturn, more than 100,000 people have been laid off from tech companies this year, and investors are warning founders of tough times ahead, so I‚Äôve decided to expand on that original post.\nThis isn‚Äôt a post about how to get thousands of followers, nor how to become the most popular LinkedIn influencer. It‚Äôs about how to use LinkedIn to get more and better quality job opportunities.\nIt comes from reading many papers about LinkedIn‚Äôs search and ranking algorithms, learning about LinkedIn Recruiter, and making lots of mistakes while working on my own profile.\nLet‚Äôs get to it!"
  },
  {
    "objectID": "posts/tips-for-standing-out-on-linkedin.html#finding-a-great-job",
    "href": "posts/tips-for-standing-out-on-linkedin.html#finding-a-great-job",
    "title": "Tips for Standing Out on LinkedIn",
    "section": "Finding a (Great) Job",
    "text": "Finding a (Great) Job\nThere are two ways to find a good job: obsess over a few companies or play the numbers game. On the plus side, the former will increase your chances of landing your dream job, while the latter will likely lead to a better financial outcome. On the other hand, the former is riskier because it places all of your eggs in one or a few baskets, whereas the latter may not lead to the job you were hoping for.\nIn this article, I‚Äôll focus on the second strategy as that‚Äôs the one I‚Äôve focused on in my career. I never had a dream job in mind because I‚Äôve always wanted to start my own business and the financial aspect was also important to me, so I prioritized learning relevant skills as well as a good salary. Despite not having a ‚Äúdream job,‚Äù I was able to work at cool places like the Boston Consulting Group, Deliveroo, and the Olympics. I‚Äôve also successfully transitioned into full-time freelancing in a very competitive industry.\nHowever, this does not imply that the approach I took is the best one. Everyone has different preferences and should choose a strategy based on what they want to optimize for."
  },
  {
    "objectID": "posts/tips-for-standing-out-on-linkedin.html#playing-the-numbers-game",
    "href": "posts/tips-for-standing-out-on-linkedin.html#playing-the-numbers-game",
    "title": "Tips for Standing Out on LinkedIn",
    "section": "Playing the Numbers Game",
    "text": "Playing the Numbers Game\nThe numbers game in job hunting consists of increasing the number of relevant job opportunities that you can access. It is not simply applying to as many job openings as possible.\nYour goal is to obtain as many relevant opportunities as possible by actively seeking them or by making your profile appealing to hiring managers and recruiters. Those who apply for jobs mindlessly are not following this strategy correctly. They‚Äôre just wasting their time.\nYou should think of your job search as a three-part funnel:\n\nLeads (job opportunities)\nInterviews\nOffers\n\nI will cover the first part of the funnel, Leads, and a will provide you with some tips for the second, Interviews. For the first part, the advice comes from my own experience, and the research I‚Äôve done about LinkedIn‚Äôs algorithms. For the second part, the advice is mostly based on my own experience.\nYou‚Äôll need to focus on two things to increase the number of relevant job opportunities or ‚Äúqualified leads‚Äù: inbound and outbound leads.\n\nInbound Leads\nInbound leads are those that come to you without your intervention. Typically, recruiters and hiring managers connect with you on LinkedIn or send you InMail messages.\nRecruiters use LinkedIn Recruiter to find candidates. They can search for terms such as ‚ÄúData Scientist‚Äù and define filters like ¬†‚Äúhas worked at Google‚Äù when looking for candidates.\nAfter a query is defined, LinkedIn Recruiter uses what they call a talent search algorithm. It works in two stages:\n\nSearch: It searches the network and defines a set of a few thousand candidates who meet the recruiter‚Äôs search criteria.\nRank: It provides the recruiter with a list of candidates ranked by how well they fit the search term and how likely they are to respond.\n\nThat‚Äôs all. If you want to get more job opportunities, you must figure out how to increase your chances of appearing in step 1 and ranking higher during step 2.\nFortunately, LinkedIn has released tons of research on its talent search algorithm. It‚Äôs not difficult to imagine what will help you stand out from the crowd. Here‚Äôs what I‚Äôve found more impactful:\n\nUse relevant keywords in your profile. You won‚Äôt show up in the results if your profile doesn‚Äôt include search terms that recruiters use to find candidates. Examine the keywords used in the job descriptions for the positions you‚Äôre interested in, and make sure you have them in your profile.\nReply to recruiters. People often don‚Äôt reply to recruiters when they‚Äôre not interested in the job opportunity. But the algorithm prioritizes those who are likely to respond over those who are not. Even if it‚Äôs just to say no, respond to recruiters!\nEngage with the brands you‚Äôre interested in on LinkedIn. Recruiters can narrow down their search to candidates who have interacted with the brand or have connections who work for that company. If you‚Äôre particularly interested in a company, follow their profile, interact with their content, and add connections who work there.\nExpand your network. LinkedIn Recruiter Lite, a cheaper version of LinkedIn Recruiter, only lets users reach out to candidates up to their third-degree network. This means that the fewer connections you have, the less likely it is a recruiter can contact you.\nIncrease your influence. If you create engaging content, have a large number of visitors to your profile, or receive endorsements and recommendations, you will rank higher. As a general rule, try to write useful content on a regular basis and solicit recommendations from relevant contacts. LinkedIn‚Äôs Social Selling Index is a good proxy for how well you‚Äôre positioning yourself.\nGet a good photo. This is based on my personal experience. But I believe people are more likely to contact you if your photo looks somewhat professional.\n\nNone of these concepts are revolutionary, but most people overlook them when creating their profiles. LinkedIn‚Äôs goal is to match recruiters with the best possible candidates. So your job is to figure out what recruiters are searching for and how to best match that.\nFurthermore, even if recruiters or hiring managers do not pay for LinkedIn Recruiter and instead use the standard search service, the suggestions above will still help you improve your profile.\n\n\nOutbound Leads\nOutbound opportunities are the ones you apply to. Usually, that means applying for jobs on LinkedIn under the Jobs tab.\nThis takes time and has a very low ROI if not done correctly. I‚Äôve discovered that the following increases its effectiveness:\n\nSet up alerts for roles you‚Äôre interested in.\nDon‚Äôt apply for jobs posted more than a week ago.\nPrioritize jobs for which you can contact the poster or someone in a relevant position within the company. In those cases, send them a personalized message expressing your interest. The best way to accomplish this is to connect with them and add a note to the connection request.\nChange your CV to fit the position you‚Äôre applying for. Read the job‚Äôs requirements and try to highlight the parts of your work history that match them.\n\nFinally, many opportunities are built through real-world connections, so reach out to people outside of LinkedIn, join relevant communities, and attend meetups."
  },
  {
    "objectID": "posts/tips-for-standing-out-on-linkedin.html#interviewing-101",
    "href": "posts/tips-for-standing-out-on-linkedin.html#interviewing-101",
    "title": "Tips for Standing Out on LinkedIn",
    "section": "Interviewing 101",
    "text": "Interviewing 101\nIt is usually simple to figure out how to prepare for an interview. The difficult part is carrying out the plan.\nI‚Äôve only worked as a Data Scientist and ML Engineer, so I can‚Äôt offer advice for interviews outside of those roles. Here are some examples of things that help you get better results for interviews for those roles.\n\nGeneral Advice\nYou should do the following for any kind of interview:\n\nResearch the company and the role. You should know what the company does, its competitors, and recent news. You should also think about what the role you‚Äôre applying for entails. Also, use Glassdoor, Reddit, or ask other people to find out what questions they typically ask during interviews.\nReview the projects you‚Äôve worked on. Make sure that you know the ins and outs of each one and have an elevator pitch for each of them. You‚Äôd be surprised how many people are rejected because they don‚Äôt fully understand key details in the projects they‚Äôve worked on or can‚Äôt clearly explain what they did.\nBe assertive. One of the worst places to sell yourself short is during an interview. While it‚Äôs obvious that lying during interviews is bad (and you will be caught sooner or later), not expressing confidence is equally bad.\nConsider how many people are willing to lie to get a job; if you sell yourself short, you give dishonest people a better chance of winning.\nSee the interview as a conversation of equals. If you‚Äôve mastered the technical bits, the only big obstacle in an interview is the mindset. They‚Äôre seeing if you‚Äôre a good fit, but you‚Äôre also seeing if they are a good fit. Don‚Äôt treat it like an exam.\nDon‚Äôt take rejections personally. Some interviews will go well, others won‚Äôt. Sometimes you‚Äôre to blame, and other times it‚Äôs due to circumstances beyond your control. When you fail an interview, consider why it didn‚Äôt go well, use the feedback to improve your next interview and move on.\n\nIn a nutshell, know yourself, know what you‚Äôre interviewing for, and be assertive.\n\n\nBehavioral interview\nThese questions are opportunities to sell yourself, so make sure you have good responses. It simply takes practice.\nHere‚Äôs what you should do:\n\nMake a list of 10-15 commonly asked questions (you can start here)\nRecord yourself answering those questions. For questions like ‚ÄúGive me an example when you did‚Ä¶‚Äù or ‚ÄúTell me about a situation when you‚Ä¶‚Äù use the STAR framework.\nSee the recordings, give yourself honest feedback, and repeat the process.\nTry practicing with a friend or colleague, and ask them for feedback.\n\nLet me say it again: practice these questions! You should make sure you get these ‚Äúeasy‚Äù points during an interview.\n\n\nTechnical interview\nTechnical interviews aren‚Äôt usually a great measure of your skills, so don‚Äôt base your identity on how you do in them. They are like tests, and all you have to do to pass them is study. And if you don‚Äôt pass, it‚Äôs not the end of the world; you‚Äôll have more chances elsewhere.\nHere‚Äôs what you should do to prepare at the very least:\n\nResearch how the company and team conduct technical interviews.\nExamine the job description to determine the most important topics to research.\nPractice questions in the programming languages used in the team on HackerRank or similar.\nUse Anki or another Spaced Repetition System (SRS) to practice key topics you might need to cover. Ideally, make your own cards using your favorite book on the topic.\n\n\n\nTake-home Challenge\nThese are hard to practice because every company does them their own way. If you‚Äôre a Data Scientist or ML Engineer, you can go to Kaggle, find a dataset that looks interesting to you, come up with some interesting questions, and then answer them. Or building a small ML model for a specific purpose.\nBonus points: Make sure your projects look nice, write a README explaining what you did, and put all that on GitHub. Also, write short posts on LinkedIn and share your projects."
  },
  {
    "objectID": "posts/tips-for-standing-out-on-linkedin.html#conclusion",
    "href": "posts/tips-for-standing-out-on-linkedin.html#conclusion",
    "title": "Tips for Standing Out on LinkedIn",
    "section": "Conclusion",
    "text": "Conclusion\nThat‚Äôs it! By now, you should have a good sense of how LinkedIn works and how you can use it to get more opportunities your way.\nIf you‚Äôre interested in learning more about the technical aspects of how the LinkedIn search works, they‚Äôve published a lot of useful material you can check:\n\nPersonalized Expertise Search at LinkedIn\nTowards Deep and Representation Learning for Talent Search at LinkedIn\nTalent Search and Recommendation Systems at LinkedIn: Practical Challenges and Lessons Learned\nDeep Natural Language Processing For LinkedIn Search\nFrom Query-By-Keyword to Query-By-Example: LinkedIn Talent Search Approach\nDeText: A Deep Text Ranking Framework with BERT\n\nI hope you find this useful. If you have any questions reach out!"
  },
  {
    "objectID": "posts/python-pyscript-101.html",
    "href": "posts/python-pyscript-101.html",
    "title": "PyScript 101",
    "section": "",
    "text": "These past few days, I‚Äôve been playing around with PyScript. I built an interactive cheat sheet for pandas and a cookbook of the most popular Python data visualization libraries.\nI‚Äôve learned a few things about how it works, so I thought of putting together these notes in case others may find them useful. This article covers the minimum you should know about PyScript to build a simple web app with it.\nI deliberately left out topics that I didn‚Äôt find as useful or interesting. But if you want a more detailed introduction, you should read this great tutorial from Real Python.\nLet‚Äôs get to it!"
  },
  {
    "objectID": "posts/python-pyscript-101.html#whats-pyscript",
    "href": "posts/python-pyscript-101.html#whats-pyscript",
    "title": "PyScript 101",
    "section": "What‚Äôs PyScript?",
    "text": "What‚Äôs PyScript?\nPyScript is a framework that lets users create Python applications in the browser. It‚Äôs built on top of Pyodide, a CPython port to WebAssembly (WASM).\nPyScript‚Äôs main selling point is that you no longer need a server to use Python and its extensive number of libraries when building a web app. You can plug in PyScript, make a few adjustments, and use many Python libraries in your web app.\nPeople often compare PyScript with Dash and Streamlit. But PyScript works quite differently. It operates on the client side, so no server is required. You could host a web app that uses PyScript for free using services like GitHub Pages.\nDash and Streamlit, on the other hand, require a server, making apps built with these libraries more difficult to deploy and usually more expensive. Furthermore, the server bears the majority of the computational load."
  },
  {
    "objectID": "posts/python-pyscript-101.html#how-to-use-pyscript",
    "href": "posts/python-pyscript-101.html#how-to-use-pyscript",
    "title": "PyScript 101",
    "section": "How to Use PyScript",
    "text": "How to Use PyScript\nThe good thing about PyScript is that there isn‚Äôt any installation required. You just need to add it to the head of your HTML file.\nI‚Äôll assume you have a file named index.html with this content for this and the following sections:\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n    &lt;head&gt;\n        &lt;meta charset=\"UTF-8\"&gt;\n        &lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"&gt;\n        &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n        &lt;title&gt;My Awesome app!&lt;/title&gt;\n    &lt;/head&gt;\n\n    &lt;body&gt;\n\n    &lt;/body&gt;\n&lt;/html&gt;\nThe code above is a minimal example we‚Äôll use as a starting point. Next, I‚Äôll show you how to use PyScript. You can do it in two ways:\n\nDownload a copy of the latest version of PyScript, and add the required files to head in your HTML document:\n\n&lt;head&gt;\n  ...\n  &lt;link rel=\"stylesheet\" href=\"path/to/pyscript.css\" /&gt;\n  &lt;script defer src=\"path/to/pyscript.js\"&gt;&lt;/script&gt;\n&lt;/head&gt;\n\nAdd the CDN version of PyScript‚Äôs JS and CSS files to head in your HTML document:\n\n&lt;head&gt;\n  ...\n  &lt;link rel=\"stylesheet\" href=\"https://pyscript.net/latest/pyscript.css\" /&gt;\n  &lt;script defer src=\"https://pyscript.net/latest/pyscript.js\"&gt;&lt;/script&gt;\n&lt;/head&gt;\nPyScript is still under heavy development. If you use the second method, your app may break when new versions are released.\nTo test your changes locally, use Live Server if you like working with VS Code or try any of these methods if you prefer other code editors."
  },
  {
    "objectID": "posts/python-pyscript-101.html#how-to-run-python-code-in-pyscript",
    "href": "posts/python-pyscript-101.html#how-to-run-python-code-in-pyscript",
    "title": "PyScript 101",
    "section": "How to Run Python Code in PyScript",
    "text": "How to Run Python Code in PyScript\nThere are two ways to run Python code in PyScript: using py-script and py-repl tags. These elements inside go inside body in your HTML document.\n\npy-script\nYou can run code using the py-script tag in two ways:\n\nWrap your code with a py-script tag.\n\n&lt;body&gt;\n  ...\n  &lt;py-script&gt;print(\"Hello, world!\")&lt;/py-script&gt;\n  ...\n&lt;/body&gt;\n\nLoad an external script using the src attribute from the py-script tag:\n\n&lt;body&gt;\n    ...\n    &lt;py-script src='path/to/script.py'&gt;&lt;/pyscript&gt;\n    ...\n&lt;/body&gt;\nThe latter is usually preferred to avoid having formatting issues. If you use an HTML auto-formatter, it may break the python code you put inside py-script.\n\n\npy-repl\nYou can also run code using an interactive interpreter, usually known as read‚Äìeval‚Äìprint loop (REPL). Using a REPL makes it feel like you‚Äôre running code in a Jupyter notebook.\nYou can create a REPL with PyScript using the py-repl tag as follows:\n&lt;body&gt;\n  &lt;py-repl&gt;print(\"Hello, world!\")&lt;/py-repl&gt;\n&lt;/body&gt;\nIf you try using multiple py-script and py-repl in the same page, you may find some unexpected behavior in the output. I came across two issues related to this but it looks like they‚Äôll get fixed soon."
  },
  {
    "objectID": "posts/python-pyscript-101.html#how-to-import-libraries",
    "href": "posts/python-pyscript-101.html#how-to-import-libraries",
    "title": "PyScript 101",
    "section": "How to Import Libraries",
    "text": "How to Import Libraries\nYou can make Python libraries available in your environment when your app loads by specifying them in the py-env tag as follows:\n&lt;head&gt;\n  ...\n  &lt;py-env&gt; - numpy - pandas - seaborn &lt;/py-env&gt;\n&lt;/head&gt;\nOr you can load them programmatically using micropip:\n&lt;py-script&gt;\n  async def main(): await micropip.install([\"numpy\", \"pandas\", \"seaborn]) await\n  loop.run_until_complete(main())\n&lt;/py-script&gt;"
  },
  {
    "objectID": "posts/python-pyscript-101.html#how-to-interact-with-html-elements",
    "href": "posts/python-pyscript-101.html#how-to-interact-with-html-elements",
    "title": "PyScript 101",
    "section": "How to Interact With HTML Elements",
    "text": "How to Interact With HTML Elements\nPyScript automatically imports a few things from JavaScript‚Äôs global namespace into your Python environment. This means that you can use use these elements without having to import them beforehand. The most important ones are window, document, and console.\nwindow, and document, let you read and make changes to elements from the Document Object Model (DOM). While console lets you interact with your browser‚Äôs integrated console.\nFor example, you could use document to select a specific element from the DOM:\n&lt;body&gt;\n  ...\n  &lt;py-script&gt;\n    # Select an element using its ID document.getElementById(\"input-code\") #\n    Select an element using its class document.querySelector(\".table\")\n  &lt;/py-script&gt;\n  ...\n&lt;/body&gt;\nOr usedocument to read attributes from elements and make changes to them, and print information using console:\n&lt;body&gt;\n  &lt;ul id=\"users-list\"&gt;\n    &lt;li&gt;Dylan&lt;/li&gt;\n    &lt;li&gt;John&lt;/li&gt;\n    &lt;li&gt;Paul&lt;/li&gt;\n    &lt;li&gt;Jane&lt;/li&gt;\n  &lt;/ul&gt;\n  &lt;py-script&gt;\n    users = [\"Dylan\", \"John\", \"Jane\"] ul = document.querySelectorAll(\"ul &gt; li\")\n    for li in ul: if li.innerHTML not in users: console.log(li.innerHTML)\n    li.remove()\n  &lt;/py-script&gt;\n&lt;/body&gt;\nYou could also make your Python code react to changes in the DOM. But that requires some magic to work because the web browser cannot natively handle Python functions when an event such as a click on a button happens.\nFor those cases, Pyodide lets you create proxies that bridge Python functions and JavaScript callbacks. You could use create_proxy to make your Python code react to changes as follows:\n&lt;body&gt;\n  &lt;input type=\"text\" name=\"\" id=\"input-text\" /&gt;\n  &lt;button type=\"submit\" id=\"button-submit\" for=\"input-text\"&gt;Submit!&lt;/button&gt;\n  &lt;div id=\"text-length\"&gt;&lt;/div&gt;\n  &lt;py-script&gt;\n    from pyodide import create_proxy input_button =\n    document.getElementById(\"button-submit\") def get_text_length(x):\n    document.getElementById(\"text-length\").innerHTML =\n    len(document.getElementById(\"input-text\").value)\n    input_button.addEventListener(\"click\", create_proxy(get_text_length))\n  &lt;/py-script&gt;\n&lt;/body&gt;\nThe code lets the user input a text and run the get_text_length Python function, after he or she clicks on the button on the screen. This function calculates the length of the text that the user entered.\nFinally, it‚Äôs worth mentioning that PyScript also provides you with the [Element](https://realpython.com/pyscript-python-in-browser/#pyscripts-adapter-for-javascript-proxy) class. It‚Äôs a basic interface that lets you select elements by ID and do certain operations on them. I found it a bit limited, which is why I didn‚Äôt cover it in this section."
  },
  {
    "objectID": "posts/python-pyscript-101.html#other-topics",
    "href": "posts/python-pyscript-101.html#other-topics",
    "title": "PyScript 101",
    "section": "Other Topics",
    "text": "Other Topics\nThere are many topics I didn‚Äôt cover in this article. So I wanted to provide some resources additional resources I‚Äôve found useful:\n\nPyScript visual components: PyScript provides a few elements you can use to build the UI of your web app. The Real Python tutorial covers them.\nAccessing the file system: Accessing files is a bit tricky. John Hanley made very detailed tutorials about this topic.\nPyScript and plotly: Getting plotly graphs to work with PyScript is tricky. This article explains how to do it.\nHosting PyScript: If you don‚Äôt need to store data from your users in a server, you can simply host a static page. The easiest way to do that is using GitHub Pages. That‚Äôs what I‚Äôve used in my projects."
  },
  {
    "objectID": "posts/python-pyscript-101.html#conclusion",
    "href": "posts/python-pyscript-101.html#conclusion",
    "title": "PyScript 101",
    "section": "Conclusion",
    "text": "Conclusion\nPyScript is an experimental Python framework that lets you run Python on the web browser. This article covers the basics of PyScript, and will show you how to:\n\nUse PyScript on a web page\nRun Python code using PyScript\nInteract with the DOM using PyScript\n\nIf you have any questions or feedback, let me know in the comments!"
  },
  {
    "objectID": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html",
    "href": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html",
    "title": "Create a Code Interpreter Chatbot with Pyodide, LangChain, and OpenAI",
    "section": "",
    "text": "OpenAI has been giving access to users to the Code Interpreter plugin and people are loving it. I wanted to replicate it outside of ChatGPT, so I created my own (simpler) version, specifically to analyze and visualize data.\nFollowing that, I figured more people might be interested in building user-facing chatbots capable of running code in the browser. So I put together this tutorial. It will teach you how to create a simple but powerful chatbot that uses Pyodide, LangChain, and OpenAI to generate and run code in the browser for a user.\nC‚Äômon, let‚Äôs get to work!"
  },
  {
    "objectID": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#prerequisites",
    "href": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#prerequisites",
    "title": "Create a Code Interpreter Chatbot with Pyodide, LangChain, and OpenAI",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo make the most of the tutorial, you should review these topics before getting started:\n\nWhat Pyodide is.\nWhat LangChain is.\nThe basics of Python backend servers such as Litestar or FastAPI.\n\nIn addition, you must create an account at OpenAI."
  },
  {
    "objectID": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#building-a-code-interpreter-chatbot",
    "href": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#building-a-code-interpreter-chatbot",
    "title": "Create a Code Interpreter Chatbot with Pyodide, LangChain, and OpenAI",
    "section": "Building a Code Interpreter Chatbot",
    "text": "Building a Code Interpreter Chatbot\nSecurity and scalability are two major challenges in developing a user-facing chatbot that‚Äôs capable of executing code. Companies like Replit must manage extremely complex infrastructures in order to provide users with online IDEs.\nWe‚Äôre fortunate, however, because what we‚Äôre doing in this tutorial isn‚Äôt as complex as Replit. We can use a simple solution: Pyodide. Pyodide is a CPython port to WebAssembly/Emscripten that lets Python to run in the browser.\nThere are some restrictions to what you can do with Pyodide, such as the fact that not every package is compatible with it and that the maximum memory it can manage is 2GB. But it is more than adequate to process small to medium datasets, which is what you‚Äôll do in this tutorial.\nThe chatbot you‚Äôll build will work as follows:\n\nA user asks a question about a preloaded dataset.\nThat question, along with a predefined prompt, is sent to the OpenAI API.\nThe API responds with a code snippet that helps answer the question.\nThe code snippet is executed on the browser using Pyodide, and the result is displayed to the user.\n\nNext, you‚Äôll set up your local environment."
  },
  {
    "objectID": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#set-up-your-local-environment",
    "href": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#set-up-your-local-environment",
    "title": "Create a Code Interpreter Chatbot with Pyodide, LangChain, and OpenAI",
    "section": "Set Up Your Local Environment",
    "text": "Set Up Your Local Environment\nBefore you begin, you must first set up a few things. Follow these steps:\n\nInstall Python 3.10.\nInstall Poetry. It‚Äôs optional but highly recommended.\nClone the project‚Äôs repository:\n\ngit clone https://github.com/dylanjcastillo/chatbot-code-interpreter\n\nFrom the root folder of the project, install the dependencies:\n\nUsing Poetry: Create the virtual environment in the same directory as the project and install the dependencies:\n\npoetry config virtualenvs.in-project true\npoetry install\n\nUsing venv and pip: Create a virtual environment and install the dependencies listed in requirements.txt:\n\npython3.10 -m venv .venv && source .venv/bin/activate\npip install -r requirements.txt\nOpen src/.env-example, add your OpenAI secret key in the corresponding variable, and save the file as .env.\n\nYou should now have a virtual environment set up with the necessary libraries and a local copy of the repository. Your project structure should look like this:\nchatbot-code-interpreter\n‚îú‚îÄ‚îÄ LICENSE\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ poetry.lock\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îú‚îÄ‚îÄ requirements.txt\n‚îú‚îÄ‚îÄ src\n‚îÇ   ‚îú‚îÄ‚îÄ app.py\n‚îÇ   ‚îú‚îÄ‚îÄ config.py\n‚îÇ   ‚îú‚îÄ‚îÄ prompts\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ system.prompt\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ user.prompt\n‚îÇ   ‚îú‚îÄ‚îÄ static\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ all_stocks.csv\n‚îÇ   ‚îú‚îÄ‚îÄ templates\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.html\n‚îÇ   ‚îú‚îÄ‚îÄ utils.py\n‚îÇ   ‚îî‚îÄ‚îÄ .env-example\n‚îî‚îÄ‚îÄ .venv/\nThese are the most relevant files and directories in the project:\n\npoetry.lock and pyproject.toml: These files contain the project‚Äôs specifications and dependencies. They‚Äôre used by Poetry to create a virtual environment.\nrequirements.txt: This file contains a list of Python packages required by the project.\nsrc/app.py: This file contains the code of the chatbot.\nsrc/config.py: This file contains project configuration details such as OpenAI‚Äôs API key (read from a .env file), and the path to the prompts used by the chatbot.\nsrc/prompts/: This directory contains the system and user prompts used by the chatbot. I‚Äôve found that keeping the prompts in text files makes it easier to manage them instead of using strings.\nsrc/static and src/templates: These files contain the data used in the example and the HTML template used for the chatbot‚Äôs interface. You‚Äôll use a dataset with prices and the volume of a group of stocks.\nsrc/utils.py: This file contains a function you use to read the prompts from src/prompts.\n.env-example: This file is a sample file that provides the required environment variables you should provide. In this case, you use it to pass OpenAI‚Äôs API key to your application and choose the name the chatbot should use behind the scenes (gpt-3.5.-turbo).\n.venv/: This directory contains the project‚Äôs virtual environment.\n\nAlright! On to the exciting stuff now."
  },
  {
    "objectID": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#run-code-on-the-browser-with-pyodide",
    "href": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#run-code-on-the-browser-with-pyodide",
    "title": "Create a Code Interpreter Chatbot with Pyodide, LangChain, and OpenAI",
    "section": "Run Code on the Browser with Pyodide",
    "text": "Run Code on the Browser with Pyodide\nIn this step, you‚Äôll configure Pyodide to run Python in the browser. You‚Äôll load pyodide.js, start it, import the required libraries, and define an event handler to process the questions asked by the user.\nTo make the analysis simpler, I‚Äôll split the code of src/templates/index.html into two sections. In the first section, you‚Äôll only look at the contents of &lt;head&gt; and &lt;body&gt;, and in the second part, you‚Äôll go through the JavaScript code defined in &lt;script&gt;.\nHere‚Äôs part one:\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"UTF-8\" /&gt;\n    &lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" /&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" /&gt;\n    &lt;title&gt;Code Runner Chatbot&lt;/title&gt;\n    &lt;link\n      rel=\"stylesheet\"\n      href=\"https://cdn.jsdelivr.net/npm/@picocss/pico@1/css/pico.min.css\"\n    /&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/pyodide/v0.23.2/full/pyodide.js\"&gt;&lt;/script&gt;\n  &lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;main class=\"container\"&gt;\n      &lt;h1 style=\"text-align:center;\"&gt;Code Runner Chatbot&lt;/h1&gt;\n      &lt;textarea\n        name=\"query\"\n        id=\"query\"\n        cols=\"30\"\n        rows=\"10\"\n        placeholder=\"Ask a question about the dataset\"\n      &gt;&lt;/textarea&gt;\n      &lt;button id=\"ask-btn\"&gt;Ask&lt;/button&gt;\n      &lt;blockquote id=\"output\"&gt;&lt;/blockquote&gt;\n    &lt;/main&gt;\n    &lt;script&gt;\n      &lt;!-- SECOND PART OF CODE --&gt;\n    &lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\nThis section of code loads the necessary libraries and styles, as well as defines the chatbot‚Äôs user interface. Here‚Äôs how it works:\n\nLines 5 to 10 set a title for the page, load pyodide.js and pico.css (a minimal CSS framework), and define a couple of standard meta tags.\nLines 14 to 20 define a simple UI that lets users input a question, and submit them by clicking on a button, to get answers about the dataset.\n\nThen, let‚Äôs go through the JavaScript code defined in &lt;script&gt;. This code will load Pyodide, install the required libraries, make the data available for use in Pyodide, and set an event handler to process the user‚Äôs question.\nHere‚Äôs how it looks:\nconst queryTextArea = document.getElementById(\"query\");\nconst outputElement = document.getElementById(\"output\");\nconst askBtn = document.getElementById(\"ask-btn\");\n\nasync function setupPyodide() {\n  let pyodide = await loadPyodide();\n\n  await pyodide.loadPackage([\"pandas\", \"numpy\"]);\n\n  const response = await fetch(\"/static/all_stocks.csv\");\n  const fileContentArrayBuffer = await response.arrayBuffer();\n  const fileContent = new Uint8Array(fileContentArrayBuffer);\n  pyodide.FS.writeFile(\"all_stocks.csv\", fileContent);\n\n  return pyodide;\n}\n\nlet pyodideReadyPromise = setupPyodide();\n\naskBtn.addEventListener(\"click\", async (event) =&gt; {\n  let pyodide = await pyodideReadyPromise;\n\n  const query = queryTextArea.value;\n  const df_info = await pyodide.runPythonAsync(`\n    import pandas as pd\n    df = pd.read_csv('all_stocks.csv')\n    pd.set_option('display.max_columns', None)\n    df.head(3).T\n    `);\n\n  const data = new FormData();\n  data.append(\"query\", query);\n  data.append(\"df_info\", df_info);\n\n  try {\n    const response = await fetch(\"/ask\", {\n      method: \"POST\",\n      body: data,\n    });\n\n    if (response.ok) {\n      const result = await response.text();\n      const output = await pyodide.runPythonAsync(result);\n\n      outputElement.innerText = output;\n    } else {\n      console.error(\"Error:\", response.statusText);\n    }\n  } catch (error) {\n    console.error(\"Error:\", error);\n  }\n});\nThis code loads Pyodide, installs the necessary libraries, makes the data accessible in Pyodide, and finally sets an event handler to process the user‚Äôs question. It works as follows:\n\nLines 1 to 3 select the elements of the DOM that you‚Äôll be interacting with.\nLines 5 to 18 define a helper function you use to load Pyodide and the data. You first load Pyodide, and install pandas and numpy. Then, you fetch the dataset and write it into Pyodide‚Äôs filesystem, to make it available to use the data in it.\nLines 20 to 52 define the event handler of a click on ask-btn. This means that whenever that button is clicked, this event will execute. The handler does a few things:\n\nLines 21 to 33 wait until Pyodide is fully loaded, extract the question that the user has asked, get the first rows of the dataset (which is how the model can know how to generate the right code), and put together the data to send in a POST request.\nLines 35 to 51 make a POST request to ‚Äú/ask‚Äù with the parameters mentioned earlier. When the server responds, that text is run using Pyodide, and the result is then saved in output.\n\n\nThat‚Äôs it! Next, you‚Äôll create the chatbot application."
  },
  {
    "objectID": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#create-the-chatbot",
    "href": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#create-the-chatbot",
    "title": "Create a Code Interpreter Chatbot with Pyodide, LangChain, and OpenAI",
    "section": "Create the Chatbot",
    "text": "Create the Chatbot\nNow, you‚Äôll create a simple application that will allow users to make questions to the chatbot about the dataset.\nTake a look at the code in src/app.py:\nfrom dataclasses import dataclass\nfrom pathlib import Path\n\nfrom langchain import LLMChain\nfrom langchain.chat_models import ChatOpenAI\nfrom litestar import Litestar, get, post\nfrom litestar.contrib.jinja import JinjaTemplateEngine\nfrom litestar.enums import RequestEncodingType\nfrom litestar.params import Body\nfrom litestar.response_containers import Template\nfrom litestar.static_files.config import StaticFilesConfig\nfrom litestar.template.config import TemplateConfig\nfrom typing_extensions import Annotated\n\nfrom config import OpenAI\nfrom utils import get_prompt\n\nchain_create = LLMChain(\n    llm=ChatOpenAI(\n        temperature=0,\n        model_name=OpenAI.model_name,\n        openai_api_key=OpenAI.secret_key,\n    ),\n    prompt=get_prompt(),\n)\n\n\n@get(path=\"/\", name=\"index\")\ndef index() -&gt; Template:\n    return Template(name=\"index.html\")\n\n\n@dataclass\nclass Query:\n    query: str\n    df_info: str\n\n\n@post(path=\"/ask\", name=\"ask\", sync_to_thread=True)\ndef ask(\n    data: Annotated[Query, Body(media_type=RequestEncodingType.MULTI_PART)],\n) -&gt; str:\n    query = data.query\n    df_info = data.df_info\n\n    chain_result = chain_create.run(\n        {\n            \"df_info\": df_info,\n            \"query\": query,\n        }\n    )\n    result = chain_result.split(\"```python\")[-1][:-3].strip()\n\n    return result\n\n\napp = Litestar(\n    route_handlers=[index, ask],\n    static_files_config=[\n        StaticFilesConfig(directories=[\"static\"], path=\"/static\", name=\"static\"),\n    ],\n    template_config=TemplateConfig(\n        engine=JinjaTemplateEngine, directory=Path(\"templates\")\n    ),\n)\nThis code provides users with a straightforward interface, allowing them to interact with the chatbot and ask questions about the dataset. Here‚Äôs how it works:\n\nLines 1 to 16 import the required libraries. The application uses Litestar, which is a bit verbose; hence, you‚Äôll notice many import statements. But there‚Äôs no real mystery to it, it‚Äôs pretty similar to FastAPI or Flask.\nLines 18 to 25 create an LLMChain to interact with the model using the prompt read from get_prompt, which is a function defined in utils.py reads the prompts defined in src/prompts. The chain takes the prompts, the user‚Äôs query, and the first three rows from the dataset, and asks the model for a completion. Make sure to read the prompt to see how they work.\nLines 28 to 30 define the index route, which returns the index.html when a user visits /.\nLines 33 to 54 define a dataclass used to validate the parameters of the request made to /ask and define /ask, which is an endpoint that helps users answer questions about the dataset by generating relevant code.\nLines 57 to 65 set up the Litestar app, incorporating the previously defined routes and the locations of the templates and static files.\n\nTo test the app, cd into src/ and run this code in a terminal within the virtual environment:\nlitestar run --reload\nIf everything goes well, you‚Äôll see an output similar to this one:\n\nNext, open http://127.0.0.1:8000 on your browser. You should see the app‚Äôs UI.\nTry asking a question to the chatbot. For example, you can ask when were the highest and lowest closing prices of AAPL.\nYou should get the following result:\n\nThat‚Äôs all! You‚Äôve built a chatbot capable of running code on your browser."
  },
  {
    "objectID": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#next-steps",
    "href": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#next-steps",
    "title": "Create a Code Interpreter Chatbot with Pyodide, LangChain, and OpenAI",
    "section": "Next steps",
    "text": "Next steps\nI won‚Äôt cover deployment in this article. This application is pretty standard, so simply choose a method that works well for you and your organization.\nI like using NGINX with Gunicorn and Uvicorn workers and wrote a tutorial about it. That tutorial uses FastAPI, but the same process would also work with Litestar."
  },
  {
    "objectID": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#conclusion",
    "href": "posts/code-interpreter-chatbot-pyodide-langchain-openai.html#conclusion",
    "title": "Create a Code Interpreter Chatbot with Pyodide, LangChain, and OpenAI",
    "section": "Conclusion",
    "text": "Conclusion\nWay to go! By now, you‚Äôve built a chatbot that can run Python code on the browser and help you answer complex questions about a dataset.\nThis is what you‚Äôve covered in this tutorial:\n\nHow to integrate Pyodide into a web app to run code in the browser.\nHow to use LangChain‚Äôs LLMChain to generate Python code.\nHow to build a simple application with Litestar.\n\nI hope this is useful. Let me know if you have questions.\nThe code for this tutorial is available on GitHub."
  },
  {
    "objectID": "til/fixing-python-not-found-error-in-macos.html",
    "href": "til/fixing-python-not-found-error-in-macos.html",
    "title": "Fixing missing ‚Äòpython‚Äô error in macOS",
    "section": "",
    "text": "After the last macOS update, I started getting the following error when trying to run poetry install:\n[Errno 2] No such file or directory: 'python'\nI went through GitHub issues, StackOverflow questions, and blog posts, but none of the suggested solutions worked.\nFinally, I found the solution somewhat hidden in this blog post.\nSo, what‚Äôs the fix?\nü•Å ü•Å ü•Å\nJust add the following line to your .zshrc file:\nexport PATH=\"$(brew --prefix python)/libexec/bin:$PATH\"\nThis gets the installation prefix for python installed via Homebrew (e.g.¬†/opt/homebrew/opt/python@3.12), gets the libexec/bin directory, and adds it to the PATH.\nIn that libexec/bin, there‚Äôs a python executable that gets called when you run python in the terminal.\nThat‚Äôs all. Hope that helps!\n\n\n\nCitationBibTeX citation:@online{castillo2024,\n  author = {Castillo, Dylan},\n  title = {Fixing Missing ‚ÄúPython‚Äù Error in {macOS}},\n  date = {2024-08-12},\n  url = {https://dylancastillo.co/til/fixing-python-not-found-error-in-macos.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCastillo, Dylan. 2024. ‚ÄúFixing Missing ‚ÄòPython‚Äô Error\nin macOS.‚Äù August 12, 2024. https://dylancastillo.co/til/fixing-python-not-found-error-in-macos.html."
  },
  {
    "objectID": "til/react-agent-pydantic-ai.html",
    "href": "til/react-agent-pydantic-ai.html",
    "title": "Using Pydantic AI to build a ReAct agent",
    "section": "",
    "text": "I wanted to get more familiar with Pydantic AI, so I decided to build a Reasoning and Acting (ReAct) agent with multiple tools.\nI‚Äôve also written other TILs about Pydantic AI:\nYou can download this notebook here."
  },
  {
    "objectID": "til/react-agent-pydantic-ai.html#setup",
    "href": "til/react-agent-pydantic-ai.html#setup",
    "title": "Using Pydantic AI to build a ReAct agent",
    "section": "Setup",
    "text": "Setup\nAfter a first failed attempt, I realized that Pydantic AI uses asyncio under the hood, so you need to enable nest_asyncio to use it in a notebook.\n\nimport nest_asyncio\n\nnest_asyncio.apply()\n\nThen, I did the imports as usual. I hadn‚Äôt used logfire for monitoring LLM applications before, so I thought it‚Äôd be a good idea to try it out.\n\nimport os\nfrom typing import Literal\n\nimport logfire\nimport requests\nfrom dotenv import load_dotenv\nfrom pydantic_ai import Agent\n\nload_dotenv()\n\nPydanticAI instrumentation uses OpenTelemetry (OTel). So it‚Äôs pretty straightforward to use it with Logfire or with any other OTel-compatible observability tool.\nYou just need to create a project in Logfire, generate a Write token and add it to the .env file. Then, you just need to run:\n\nlogfire.configure(\n    token=os.getenv('LOGFIRE_TOKEN'),\n)\nlogfire.instrument_pydantic_ai()\n\nThis will ask you to select a project the first time you run it. It will generate a logfire_credentials.json file in your working directory. In following runs, it will automatically use the credentials from the file."
  },
  {
    "objectID": "til/react-agent-pydantic-ai.html#react-agent",
    "href": "til/react-agent-pydantic-ai.html#react-agent",
    "title": "Using Pydantic AI to build a ReAct agent",
    "section": "ReAct Agent",
    "text": "ReAct Agent\nI decided to make an agent that had access to a tool to get the weather and another one that checks if the response that‚Äôs going to be sent to the user follows the company guidelines.\nHere‚Äôs the code:\n\nfrom pydantic import BaseModel\n\nclass Feedback(BaseModel):\n    feedback: str\n    status: Literal['OK', 'REQUIRES FIXING']\n\nevaluator_agent = Agent(\n    'openai:gpt-4.1-mini',\n    system_prompt=(\n        \"You're a helpful assistant. Your task is to check if a given response follows the company guidelines. The company guidelines are that responses should be written in the style of a haiku. You should reply with 'OK' or 'REQUIRES FIXING' and a short explanation.\"\n    ),\n    output_type=Feedback,\n)\n\nreact_agent = Agent(  \n    'openai:gpt-4.1-mini',\n    system_prompt=(\n        \"You're a helpful assistant. Use the tools provided when relevant. Then draft a response and check if it follows the company guidelines. Only respond to the user after you've validated and modified the response if needed.\"\n    ),\n)\n\n\n@react_agent.tool_plain\ndef get_weather(latitude: float, longitude: float) -&gt; str:\n    \"\"\"Get the weather of a given latitude and longitude\"\"\"\n    response = requests.get(\n        f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\"\n    )\n    data = response.json()\n    return str(data[\"current\"][\"temperature_2m\"])\n\n\n@react_agent.tool_plain\ndef check_guidelines(drafted_response: str) -&gt; str:\n    \"\"\"Check if a given response follows the company guidelines\"\"\"\n    response = evaluator_agent.run_sync(drafted_response)\n    return response.output\n\n\nresponse = react_agent.run_sync(\"What is the temperature in Madrid?\")\n\n18:57:42.206 react_agent run\n18:57:42.208   chat gpt-4.1-mini\n18:57:43.191   running 1 tool\n18:57:43.192     running tool: get_weather\n18:57:43.408   chat gpt-4.1-mini\n18:57:44.117   running 1 tool\n18:57:44.118     running tool: check_guidelines\n18:57:44.120       evaluator_agent run\n18:57:44.120         chat gpt-4.1-mini\n18:57:46.291   chat gpt-4.1-mini\n\n\nAnd here‚Äôs the output:\n\nprint(response.output)\n\nIn Madrid sunshine,\nTemperature climbs so high,\nThirty-four degrees.\n\n\nThe output in Logfire looks like typical observability tools:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†1: Traces in Logfire\n\n\n\nThat‚Äôs all!\nYou can access this notebook here."
  },
  {
    "objectID": "til/install-alacritty-and-zellij-in-macos.html",
    "href": "til/install-alacritty-and-zellij-in-macos.html",
    "title": "Installing Alacritty, Zellij, and Neovim in macOS",
    "section": "",
    "text": "Ever since I saw The Matrix, I‚Äôve wanted to be a hacker.\nWhen I say hacker, I mean having a cool-looking terminal. The kind that makes people think I‚Äôm a stealing millions from banks when, in reality, I‚Äôm just struggling to exit vim.\nI use macOS. The closest I‚Äôve been to being a hacker is using vim hotkeys in VSCode.\nIt‚Äôs not that I haven‚Äôt tried to look the part. I was a just one audio driver away from saying ‚ÄúI use Arch btw‚Äù. I did succeed with Ubuntu, but honestly, using Linux as my main OS always felt like too much work1. So that didn‚Äôt last long.\nBut, today, after reading about DHH‚Äôs Omakub, I though it was time to give my hacker dreams a second shot.\nInstalling Ubuntu felt like a bit too much work2, so I decided to settle on just upgrading my terminal.\nI decided to set up Alacritty, Zellij, and Neovim on my M3 MacBook Pro.\nSure, I have client projects to deliver. But how could I let go of one-in-a-lifetime opportunity to procrastinate and imagine I‚Äôm a genius hacker for an afternoon?"
  },
  {
    "objectID": "til/install-alacritty-and-zellij-in-macos.html#installing-alacritty",
    "href": "til/install-alacritty-and-zellij-in-macos.html#installing-alacritty",
    "title": "Installing Alacritty, Zellij, and Neovim in macOS",
    "section": "Installing Alacritty",
    "text": "Installing Alacritty\nAlacritty is a terminal emulator, similar to Iterm2 and others. It‚Äôs selling point is that it‚Äôs very fast due to GPU-acceleration. Plus, you also get 256 colors support by default.\nThe best way to install it is using Homebrew:\nbrew install --cask alacritty\nThen, you you can customize it to your liking by creating a ~/.config/alacritty/alacritty.toml file.\nI ended up modifying just a couple of things:\n\nIncrease padding.\nChange the font.\nChange the color scheme. I used One Dark from this gist.\n\n\n\nShow the code\n\n[window]\npadding.x = 16\npadding.y = 14\ndecorations = \"none\" # Removes the window decoration (title bar, etc.)\n\n[font]\nsize = 13\n\n# FiraCode Nerd Font\nnormal = { family = \"FiraCode Nerd Font\", style = \"Regular\" }\nbold = { family = \"FiraCode Nerd Font\", style = \"Bold\" }\nitalic = { family = \"FiraCode Nerd Font\", style = \"Italic\" }\n\n# One Dark theme\n[colors]\n[colors.primary]\nbackground = '0x1e2127'\nforeground = '0xabb2bf'\nbright_foreground = '0xe6efff'\n\n# Normal colors\n[colors.normal]\nblack = '0x1e2127'\nred = '0xe06c75'\ngreen = '0x98c379'\nyellow = '0xd19a66'\nblue = '0x61afef'\nmagenta = '0xc678dd'\ncyan = '0x56b6c2'\nwhite = '0x828791'\n\n# Bright colors\n[colors.bright]\nblack = '0x5c6370'\nred = '0xe06c75'\ngreen = '0x98c379'\nyellow = '0xd19a66'\nblue = '0x61afef'\nmagenta = '0xc678dd'\ncyan = '0x56b6c2'\nwhite = '0xe6efff'\n\n# Dim colors\n[colors.dim]\nblack = '0x1e2127'\nred = '0xe06c75'\ngreen = '0x98c379'\nyellow = '0xd19a66'\nblue = '0x61afef'\nmagenta = '0xc678dd'\ncyan = '0x56b6c2'\nwhite = '0x828791'\n\nI use FiraCode Nerd Font. Nerd Fonts are a collection of fonts that include glyphs such as icons that represent folders, file types, weird arrows, etc.\nYou can install them using Homebrew:\nbrew install font-&lt;name-of-the-font&gt;-nerd-font # For example, font-fira-code-nerd-font"
  },
  {
    "objectID": "til/install-alacritty-and-zellij-in-macos.html#installing-zellij",
    "href": "til/install-alacritty-and-zellij-in-macos.html#installing-zellij",
    "title": "Installing Alacritty, Zellij, and Neovim in macOS",
    "section": "Installing Zellij",
    "text": "Installing Zellij\nZellij is an improved version of tmux. Whenever I used tmux, I spent half my time figuring out the key bindings. Zellij shows the key bindings on screen (unless you disable it), which greatly improves the user experience.\nThis is what it looks like:\n\n\n\nZellij\n\n\nSame as before, the best way to install it is using Homebrew.\nbrew install zellij\nYou can customize it by creating a ~/.config/zellij/config.kdl file.\nI just copied the One Half Dark theme they provide.\ntheme \"one-half-dark\"\n\nthemes {\n    one-half-dark {\n        fg 169 177 214\n        bg 26 27 38\n        black 56 62 90\n        red 249 51 87\n        green 158 206 106\n        yellow 224 175 104\n        blue 122 162 247\n        magenta 187 154 247\n        cyan 42 195 222\n        white 192 202 245\n        orange 255 158 100\n    }\n}\nIs One Half Dark the same as One Dark? I don‚Äôt really know. I like to live dangerously.\n\nMaking Zellij play nice with Alacritty\nGetting Alacritty to work with Zellij took me a while to figure out. But luckily it‚Äôs as simple as adding the full path to the zellij binary in the ~/.config/alacritty/alacritty.toml file.\n[terminal.shell]\n     program = \"/opt/homebrew/bin/zellij\""
  },
  {
    "objectID": "til/install-alacritty-and-zellij-in-macos.html#installing-neovim-and-lazyvim",
    "href": "til/install-alacritty-and-zellij-in-macos.html#installing-neovim-and-lazyvim",
    "title": "Installing Alacritty, Zellij, and Neovim in macOS",
    "section": "Installing Neovim and LazyVim",
    "text": "Installing Neovim and LazyVim\nNeovim is a fork from Vim, that solves some of Vim‚Äôs issues3. I didn‚Äôt really know if it was better or worse than Vim, but given that DHH recommended it, I thought it was a good idea to give it a shot.\nLazyVim is a premade configuration for Neovim. The purists will probably hate it, but it‚Äôs a good start.\nI had my own .vimrc, and after trying LazyVim for 30 minutes or so, I realized my config sucked.\nFirst, install Neovim using Homebrew.\nbrew install neovim\nThen, install LazyVim.\ngit clone https://github.com/LazyVim/starter ~/.config/nvim\nSo far, I‚Äôve only made a few changes:\n\nInstalled Copilot, CopilotChat, and mini-surround.\nInstalled One Dark color scheme.\n\nThe plugins I mentioned are available in :LazyExtras, so it‚Äôs very easy to install them. Run :LazyExtras, select the plugins, and then install them with II.\nTo install One Dark, you must create a new file in ~/.config/nvim/lua/plugins/ containing the following code:\nreturn {\n  { \"navarasu/onedark.nvim\" },\n\n  -- Configure LazyVim to load One Dark\n  {\n    \"LazyVim/LazyVim\",\n    opts = {\n      colorscheme = \"onedark\",\n    },\n  },\n}"
  },
  {
    "objectID": "til/install-alacritty-and-zellij-in-macos.html#other-useful-tools",
    "href": "til/install-alacritty-and-zellij-in-macos.html#other-useful-tools",
    "title": "Installing Alacritty, Zellij, and Neovim in macOS",
    "section": "Other useful tools",
    "text": "Other useful tools\nWhile exploring Omakub‚Äôs repository, I also learned about other useful tools that I‚Äôve now included in my daily workflow:\n\nlazydocker\nlazydocker: A simple terminal UI to manage everything Docker. Much better than everything else I‚Äôve used.\nDocker is great. But, until now, the experience of managing Docker containers sucked.\nTake a look at the main screen:\n\n\n\nlazydocker\n\n\n\n\nlazygit\nlazygit is a simple terminal UI for git.\nI‚Äôve found it better than GitHub Desktop when doing complex operations.\nHere‚Äôs a screenshot of the main screen:\n\n\n\nlazygit\n\n\n\n\neza\neza is an improved version of ls.\nI also added a couple of aliases from Omakub:\nalias ls='eza -lh --group-directories-first --icons --hyperlink'\nalias lsa='ls -a'\nalias lt='eza --tree --level=2 --long --icons --git'\nalias lta='lt -a'\nIf you run lta you‚Äôll get a nice view of the current directory.\n\n\n\neza"
  },
  {
    "objectID": "til/install-alacritty-and-zellij-in-macos.html#conclusion",
    "href": "til/install-alacritty-and-zellij-in-macos.html#conclusion",
    "title": "Installing Alacritty, Zellij, and Neovim in macOS",
    "section": "Conclusion",
    "text": "Conclusion\nThat‚Äôs all. I hope you learned something from this post or, at least, got a cool looking terminal.\nThere‚Äôs nothing else to say except that I‚Äôm never going to recover those 4 hours of my life.\nI should get back to work."
  },
  {
    "objectID": "til/install-alacritty-and-zellij-in-macos.html#footnotes",
    "href": "til/install-alacritty-and-zellij-in-macos.html#footnotes",
    "title": "Installing Alacritty, Zellij, and Neovim in macOS",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI‚Äôm sure you‚Äôre itching to explain how I‚Äôm completely wrong about this, and how you haven‚Äôt had to fix anything in years. Please reach me at elon@x.com‚Ü©Ô∏é\nCan you even install Ubuntu on a Mac?‚Ü©Ô∏é\nIncluding its horrible website.‚Ü©Ô∏é"
  },
  {
    "objectID": "til/routing-pydantic-ai.html",
    "href": "til/routing-pydantic-ai.html",
    "title": "Routing workflow with Pydantic AI",
    "section": "",
    "text": "I‚Äôm trying to get more familiar with Pydantic AI, so I‚Äôve been re-implementing typical patterns for building agentic systems.\nIn this post, I‚Äôll build a routing workflow. I won‚Äôt cover the basics of agentic workflows, so if you‚Äôre not familiar with the concept, I recommend you to read this post first.\nI‚Äôve also written other TILs about Pydantic AI:\nYou can download this notebook here."
  },
  {
    "objectID": "til/routing-pydantic-ai.html#what-is-router",
    "href": "til/routing-pydantic-ai.html#what-is-router",
    "title": "Routing workflow with Pydantic AI",
    "section": "What is router?",
    "text": "What is router?\nRouting is a workflow pattern that takes the input, classifies it and then sends it to the right place for the best handling. This process can be managed by an LLM or a traditional classification model. It makes sense to use when a system needs to apply different logic to different types of queries.\nIt looks like this:\n\n\n\n\n\nflowchart LR \n    In([In]) --&gt; Router[\"LLM Call Router\"]\n\n    Router --&gt;|Route 1| LLM1[\"LLM Call 1\"]\n    Router --&gt;|Route 2| LLM2[\"LLM Call 2\"]\n    Router --&gt;|Route 3| LLM3[\"LLM Call 3\"]\n\n    LLM1 --&gt; Out([Out])\n    LLM2 --&gt; Out\n    LLM3 --&gt; Out\n\n\n\n\n\n\n\nExamples:\n\nClassify complexity of question and adjust model depending on it\nClassify type of query and use specialized tools (e.g., indexes, prompts)\n\nLet‚Äôs see how this looks like in code."
  },
  {
    "objectID": "til/routing-pydantic-ai.html#setup",
    "href": "til/routing-pydantic-ai.html#setup",
    "title": "Routing workflow with Pydantic AI",
    "section": "Setup",
    "text": "Setup\nI will implement a workflow that will take a query from a user and will route it to the appropriate agent.\nThere will be three agents in the workflow:\n\nAgent TOC: Generate a table of contents for the article\nAgent Writer: Generate the content of the article\nAgent Editor: Update the content of the article if it‚Äôs too long\n\nBecause Pydantic AI uses asyncio under the hood, you need to enable nest_asyncio to use it in a notebook:\n\nimport nest_asyncio\n\nnest_asyncio.apply()\n\nThen, you need to import the required libraries. Logfire is part of the Pydantic ecosystem, so I thought it‚Äôd be good to use it for observability.\n\nimport os\nfrom typing import Literal\n\nimport logfire\nimport requests\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent, RunContext\n\nload_dotenv()\n\nTrue\n\n\nPydanticAI is compatible with OpenTelemetry (OTel). It‚Äôs straightforward to use it with Logfire or with any other OTel-compatible observability tool (e.g., Langfuse).\nTo enable tracking, create a project in Logfire, generate a Write token and add it to the .env file. Then, you just need to run:\n\nlogfire.configure(\n    token=os.getenv(\"LOGFIRE_TOKEN\"),\n)\nlogfire.instrument_pydantic_ai()\n\nThe first time you run this, it will ask you to create a project in Logfire. From it, it will generate a logfire_credentials.json file in your working directory. In following runs, it will automatically use the credentials from the file."
  },
  {
    "objectID": "til/routing-pydantic-ai.html#prompt-chaining-workflow",
    "href": "til/routing-pydantic-ai.html#prompt-chaining-workflow",
    "title": "Routing workflow with Pydantic AI",
    "section": "Prompt chaining workflow",
    "text": "Prompt chaining workflow\nAs mentioned before, the workflow will be composed of three agents. So I created three Agent instances. Each one takes care of one of the tasks\nHere‚Äôs the code:\n\nclass RouterOutput(BaseModel):\n    category: Literal[\"write_article\", \"generate_table_of_contents\", \"review_article\"]\n\n\nrouter_agent = Agent(\n    \"openai:gpt-4.1-mini\",\n    system_prompt=(\n        \"You are a helpful assistant. You will classify the message into one of the following categories: 'write_article', 'generate_table_of_contents', 'review_article'.\"\n    ),\n    output_type=RouterOutput,\n)\n\nagent_writer = Agent(\n    \"openai:gpt-4.1-mini\",\n    system_prompt=(\n        \"You are a writer. You will write an article about the topic provided.\"\n    ),\n)\n\nagent_toc = Agent(\n    \"openai:gpt-4.1-mini\",\n    system_prompt=(\n        \"You are an expert writer specialized in SEO. Provided with a topic, you will generate the table of contents for a short article.\"\n    ),\n)\n\nagent_reviewer = Agent(\n    \"openai:gpt-4.1-mini\",\n    system_prompt=(\n        \"You are a writer. You will review the article for the topic provided.\"\n    ),\n)\n\nLogfire project URL: https://logfire-us.pydantic.dev/dylanjcastillo/blog\n\n\n\n\n@logfire.instrument(\"Run workflow\")\ndef run_workflow(topic: str) -&gt; str:\n    router_output = router_agent.run_sync(\n        f\"Classify the message: {topic}\"\n    )\n    category = router_output.output.category \n    if category == \"write_article\":\n        return agent_writer.run_sync(f\"Write an article about {topic}\").output\n    elif category == \"generate_table_of_contents\":\n        return agent_toc.run_sync(f\"Generate the table of contents of an article about {topic}\").output\n    else:\n        return agent_reviewer.run_sync(f\"Review the article for the topic {topic}\").output\n\nYou can run the workflow and it will route your message and use the appropriate agent. For example, try to generate a table of contents for an article about AI:\n\ntoc = run_workflow(\"Generate a table of contents for an article about AI\")\n\n20:08:05.547 Run workflow\n20:08:05.548   router_agent run\n20:08:05.549     chat gpt-4.1-mini\n20:08:06.810   agent_toc run\n20:08:06.811     chat gpt-4.1-mini\n\n\n\nprint(toc)\n\nTable of Contents\n\n1. Introduction to Artificial Intelligence  \n2. History and Evolution of AI  \n3. Types of Artificial Intelligence  \n4. Key Technologies Behind AI  \n5. Applications of AI in Various Industries  \n6. Benefits and Challenges of AI  \n7. Future Trends in Artificial Intelligence  \n8. Ethical Considerations in AI Development  \n9. Conclusion\n\n\nOr, ask the workflow to review a social media post.\n\nreview = run_workflow(\"Review this post: 'There are times where there's no time, so you don't have time to write an article about it.'\")\n\n20:08:08.917 Run workflow\n20:08:08.918   router_agent run\n20:08:08.919     chat gpt-4.1-mini\n20:08:09.691   agent_reviewer run\n20:08:09.692     chat gpt-4.1-mini\n\n\n\nprint(review)\n\nThe post \"There are times where there's no time, so you don't have time to write an article about it.\" offers a succinct reflection on the challenges of time constraints, especially in tasks like writing. Its brevity captures the irony of not having enough time to address a situation‚Äîin this case, the lack of time itself. The message resonates with anyone who has felt overwhelmed by deadlines or competing priorities.\n\nHowever, as a piece intended for a broader audience or a formal article, it could benefit from expansion. Elaborating on scenarios where time scarcity impacts productivity, or providing strategies for managing pressing tasks despite limited time, would add depth and practical value. Additionally, refining the sentence for clarity and flow could enhance its impact‚Äîfor example: \"Sometimes, we're so pressed for time that we can't even write about the very pressure we're under.\"\n\nIn summary, the post effectively conveys a common frustration with time limitations in a clever and relatable way but serves better as a starting point for a more detailed discussion rather than a standalone article.\n\n\nThat‚Äôs all!\nYou can access this notebook here.\nIf you have any questions or feedback, please let me know in the comments below."
  },
  {
    "objectID": "til/django-poetry-dockerfile.html",
    "href": "til/django-poetry-dockerfile.html",
    "title": "A Dockerfile for a Django app using Poetry",
    "section": "",
    "text": "Pieter Levels makes over $100k/month with a single VPS using PHP and jQuery. And until very recently, his deployment process was simply uploading files via FTP.\nIf you focus on what your users want and know how to market your product, you can make a lot of money.\nWhich is why I decided to stay poor and spent an inordinate amount of time improving my deployment process.\nWho needs money when you can get the satisfaction of that beautiful green check mark after you‚Äôve run your CI/CD pipeline?\nAnyways‚Ä¶\nThese days, I‚Äôm using kamal to deploy most of my projects.\nI used to hate Docker. But, like with Gollum, I‚Äôve come to realize that it‚Äôs not that bad after all.\nI wanted to create a simple Dockerfile to run a Django app using Poetry, with a SQLite database, and hot reload. Additionally, I wanted to switch between the development and production versions of the container.\nSo here‚Äôs a simple Dockerfile that does just that."
  },
  {
    "objectID": "til/django-poetry-dockerfile.html#project-structure",
    "href": "til/django-poetry-dockerfile.html#project-structure",
    "title": "A Dockerfile for a Django app using Poetry",
    "section": "Project structure",
    "text": "Project structure\nThis is my project structure:\n.\n‚îú‚îÄ‚îÄ db/\n‚îú‚îÄ‚îÄ src/\n‚îú‚îÄ‚îÄ entrypoint.sh\n‚îú‚îÄ‚îÄ Dockerfile\n‚îú‚îÄ‚îÄ docker-compose.yml\n‚îú‚îÄ‚îÄ poetry.lock\n‚îî‚îÄ‚îÄ pyproject.toml\nThe src/ directory contains the Django project. The db/ directory contains the SQLite database. The entrypoint.sh file is the entrypoint for the Docker container.\nIf your project is not structured in a similar way, you might need to adapt the files below to your needs."
  },
  {
    "objectID": "til/django-poetry-dockerfile.html#dockerfile",
    "href": "til/django-poetry-dockerfile.html#dockerfile",
    "title": "A Dockerfile for a Django app using Poetry",
    "section": "Dockerfile",
    "text": "Dockerfile\nI created a Dockerfile that fulfilled this:\n\nA base stage with Python 3.10 and Poetry installed.\nA builder stage that installs the dependencies.\nA runner stage that copies the virtual environment from the builder stage.\nA development stage that runs the entrypoint as a root user.\nA production stage that runs the entrypoint as a non-root user.\n\nHere‚Äôs the full Dockerfile:\n\n\nDockerfile\n\nFROM python:3.10-slim AS base\n\nENV POETRY_HOME=/opt/poetry\nENV PATH=${POETRY_HOME}/bin:${PATH}\n\nRUN apt-get update \\\n    && apt-get install --no-install-recommends -y \\\n    curl \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\nRUN curl -sSL https://install.python-poetry.org | python3 - && poetry --version\n\nFROM base AS builder\n\nWORKDIR /app\nCOPY poetry.lock pyproject.toml ./\nRUN poetry config virtualenvs.in-project true\nRUN poetry install --only main --no-interaction\n\nFROM base AS runner\n\nWORKDIR /app\nCOPY --from=builder /app/.venv/ /app/.venv/\n\nCOPY . /app\nRUN mkdir -p /db\n\nEXPOSE 8000\n\nRUN chmod +x /app/src/entrypoint.sh\n\nFROM runner AS development\n\nWORKDIR /app/src\nENTRYPOINT [ \"/app/src/entrypoint.sh\" ]\n\nFROM runner AS production\n\n# Set user and group\nARG user=django\nARG group=django\nARG uid=1000\nARG gid=1000\nRUN groupadd -g ${gid} ${group}\nRUN useradd -u ${uid} -g ${group} -s /bin/sh -m ${user}\n\n# Switch to user\nRUN chown -R ${uid}:${gid} /app\nRUN chown -R ${uid}:${gid} /db\n\nUSER ${uid}:${gid}\n\nWORKDIR /app/src\nENTRYPOINT [ \"/app/src/entrypoint.sh\" ]"
  },
  {
    "objectID": "til/django-poetry-dockerfile.html#entrypoint",
    "href": "til/django-poetry-dockerfile.html#entrypoint",
    "title": "A Dockerfile for a Django app using Poetry",
    "section": "Entrypoint",
    "text": "Entrypoint\nFor entrypoint.sh I use this:\n#!/bin/sh\n\nif [ \"$ENVIRONMENT\" = \"production\" ]; then\n    echo \"Running in production mode\"\n    exec poetry run gunicorn -c gunicorn.conf.py\nelif [ \"$ENVIRONMENT\" = \"development\" ]; then\n    echo \"Running in development mode\"\n    exec poetry run python manage.py runserver 0.0.0.0:8000\nelse\n    echo \"ENVIRONMENT variable is not set\"\nfi\nIf ENVIRONMENT is set to production, the container will run the production server using gunicorn. If it is development, the container will run Django‚Äôs development server."
  },
  {
    "objectID": "til/django-poetry-dockerfile.html#docker-compose",
    "href": "til/django-poetry-dockerfile.html#docker-compose",
    "title": "A Dockerfile for a Django app using Poetry",
    "section": "Docker-compose",
    "text": "Docker-compose\nThen, I have a docker-compose that lets you run the development and production containers:\nservices:\n  app:\n    build:\n      context: .\n      target: ${ENVIRONMENT}\n    platform: linux/amd64\n    environment:\n      - DJANGO_SECRET_KEY=${DJANGO_SECRET_KEY}\n      - DJANGO_DEBUG=${DJANGO_DEBUG}\n      - DJANGO_SECURE_SSL_REDIRECT=${DJANGO_SECURE_SSL_REDIRECT}\n      - DJANGO_SECURE_HSTS_SECONDS=${DJANGO_SECURE_HSTS_SECONDS}\n      - DJANGO_SECURE_HSTS_INCLUDE_SUBDOMAINS=${DJANGO_SECURE_HSTS_INCLUDE_SUBDOMAINS}\n      - DJANGO_SECURE_HSTS_PRELOAD=${DJANGO_SECURE_HSTS_PRELOAD}\n      - DJANGO_SESSION_COOKIE_SECURE=${DJANGO_SESSION_COOKIE_SECURE}\n      - DJANGO_CSRF_COOKIE_SECURE=${DJANGO_CSRF_COOKIE_SECURE}\n      - CACHE_REDIS_URL=${CACHE_REDIS_URL}\n      - ENVIRONMENT=${ENVIRONMENT}\n    env_file:\n      - .env\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - \"./db/:/app/db/\"\n    develop:\n      watch:\n        - action: sync\n          path: ./src/\n          target: /app/src\n        - action: rebuild\n          path: pyproject.toml\nIn this docker-compose, I use ENVIRONMENT to switch between the development and production containers.\nI also use the Compose Watch to reload the container when I make changes to the code and to rebuild the container when I make changes to the pyproject.toml file."
  },
  {
    "objectID": "til/django-poetry-dockerfile.html#conclusion",
    "href": "til/django-poetry-dockerfile.html#conclusion",
    "title": "A Dockerfile for a Django app using Poetry",
    "section": "Conclusion",
    "text": "Conclusion\nThat‚Äôs it. I hope you find this useful.\nAnd remember, while Pieter is busy counting his cash, here you are counting the number of successful builds."
  },
  {
    "objectID": "til/evaluator-optimizer-pydantic-ai.html",
    "href": "til/evaluator-optimizer-pydantic-ai.html",
    "title": "Evaluator-optimizer workflow with Pydantic AI",
    "section": "",
    "text": "I‚Äôm doing a deep dive into Pydantic AI, so I‚Äôve been re-implementing typical patterns for building agentic systems.\nIn this post, I‚Äôll explore how to build a evaluator-optimizer workflow. I won‚Äôt cover the basics of agentic workflows, so if you‚Äôre not familiar with the concept, I recommend you to read this post first."
  },
  {
    "objectID": "til/evaluator-optimizer-pydantic-ai.html#what-is-evaluator-optimizer",
    "href": "til/evaluator-optimizer-pydantic-ai.html#what-is-evaluator-optimizer",
    "title": "Evaluator-optimizer workflow with Pydantic AI",
    "section": "What is evaluator-optimizer?",
    "text": "What is evaluator-optimizer?\nEvaluator-optimizer is a pattern that has an LLM generator and an LLM evaluator. The generator generates a solution and the evaluator evaluates if the solution is good enough. If it‚Äôs not, the generator is given feedback and it generates a new solution. This process is repeated until the solution is good enough.\nIt looks like this:\n\n\n\n\n\nflowchart LR\n    In([In]) --&gt; Gen[\"Generator (LLM)\"]\n    Gen -- \"Solution\" --&gt; Eval[\"Evaluator (LLM)\"]\n    Eval -- \"Accepted\" --&gt; Out([Out])\n    Eval -- \"Rejected + Feedback\" --&gt; Gen\n\n\n\n\n\n\n\nExamples:\n\nContent generation that must match certain guidelines such as writing with a particular style.\nImproving search results iteratively\n\nLet‚Äôs see how this looks in practice."
  },
  {
    "objectID": "til/evaluator-optimizer-pydantic-ai.html#setup",
    "href": "til/evaluator-optimizer-pydantic-ai.html#setup",
    "title": "Evaluator-optimizer workflow with Pydantic AI",
    "section": "Setup",
    "text": "Setup\nI will implement a simple workflow:\n\nGenerate a candidate article\nEvaluate if the article is good enough\nIf it‚Äôs not, provide feedback and generate a new article\nRepeat until the article is good enough\n\nBefore we start, because Pydantic AI uses asyncio under the hood, you need to enable nest_asyncio to use it in a notebook:\n\nimport nest_asyncio\n\nnest_asyncio.apply()\n\nThen, you need to import the required libraries. I‚Äôm using Logfire to monitor the workflow.\n\nimport os\nfrom typing import Literal\n\nimport logfire\nimport requests\nfrom dotenv import load_dotenv\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic import BaseModel, Field\n\nload_dotenv()\n\nTrue\n\n\nPydanticAI is compatible with OpenTelemetry (OTel). So it‚Äôs pretty easy to use it with Logfire or with any other OTel-compatible observability tool (e.g., Langfuse).\nTo enable tracking, create a project in Logfire, generate a Write token and add it to the .env file. Then, you just need to run:\n\nlogfire.configure(\n    token=os.getenv('LOGFIRE_TOKEN'),\n)\nlogfire.instrument_pydantic_ai()\n\nThe first time you run this, it will ask you to create a project in Logfire. From it, it will generate a logfire_credentials.json file in your working directory. In following runs, it will automatically use the credentials from the file."
  },
  {
    "objectID": "til/evaluator-optimizer-pydantic-ai.html#evaluator-optimizer-workflow",
    "href": "til/evaluator-optimizer-pydantic-ai.html#evaluator-optimizer-workflow",
    "title": "Evaluator-optimizer workflow with Pydantic AI",
    "section": "Evaluator-optimizer workflow",
    "text": "Evaluator-optimizer workflow\nThe workflow is composed of two steps:\n\nText generator: Generates a candidate article.\nEvaluator: Evaluates if the article is good enough.\n\nI‚Äôll split the text generation into two agents: generator and fixer. The generator will generate a candidate article and the fixer will fix the article, when provided with feedback.\n\ngenerator = Agent(\n    'openai:gpt-4.1-mini',\n    system_prompt=(\n        \"You are an expert writer. Provided with a topic, you will generate an engaging article with less than 500 words\"\n    ),\n)\n\nfixer = Agent(\n    'openai:gpt-4.1-mini',\n    system_prompt=(\n        \"You are an expert writer. Provided with a text and feedback, you wil improve the text.\"\n    ),\n)\n\nLogfire project URL: https://logfire-us.pydantic.dev/dylanjcastillo/blog\n\n\n\nNext, I‚Äôll create the Evaluator agent. It will take a text and it will evaluate if it‚Äôs good enough. It‚Äôll produce an Evaluation object as the output.\n\nclass Evaluation(BaseModel):\n    explanation: str = Field(\n        description=\"Explain why the text evaluated matches or not the evaluation criteria\"\n    )\n    feedback: str = Field(\n        description=\"Provide feedback to the writer to improve the text\"\n    )\n    is_correct: bool = Field(\n        description=\"Whether the text evaluated matches or not the evaluation criteria\"\n    )\n\nevaluator = Agent(\n    'openai:gpt-4.1-mini',\n    system_prompt=(\n        \"You are an expert evaluator. Provided with a text, you will evaluate if it's written in British English and if it's appropriate for a young audience. The text must always use British spelling and grammar. Make sure the text doesn't include any em dashes.\"\n    ),\n    output_type=Evaluation,\n)\n\nFinally, you can encapsulate all the logic in a single function:\n\n@logfire.instrument(\"Run workflow\")\ndef run_workflow(topic: str) -&gt; str:\n    text = generator.run_sync(f\"Generate an article about '{topic}'\")\n    evaluation = evaluator.run_sync(f\"Evaluate the following text: {text.output}\")\n    for _ in range(3):\n        if not evaluation.output.is_correct:\n            text = fixer.run_sync(f\"Fix the text: {text.output} with the following feedback: {evaluation.output.feedback}\")\n            evaluation = evaluator.run_sync(f\"Evaluate the following text: {text.output}\")\n        else:\n            return text.output\n    return text.output\n\noutput = run_workflow(\"Consumption of hard drugs\")\n\n11:28:25.995 Run workflow\n11:28:25.995   generator run\n11:28:25.996     chat gpt-4.1-mini\n11:28:36.293   evaluator run\n11:28:36.294     chat gpt-4.1-mini\n\n\nAnd here‚Äôs the output:\n\nprint(output)\n\n**The Complex Reality of Hard Drug Consumption**\n\nHard drugs ‚Äî substances such as heroin, cocaine, methamphetamines, and crack ‚Äî have long been a subject of concern worldwide due to their profound impact on individuals and society. The consumption of these drugs is not merely a matter of personal choice but a complex issue influenced by social, economic, psychological, and cultural factors.\n\n**Understanding Hard Drugs and Their Effects**\n\nHard drugs are characterized by their high potential for addiction and severe physical and psychological effects. Unlike softer substances such as marijuana or alcohol (when consumed responsibly), hard drugs often disrupt brain function dramatically, leading to addiction, mental health disorders, and significant physical health problems. Users may experience paranoia, hallucinations, heart issues, and even fatal overdoses.\n\nThe allure of hard drugs often stems from their ability to produce intense euphoria or numb emotional pain temporarily. However, this fleeting escape comes at a steep cost. Dependence quickly sets in, making cessation incredibly difficult and often trapping users in a cycle of abuse.\n\n**Social and Economic Implications**\n\nThe ramifications of hard drug consumption ripple beyond the individual. Families endure emotional and financial strain, communities face increased crime rates and reduced public safety, and healthcare systems are burdened with treating overdoses and long-term complications. Moreover, productivity declines as addiction interferes with employment, contributing to broader economic challenges.\n\nMany users come from marginalized backgrounds, where poverty, trauma, and lack of education or opportunity make drugs seem like a refuge or an escape. This correlation highlights that addressing drug consumption isn't only a matter of law enforcement but of social equity and support.\n\n**Challenges in Addressing Hard Drug Use**\n\nEfforts to reduce hard drug consumption have varied widely, from strict punitive measures to harm reduction strategies. While criminalization seeks to deter use, it often leads to overcrowded prisons and can exacerbate social stigma, making it harder for users to seek help. Conversely, approaches like supervised consumption sites, needle exchange programs, and accessible addiction treatment aim to minimize harm and promote recovery.\n\nPrevention and education are critical components. Informing communities about the risks of hard drugs and providing mental health support can reduce initial experimentation and help those at risk before addiction takes hold.\n\n**Moving Towards Compassionate Solutions**\n\nUltimately, the consumption of hard drugs is a multifaceted issue requiring balanced and compassionate responses. Policymakers, healthcare providers, and communities must work together to create environments that prioritize treatment over punishment, recognize addiction as a health issue, and promote social support.\n\nBy understanding the complex realities behind hard drug use, society can better address its consequences and help those affected find a path to recovery and hope.\n\n\nThat‚Äôs all!\nYou can access this notebook here.\nIf you have any questions or feedback, please let me know in the comments below."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I‚Äôm Dylan ‚Äì I‚Äôm an independent AI consultant. I‚Äôve worked in the AI & ML space for almost a decade.\nI‚Äôve delivered successful projects for large organizations like Deliveroo, Boston Consulting Group (BCG), and the Olympics and startups such as REMATIQ or Neurolabs. You can read testimonials from some of these projects on my LinkedIn.\nIn my free time, I like working on random side projects (open source, personal, or otherwise). Some of them have been featured in The Economist, praised by the leaders at Zapier and HubSpot, and reached hundreds of stars on GitHub.\nI also write regularly about topics I‚Äôm interested in. I mostly cover technical subjects, such as data science and machine learning, but every once in a while I write about topics like financial independence or my end of year reviews.\nIf you‚Äôre interested in working with me or just want to chat, feel free to reach out to me at dylan@iwanalabs.com."
  }
]