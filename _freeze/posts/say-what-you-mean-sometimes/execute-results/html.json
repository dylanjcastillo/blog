{
  "hash": "fae32f8e8aa83abc2d540c4ea9bb4432",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Say What You Mean... Sometimes\"\ndate: \"12/08/2024\"\ndate-modified: last-modified\ndescription-meta: \"A look at the impact of structured outputs on the performance of LLMs.\"\ntoc: true\ntoc-depth: 3\nlightbox: true\nfig-cap-location: margin\ncategories:\n  - llm\n  - openai\n  - pydantic\n  - python\nauthor:\n  - name: Dylan Castillo\n    url: https://dylancastillo.co\n    affiliation: Iwana Labs\n    affiliation-url: https://iwanalabs.com\ncitation: true\ncomments:\n  utterances:\n    repo: dylanjcastillo/blog_comments\n    theme: dark-blue\n    issue-term: pathname\n---\n\n\nWhen I read [Let Me Speak Freely?](https://arxiv.org/abs/2408.02442) by Tam et al. I thought they raised an interesting question: does constraining LLM outputs to structured formats impact the quality of their responses?\n\nIn both the original study and their recent update, Tam et al. concluded that is the case. They found that \"structured generation constraints significantly impact LLM performance across various tasks\".\n\nBut the study had major flaws. The [.txt](https://dottxt.co/) team wrote a very compelling [rebuttal](https://dottxt.co/blog/let-me-speak-freely) to the paper. For _Llama-3-8B-Instruct_, they demonstrate that Tam, et al. results were mostly due to poor prompting, unfair comparisons and the improper use of an \"AI parser\" rather than the use of structured outputs.\n\nI liked the rebuttal but it still left me wondering how well their results generalize. They focused on a single model[^1], which represents a small fraction of the LLMs powering applications in production today. Open-weight models offer more flexibility on how to _structure_ your output, such as letting users specify [regex expressions](https://dottxt-ai.github.io/outlines/latest/reference/generation/regex/) to constrain the output. Proprietary models lack this. Right now, JSON is the only structured output format guaranteed to work across most popular providers.\n\n[^1]: Although, they've also [shared results](https://blog.dottxt.co/performance-gsm8k.html) of other open-weight models using a different setup.\n\nGiven this constraint, would the .txt team’s results still hold?\n\nPlus, both the original study and the rebuttal focused on tasks that might not be a good proxy for the full range of tasks people use LLMs for. Would the rebuttal results be different in settings outside of simple reasoning tasks?\n\nSo I decided to:\n\n1.  Replicate the results from .txt's rebuttal using _LLaMA3-8B-Instruct_.\n2.  Replicate the same tasks using a proprietary model _GPT-4o-mini_.\n3.  Test results on a broader set of tasks such as [LiveBench](https://livebench.ai/).\n\nThis article presents the results of the first two steps. All the code is available on [Github](https://github.com/dylanjcastillo/blog/tree/main/_extras/say-what-you-mean-sometimes).\n\n## Results\n\nIf you're short on time, here are the main results:\n\n1. Tam et al.’s conclusions about structured outputs might still hold, even if they did not properly test for it. There are cases where structured outputs perform worse than unstructured outputs.\n2. .txt's rebuttal is correct, and shows that structured outputs are as good or better than unstructured outputs for _LLaMA3-8B-Instruct_. But the same approach does not hold for _GPT-4o-mini_ (and possibly other models).\n\nIn the figure below you can see the results for _GPT-4o-mini_, using .txt's fixes to the prompts and additional improvements I implemented.\n\n::: {#866a4dbb .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>                            <div id=\"a1bce30a-ff83-4b3b-88bc-af0933193ebb\" class=\"plotly-graph-div\" style=\"height:400px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a1bce30a-ff83-4b3b-88bc-af0933193ebb\")) {                    Plotly.newPlot(                        \"a1bce30a-ff83-4b3b-88bc-af0933193ebb\",                        [{\"hoverinfo\":\"skip\",\"name\":\"Structured\",\"text\":[\"93.86\",\"94.67\",\"89.84\"],\"textposition\":\"outside\",\"texttemplate\":\"%{text:.2f}%\",\"x\":[\"GSM8k\",\"Last Letter\",\"Shuffled Objects\"],\"y\":[93.86,94.67,89.84],\"type\":\"bar\"},{\"hoverinfo\":\"skip\",\"name\":\"Unstructured\",\"text\":[\"94.31\",\"92.0\",\"95.12\"],\"textposition\":\"outside\",\"texttemplate\":\"%{text:.2f}%\",\"x\":[\"GSM8k\",\"Last Letter\",\"Shuffled Objects\"],\"y\":[94.31,92.0,95.12],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#f2f5fa\"},\"error_y\":{\"color\":\"#f2f5fa\"},\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"baxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#506784\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"header\":{\"fill\":{\"color\":\"#2a3f5f\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#f2f5fa\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#f2f5fa\"},\"geo\":{\"bgcolor\":\"rgb(17,17,17)\",\"lakecolor\":\"rgb(17,17,17)\",\"landcolor\":\"rgb(17,17,17)\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#506784\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"dark\"},\"margin\":{\"b\":0,\"l\":0,\"r\":0,\"t\":30},\"paper_bgcolor\":\"rgb(17,17,17)\",\"plot_bgcolor\":\"rgb(17,17,17)\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"bgcolor\":\"rgb(17,17,17)\",\"radialaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"},\"yaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"},\"zaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"}},\"shapedefaults\":{\"line\":{\"color\":\"#f2f5fa\"}},\"sliderdefaults\":{\"bgcolor\":\"#C8D4E3\",\"bordercolor\":\"rgb(17,17,17)\",\"borderwidth\":1,\"tickwidth\":0},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"bgcolor\":\"rgb(17,17,17)\",\"caxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"updatemenudefaults\":{\"bgcolor\":\"#506784\",\"borderwidth\":0},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"zerolinewidth\":2}}},\"margin\":{\"l\":50,\"r\":50,\"t\":50,\"b\":50,\"pad\":10},\"yaxis\":{\"title\":{\"text\":\"Score (%)\"},\"range\":[0,105]},\"legend\":{\"orientation\":\"h\",\"yanchor\":\"bottom\",\"y\":1.05,\"xanchor\":\"center\",\"x\":0.5},\"modebar\":{\"remove\":[\"zoom\",\"pan\",\"select\",\"lasso2d\",\"zoomIn2d\",\"zoomOut2d\",\"autoScale2d\",\"resetScale2d\"]},\"barmode\":\"group\",\"height\":400,\"xaxis\":{\"title\":{\"text\":\"Task\"}},\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('a1bce30a-ff83-4b3b-88bc-af0933193ebb');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>\n```\n:::\n:::\n\n\nFor **GSM8K** and **Last Letter**, structured and unstructured methods scored similarly. But for **Shuffled Objects**, unstructured outputs clearly surpassed a structured format.\n\nThe rest of the article will explain the approach I took to get these results.\n\n## Study design\n\nTam et al. evaluated structured and unstructured outputs across three reasoning tasks and six classification tasks. They used exact match to evaluate reasoning tasks and accuracy to evaluate classification tasks. They ran the experiments using the following models:\n\n1.  **Proprietary models**: _gpt-3.5-turbo-0125_, _claude-3-haiku-20240307_, _gemini-1.5-flash_, and _gpt-4o-mini-2024-07-18_.\n2.  **Open-weight models**: _LLaMA3-8B-Instruct_, and _Gemma-2-9B-Instruct_.\n\n.txt used a similar setup, but only focused on the reasoning tasks and evaluating _LLaMA3-8B-Instruct_. They did not include classification tasks because Tam et al. observed that structured outputs resulted in better performance in these tasks, so there was no need to test for it.\n\nI also believe that structured outputs are better for classification tasks. So, I excluded them from my analysis as well.\n\nThe reasoning tasks were:\n\n1.  [GSM8K](https://huggingface.co/datasets/openai/gsm8k): A dataset from of grade school math word problems.\n2.  [Last Letter](https://huggingface.co/datasets/ChilleD/LastLetterConcat): A dataset of simple word puzzles that require concatening the last letters of a list of names.\n3.  [Shuffled Objects](https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/tracking_shuffled_objects): A dataset that requires reasoning about the state of a system after a sequence of shuffling operations.\n\nThis article will focus on replicating the results from .txt's rebuttal on these tasks and evaluating the same tasks using a proprietary model.\n\n## Replicating .txt's rebuttal\n\n.txt made it very easy to reproduce their results by sharing their [code on Github](https://github.com/dottxt-ai/demos/tree/main/say-what-you-mean). I just set up a machine at [Modal](https://modal.com/) and ran the code.\n\nWhile going through the code, I noticed some small issues with the prompts. So I decided to tweak them a bit.\n\nBelow are .txt's original results compared to mine, after the prompt adjustments:\n\n\n```{=html}\n<table>\n  <tr>\n    <th rowspan=\"2\"><strong>Task</strong></th>\n    <th colspan=\"2\"><strong>.txt</strong></th>\n    <th colspan=\"2\"><strong>Me, 3-shot</strong></th>\n  </tr>\n  <tr>\n    <th><strong>Unstructured</strong></th>\n    <th><strong>Structured</strong></th>\n    <th><strong>Unstructured</strong></th>\n    <th><strong>Structured</strong></th>\n  </tr>\n  <tr>\n    <td><strong>GSM8K</strong></td>\n    <td>77.18</td>\n    <td>77.79</td>\n    <td>79.98</td>\n    <td>79.45</td>\n  </tr>\n  <tr>\n    <td><strong>Last Letter</strong></td>\n    <td>73.33</td>\n    <td>77.33</td>\n    <td>74.00</td>\n    <td>78.00</td>\n  </tr>\n  <tr>\n    <td><strong>Shuffled Objects</strong></td>\n    <td>40.72</td>\n    <td>44.35</td>\n    <td>42.68</td>\n    <td>43.90</td>\n  </tr>\n</table>\n```\n\n\nExcept for **Structured** in the **Shuffled Objects** task, I was able to improve all the metrics. In **GSM8K's** case, even reversing .txt's result, with **Unstructured** outperforming **Structured** by a small margin.\n\nBut I don't think this matters much.\n\nTheir conclusion still holds: structured outputs are either as good as or better than unstructured outputs, in the tasks considered.\n\nI'll explain the prompt changes I made below, so that you can judge for yourself if they make sense.\n\n### Formatting few-shot examples\n\nIn the **GSM8K** and **Last Letter** tasks, the few-shot prompt for both unstructured and structured used examples formatted as JSON objects and asked the LLM to produce the output in the same format, from which the answer was extracted.\n\nThat felt unfair. Even though you're not formally constraining the LLM to produce a JSON object, you're still asking it to format its response in somewhat unnatural way.\n\nI adjusted the prompts to be as similar as possible for both unstructured and structured outputs while still trying to get the most out of each approach.\n\nFor example, in **GSM8K**, the unstructured prompt is:\n\n> You are an expert in solving grade school math tasks. You will be presented with a grade-school math word problem and be asked to solve it.\n> You will always respond in the following format:\n>\n> <str, reasoning about the answer>\n>\n> ANSWER: <int, final answer>\n>\n> First, provide your step by step reasoning. Then, in ANSWER, provide an integer that corresponds to the correct answer to the question. Don't include any other text in ANSWER.\n\nAnd the structured prompt is:\n\n> You are an expert in solving grade school math tasks. You will be presented with a grade-school math word problem and be asked to solve it.\n> You will always respond in the following format:\n>\n> {\"reasoning\": <str, reasoning about the answer>, \"answer\": <int, final answer>}\n>\n> First, provide your step by step reasoning in the \"reasoning\" field. Then, in the \"answer\" field, provide an integer that corresponds to the correct answer to the question. Don't include any other text in the \"answer\" field.\n\nFinally, for all the tasks, I used a 3-shot prompt.\n\n### Clarifying the task\n\nI also tried to make the prompts clearer. The description of the task in the original **Last Letter** prompt was:\n\n> You are an expert in solving simple word puzzles using reasoning steps. Your specific task is going to be to take a list of 4 names and reason about the last letter of each ., then you will concatenate those letters into a word.\n\nI changed it to:\n\n> You are an expert in solving word puzzles. Your specific task is going to be to take a list of 4 names, get the last letter of each and concatenate these letters into a word.\n\nThe original prompt was reasonable, but I thought the new version was clearer. Through trial and error, I've learned that when working with LLMs, it's best to be as clear and direct as possible.\n\n## Evaluating GPT-4o-mini\n\nUsing the same setup as before, I ran the same tasks with `gpt-4o-mini-2024-07-18`.\n\nBelow are the results, including the original results from Tam et al. for comparison:\n\n| **Task**          | **Method**      | **NL** | **FRI** | **JSON-Mode** | **JSON-Schema** |\n| ----------------- | --------------- | ------ | ------- | ------------- | --------------- |\n| **GSM8K**         | **Tam et al.**  | 94.57  | 87.17   | 86.95         | 91.71           |\n|                   | **Me (0-shot)** | 94.31  | 92.12   | 93.33         | 93.48           |\n|                   | **Me (3-shot)** | 93.86  | 92.72   | 93.25         | 92.95           |\n| **Last Letter**   | **Tam et al.**  | 83.11  | 84.73   | 76.00         | 86.07           |\n|                   | **Me (0-shot)** | 87.33  | 88.00   | 90.00         | 87.33           |\n|                   | **Me (3-shot)** | 92.00  | 94.67   | 90.00         | 93.33           |\n| **Shuffled Obj.** | **Tam et al.**  | 82.85  | 81.46   | 76.43         | 81.77           |\n|                   | **Me (0-shot)** | 95.12  | 79.67   | 81.71         | 89.84           |\n|                   | **Me (3-shot)** | 92.68  | 69.51   | 62.60         | 65.85           |\n\n_NL_ stands for \"Natural Language\", which would correspond to the _Unstructured_ method in the previous table.\n\n_FRI_ stands for \"Format Restricting Instructions\", which is a JSON generated through the OpenAI's [function calling](https://platform.openai.com/docs/guides/function-calling). _JSON-Mode_ is a JSON generated through the OpenAI's [JSON mode](https://platform.openai.com/docs/guides/structured-outputs#json-mode). _JSON-Schema_ is a JSON generated using [constrained decoding](https://openai.com/index/introducing-structured-outputs-in-the-api/).\n\n_JSON-Schema_ is the closest equivalent to **Structured** as referenced in the previous table. But, in real-life applications, you don't really care about how the output was generated. You just want to get the output in the format you want. So, for the sake of comparison, I will consider the three other methods equivalent to **Structured** as well.\n\n### Adjusting for proprietary models\n\nIn this case, I allowed for 3 retries in the case of parsing errors. I allowed for this because function calling had high error rates in the zero-shot prompting scenario.\n\nThese retries primarily affected **FRI** results. This might make the comparisons in **Last Letter** biased in favor of structured outputs (**FRI** was the best method in this case). But since **JSON-Schema** also outperformed **NL** in this case, this adjustment does not alter the overall conclusions. The other methods maintained error rates of \\<0.5% in **GSM8K** and 0% in **Last Letter** and **Shuffled Objects**.\n\nI used slightly different parsing functions for **Unstructured** and **Structured** outputs. The **Unstructured** parser was more lenient, removing commas and periods at the end of responses. But I believe this remains a fair comparison given that in the **Structured** cases you provide a JSON schema which is more informative.\n\n### Analyzing the results\n\nSimilar to what the .txt team found, after adjusting the prompts, the performance of structured outputs increases substantially compared to Tam et al.\n\nExcept for _NL_ in **GSM8k** and _FRI_ in **Last Letter**, I was able to improve all the metrics for both unstructured and structured outputs using a 0-shot prompt. For 3-shot prompts, I improved **GSM8k** and **Last Letter** across all methods, and _NL_ in **Shuffled Objects**.\n\nFor **GSM8k** and **Last Letter**, the results were very similar between unstructured and structured outputs. There was a slight edge for unstructured outputs in **GSM8k** and for structured outputs in **Last Letter**. In these cases, it’s not clear that one approach definitively outperforms the other.\n\nOn the other hand, **Shuffled Objects** shows a clear advantage for unstructured outputs over structured outputs. This was unexpected, and even after tweaking the prompts, I couldn’t fully close the gap.\n\nDespite the issues in Tam et al.’s study, their conclusion appears to hold. In this particular scenario, using a fairly popular model with reasonable prompts, there is a significant difference in performance between structured and unstructured outputs.\n\n::: callout-note\nIn **GSM8k** and **Last Letter**, few-shot prompting generally decreased performance. This is in line with [other analyses](https://python.useinstructor.com/blog/2024/09/26/bad-schemas-could-break-your-llm-structured-outputs/?h=bad+sc#modes-and-models).\n:::\n\n## Conclusion\n\nYou're here because you want to know whether to use structured or unstructured outputs. As a developer, I'm glad to say the answer is: [it depends](https://www.reddit.com/r/orlybooks/comments/50meb5/it_depends/).\n\nI love using structured outputs in my daily work, because it makes it much easier to work with the output of LLMs. I always encourage [clients](https://iwanalabs.com/) who aren't using them yet to give them a try.\n\nThat said, until there's strong evidence favoring one approach over the other, the best course of action is to test things for yourself. Run your own [evals](https://hamel.dev/blog/posts/evals/) and make a decision based on data.\n\nI expect that in most cases, structured outputs will have similar performance to unstructured outputs. But, if you blindly assume that structured outputs are always equal to or better than unstructured ones, you might be missing out on easy performance gains.\n\nTake the example of **Shuffled Objects** with _GPT-4o-mini_. You could potentially reduce the gap between the two methods by continuing improving the prompts or by switching to a more powerful model. But the effort and/or costs might outweigh the benefits compared to simply using unstructured outputs.\n\nI don’t think unstructured outputs are inherently better or worse than structured outputs across the board. The right choice depends on your task, the model, and your prompt engineering skills. Test for yourself to determine if a difference exists, and if it does, decide which option works best for you.\n\n",
    "supporting": [
      "say-what-you-mean-sometimes_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n<script type=\"text/javascript\">\nwindow.PlotlyConfig = {MathJaxConfig: 'local'};\nif (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\nif (typeof require !== 'undefined') {\nrequire.undef(\"plotly\");\nrequirejs.config({\n    paths: {\n        'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n    }\n});\nrequire(['plotly'], function(Plotly) {\n    window._Plotly = Plotly;\n});\n}\n</script>\n\n"
      ]
    }
  }
}