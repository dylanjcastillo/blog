{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"LangSmith 101\"\n",
    "date: 2025-08-10\n",
    "date-modified: 2025-08-10\n",
    "description-meta: \"A beginner's guide to LangSmith, an observability and evaluation platform for LLM applications.\"\n",
    "categories:\n",
    "  - llm\n",
    "  - python\n",
    "  - openai\n",
    "  - langsmith \n",
    "---\n",
    "\n",
    "In my first AI projects, I didn't have access to proper observability tools and didn't know how to evaluate the performance of LLM pipelines. I struggled to figure out what to improve and even when I knew what to improve, it was hard to do so, without breaking other things. Many of those projects failed miserably.\n",
    "\n",
    "Those failed projects made me start looking for better ways and tools to build AI applications. Over time, tools such as [LangSmith](https://smith.langchain.com/), [Langfuse](https://langfuse.com), or [Logfire](https://logfire.pydantic.dev) became key components of my AI toolkit. I can no longer imagine building an AI application without them.\n",
    "\n",
    "In this tutorial, I'll walk you through the basics of using LangSmith to monitor and evaluate your LLM applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To complete this tutorial, you need to: \n",
    "\n",
    "1. Sign up and generate [OpenAI](https://platform.openai.com/docs/overview) and [LangSmith](https://smith.langchain.com/) API keys.\n",
    "2. Create a `.env` file in the root directory of your project and add the following lines:\n",
    "```bash\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "LANGSMITH_TRACING=true\n",
    "LANGSMITH_PROJECT=your_langchain_project_name\n",
    "LANGSMITH_API_KEY=your_langsmith_api_key\n",
    "```\n",
    "3. Create a virtual environment in Python and install the following packages: \n",
    "```bash\n",
    "uv venv\n",
    "uv add langchain langchain-openai langsmith openai jupyter python-dotenv \n",
    "```\n",
    "\n",
    "I'm assuming you're familiar with the basics of LLMs. If you need a refresher, you can check out some of [my](https://dylancastillo.co/posts/function-calling-structured-outputs.html) [older](https://dylancastillo.co/posts/key-parameters-llms.html) [posts](https://dylancastillo.co/posts/prompt-engineering-101.html). Also, if you don't want to copy and paste the code, you can  download this post's [notebook](https://github.com/dylanjcastillo/blog/tree/main/posts/synthetic-data-rag.ipynb) and follow along.\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "As usual, you should start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | output: false\n",
    "\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import Client, trace, traceable\n",
    "from langsmith.run_trees import RunTree\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will import all the libraries required for the next sections:\n",
    "\n",
    "1. `datasets` for loading the sample dataset we'll use to run evaluations.\n",
    "2. `langchain` libraries and `openai` for working with LLMs\n",
    "3. `langsmith` for tracing and evaluating the pipeline\n",
    "4. `dotenv` and `pydantic` for environment variable management and data validation\n",
    "\n",
    "Next, you will create your first trace on LangSmith."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracing and monitoring \n",
    "\n",
    "A LangSmith **trace** captures the full execution path of a single operation. It consists of a sequence of steps, which are called **runs**. Each trace contains the top-level inputs and outputs, as well as metadata such as runtime version and operating system details.\n",
    "\n",
    "There are four ways to create traces in LangSmith:\n",
    "\n",
    "1. Using `@traceable`\n",
    "2. Using a wrapped client\n",
    "3. Using a `trace` context manager\n",
    "4. Manually creating traces with `RunTree`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `@traceable`\n",
    "\n",
    "The simplest way is to encapsulate your pipeline in a function and use the `traceable` decorator: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am ChatGPT, an AI language model created by OpenAI. I'm here to assist you with information, answer questions, and help with a wide range of topics. How can I help you today?\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | output: false\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "@traceable\n",
    "def format_messages(question: str) -> list[dict]:\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"You're a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_llm(messages: list[dict]):\n",
    "    response = client.chat.completions.create(model=\"gpt-4.1-mini\", messages=messages)\n",
    "    return response\n",
    "\n",
    "\n",
    "@traceable\n",
    "def run_pipeline(question: str):\n",
    "    messages = format_messages(question)\n",
    "    response = call_llm(messages)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "run_pipeline(\"Who are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will automatically log the input and ouput of the functions decorated with `traceable`. It will also handle the nesting for you, so that `format_messages` and `call_llm` are steps within the `run_pipeline` function.  \n",
    "\n",
    "In `traceable` you can customize xyz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a `trace` context manager\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, to the `traceable` decorator, you can also use the `trace` context manager to create traces. You can easily combine both as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "@traceable\n",
    "def format_messages(question: str) -> list[dict]:\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"You're a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_llm(messages: list[dict]):\n",
    "    response = client.chat.completions.create(model=\"gpt-4.1-mini\", messages=messages)\n",
    "    return response\n",
    "\n",
    "\n",
    "app_inputs = {\"question\": \"Who are you?\"}\n",
    "\n",
    "with trace(\"run_pipeline\", inputs=app_inputs) as rt:\n",
    "    messages = format_messages(app_inputs[\"question\"])\n",
    "    response = call_llm(messages)\n",
    "    output = response.choices[0].message.content\n",
    "    rt.end(outputs={\"output\": output})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will generate a trace called \"LLM Pipeline\" with the input and output of the entire pipeline. Within this trace, you will find the individual traces for each function call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a wrapped client\n",
    "\n",
    "For `OpenAI` and `Anthropic` models, LangSmith offers a wrapped client that automatically instruments calls to the API with tracing. Any call to the LLM will automatically handled by LangSmith. This plays well with using `traceable` for the rest of the part in your pipeline. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am ChatGPT, an AI language model created by OpenAI. I'm here to assist you with information, answer your questions, and help with a variety of tasks. How can I help you today?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | output: false\n",
    "client = wrap_openai(OpenAI())  # Added client wrapper\n",
    "\n",
    "\n",
    "@traceable\n",
    "def format_messages(question: str) -> list[dict]:\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"You're a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "\n",
    "\n",
    "# Removed @traceable\n",
    "def call_llm(messages: list[dict]):\n",
    "    response = client.chat.completions.create(model=\"gpt-4.1-mini\", messages=messages)\n",
    "    return response\n",
    "\n",
    "\n",
    "@traceable\n",
    "def run_pipeline(question: str):\n",
    "    messages = format_messages(question)\n",
    "    response = call_llm(messages)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "run_pipeline(\"Who are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will automatically log the LLM calls made within `run_pipeline`, so you no longer need to add the `traceable` decorator to each call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually creating traces with `RunTree`\n",
    "\n",
    "If you want to have more control over the tracing, you can use [`RunTree`](https://docs.smith.langchain.com/reference/python/run_trees/langsmith.run_trees.RunTree). It provides the most flexibility but requires more setup.\n",
    "\n",
    "Here's the `RunTree` version of the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def format_messages(question: str, parent_run: RunTree):\n",
    "    format_message_step = parent_run.create_child(\n",
    "        name=\"format_messages\", run_type=\"tool\", inputs={\"question\": question}\n",
    "    )\n",
    "    format_message_step.post()\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    format_message_step.end(outputs={\"messages\": messages})\n",
    "    format_message_step.patch()\n",
    "    return messages\n",
    "\n",
    "\n",
    "def call_llm(messages: list[dict], parent_run: RunTree):\n",
    "    call_llm_step = parent_run.create_child(\n",
    "        name=\"call_llm\",\n",
    "        run_type=\"llm\",\n",
    "        inputs={\"messages\": messages},\n",
    "    )\n",
    "    call_llm_step.post()\n",
    "    response = client.chat.completions.create(model=\"gpt-4.1-mini\", messages=messages)\n",
    "    call_llm_step.end(outputs=response)\n",
    "    call_llm_step.patch()\n",
    "    return response\n",
    "\n",
    "\n",
    "def run_pipeline(question: str):\n",
    "    parent_run = RunTree(name=\"run_pipeline\", inputs={\"question\": question})\n",
    "    parent_run.post()\n",
    "\n",
    "    messages = format_messages(question, parent_run)\n",
    "    response = call_llm(messages, parent_run)\n",
    "\n",
    "    parent_run.end(outputs={\"answer\": response.choices[0].message.content})\n",
    "    parent_run.patch()\n",
    "\n",
    "\n",
    "run_pipeline(\"Who are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will result in a similar trace, but in this case you have more control over when/what to send in each step. \n",
    "\n",
    "For all of these methods, you should've obtained a trace that looks like this:\n",
    "\n",
    "![](./images/langsmith-101/traces.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the left of the image, you should see the trace for the `run_pipeline` function, which includes all the steps taken during the execution of the function, including the formatting of messages and the call to the LLM. To the right, you will see the input and output for the full trace.\n",
    "\n",
    "Then, you can click on each individual step to view more details about that step, including the inputs, outputs, and any errors that may have occurred.\n",
    "\n",
    "Here's `format_messages`: \n",
    "\n",
    "![](./images/langsmith-101/format_messages.png)\n",
    "\n",
    "And here's `call_llm`:\n",
    "\n",
    "![image.png](./images/langsmith-101/call_llm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend you explore the traces on your own. Just looking at the images in this post won't be enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n",
    "\n",
    "LangSmith lets you evaluate your LLM pipelines by providing you with a way to upload evaluation datasets, define evaluation metrics, and view the results of your experiments.\n",
    "\n",
    "Let's explore this by running a set of evals on a sample dataset. You'll use the [`AIMO Validation AIME`](https://huggingface.co/datasets/AI-MO/aimo-validation-aime) dataset that contains questions, answers and detailed solutions from the 2022, 2023, and 2024 AIME competitions.\n",
    "\n",
    "You should start by creating a dataset on LangSmith:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset AIME Example Dataset (sample) already exists. Error: Conflict for /datasets. HTTPError('409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets', '{\"detail\":\"Dataset with this name already exists.\"}')\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"AI-MO/aimo-validation-aime\")\n",
    "examples = [\n",
    "    {\"inputs\": {\"question\": d[\"problem\"]}, \"outputs\": {\"answer\": int(d[\"answer\"])}}\n",
    "    for d in ds[\"train\"]\n",
    "][:15]\n",
    "\n",
    "client = Client()\n",
    "\n",
    "dataset_name = \"AIME Example Dataset (sample)\"\n",
    "\n",
    "try:\n",
    "    dataset = client.create_dataset(dataset_name)\n",
    "    client.create_examples(dataset_id=dataset.id, examples=examples)\n",
    "except Exception as e:\n",
    "    print(f\"Dataset {dataset_name} already exists. Error: {e}\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a dataset with the first 15 examples from the AIMO Validation AIME dataset. I only included a a sample of the dataset to keep costs down. You can always add more examples later if needed.\n",
    "\n",
    "The dataset will be available under `Datasets & Experiments`:\n",
    "\n",
    "![image.png](./images/langsmith-101/dataset.png)\n",
    "\n",
    "Then, you'll define a pipeline that takes the user question, and provides a response using a structured output: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response(BaseModel):\n",
    "    explanation: str = Field(description=\"The explanation of the answer\")\n",
    "    answer: int = Field(\n",
    "        description=\"The answer to the question. It should be an integer.\"\n",
    "    )\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "model_with_structure = model.with_structured_output(Response, method=\"function_calling\")\n",
    "\n",
    "\n",
    "def get_response(question: str) -> Response:\n",
    "    max_retries = 3\n",
    "    for _ in range(max_retries):\n",
    "        try:\n",
    "            messages = [\n",
    "                SystemMessage(\n",
    "                    \"You're a math expert. You will always respond in a JSON format with the following fields: explanation and answer.\"\n",
    "                ),\n",
    "                HumanMessage(question),\n",
    "            ]\n",
    "            response = model_with_structure.invoke(messages)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "    raise ValueError(\"Failed to get a valid response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I included a simple retry mechanism, as I often found that the model sometime failed to generate a valid response.\n",
    "\n",
    "Next, you should define the evaluation metrics you'll use to measure the performance of your pipeline. You could define a simple accuracy metric that checks if the answer is the same as the expected answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    return outputs[\"answer\"] == reference_outputs[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define an evaluation metric in LangSmith, you must create a function that takes the inputs, outputs, and reference outputs as arguments and returns a boolean or a numeric value. \n",
    "\n",
    "For accuracy, the function checks if the answer provided by the model matches the expected answer from the dataset, and returns a boolean value indicating whether the evaluation passed or failed. \n",
    "\n",
    "You can also define more complex metrics, such as an LLM judge to evaluate the clarity of the solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClarityResponse(BaseModel):\n",
    "    explanation: str = Field(description=\"The explanation of the answer\")\n",
    "    clarity: int = Field(description=\"The clarity of the explanation\", ge=1, le=5)\n",
    "\n",
    "\n",
    "def clarity(inputs: dict, outputs: dict, reference_outputs: dict) -> int:\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that evaluates the clarity of the explanation of the answer. You will always return a number between 1 and 5, where 1 is the lowest clarity and 5 is the highest clarity.\"\n",
    "        ),\n",
    "        HumanMessage(content=f\"Explanation: {outputs['explanation']}\"),\n",
    "    ]\n",
    "    model_with_clarity_structure = model.with_structured_output(ClarityResponse)\n",
    "    response = model_with_clarity_structure.invoke(messages)\n",
    "    return response.clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `clarity` metric evaluates the clarity of the explanation provided by the model. It uses a scale from 1 to 5, where 1 indicates low clarity and 5 indicates high clarity. \n",
    "\n",
    "Finally, you can run the evaluation using `client.evaluate()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "\n",
    "def ls_wrapper(inputs: dict) -> dict:\n",
    "    response = get_response(inputs[\"question\"])\n",
    "    return response.model_dump()\n",
    "\n",
    "\n",
    "experiment_results = client.aevaluate(\n",
    "    ls_wrapper, data=dataset_name, evaluators=[accuracy, clarity], max_concurrency=15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangSmith requires you to define a function that wraps your pipeline function. It should take an input dictionary that contains the necessary parameters for your pipeline and return a dictionary with the results. You can also specify a `evaluators` parameter that includes the evaluation metrics you want to use.\n",
    "\n",
    "\n",
    "After you've run the evaluation, you'll be able to inspect the results of the experiment:\n",
    "\n",
    "![image.png](./images/langsmith-101/results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also investigate single runs:\n",
    "\n",
    "![image.png](./images/langsmith-101/single_run.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or see how results look over time:\n",
    "\n",
    "![image.png](./images/langsmith-101/results_over_time.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, I suggest you go explore the results in the LangSmith UI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all! We've covered the basics of using LangSmith to trace and evaluate your LLM applications. \n",
    "\n",
    "By now, you should have a good understanding of how to create traces, define evaluation metrics, and run experiments.\n",
    "\n",
    "If you have any questions or feedback, let me know in the comments below. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
