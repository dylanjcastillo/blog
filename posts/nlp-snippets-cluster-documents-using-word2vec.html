<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Dylan Castillo">
<meta name="dcterms.date" content="2021-01-18">
<meta name="description" content="Learn how to cluster documents using Word2Vec. In this tutorial, you’ll train a Word2Vec model, generate word embeddings, and use K-means to create groups of news articles.">

<title>How to Cluster Documents Using Word2Vec and K-means – Dylan Castillo</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../images/logo.webp" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-8ea72dc5fed832574809a9c94082fbbb.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-ae853c3ebf3e379b3ce4995fdc763be4.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="dark">
<script src="../site_libs/quarto-contrib/iconify-2.1.0/iconify-icon.min.js"></script>
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="https://fonts.googleapis.com/css2?family=Merriweather:ital,wght@0,300;0,400;0,700;0,900;1,300;1,400;1,700;1,900&amp;display=swap" rel="stylesheet">
<script src="https://cdn.usefathom.com/script.js" data-site="ZJFQREIA" defer=""></script>


<meta property="og:title" content="How to Cluster Documents Using Word2Vec and K-means – Dylan Castillo">
<meta property="og:description" content="">
<meta property="og:image" content="https://dylancastillo.co/posts/images/social_media_card.webp">
<meta property="og:site_name" content="Dylan Castillo">
<meta name="twitter:title" content="How to Cluster Documents Using Word2Vec and K-means – Dylan Castillo">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="https://dylancastillo.co/posts/images/social_media_card.webp">
<meta name="twitter:creator" content="@dylanjcastillo">
<meta name="twitter:site" content="@dylanjcastillo">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../projects.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/dylanjcastillo"> 
<span class="menu-text"><iconify-icon role="img" inline="" icon="fa6-brands:github" aria-label="Icon github from fa6-brands Iconify.design set." title="Icon github from fa6-brands Iconify.design set."></iconify-icon></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.linkedin.com/in/dylanjcastillo/"> 
<span class="menu-text"><iconify-icon role="img" inline="" icon="fa6-brands:linkedin" aria-label="Icon linkedin from fa6-brands Iconify.design set." title="Icon linkedin from fa6-brands Iconify.design set."></iconify-icon></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://bsky.app/profile/dylancastillo.co"> 
<span class="menu-text"><iconify-icon role="img" inline="" icon="fa6-brands:bluesky" aria-label="Icon bluesky from fa6-brands Iconify.design set." title="Icon bluesky from fa6-brands Iconify.design set."></iconify-icon></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#how-to-cluster-documents" id="toc-how-to-cluster-documents" class="nav-link active" data-scroll-target="#how-to-cluster-documents">How to Cluster Documents</a></li>
  <li><a href="#sample-project-clustering-news-articles" id="toc-sample-project-clustering-news-articles" class="nav-link" data-scroll-target="#sample-project-clustering-news-articles">Sample Project: Clustering News Articles</a>
  <ul class="collapse">
  <li><a href="#set-up-your-local-environment" id="toc-set-up-your-local-environment" class="nav-link" data-scroll-target="#set-up-your-local-environment">Set Up Your Local Environment</a></li>
  <li><a href="#import-the-required-libraries" id="toc-import-the-required-libraries" class="nav-link" data-scroll-target="#import-the-required-libraries">Import the Required Libraries</a></li>
  <li><a href="#clean-and-tokenize-data" id="toc-clean-and-tokenize-data" class="nav-link" data-scroll-target="#clean-and-tokenize-data">Clean and Tokenize Data</a></li>
  <li><a href="#generate-document-vectors" id="toc-generate-document-vectors" class="nav-link" data-scroll-target="#generate-document-vectors">Generate Document Vectors</a></li>
  <li><a href="#cluster-documents-using-mini-batches-k-means" id="toc-cluster-documents-using-mini-batches-k-means" class="nav-link" data-scroll-target="#cluster-documents-using-mini-batches-k-means">Cluster Documents Using (Mini-batches) K-means</a></li>
  </ul></li>
  <li><a href="#other-approaches" id="toc-other-approaches" class="nav-link" data-scroll-target="#other-approaches">Other Approaches</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    <div class="quarto-margin-footer"><div class="margin-footer-item">
<button onclick="window.location.href='https://subscribe.dylancastillo.co'" style="background-color: #eb841b; color: white; padding: 12px 24px; border: none; border-radius: 6px; font-size: 12px; font-weight: bold; cursor: pointer; transition: background-color 0.3s ease;">
Subscribe to my newsletter
</button>
</div></div></div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">How to Cluster Documents Using Word2Vec and K-means</h1>
  <div class="quarto-categories">
    <div class="quarto-category">python</div>
    <div class="quarto-category">nlp</div>
    <div class="quarto-category">ml</div>
  </div>
  </div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://dylancastillo.co">Dylan Castillo</a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://iwanalabs.com">
            Iwana Labs
            </a>
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 18, 2021</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">February 16, 2025</p>
    </div>
  </div>
    
  </div>
  


</header>


<p>There are more sentiment analysis tutorials online than people doing sentiment analysis in their day jobs. Don’t get me wrong. I’m not saying those tutorials aren’t useful. I just want to highlight that <strong>supervised learning</strong> receives much more attention than any other Natural Language Processing (NLP) method.</p>
<p>Oddly enough, there’s a big chance that most of the text data you’ll use in your next projects won’t have ground truth labels. So supervised learning might not be a solution you can immediately apply to your data problems.</p>
<p>What can you do then? Use <strong>unsupervised learning</strong> algorithms.</p>
<p>In this tutorial, you’ll learn to apply unsupervised learning to generate value from your text data. You’ll cluster documents by training a word embedding (Word2Vec) and applying the K-means algorithm.</p>
<p>Please be aware that the next sections focus on practical manners. You won’t find much theory in them besides brief definitions of relevant ideas.</p>
<p>To make the most of this tutorial, you should be familiar with these topics:</p>
<ul>
<li><a href="https://www.ibm.com/cloud/blog/supervised-vs-unsupervised-learning">Supervised and unsupervised learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Cluster_analysis">Clustering</a> (particularly, <a href="https://realpython.com/k-means-clustering-python/">K-means</a>)</li>
<li><a href="https://jalammar.github.io/illustrated-word2vec/">Word2Vec</a></li>
</ul>
<p>Let’s get to it!</p>
<section id="how-to-cluster-documents" class="level2">
<h2 class="anchored" data-anchor-id="how-to-cluster-documents">How to Cluster Documents</h2>
<p>You can think of the process of clustering documents in three steps:</p>
<ol type="1">
<li><a href="https://dylancastillo.co/nlp-snippets-clean-and-tokenize-text-with-python/"><strong>Cleaning and tokenizing data</strong></a> usually involves lowercasing text, removing non-alphanumeric characters, or stemming words.</li>
<li><strong>Generating vector representations of the documents</strong> concerns the mapping of documents from words into numerical vectors—some common ways of doing this include using bag-of-words models or word embeddings.</li>
<li><strong>Applying a clustering algorithm on the document vectors</strong> requires selecting and applying a clustering algorithm to find the best possible groups using the document vectors. Some frequently used algorithms include K-means, DBSCAN, or Hierarchical Clustering.</li>
</ol>
<p>That’s it! Now, you’ll see how that looks in practice.</p>
</section>
<section id="sample-project-clustering-news-articles" class="level2">
<h2 class="anchored" data-anchor-id="sample-project-clustering-news-articles">Sample Project: Clustering News Articles</h2>
<p>In this section, you’ll learn how to cluster documents by working through a small project. You’ll group news articles into categories using a <a href="https://www.kaggle.com/szymonjanowski/internet-articles-data-with-users-engagement">dataset</a> published by <a href="https://github.com/sleter">Szymon Janowski</a>.</p>
<section id="set-up-your-local-environment" class="level3">
<h3 class="anchored" data-anchor-id="set-up-your-local-environment">Set Up Your Local Environment</h3>
<p>To follow along with the tutorial examples, you’ll need to download the data and install a few libraries. You can do it by following these steps:</p>
<ol type="1">
<li>Clone the <a href="https://github.com/dylanjcastillo/nlp-snippets/">nlp-snippets repository</a> locally.</li>
<li>Create a new virtual environment using <code>venv</code> or <code>conda</code>.</li>
<li>Activate your new virtual environment.</li>
<li>Install the required libraries.</li>
<li>Start a Jupyter notebook.</li>
</ol>
<p>If you’re using <code>venv</code>, then you need to run these commands:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1"></a><span class="fu">git</span> clone https://github.com/dylanjcastillo/nlp-snippets.git</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="ex">python3</span> <span class="at">-m</span> venv venv</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="bu">source</span> venv/bin/activate</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="ex">pip</span> install <span class="at">-r</span> requirements</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="ex">jupyter</span> notebook</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If you’re using <code>conda</code>, then you need to run these commands:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1"></a><span class="fu">git</span> clone https://github.com/dylanjcastillo/nlp-snippets.git</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="ex">conda</span> create <span class="at">--name</span> venv</span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="ex">conda</span> activate venv</span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="ex">pip</span> install <span class="at">-r</span> requirements</span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="ex">jupyter</span> notebook</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Next, open Jupyter Notebook. Then, create a new notebook in the root folder and set its name to <code>clustering_word2vec.ipynb</code>.</p>
<p>By now, your project structure should look like this:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1"></a><span class="ex">nlp-snippets/</span></span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="ex">│</span></span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="ex">├──</span> clustering/</span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="ex">│</span></span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="ex">├──</span> data/</span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="ex">│</span></span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="ex">├──</span> ds_utils/</span>
<span id="cb3-8"><a href="#cb3-8"></a><span class="ex">│</span></span>
<span id="cb3-9"><a href="#cb3-9"></a><span class="ex">├──</span> preprocessing/</span>
<span id="cb3-10"><a href="#cb3-10"></a><span class="ex">│</span></span>
<span id="cb3-11"><a href="#cb3-11"></a><span class="ex">├──</span> venv/ <span class="co"># (If you're using venv)</span></span>
<span id="cb3-12"><a href="#cb3-12"></a><span class="ex">│</span></span>
<span id="cb3-13"><a href="#cb3-13"></a><span class="ex">├──</span> clustering_word2vec.ipynb</span>
<span id="cb3-14"><a href="#cb3-14"></a><span class="ex">├──</span> LICENSE</span>
<span id="cb3-15"><a href="#cb3-15"></a><span class="ex">├──</span> README.md</span>
<span id="cb3-16"><a href="#cb3-16"></a><span class="ex">└──</span> requirements.txt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is your project’s structure. It includes these directories and files:</p>
<ul>
<li><code>clustering/</code>: Examples of clustering text data using bag-of-words, training a word2vec model, and using a pretrained fastText embeddings.</li>
<li><code>data/</code>: Data used for the clustering examples.</li>
<li><code>ds_utils/</code>: Common utility functions used in the sample notebooks in the repository.</li>
<li><code>preprocessing/</code>: Frequently used code snippets for preprocessing text.</li>
<li><code>venv/</code>: If you used <code>venv</code>, then this directory will contain the files related to your virtual environment.</li>
<li><code>requirements.txt</code>: Libraries used in the examples provided.</li>
<li><code>README</code> and <code>License</code>: Information about the repository and its license.</li>
</ul>
<p>For now, you’ll use the notebook you created (<code>clustering_word2vec.ipynb</code>) and the news dataset in <code>data/</code>. The notebooks in <code>clustering/</code> and <code>preprocessing/</code> include additional code snippets that might be useful for NLP tasks. You can review those on your own.</p>
<p>In the next section, you’ll create the whole pipeline from scratch. If you’d like to download the full and cleaner version of the code in the examples, go to the <a href="https://github.com/dylanjcastillo/nlp-snippets/blob/main/clustering/word2vec.ipynb">NLP Snippets repository</a>.</p>
<p>That’s it for setup! Next, you’ll define your imports.</p>
</section>
<section id="import-the-required-libraries" class="level3">
<h3 class="anchored" data-anchor-id="import-the-required-libraries">Import the Required Libraries</h3>
<p>Once you finish setting up your local environment, it’s time to start writing code in your notebook. Open <code>clustering_word2vec.ipynb</code>, and copy the following code in the first cell:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="im">import</span> os</span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="im">import</span> random</span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="im">import</span> re</span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="im">import</span> string</span>
<span id="cb4-5"><a href="#cb4-5"></a></span>
<span id="cb4-6"><a href="#cb4-6"></a><span class="im">import</span> nltk</span>
<span id="cb4-7"><a href="#cb4-7"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-8"><a href="#cb4-8"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-9"><a href="#cb4-9"></a></span>
<span id="cb4-10"><a href="#cb4-10"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb4-11"><a href="#cb4-11"></a></span>
<span id="cb4-12"><a href="#cb4-12"></a><span class="im">from</span> nltk <span class="im">import</span> word_tokenize</span>
<span id="cb4-13"><a href="#cb4-13"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb4-14"><a href="#cb4-14"></a></span>
<span id="cb4-15"><a href="#cb4-15"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> MiniBatchKMeans</span>
<span id="cb4-16"><a href="#cb4-16"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> silhouette_samples, silhouette_score</span>
<span id="cb4-17"><a href="#cb4-17"></a></span>
<span id="cb4-18"><a href="#cb4-18"></a>nltk.download(<span class="st">"stopwords"</span>)</span>
<span id="cb4-19"><a href="#cb4-19"></a>nltk.download(<span class="st">"punkt"</span>)</span>
<span id="cb4-20"><a href="#cb4-20"></a></span>
<span id="cb4-21"><a href="#cb4-21"></a>SEED <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb4-22"><a href="#cb4-22"></a>random.seed(SEED)</span>
<span id="cb4-23"><a href="#cb4-23"></a>os.environ[<span class="st">"PYTHONHASHSEED"</span>] <span class="op">=</span> <span class="bu">str</span>(SEED)</span>
<span id="cb4-24"><a href="#cb4-24"></a>np.random.seed(SEED)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>These are the libraries you need for the sample project. Here’s what you do with each of them:</p>
<ul>
<li><code>os</code> and <code>random</code> help you define a random seed to make the code deterministically reproducible.</li>
<li><code>re</code> and <code>string</code> provide you with easy ways to clean the data.</li>
<li><code>pandas</code>helps you read the data.</li>
<li><code>numpy</code>provides you with linear algebra utilities you’ll use to evaluate results. Also, it’s used for setting a random seed to make the code deterministically reproducible.</li>
<li><code>gensim</code> makes it easy for you to train a word embedding from scratch using the <code>Word2Vec</code> class.</li>
<li><code>nltk</code>aids you in cleaning and tokenizing data through the <code>word_tokenize</code> method and the <code>stopword</code> list.</li>
<li><code>sklearn</code>gives you an easy interface to the clustering model, <code>MiniBatchKMeans</code>, and the metrics to evaluate the quality of its results, <code>silhouette_samples</code> and <code>silhouette_score</code>.</li>
</ul>
<p>In addition to importing the libraries, you download English stopwords using <code>nltk.download("stopwords")</code>, you define <code>SEED</code> and set it as a random seed using <code>numpy</code>, <code>random</code>, and the <code>PYTHONHASHSEED</code> environment variable. This last step makes sure your code is reproducible across systems.</p>
<p>Run this cell and make sure you don’t get any errors. In the next section, you’ll prepare your text data.</p>
</section>
<section id="clean-and-tokenize-data" class="level3">
<h3 class="anchored" data-anchor-id="clean-and-tokenize-data">Clean and Tokenize Data</h3>
<p>After you import the required libraries, you need to read and preprocess the data you’ll use in your clustering algorithm. The preprocessing consists of cleaning and tokenizing the data. To do that, copy the following function in a new cell in your notebook:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw">def</span> clean_text(text, tokenizer, stopwords):</span>
<span id="cb5-2"><a href="#cb5-2"></a>    <span class="co">"""Pre-process text and generate tokens</span></span>
<span id="cb5-3"><a href="#cb5-3"></a></span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="co">    Args:</span></span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="co">        text: Text to tokenize.</span></span>
<span id="cb5-6"><a href="#cb5-6"></a></span>
<span id="cb5-7"><a href="#cb5-7"></a><span class="co">    Returns:</span></span>
<span id="cb5-8"><a href="#cb5-8"></a><span class="co">        Tokenized text.</span></span>
<span id="cb5-9"><a href="#cb5-9"></a><span class="co">    """</span></span>
<span id="cb5-10"><a href="#cb5-10"></a>    text <span class="op">=</span> <span class="bu">str</span>(text).lower()  <span class="co"># Lowercase words</span></span>
<span id="cb5-11"><a href="#cb5-11"></a>    text <span class="op">=</span> re.sub(<span class="vs">r"\[(.*?)\]"</span>, <span class="st">""</span>, text)  <span class="co"># Remove [+XYZ chars] in content</span></span>
<span id="cb5-12"><a href="#cb5-12"></a>    text <span class="op">=</span> re.sub(<span class="vs">r"\s+"</span>, <span class="st">" "</span>, text)  <span class="co"># Remove multiple spaces in content</span></span>
<span id="cb5-13"><a href="#cb5-13"></a>    text <span class="op">=</span> re.sub(<span class="vs">r"\w+…|…"</span>, <span class="st">""</span>, text)  <span class="co"># Remove ellipsis (and last word)</span></span>
<span id="cb5-14"><a href="#cb5-14"></a>    text <span class="op">=</span> re.sub(<span class="vs">r"(?&lt;=\w)-(?=\w)"</span>, <span class="st">" "</span>, text)  <span class="co"># Replace dash between words</span></span>
<span id="cb5-15"><a href="#cb5-15"></a>    text <span class="op">=</span> re.sub(</span>
<span id="cb5-16"><a href="#cb5-16"></a>        <span class="ss">f"[</span><span class="sc">{</span>re<span class="sc">.</span>escape(string.punctuation)<span class="sc">}</span><span class="ss">]"</span>, <span class="st">""</span>, text</span>
<span id="cb5-17"><a href="#cb5-17"></a>    )  <span class="co"># Remove punctuation</span></span>
<span id="cb5-18"><a href="#cb5-18"></a></span>
<span id="cb5-19"><a href="#cb5-19"></a>    tokens <span class="op">=</span> tokenizer(text)  <span class="co"># Get tokens from text</span></span>
<span id="cb5-20"><a href="#cb5-20"></a>    tokens <span class="op">=</span> [t <span class="cf">for</span> t <span class="kw">in</span> tokens <span class="cf">if</span> <span class="kw">not</span> t <span class="kw">in</span> stopwords]  <span class="co"># Remove stopwords</span></span>
<span id="cb5-21"><a href="#cb5-21"></a>    tokens <span class="op">=</span> [<span class="st">""</span> <span class="cf">if</span> t.isdigit() <span class="cf">else</span> t <span class="cf">for</span> t <span class="kw">in</span> tokens]  <span class="co"># Remove digits</span></span>
<span id="cb5-22"><a href="#cb5-22"></a>    tokens <span class="op">=</span> [t <span class="cf">for</span> t <span class="kw">in</span> tokens <span class="cf">if</span> <span class="bu">len</span>(t) <span class="op">&gt;</span> <span class="dv">1</span>]  <span class="co"># Remove short tokens</span></span>
<span id="cb5-23"><a href="#cb5-23"></a>    <span class="cf">return</span> tokens</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This code cleans and tokenizes a <code>text</code> input, using a predefined <code>tokenizer</code> and a list of <code>stopwords</code>. It helps you perform these operations:</p>
<ol type="1">
<li><strong>Line 10:</strong> Transform the input into a string and lowercase it.</li>
<li><strong>Line 11:</strong> Remove substrings like “[+300 chars]” I found while reviewing the data.</li>
<li><strong>Line 12:</strong> Remove multiple spaces, tabs, and line breaks.</li>
<li><strong>Line 13:</strong> Remove ellipsis characters.</li>
<li><strong>Lines 14-17:</strong> Replace dashes between words with a space and remove punctuation.</li>
<li><strong>Lines 19-20:</strong> Tokenize text and remove tokens using a list of stop words.</li>
<li><strong>Lines 21-22:</strong> Remove digits and tokens whose length is too short.</li>
</ol>
<p>Then, in the next cell, copy the following code to read the data and apply that function to the text columns:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>custom_stopwords <span class="op">=</span> <span class="bu">set</span>(stopwords.words(<span class="st">"english"</span>) <span class="op">+</span> [<span class="st">"news"</span>, <span class="st">"new"</span>, <span class="st">"top"</span>])</span>
<span id="cb6-2"><a href="#cb6-2"></a>text_columns <span class="op">=</span> [<span class="st">"title"</span>, <span class="st">"description"</span>, <span class="st">"content"</span>]</span>
<span id="cb6-3"><a href="#cb6-3"></a></span>
<span id="cb6-4"><a href="#cb6-4"></a>df_raw <span class="op">=</span> pd.read_csv(<span class="st">"data/news_data.csv"</span>)</span>
<span id="cb6-5"><a href="#cb6-5"></a>df <span class="op">=</span> df_raw.copy()</span>
<span id="cb6-6"><a href="#cb6-6"></a>df[<span class="st">"content"</span>] <span class="op">=</span> df[<span class="st">"content"</span>].fillna(<span class="st">""</span>)</span>
<span id="cb6-7"><a href="#cb6-7"></a></span>
<span id="cb6-8"><a href="#cb6-8"></a><span class="cf">for</span> col <span class="kw">in</span> text_columns:</span>
<span id="cb6-9"><a href="#cb6-9"></a>    df[col] <span class="op">=</span> df[col].astype(<span class="bu">str</span>)</span>
<span id="cb6-10"><a href="#cb6-10"></a></span>
<span id="cb6-11"><a href="#cb6-11"></a><span class="co"># Create text column based on title, description, and content</span></span>
<span id="cb6-12"><a href="#cb6-12"></a>df[<span class="st">"text"</span>] <span class="op">=</span> df[text_columns].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">" | "</span>.join(x), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-13"><a href="#cb6-13"></a>df[<span class="st">"tokens"</span>] <span class="op">=</span> df[<span class="st">"text"</span>].<span class="bu">map</span>(<span class="kw">lambda</span> x: clean_text(x, word_tokenize, custom_stopwords))</span>
<span id="cb6-14"><a href="#cb6-14"></a></span>
<span id="cb6-15"><a href="#cb6-15"></a><span class="co"># Remove duplicated after preprocessing</span></span>
<span id="cb6-16"><a href="#cb6-16"></a>_, idx <span class="op">=</span> np.unique(df[<span class="st">"tokens"</span>], return_index<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-17"><a href="#cb6-17"></a>df <span class="op">=</span> df.iloc[idx, :]</span>
<span id="cb6-18"><a href="#cb6-18"></a></span>
<span id="cb6-19"><a href="#cb6-19"></a><span class="co"># Remove empty values and keep relevant columns</span></span>
<span id="cb6-20"><a href="#cb6-20"></a>df <span class="op">=</span> df.loc[df.tokens.<span class="bu">map</span>(<span class="kw">lambda</span> x: <span class="bu">len</span>(x) <span class="op">&gt;</span> <span class="dv">0</span>), [<span class="st">"text"</span>, <span class="st">"tokens"</span>]]</span>
<span id="cb6-21"><a href="#cb6-21"></a></span>
<span id="cb6-22"><a href="#cb6-22"></a>docs <span class="op">=</span> df[<span class="st">"text"</span>].values</span>
<span id="cb6-23"><a href="#cb6-23"></a>tokenized_docs <span class="op">=</span> df[<span class="st">"tokens"</span>].values</span>
<span id="cb6-24"><a href="#cb6-24"></a></span>
<span id="cb6-25"><a href="#cb6-25"></a><span class="bu">print</span>(<span class="ss">f"Original dataframe: </span><span class="sc">{</span>df_raw<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-26"><a href="#cb6-26"></a><span class="bu">print</span>(<span class="ss">f"Pre-processed dataframe: </span><span class="sc">{</span>df<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is how you read and preprocess the data. This code applies the cleaning function you defined earlier, removes duplicates and nulls, and drops irrelevant columns.</p>
<p>You apply these steps to a new data frame (<code>df</code>). It contains a column with the raw documents called <code>text</code> and another one with the preprocessed documents called <code>tokens</code>. You save the values of those columns into two variables, <code>docs</code> and <code>tokenized_docs</code>, to use in the next code snippets.</p>
<p>If you execute the two cells you defined, then you should get the following output:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1"></a><span class="ex">Original</span> dataframe: <span class="er">(</span><span class="ex">10437,</span> 15<span class="kw">)</span></span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="ex">Pre-processed</span> dataframe: <span class="er">(</span><span class="ex">9882,</span> 2<span class="kw">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Next, you’ll create document vectors using Word2Vec.</p>
</section>
<section id="generate-document-vectors" class="level3">
<h3 class="anchored" data-anchor-id="generate-document-vectors">Generate Document Vectors</h3>
<p>After you’ve cleaned and tokenized the text, you’ll use the documents’ tokens to create vectors using Word2Vec. This process consists of two steps:</p>
<ol type="1">
<li>Train a Word2Vec model using the tokens you generated earlier. Alternatively, you could load a pre-trained Word2Vec model (I’ll also show you how to do it).</li>
<li>Generate a vector per document based on its individual word vectors.</li>
</ol>
<p>In this section, you’ll go through these steps.</p>
<section id="train-word2vec-model" class="level4">
<h4 class="anchored" data-anchor-id="train-word2vec-model">Train Word2Vec Model</h4>
<p>The following code will help you train a Word2Vec model. Copy it into a new cell in your notebook:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>model <span class="op">=</span> Word2Vec(sentences<span class="op">=</span>tokenized_docs, vector_size<span class="op">=</span><span class="dv">100</span>, workers<span class="op">=</span><span class="dv">1</span>, seed<span class="op">=</span>SEED)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>You use this code to train a Word2Vec model based on your tokenized documents. For this example, you specified the following parameters in the <code>Word2Vec</code> class:</p>
<ul>
<li><code>sentences</code> expects a list of lists with the tokenized documents.</li>
<li><code>vector_size</code> defines the size of the word vectors. In this case, you set it to 100.</li>
<li><code>workers</code> defines how many cores you use for training. I set it to 1 to make sure the code is deterministically reproducible.</li>
<li><code>seed</code> sets the seed for random number generation. It’s set to the constant <code>SEED</code> you defined in the first cell.</li>
</ul>
<p>There are other parameters you can tune when training the Word2Vec model. See <a href="https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec">gensim’s documentation</a> if you’d like to learn more about them.</p>
<p><strong>Note:</strong> In many cases, you might want to use a pre-trained model instead of training one yourself. If that’s the case, gensim provides you with an easy way to access some of the <a href="https://github.com/RaRe-Technologies/gensim-data#models">most popular pre-trained word embeddings</a>.</p>
<p>You can load a pre-trained Word2Vec model as follows:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a>wv <span class="op">=</span> api.load(<span class="st">'word2vec-google-news-300'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>One last thing, if you’re following this tutorial and decide to use a pre-trained model, you’ll need to replace <code>model.wv</code> by <code>wv</code> in the code snippets from here on. Otherwise, you’ll get an error.</p>
<p>Next, run the cell you just created in your notebook. It might take a couple of minutes. After it’s done, you can validate that the results make sense by <a href="https://machinelearningmastery.com/develop-word-embeddings-python-gensim/">plotting the vectors</a> or reviewing the similarity results for relevant words. You can do the latter by copying and running this code in a cell in your notebook:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>model.wv.most_similar(<span class="st">"trump"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If you run this code, then you’ll get this output:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>[(<span class="st">'trumps'</span>, <span class="fl">0.988541841506958</span>),</span>
<span id="cb11-2"><a href="#cb11-2"></a> (<span class="st">'president'</span>, <span class="fl">0.9746493697166443</span>),</span>
<span id="cb11-3"><a href="#cb11-3"></a> (<span class="st">'donald'</span>, <span class="fl">0.9274922013282776</span>),</span>
<span id="cb11-4"><a href="#cb11-4"></a> (<span class="st">'ivanka'</span>, <span class="fl">0.9203903079032898</span>),</span>
<span id="cb11-5"><a href="#cb11-5"></a> (<span class="st">'impeachment'</span>, <span class="fl">0.9195784330368042</span>),</span>
<span id="cb11-6"><a href="#cb11-6"></a> (<span class="st">'pences'</span>, <span class="fl">0.9152231812477112</span>),</span>
<span id="cb11-7"><a href="#cb11-7"></a> (<span class="st">'avlon'</span>, <span class="fl">0.9148306846618652</span>),</span>
<span id="cb11-8"><a href="#cb11-8"></a> (<span class="st">'biden'</span>, <span class="fl">0.9146010279655457</span>),</span>
<span id="cb11-9"><a href="#cb11-9"></a> (<span class="st">'breitbart'</span>, <span class="fl">0.9144087433815002</span>),</span>
<span id="cb11-10"><a href="#cb11-10"></a> (<span class="st">'vice'</span>, <span class="fl">0.9067237973213196</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>That’s it! You’ve trained your Word2Vec model, now, you’ll use it to generate document vectors.</p>
</section>
<section id="create-document-vectors-from-word-embedding" class="level4">
<h4 class="anchored" data-anchor-id="create-document-vectors-from-word-embedding">Create Document Vectors from Word Embedding</h4>
<p>Now you’ll generate document vectors using the Word2Vec model you trained. The idea is straightforward. From the Word2Vec model, you’ll get numerical vectors per word in a document, so you need to find a way of generating a single vector out of them.</p>
<p>For <a href="https://arxiv.org/pdf/1607.00570.pdf">short texts</a>, a common approach is to use the <a href="https://stats.stackexchange.com/a/318891">average</a> of the vectors. There’s no clear consensus on what will work well for longer texts. Though, using a <a href="https://openreview.net/forum?id=SyK00v5xx">weighted average of the vectors</a> might help.</p>
<p>The following code will help you create a vector per document by averaging its word vectors. Create a new cell in your notebook and copy this code there:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="kw">def</span> vectorize(list_of_docs, model):</span>
<span id="cb12-2"><a href="#cb12-2"></a>    <span class="co">"""Generate vectors for list of documents using a Word Embedding</span></span>
<span id="cb12-3"><a href="#cb12-3"></a></span>
<span id="cb12-4"><a href="#cb12-4"></a><span class="co">    Args:</span></span>
<span id="cb12-5"><a href="#cb12-5"></a><span class="co">        list_of_docs: List of documents</span></span>
<span id="cb12-6"><a href="#cb12-6"></a><span class="co">        model: Gensim's Word Embedding</span></span>
<span id="cb12-7"><a href="#cb12-7"></a></span>
<span id="cb12-8"><a href="#cb12-8"></a><span class="co">    Returns:</span></span>
<span id="cb12-9"><a href="#cb12-9"></a><span class="co">        List of document vectors</span></span>
<span id="cb12-10"><a href="#cb12-10"></a><span class="co">    """</span></span>
<span id="cb12-11"><a href="#cb12-11"></a>    features <span class="op">=</span> []</span>
<span id="cb12-12"><a href="#cb12-12"></a></span>
<span id="cb12-13"><a href="#cb12-13"></a>    <span class="cf">for</span> tokens <span class="kw">in</span> list_of_docs:</span>
<span id="cb12-14"><a href="#cb12-14"></a>        zero_vector <span class="op">=</span> np.zeros(model.vector_size)</span>
<span id="cb12-15"><a href="#cb12-15"></a>        vectors <span class="op">=</span> []</span>
<span id="cb12-16"><a href="#cb12-16"></a>        <span class="cf">for</span> token <span class="kw">in</span> tokens:</span>
<span id="cb12-17"><a href="#cb12-17"></a>            <span class="cf">if</span> token <span class="kw">in</span> model.wv:</span>
<span id="cb12-18"><a href="#cb12-18"></a>                <span class="cf">try</span>:</span>
<span id="cb12-19"><a href="#cb12-19"></a>                    vectors.append(model.wv[token])</span>
<span id="cb12-20"><a href="#cb12-20"></a>                <span class="cf">except</span> <span class="pp">KeyError</span>:</span>
<span id="cb12-21"><a href="#cb12-21"></a>                    <span class="cf">continue</span></span>
<span id="cb12-22"><a href="#cb12-22"></a>        <span class="cf">if</span> vectors:</span>
<span id="cb12-23"><a href="#cb12-23"></a>            vectors <span class="op">=</span> np.asarray(vectors)</span>
<span id="cb12-24"><a href="#cb12-24"></a>            avg_vec <span class="op">=</span> vectors.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb12-25"><a href="#cb12-25"></a>            features.append(avg_vec)</span>
<span id="cb12-26"><a href="#cb12-26"></a>        <span class="cf">else</span>:</span>
<span id="cb12-27"><a href="#cb12-27"></a>            features.append(zero_vector)</span>
<span id="cb12-28"><a href="#cb12-28"></a>    <span class="cf">return</span> features</span>
<span id="cb12-29"><a href="#cb12-29"></a></span>
<span id="cb12-30"><a href="#cb12-30"></a>vectorized_docs <span class="op">=</span> vectorize(tokenized_docs, model<span class="op">=</span>model)</span>
<span id="cb12-31"><a href="#cb12-31"></a><span class="bu">len</span>(vectorized_docs), <span class="bu">len</span>(vectorized_docs[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This code will get all the word vectors of each document and average them to generate a vector per each document. Here’s what’s happening there:</p>
<ol type="1">
<li>You define the <code>vectorize</code> function that takes a list of documents and a <code>gensim</code> model as input, and generates a feature vector per document as output.</li>
<li>You apply the function to the documents’ tokens in <code>tokenized_doc</code>, using the Word2Vec <code>model</code> you trained earlier.</li>
<li>You print the length of the list of documents and the size of the generated vectors.</li>
</ol>
<p>Next, you’ll cluster the documents using Mini-batches K-means.</p>
</section>
</section>
<section id="cluster-documents-using-mini-batches-k-means" class="level3">
<h3 class="anchored" data-anchor-id="cluster-documents-using-mini-batches-k-means">Cluster Documents Using (Mini-batches) K-means</h3>
<p>To cluster the documents, you’ll use the <strong>Mini-batches K-means</strong> algorithm. This K-means variant uses random input data samples to reduce the time required during training. The upside is that it shares the same objective function with the original algorithm, so, in practice, the results are <a href="https://scikit-learn.org/stable/modules/clustering.html#mini-batch-k-means">just a bit worse than K-means</a>.</p>
<p>In the code snippet below, you can see the function you’ll use to create the clusters using Mini-batches K-means. Create a new cell in your notebook, and copy the following code there:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="kw">def</span> mbkmeans_clusters(</span>
<span id="cb13-2"><a href="#cb13-2"></a>    X,</span>
<span id="cb13-3"><a href="#cb13-3"></a>    k,</span>
<span id="cb13-4"><a href="#cb13-4"></a>    mb,</span>
<span id="cb13-5"><a href="#cb13-5"></a>    print_silhouette_values,</span>
<span id="cb13-6"><a href="#cb13-6"></a>):</span>
<span id="cb13-7"><a href="#cb13-7"></a>    <span class="co">"""Generate clusters and print Silhouette metrics using MBKmeans</span></span>
<span id="cb13-8"><a href="#cb13-8"></a></span>
<span id="cb13-9"><a href="#cb13-9"></a><span class="co">    Args:</span></span>
<span id="cb13-10"><a href="#cb13-10"></a><span class="co">        X: Matrix of features.</span></span>
<span id="cb13-11"><a href="#cb13-11"></a><span class="co">        k: Number of clusters.</span></span>
<span id="cb13-12"><a href="#cb13-12"></a><span class="co">        mb: Size of mini-batches.</span></span>
<span id="cb13-13"><a href="#cb13-13"></a><span class="co">        print_silhouette_values: Print silhouette values per cluster.</span></span>
<span id="cb13-14"><a href="#cb13-14"></a></span>
<span id="cb13-15"><a href="#cb13-15"></a><span class="co">    Returns:</span></span>
<span id="cb13-16"><a href="#cb13-16"></a><span class="co">        Trained clustering model and labels based on X.</span></span>
<span id="cb13-17"><a href="#cb13-17"></a><span class="co">    """</span></span>
<span id="cb13-18"><a href="#cb13-18"></a>    km <span class="op">=</span> MiniBatchKMeans(n_clusters<span class="op">=</span>k, batch_size<span class="op">=</span>mb).fit(X)</span>
<span id="cb13-19"><a href="#cb13-19"></a>    <span class="bu">print</span>(<span class="ss">f"For n_clusters = </span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-20"><a href="#cb13-20"></a>    <span class="bu">print</span>(<span class="ss">f"Silhouette coefficient: </span><span class="sc">{</span>silhouette_score(X, km.labels_)<span class="sc">:0.2f}</span><span class="ss">"</span>)</span>
<span id="cb13-21"><a href="#cb13-21"></a>    <span class="bu">print</span>(<span class="ss">f"Inertia:</span><span class="sc">{</span>km<span class="sc">.</span>inertia_<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-22"><a href="#cb13-22"></a></span>
<span id="cb13-23"><a href="#cb13-23"></a>    <span class="cf">if</span> print_silhouette_values:</span>
<span id="cb13-24"><a href="#cb13-24"></a>        sample_silhouette_values <span class="op">=</span> silhouette_samples(X, km.labels_)</span>
<span id="cb13-25"><a href="#cb13-25"></a>        <span class="bu">print</span>(<span class="ss">f"Silhouette values:"</span>)</span>
<span id="cb13-26"><a href="#cb13-26"></a>        silhouette_values <span class="op">=</span> []</span>
<span id="cb13-27"><a href="#cb13-27"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb13-28"><a href="#cb13-28"></a>            cluster_silhouette_values <span class="op">=</span> sample_silhouette_values[km.labels_ <span class="op">==</span> i]</span>
<span id="cb13-29"><a href="#cb13-29"></a>            silhouette_values.append(</span>
<span id="cb13-30"><a href="#cb13-30"></a>                (</span>
<span id="cb13-31"><a href="#cb13-31"></a>                    i,</span>
<span id="cb13-32"><a href="#cb13-32"></a>                    cluster_silhouette_values.shape[<span class="dv">0</span>],</span>
<span id="cb13-33"><a href="#cb13-33"></a>                    cluster_silhouette_values.mean(),</span>
<span id="cb13-34"><a href="#cb13-34"></a>                    cluster_silhouette_values.<span class="bu">min</span>(),</span>
<span id="cb13-35"><a href="#cb13-35"></a>                    cluster_silhouette_values.<span class="bu">max</span>(),</span>
<span id="cb13-36"><a href="#cb13-36"></a>                )</span>
<span id="cb13-37"><a href="#cb13-37"></a>            )</span>
<span id="cb13-38"><a href="#cb13-38"></a>        silhouette_values <span class="op">=</span> <span class="bu">sorted</span>(</span>
<span id="cb13-39"><a href="#cb13-39"></a>            silhouette_values, key<span class="op">=</span><span class="kw">lambda</span> tup: tup[<span class="dv">2</span>], reverse<span class="op">=</span><span class="va">True</span></span>
<span id="cb13-40"><a href="#cb13-40"></a>        )</span>
<span id="cb13-41"><a href="#cb13-41"></a>        <span class="cf">for</span> s <span class="kw">in</span> silhouette_values:</span>
<span id="cb13-42"><a href="#cb13-42"></a>            <span class="bu">print</span>(</span>
<span id="cb13-43"><a href="#cb13-43"></a>                <span class="ss">f"    Cluster </span><span class="sc">{</span>s[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">: Size:</span><span class="sc">{</span>s[<span class="dv">1</span>]<span class="sc">}</span><span class="ss"> | Avg:</span><span class="sc">{</span>s[<span class="dv">2</span>]<span class="sc">:.2f}</span><span class="ss"> | Min:</span><span class="sc">{</span>s[<span class="dv">3</span>]<span class="sc">:.2f}</span><span class="ss"> | Max: </span><span class="sc">{</span>s[<span class="dv">4</span>]<span class="sc">:.2f}</span><span class="ss">"</span></span>
<span id="cb13-44"><a href="#cb13-44"></a>            )</span>
<span id="cb13-45"><a href="#cb13-45"></a>    <span class="cf">return</span> km, km.labels_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This function creates the clusters using the Mini-batches K-means algorithm. It takes the following arguments:</p>
<ul>
<li><code>**X**</code>: Matrix of features. In this case, it’s your vectorized documents.</li>
<li><code>**k**</code>:Number of clusters you’d like to create.</li>
<li><code>**mb**</code>: Size of mini-batches.</li>
<li><code>**print_silhouette_values**</code>: Defines if the Silhouette Coefficient is printed for each cluster. If you haven’t heard about this coefficient, don’t worry, you’ll learn about it in a bit!</li>
</ul>
<p><code>mbkmeans_cluster</code> takes these arguments and returns the fitted clustering model and the labels for each document.</p>
<p>Run the cell where you copied the function. Next, you’ll apply this function to your vectorized documents.</p>
<section id="definition-of-clusters" class="level4">
<h4 class="anchored" data-anchor-id="definition-of-clusters">Definition of Clusters</h4>
<p>Now, you need to execute <code>mbkmean_clusters</code> providing it with the vectorized documents and the number of clusters. You’ll print the Silhouette Coefficients per cluster to review the quality of your clusters.</p>
<p>Create a new cell and copy this code there:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a>clustering, cluster_labels <span class="op">=</span> mbkmeans_clusters(</span>
<span id="cb14-2"><a href="#cb14-2"></a>    X<span class="op">=</span>vectorized_docs,</span>
<span id="cb14-3"><a href="#cb14-3"></a>    k<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb14-4"><a href="#cb14-4"></a>    mb<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb14-5"><a href="#cb14-5"></a>    print_silhouette_values<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb14-6"><a href="#cb14-6"></a>)</span>
<span id="cb14-7"><a href="#cb14-7"></a>df_clusters <span class="op">=</span> pd.DataFrame({</span>
<span id="cb14-8"><a href="#cb14-8"></a>    <span class="st">"text"</span>: docs,</span>
<span id="cb14-9"><a href="#cb14-9"></a>    <span class="st">"tokens"</span>: [<span class="st">" "</span>.join(text) <span class="cf">for</span> text <span class="kw">in</span> tokenized_docs],</span>
<span id="cb14-10"><a href="#cb14-10"></a>    <span class="st">"cluster"</span>: cluster_labels</span>
<span id="cb14-11"><a href="#cb14-11"></a>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This code will fit the clustering model, print the Silhouette Coefficient per cluster, and return the fitted model and the labels per cluster. It’ll also create a data frame you can use to review the results.</p>
<p>There are a few things to consider when setting the input arguments:</p>
<ul>
<li><code>print_silhouette_values</code> is straightforward. In this case, you set it to <code>True</code> to print the evaluation metric per cluster. This will help you review the results.</li>
<li><code>mb</code> depends on the size of your dataset. You need to ensure that it is not too small to avoid a significant impact on the quality of results and not too big to avoid making the execution too slow. In this case, you set it to 500 observations.</li>
<li><code>k</code> is trickier. In general, it involves a mix of qualitative analysis and quantitative metrics. After a few experiments on my side, I found that 50 seemed to work well. But that is more or less arbitrary.</li>
</ul>
<p>You could use metrics like the Silhouette Coefficient for the quantitative evaluation of the number of clusters. This coefficient is an evaluation metric frequently used in problems where ground truth labels are unknown. It’s calculated using the mean intra-cluster distance and the mean nearest-cluster distance and goes from -1 to 1. Well-defined clusters result in positive values of this coefficient, while incorrect clusters will result in negative values. If you’d like to learn more about it, look at <a href="https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient">scikit-learn’s documentation</a>.</p>
<p>The qualitative part generally requires you to have domain knowledge of the subject matter so you can sense-check your clustering algorithm’s results. In the next section, I’ll show you two approaches you can use to check your results qualitatively.</p>
<p>After executing the cell you just created, the output should look like this:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1"></a><span class="ex">For</span> n_clusters = 50</span>
<span id="cb15-2"><a href="#cb15-2"></a><span class="ex">Silhouette</span> coefficient: 0.11</span>
<span id="cb15-3"><a href="#cb15-3"></a><span class="ex">Inertia:3568.342791047967</span></span>
<span id="cb15-4"><a href="#cb15-4"></a><span class="ex">Silhouette</span> values:</span>
<span id="cb15-5"><a href="#cb15-5"></a>    <span class="ex">Cluster</span> 29: Size:50 <span class="kw">|</span> <span class="ex">Avg:0.39</span> <span class="kw">|</span> <span class="ex">Min:0.01</span> <span class="kw">|</span> <span class="ex">Max:</span> 0.59</span>
<span id="cb15-6"><a href="#cb15-6"></a>    <span class="ex">Cluster</span> 35: Size:30 <span class="kw">|</span> <span class="ex">Avg:0.34</span> <span class="kw">|</span> <span class="ex">Min:0.05</span> <span class="kw">|</span> <span class="ex">Max:</span> 0.54</span>
<span id="cb15-7"><a href="#cb15-7"></a>    <span class="ex">Cluster</span> 37: Size:58 <span class="kw">|</span> <span class="ex">Avg:0.32</span> <span class="kw">|</span> <span class="ex">Min:0.09</span> <span class="kw">|</span> <span class="ex">Max:</span> 0.51</span>
<span id="cb15-8"><a href="#cb15-8"></a>    <span class="ex">Cluster</span> 39: Size:81 <span class="kw">|</span> <span class="ex">Avg:0.31</span> <span class="kw">|</span> <span class="ex">Min:-0.05</span> <span class="kw">|</span> <span class="ex">Max:</span> 0.52</span>
<span id="cb15-9"><a href="#cb15-9"></a>    <span class="ex">Cluster</span> 27: Size:63 <span class="kw">|</span> <span class="ex">Avg:0.28</span> <span class="kw">|</span> <span class="ex">Min:0.02</span> <span class="kw">|</span> <span class="ex">Max:</span> 0.46</span>
<span id="cb15-10"><a href="#cb15-10"></a>    <span class="ex">Cluster</span> 6: Size:101 <span class="kw">|</span> <span class="ex">Avg:0.27</span> <span class="kw">|</span> <span class="ex">Min:0.02</span> <span class="kw">|</span> <span class="ex">Max:</span> 0.46</span>
<span id="cb15-11"><a href="#cb15-11"></a>    <span class="ex">Cluster</span> 24: Size:120 <span class="kw">|</span> <span class="ex">Avg:0.26</span> <span class="kw">|</span> <span class="ex">Min:-0.04</span> <span class="kw">|</span> <span class="ex">Max:</span> 0.46</span>
<span id="cb15-12"><a href="#cb15-12"></a>    <span class="ex">Cluster</span> 49: Size:65 <span class="kw">|</span> <span class="ex">Avg:0.26</span> <span class="kw">|</span> <span class="ex">Min:-0.03</span> <span class="kw">|</span> <span class="ex">Max:</span> 0.47</span>
<span id="cb15-13"><a href="#cb15-13"></a>    <span class="ex">Cluster</span> 47: Size:53 <span class="kw">|</span> <span class="ex">Avg:0.23</span> <span class="kw">|</span> <span class="ex">Min:0.01</span> <span class="kw">|</span> <span class="ex">Max:</span> 0.45</span>
<span id="cb15-14"><a href="#cb15-14"></a>    <span class="ex">Cluster</span> 22: Size:78 <span class="kw">|</span> <span class="ex">Avg:0.22</span> <span class="kw">|</span> <span class="ex">Min:-0.01</span> <span class="kw">|</span> <span class="ex">Max:</span> 0.43</span>
<span id="cb15-15"><a href="#cb15-15"></a>    <span class="ex">Cluster</span> 45: Size:38 <span class="kw">|</span> <span class="ex">Avg:0.21</span> <span class="kw">|</span> <span class="ex">Min:-0.07</span> <span class="kw">|</span> <span class="ex">Max:</span> 0.41</span>
<span id="cb15-16"><a href="#cb15-16"></a><span class="ex">...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is the output of your clustering algorithm. The sizes and Silhouette Coefficients per cluster are the most relevant metrics. The clusters are printed by the value of the Silhouette coefficient in descending order. A higher score means denser – and thus better – clusters. In this case, you can see that clusters 29, 35, and 37 seem to be the top ones.</p>
<p>Next, you’ll learn how to check what’s in each cluster.</p>
</section>
<section id="qualitative-review-of-clusters" class="level4">
<h4 class="anchored" data-anchor-id="qualitative-review-of-clusters">Qualitative Review of Clusters</h4>
<p>There are a few ways you can qualitatively analyze the results. During the earlier sections, our approach resulted in vector representations of tokens and documents, and vectors of the clusters’ centroids. You can find the most representative tokens and documents to analyze the results by looking for the vectors closest to the clusters’ centroids.</p>
<p>Here’s how you obtain the most representative tokens per cluster:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="bu">print</span>(<span class="st">"Most representative terms per cluster (based on centroids):"</span>)</span>
<span id="cb16-2"><a href="#cb16-2"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">50</span>):</span>
<span id="cb16-3"><a href="#cb16-3"></a>    tokens_per_cluster <span class="op">=</span> <span class="st">""</span></span>
<span id="cb16-4"><a href="#cb16-4"></a>    most_representative <span class="op">=</span> model.wv.most_similar(positive<span class="op">=</span>[clustering.cluster_centers_[i]], topn<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb16-5"><a href="#cb16-5"></a>    <span class="cf">for</span> t <span class="kw">in</span> most_representative:</span>
<span id="cb16-6"><a href="#cb16-6"></a>        tokens_per_cluster <span class="op">+=</span> <span class="ss">f"</span><span class="sc">{</span>t[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> "</span></span>
<span id="cb16-7"><a href="#cb16-7"></a>    <span class="bu">print</span>(<span class="ss">f"Cluster </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>tokens_per_cluster<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For the top clusters we identified earlier – 29, 35, and 37 – these are the results:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb17-1"><a href="#cb17-1"></a><span class="ex">Cluster</span> 29: noaa sharpie claim assertions forecasters</span>
<span id="cb17-2"><a href="#cb17-2"></a><span class="ex">Cluster</span> 35: eye lilinow path halts projected</span>
<span id="cb17-3"><a href="#cb17-3"></a><span class="ex">Cluster</span> 37: cnnpolitics complaint clinton pences whistleblower</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Next, we can do the same analysis with documents instead of tokens. This is how you find the most representative documents for cluster 29:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a>test_cluster <span class="op">=</span> <span class="dv">29</span></span>
<span id="cb18-2"><a href="#cb18-2"></a>most_representative_docs <span class="op">=</span> np.argsort(</span>
<span id="cb18-3"><a href="#cb18-3"></a>    np.linalg.norm(vectorized_docs <span class="op">-</span> clustering.cluster_centers_[test_cluster], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-4"><a href="#cb18-4"></a>)</span>
<span id="cb18-5"><a href="#cb18-5"></a><span class="cf">for</span> d <span class="kw">in</span> most_representative_docs[:<span class="dv">3</span>]:</span>
<span id="cb18-6"><a href="#cb18-6"></a>    <span class="bu">print</span>(docs[d])</span>
<span id="cb18-7"><a href="#cb18-7"></a>    <span class="bu">print</span>(<span class="st">"-------------"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And these are the 3 most representative documents in that cluster:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb19-1"><a href="#cb19-1"></a><span class="ex">Dorian,</span> Comey and Debra Messing: What Trump tweeted on Labor Day weekend <span class="kw">|</span> <span class="ex">President</span> Donald Trump axed his visit to Poland over the weekend to monitor Hurricane Dorian from Camp David with emergency management staff, but if the President<span class="st">'s more than 120 tweets are any indication, he had more than just the storm on his mind. | Washington (CNN)President Donald Trump axed his visit to Poland over the weekend to monitor Hurricane Dorian from Camp David with emergency management staff, but if the President'</span>s more than 120 tweets are any indication, he had more than just the storm on hi… [+3027 chars]</span>
<span id="cb19-2"><a href="#cb19-2"></a><span class="ex">-------------</span></span>
<span id="cb19-3"><a href="#cb19-3"></a><span class="ex">Ross</span> Must Resign If Report He Threatened NOAA Officials Is True: Democrat <span class="kw">|</span> <span class="ex">As</span> President Donald Trump claimed Hurricane Dorian could hit Alabama, the National Weather Service tweeted to correct the rumors. <span class="kw">|</span> <span class="ex">Commerce</span> Secretary Wilbur Ross is facing calls to resign over a report alleging that he threatened to fire top officials at NOAA for a tweet disputing President Donald Trump<span class="st">'s claim that Hurricane Dorian would hit Alabama.</span></span>
<span id="cb19-4"><a href="#cb19-4"></a><span class="st">"If that story is true, and I don'</span>t… [+3828 chars]</span>
<span id="cb19-5"><a href="#cb19-5"></a><span class="ex">-------------</span></span>
<span id="cb19-6"><a href="#cb19-6"></a><span class="ex">Federal</span> weather workers are furious at the NOAA<span class="st">'s '</span>utterly disgusting<span class="st">' statement defending Trump'</span>s claim Hurricane Dorian would hit Alabama <span class="kw">|</span> <span class="ex">Federal</span> weather workers have reacted furiously to the National Oceanic and Atmospheric Administration<span class="st">'s (NOAA) defence of US President Donald Trump'</span>s repeated assertions that Hurricane Dorian was set to hit Alabama. <span class="st">"Never ever before has their management thr… | Federal weather workers have reacted furiously to the National Oceanic and Atmospheric Administration's (NOAA) defence of US President Donald Trump's repeated assertions that Hurricane Dorian was set to hit Alabama, saying they have been "</span>thrown under the bus… [+3510 chars]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Most of the results seem to be related to a dispute between Donald Trump and the National Oceanic and Atmospheric Agency (NOAA). It was a famous controversy that people referred to as <a href="https://en.wikipedia.org/wiki/Hurricane_Dorian%E2%80%93Alabama_controversy">Sharpiegate</a>.</p>
<p>You could also explore other approaches like generating word frequencies per cluster or reviewing random samples of documents per cluster.</p>
</section>
</section>
</section>
<section id="other-approaches" class="level2">
<h2 class="anchored" data-anchor-id="other-approaches">Other Approaches</h2>
<p>There are other approaches you could take to cluster text data like:</p>
<ul>
<li>Use a <a href="https://keras.io/examples/nlp/pretrained_word_embeddings/">pre-trained word embedding</a> instead of training your own. In this tutorial, you trained a Word2Vec model from scratch, but it’s very common to use a pre-trained model.</li>
<li>Generating <a href="https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html?highlight=clustering%20bag%20words">feature vectors using a bag-of-words</a> approach instead of word embeddings.</li>
<li><a href="https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html?highlight=clustering%20bag%20words">Reducing dimensionality</a> of feature vectors. This is very useful if you use a bag-of-words approach.</li>
<li>Clustering documents using other algorithms like HDBSCAN or Hierarchical Clustering.</li>
<li>Using <a href="https://github.com/UKPLab/sentence-transformers">BERT sentence embeddings</a> to generate the feature vectors. Or generating the topics with <a href="https://github.com/MaartenGr/BERTopic">BERTopic</a>.</li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Way to go! You just learned how to cluster documents using Word2Vec. You went through an end-to-end project, where you learned all the steps required for clustering a corpus of text.</p>
<p>You learned how to:</p>
<ul>
<li><strong>Preprocess</strong> data to use with a Word2Vec model</li>
<li><strong>Train</strong> a Word2Vec model</li>
<li>Use quantitative metrics, such as the <strong>Silhouette score</strong> to evaluate the quality of your clusters.</li>
<li>Find the most <strong>representative tokens</strong> and <strong>documents</strong> in your clusters</li>
</ul>
<p>I hope you find this tutorial useful. Shoot me a message if you have any questions!</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{castillo2021,
  author = {Castillo, Dylan},
  title = {How to {Cluster} {Documents} {Using} {Word2Vec} and
    {K-means}},
  date = {2021-01-18},
  url = {https://dylancastillo.co/posts/nlp-snippets-cluster-documents-using-word2vec.html},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-castillo2021" class="csl-entry quarto-appendix-citeas" role="listitem">
Castillo, Dylan. 2021. <span>“How to Cluster Documents Using Word2Vec
and K-Means.”</span> January 18, 2021. <a href="https://dylancastillo.co/posts/nlp-snippets-cluster-documents-using-word2vec.html">https://dylancastillo.co/posts/nlp-snippets-cluster-documents-using-word2vec.html</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/dylancastillo\.co");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="dylanjcastillo/blog_comments" issue-term="pathname" theme="dark-blue" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2024, Dylan Castillo</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>