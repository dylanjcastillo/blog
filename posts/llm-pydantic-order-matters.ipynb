{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Structured outputs: don't put the cart before the horse\"\n",
    "date: \"11/09/2024\"\n",
    "date-modified: last-modified\n",
    "description-meta: \"How the order of fields in your response model can impact the quality of the responses you get from an LLM.\"\n",
    "toc: true\n",
    "toc-depth: 3\n",
    "lightbox: true\n",
    "fig-cap-location: margin\n",
    "categories:\n",
    "  - llm\n",
    "  - openai\n",
    "  - pydantic\n",
    "  - python\n",
    "author:\n",
    "  - name: Dylan Castillo\n",
    "    url: https://dylancastillo.co\n",
    "    affiliation: Iwana Labs\n",
    "    affiliation-url: https://iwanalabs.com\n",
    "citation: true\n",
    "comments:\n",
    "  utterances:\n",
    "    repo: dylanjcastillo/blog_comments\n",
    "    theme: dark-blue\n",
    "    issue-term: pathname\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not long ago, we couldn't reliably ask LLMs to provide a response using a specific format. Building tools that used LLM outputs was painful. \n",
    "\n",
    "Eventually, first through [function calling](https://platform.openai.com/docs/guides/function-calling) and then through [structured outputs](https://platform.openai.com/docs/guides/structured-outputs), we could instruct LLMs to respond in specific formats^[I'm referring to OpenAI models here. Open weight models allowed this using [grammars](https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md).]. So, extracting information from LLM outputs in a reliable way stopped being a problem.\n",
    "\n",
    "But then I started noticing that structured outputs were not always the [silver bullet](https://arxiv.org/abs/2408.02442) people think they are. Defining response formats adds a sort of safety net, and people often forget that underneath, they're still dealing with an LLM. Setting up a Pydantic model for your API calls is not the same as setting up a Pydantic model for your LLM outputs.\n",
    "\n",
    "For example, take the following question from the [LiveBench](https://huggingface.co/datasets/livebench/reasoning) dataset:\n",
    "\n",
    "> Suppose I have a physical, solid, equilateral triangle, and I make two cuts. The two cuts are from two parallel lines, and both cuts pass through the interior of the triangle. How many pieces are there after the cuts? Think step by step, and then put your answer in **bold** as a single integer (for example, **0**). If you don't know, guess.\n",
    "\n",
    "Let's say I write a simple system prompt and two Pydantic models to format the responses:\n",
    "\n",
    "```python\n",
    "system_prompt = (\n",
    "    \"You're a helpful assistant. You will help me answer a question.\"\n",
    "    \"\\nYou will use this JSON schema for your response:\"\n",
    "    \"\\n{response_format}\"\n",
    ")\n",
    "\n",
    "class ResponseFormatA(BaseModel):\n",
    "    reasoning: str\n",
    "    answer: str\n",
    "\n",
    "class ResponseFormatB(BaseModel):\n",
    "    answer: str\n",
    "    reasoning: str\n",
    "```\n",
    "\n",
    "Do you think that there will be a difference in performance between `ResponseFormatA` and `ResponseFormatB`? If so, which one do you think will perform better?\n",
    "\n",
    "Not sure? Well, you're in luck! Let's run some experiments to find out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from asyncio import Semaphore\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from openai import AsyncOpenAI\n",
    "from pydantic import BaseModel\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = wrap_openai(AsyncOpenAI())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will set up all the necessary infrastructure to run the experiments. I like using [LangSmith](https://www.langchain.com/langsmith) to track [runs](https://smith.langchain.com/public/57df6d66-505d-4bae-a54b-91d6954e5344/runs).\n",
    "\n",
    "To run the experiment, you need some data. I ended up using a subset of the [reasoning questions](https://huggingface.co/datasets/livebench/reasoning) from LiveBench. You can download it and save it in the `data` directory.\n",
    "\n",
    "Then, you can read it into a pandas `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path().absolute().parent / \"data\" / \"live_bench\"\n",
    "reasoning_dir = data_dir / \"reasoning\"\n",
    "live_bench_json = reasoning_dir / \"question.jsonl\"\n",
    "\n",
    "df = (\n",
    "    pd.read_json(live_bench_json, lines=True)\n",
    "    .query(\"livebench_release_date == '2024-07-26'\")\n",
    "    .assign(\n",
    "        turns_str=lambda x: x.turns.str[0], \n",
    "        expects_integer=lambda x: x.turns.str[0].str.contains(\"integer\", case=False)\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"data_point_id\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the system prompt and the Pydantic models you'll use to format the responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_template = (\n",
    "    \"You're a helpful assistant. You will help me answer a question.\"\n",
    "    \"\\nYou will use this JSON schema for your response:\"\n",
    "    \"\\n{response_format}\"\n",
    ")\n",
    "\n",
    "class ResponseFormatA(BaseModel):\n",
    "    reasoning: str\n",
    "    answer: str \n",
    "\n",
    "class ResponseFormatB(BaseModel):\n",
    "    answer: str \n",
    "    reasoning: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the system prompt you send to the LLM, you'll replace `{response_format}` with the JSON schema of the response format you want to use.\n",
    "\n",
    "Then, let's define a few helper functions to run the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_response(response_json, response_format):\n",
    "    response_dict = json.loads(response_json)\n",
    "    expected_keys = list(response_format.model_json_schema()[\"properties\"].keys())\n",
    "    actual_keys = list(response_dict.keys())\n",
    "    if actual_keys != expected_keys:\n",
    "        raise ValueError(f\"Response keys {actual_keys} do not match expected keys {expected_keys}\")\n",
    "    return response_format.model_validate_json(response_json)\n",
    "\n",
    "@traceable\n",
    "async def process_row(\n",
    "    row: pd.Series, \n",
    "    response_format: ResponseFormatA | ResponseFormatB, \n",
    "    semaphore: Semaphore\n",
    ") -> ResponseFormatA | ResponseFormatB:\n",
    "    system_prompt = system_prompt_template.format(\n",
    "        response_format=response_format.model_json_schema()\n",
    "    )\n",
    "    async with semaphore:\n",
    "        for _ in range(3):\n",
    "            try:\n",
    "                response = await client.chat.completions.create(\n",
    "                    model=\"gpt-4o\", \n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": f\"Question:\\n{row.turns_str}\"}\n",
    "                    ],\n",
    "                    response_format={\"type\": \"json_object\"}\n",
    "                )\n",
    "                response_json = response.choices[0].message.content\n",
    "                return validate_response(response_json, response_format)\n",
    "            except Exception:\n",
    "                pass\n",
    "        raise Exception(\"Failed to generate a valid response\")\n",
    "\n",
    "@traceable\n",
    "async def main(df, response_format, concurrency: int = 30):\n",
    "    semaphore = Semaphore(concurrency)\n",
    "    tasks = [process_row(row, response_format, semaphore) for _, row in df.iterrows()]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "\n",
    "    return responses\n",
    "\n",
    "def extract_answer(answer):\n",
    "    return str(answer).strip(\"**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, `validate_response` is used to check if the response is valid (i.e. it matches the JSON schema in the same order). If it is, it returns the response. Otherwise, it raises an exception.\n",
    "\n",
    "`extract_answer` is used to remove ** from the answer if it exists in the response. Some of the questions in the LiveBench dataset included instructions to put the answer in bold, which is why we need to remove it.\n",
    "\n",
    "`process_row` is used to process a single row of the DataFrame. It sends the system prompt to the LLM and validates the response. It includes a simple retry mechanism in case the validation fails. Each run is tracked in LangSmith.\n",
    "\n",
    "Finally, `main` is used to run the experiment. It runs the `process_row` function concurrently for each row in the DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the experiment\n",
    "\n",
    "Now, you can run the experiment using the two response formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/3\n",
      "Run 2/3\n",
      "Run 3/3\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "n_runs = 3\n",
    "df_runs = []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"Run {run + 1}/{n_runs}\")\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    responses_A = asyncio.run(main(df_copy, ResponseFormatA))\n",
    "    df_copy[\"raw_answer_A\"] = [r.answer for r in responses_A]\n",
    "    df_copy[\"response_A\"] = df_copy[\"raw_answer_A\"].apply(extract_answer)\n",
    "    df_copy[\"is_correct_A\"] = (df_copy[\"response_A\"] == df_copy[\"ground_truth\"]).astype(int)\n",
    "    \n",
    "    responses_B = asyncio.run(main(df_copy, ResponseFormatB))\n",
    "    df_copy[\"raw_answer_B\"] = [r.answer for r in responses_B]\n",
    "    df_copy[\"response_B\"] = df_copy[\"raw_answer_B\"].apply(extract_answer)\n",
    "    df_copy[\"is_correct_B\"] = (df_copy[\"response_B\"] == df_copy[\"ground_truth\"]).astype(int)\n",
    "    \n",
    "    df_copy[\"run\"] = run\n",
    "    df_run = df_copy[[\"data_point_id\", \"ground_truth\", \"is_correct_A\", \"is_correct_B\", \"run\"]]\n",
    "    \n",
    "    df_runs.append(df_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the experiment multiple times with the same inputs to account for the randomness in the LLM's responses. Ideally, we should run it more than three times, but I'm poor. So, we'll just do it 3 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response format A - Mean: 46.67% CI: 34.67% - 58.67%\n",
      "Response format B - Mean: 30.67% CI: 20.67% - 41.33%\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "df_all_runs = pd.concat(df_runs, ignore_index=True)\n",
    "\n",
    "n_bootstraps = 10000\n",
    "bootstrap_accuracies_A = []\n",
    "bootstrap_accuracies_B = []\n",
    "\n",
    "data_point_ids = df_all_runs['data_point_id'].unique()\n",
    "n_data_points = len(data_point_ids)\n",
    "\n",
    "grouped_A = df_all_runs.groupby('data_point_id')['is_correct_A']\n",
    "grouped_B = df_all_runs.groupby('data_point_id')['is_correct_B']\n",
    "\n",
    "df_correct_counts_A = grouped_A.sum()\n",
    "df_total_counts_A = grouped_A.count()\n",
    "df_correct_counts_B = grouped_B.sum()\n",
    "df_total_counts_B = grouped_B.count()\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    sampled_ids = np.random.choice(data_point_ids, size=n_data_points, replace=True)\n",
    "    sampled_counts = pd.Series(sampled_ids).value_counts()\n",
    "    counts_index = sampled_counts.index\n",
    "    \n",
    "    total_correct_counts_A = (df_correct_counts_A.loc[counts_index] * sampled_counts).sum()\n",
    "    total_observations_A = (df_total_counts_A.loc[counts_index] * sampled_counts).sum()\n",
    "    mean_accuracy_A = total_correct_counts_A / total_observations_A\n",
    "    bootstrap_accuracies_A.append(mean_accuracy_A)\n",
    "    \n",
    "    total_correct_counts_B = (df_correct_counts_B.loc[counts_index] * sampled_counts).sum()\n",
    "    total_observations_B = (df_total_counts_B.loc[counts_index] * sampled_counts).sum()\n",
    "    mean_accuracy_B = total_correct_counts_B / total_observations_B\n",
    "    bootstrap_accuracies_B.append(mean_accuracy_B)\n",
    "\n",
    "ci_A = np.percentile(bootstrap_accuracies_A, [2.5, 97.5])\n",
    "ci_B = np.percentile(bootstrap_accuracies_B, [2.5, 97.5])\n",
    "\n",
    "mean_accuracy_A = df_all_runs['is_correct_A'].mean()\n",
    "mean_accuracy_B = df_all_runs['is_correct_B'].mean()\n",
    "\n",
    "print(\n",
    "    f\"Response format A - Mean: {mean_accuracy_A * 100:.2f}% CI: {ci_A[0] * 100:.2f}% - {ci_A[1] * 100:.2f}%\"\n",
    ")\n",
    "print(\n",
    "    f\"Response format B - Mean: {mean_accuracy_B * 100:.2f}% CI: {ci_B[0] * 100:.2f}% - {ci_B[1] * 100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can build bootstrap confidence intervals for the accuracies of the two response formats. Given that I'm asking the LLM the same question multiple times, I went with an approach called [cluster bootstrapping](https://pmc.ncbi.nlm.nih.gov/articles/PMC5965657/), which accounts for the fact that the data points are not independent.\n",
    "\n",
    "It should take a few minutes to run. Once it's done, you should see output like the following:\n",
    "\n",
    "| Response Format | Mean (95% CI)           |\n",
    "|-----------------|-------------------------|\n",
    "| A        | 46.67% (34.67% – 58.67%) |\n",
    "| B        | 30.67% (20.67% – 41.33%) |\n",
    "\n",
    "These results suggest that the order of the fields in the JSON schema does matter.\n",
    "\n",
    "But if you're still unsure, you can perform a t-test to see if the two response formats are statistically different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic: 3.2796489996607274, p-value: 0.0009586764318774877\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "accuracies_A = df_all_runs.pivot(index='data_point_id', columns='run', values='is_correct_A')\n",
    "accuracies_B = df_all_runs.pivot(index='data_point_id', columns='run', values='is_correct_B')\n",
    "\n",
    "mean_accuracies_A = accuracies_A.mean(axis=1)\n",
    "mean_accuracies_B = accuracies_B.mean(axis=1)\n",
    "\n",
    "t_stat, p_value = stats.ttest_rel(mean_accuracies_A, mean_accuracies_B, alternative='greater')\n",
    "\n",
    "print(f\"t-statistic: {t_stat}, p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got a p-value <0.001, meaning I can reject the null hypothesis that the two response formats are the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Based on the results of the experiment, we can safely say that `ResponseFormatA` is better than `ResponseFormatB`.\n",
    "\n",
    "But why?\n",
    "\n",
    "In this case, it's simple. \n",
    "\n",
    "These response formats are meant to help the LLM reason step by step to arrive at the answer. This is known as [chain of thought reasoning](https://en.wikipedia.org/wiki/Chain_of_thought_reasoning). However, for it to work, we need the LLM to first provide us with the reasoning of how it arrived at the answer and then the answer.\n",
    "\n",
    "In `ResponseFormatB`, we defined our Pydantic model with the answer first and the reasoning second. This means that the LLM will give us the answer first, and then adjust the reasoning to match that answer. `ResponseFormatA` does the opposite, which is why it performs better.\n",
    "\n",
    "So, to summarize, when using structured outputs, don't put the cart before the horse.\n",
    "\n",
    "That's all! Let me know if you have any questions in the comments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
