<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Dylan Castillo">
<meta name="dcterms.date" content="2023-03-03">
<meta name="description" content="In this tutorial, you’ll learn how to use FastAPI, Qdrant, Sentence Transformers, and ChatGPT to create an AI search engine.">

<title>Build an AI Search Engine Using FastAPI, Qdrant, and ChatGPT – Dylan Castillo</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../images/logo.webp" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-d166b450ba5a8e9f7a0ab969bf6592c1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-a4c2714d643bba16a42772a6f5e54e5d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="dark">
<script src="../site_libs/quarto-contrib/iconify-2.1.0/iconify-icon.min.js"></script>
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="https://fonts.googleapis.com/css2?family=Merriweather:ital,wght@0,300;0,400;0,700;0,900;1,300;1,400;1,700;1,900&amp;display=swap" rel="stylesheet">
<script src="https://cdn.usefathom.com/script.js" data-site="ZJFQREIA" defer=""></script>


<meta property="og:title" content="Build an AI Search Engine Using FastAPI, Qdrant, and ChatGPT – Dylan Castillo">
<meta property="og:description" content="">
<meta property="og:image" content="https://dylancastillo.co/posts/images/ai-search-engine-fastapi-qdrant-chatgpt/image-8.png">
<meta property="og:site_name" content="Dylan Castillo">
<meta property="og:image:height" content="331">
<meta property="og:image:width" content="688">
<meta name="twitter:title" content="Build an AI Search Engine Using FastAPI, Qdrant, and ChatGPT – Dylan Castillo">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="https://dylancastillo.co/posts/images/ai-search-engine-fastapi-qdrant-chatgpt/image-8.png">
<meta name="twitter:creator" content="@dylanjcastillo">
<meta name="twitter:site" content="@dylanjcastillo">
<meta name="twitter:image-height" content="331">
<meta name="twitter:image-width" content="688">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../projects.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/dylanjcastillo"> 
<span class="menu-text"><iconify-icon role="img" inline="" icon="fa6-brands:github" aria-label="Icon github from fa6-brands Iconify.design set." title="Icon github from fa6-brands Iconify.design set."></iconify-icon></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://www.linkedin.com/in/dylanjcastillo/"> 
<span class="menu-text"><iconify-icon role="img" inline="" icon="fa6-brands:linkedin" aria-label="Icon linkedin from fa6-brands Iconify.design set." title="Icon linkedin from fa6-brands Iconify.design set."></iconify-icon></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://bsky.app/profile/dylancastillo.co"> 
<span class="menu-text"><iconify-icon role="img" inline="" icon="fa6-brands:bluesky" aria-label="Icon bluesky from fa6-brands Iconify.design set." title="Icon bluesky from fa6-brands Iconify.design set."></iconify-icon></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#prerequisites" id="toc-prerequisites" class="nav-link active" data-scroll-target="#prerequisites">Prerequisites</a></li>
  <li><a href="#designing-a-tiny-search-engine-with-chatgpt" id="toc-designing-a-tiny-search-engine-with-chatgpt" class="nav-link" data-scroll-target="#designing-a-tiny-search-engine-with-chatgpt">Designing a (Tiny) Search Engine with ChatGPT</a></li>
  <li><a href="#set-up-your-local-environment" id="toc-set-up-your-local-environment" class="nav-link" data-scroll-target="#set-up-your-local-environment">Set Up Your Local Environment</a></li>
  <li><a href="#configure-qdrant-and-openai" id="toc-configure-qdrant-and-openai" class="nav-link" data-scroll-target="#configure-qdrant-and-openai">Configure Qdrant and OpenAI</a>
  <ul class="collapse">
  <li><a href="#qdrant" id="toc-qdrant" class="nav-link" data-scroll-target="#qdrant">Qdrant</a></li>
  <li><a href="#openai" id="toc-openai" class="nav-link" data-scroll-target="#openai">OpenAI</a></li>
  </ul></li>
  <li><a href="#extract-data" id="toc-extract-data" class="nav-link" data-scroll-target="#extract-data">Extract Data</a></li>
  <li><a href="#vectorize-and-index-data" id="toc-vectorize-and-index-data" class="nav-link" data-scroll-target="#vectorize-and-index-data">Vectorize and Index Data</a></li>
  <li><a href="#create-a-server-with-fastapi" id="toc-create-a-server-with-fastapi" class="nav-link" data-scroll-target="#create-a-server-with-fastapi">Create a Server with FastAPI</a></li>
  <li><a href="#deploy-your-app" id="toc-deploy-your-app" class="nav-link" data-scroll-target="#deploy-your-app">Deploy Your App</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    <div class="quarto-margin-footer"><div class="margin-footer-item">
<button onclick="window.location.href='https://subscribe.dylancastillo.co'" style="background-color: #eb841b; color: white; padding: 12px 24px; border: none; border-radius: 6px; font-size: 12px; font-weight: bold; cursor: pointer; transition: background-color 0.3s ease;">
Subscribe to my newsletter
</button>
</div></div></div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Build an AI Search Engine Using FastAPI, Qdrant, and ChatGPT</h1>
  <div class="quarto-categories">
    <div class="quarto-category">python</div>
    <div class="quarto-category">llm</div>
    <div class="quarto-category">openai</div>
  </div>
  </div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://dylancastillo.co">Dylan Castillo</a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://iwanalabs.com">
            Iwana Labs
            </a>
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 3, 2023</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">December 10, 2024</p>
    </div>
  </div>
    
  </div>
  


</header>


<p>With all the buzz surrounding <a href="https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/">Bing AI</a> and <a href="https://blog.google/technology/ai/bard-google-ai-search-updates/">Bard</a>, I was keen on building a (tiny) AI search engine myself. After a few days of tinkering, I released <a href="https://seneca.dylancastillo.co/">Ask Seneca</a>. It’s a small app that allows you to consult a GPT-based Seneca who answers your questions and cites his sources.</p>
<p>When a user asks a question, Ask Seneca searches for Seneca’s most relevant writings to answer that question and then summarizes those writings into a coherent answer. I built it using <a href="fastapi.tiangolo.com/">FastAPI</a>, <a href="https://qdrant.tech/">Qdrant</a>, <a href="https://www.sbert.net/">Sentence Transformers</a>, and <a href="https://openai.com/product">GPT-3</a>. I recently updated it to use the <a href="https://openai.com/blog/introducing-chatgpt-and-whisper-apis">ChatGPT API</a>.</p>
<p>Despite <a href="https://www.reuters.com/technology/google-ai-chatbot-bard-offers-inaccurate-information-company-ad-2023-02-08/">the</a> <a href="https://simonwillison.net/2023/Feb/15/bing/">setbacks</a> that Bing AI and Bard are facing, the potential for this technology is vast - you could build tools for quick and efficient searches through legal documents, internal knowledge bases, product manuals, and more.</p>
<p>In this tutorial, I’ll show you how to build your own AI search engine. You’ll create an app that lets users ask questions to a GPT-based Marcus Aurelius, and provides them with concise answers and references to his <a href="https://en.wikisource.org/wiki/Marcus_Aurelius_Antoninus_-_His_Meditations_concerning_himselfe">Meditations</a>.</p>
<p>Let’s get to it!</p>
<section id="prerequisites" class="level2">
<h2 class="anchored" data-anchor-id="prerequisites">Prerequisites</h2>
<p>To make the most out of this tutorial, you should know:</p>
<ol type="1">
<li>What <a href="https://blog.dataiku.com/semantic-search-an-overlooked-nlp-superpower">semantic search</a> is.</li>
<li>What <a href="https://www.pinecone.io/learn/vector-database/">vector databases</a> are.</li>
<li>What <a href="https://fastapi.tiangolo.com/">FastAPI</a> is and how to use it.</li>
</ol>
<p>You don’t have to be an expert in any of these areas, but familiarity with them will help you understand the sections that follow.</p>
</section>
<section id="designing-a-tiny-search-engine-with-chatgpt" class="level2">
<h2 class="anchored" data-anchor-id="designing-a-tiny-search-engine-with-chatgpt">Designing a (Tiny) Search Engine with ChatGPT</h2>
<p>Before you get started, you should understand the overall approach you’ll take to build your AI search engine. There are three parts to it:</p>
<ol type="1">
<li><strong>Extraction:</strong> This part consists of extracting the data that you want users to be able to search. In this case, that means parsing <a href="https://en.wikisource.org/wiki/Marcus_Aurelius_Antoninus_-_His_Meditations_concerning_himselfe">Meditations</a>. I won’t go into detail about this because it is very project-specific. The <a href="https://github.com/dylanjcastillo/ai-search-fastapi-qdrant-chatgpt/blob/main/data/processed/Marcus_Aurelius_Antoninus_-_His_Meditations_concerning_himselfe/Marcus_Aurelius_Antoninus_-_His_Meditations_concerning_himselfe.json">parsed data</a> is available in the repository.</li>
<li><strong>Indexing:</strong> This entails indexing the extracted data so that it can be accessed later when running searches. In this case, you’ll use a semantic search approach, which means you’ll search the data based on its meaning rather than keywords. That is, if you search for “How can I be happy?” you should get passages from Meditations that discuss happiness or feeling good, not just those that contain the exact words from the query.</li>
<li><strong>Search:</strong> This consists of a backend service that processes the user’s query, vectorizes it, finds vectors in the index that are the most similar to it, and then calls OpenAI’s API to generate a summarized answer for the user.</li>
</ol>
<p>Here’s a visual representation of how the parts of the application you’ll build in this tutorial fit together:</p>
<p><a href="images/ai-search-engine-fastapi-qdrant-chatgpt/image-8.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="images/ai-search-engine-fastapi-qdrant-chatgpt/image-8.png" class="img-fluid"></a></p>
<p>That’s all. Let’s continue!</p>
</section>
<section id="set-up-your-local-environment" class="level2">
<h2 class="anchored" data-anchor-id="set-up-your-local-environment">Set Up Your Local Environment</h2>
<p>Take the following steps to prepare your local environment:</p>
<ol type="1">
<li>Install <a href="https://www.python.org/downloads/">Python 3.10</a>.</li>
<li>Install <a href="https://python-poetry.org/docs/#installation">Poetry</a>. It’s not mandatory but I highly recommend it.</li>
<li>Clone the repository with the sample app:</li>
</ol>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1"></a><span class="fu">git</span> clone https://github.com/dylanjcastillo/ai-search-fastapi-qdrant-chatgpt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="4" type="1">
<li><p>Go to the root folder of the project and install the dependencies with:</p>
<ul>
<li><strong>Poetry:</strong> Create the virtual environment in the same directory as the project and install the dependencies:</li>
</ul>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1"></a><span class="ex">poetry</span> config virtualenvs.in-project true</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="ex">poetry</span> install</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>venv and pip:</strong> Create a virtual environment and install the dependencies listed in <code>requirements.txt</code>:</li>
</ul>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1"></a><span class="ex">python3.10</span> <span class="at">-m</span> venv .venv <span class="kw">&amp;&amp;</span> <span class="bu">source</span> .venv/bin/activate</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="ex">pip</span> install <span class="at">-r</span> requirements.txt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Because PyTorch <a href="https://github.com/pytorch/pytorch/issues/86566">does not yet support Python 3.11 in MacOS and Windows</a>, this tutorial will not work if you are running Python 3.11 on those operating systems.</p>
</div>
</div>
<p>If everything went well, you should have a virtual environment with all of the necessary libraries and a project structure that looks like this:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1"></a><span class="ex">ai-search-fastapi-qdrant-gpt3</span></span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="ex">│</span></span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="ex">├──</span> README.md</span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="ex">├──</span> config.py</span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="ex">├──</span> data</span>
<span id="cb4-6"><a href="#cb4-6"></a><span class="ex">│&nbsp;&nbsp;</span> ├── processed</span>
<span id="cb4-7"><a href="#cb4-7"></a><span class="ex">│&nbsp;&nbsp;</span> │&nbsp;&nbsp; └── Marcus_Aurelius_Antoninus...</span>
<span id="cb4-8"><a href="#cb4-8"></a><span class="ex">│&nbsp;&nbsp;</span> │&nbsp;&nbsp;     └── Marcus_Aurelius_Antoninus...json</span>
<span id="cb4-9"><a href="#cb4-9"></a><span class="ex">│&nbsp;&nbsp;</span> └── unzipped</span>
<span id="cb4-10"><a href="#cb4-10"></a><span class="ex">│&nbsp;&nbsp;</span>     └── Marcus_Aurelius_Antoninus...</span>
<span id="cb4-11"><a href="#cb4-11"></a><span class="ex">│&nbsp;&nbsp;</span>         ├── index.html</span>
<span id="cb4-12"><a href="#cb4-12"></a><span class="ex">│&nbsp;&nbsp;</span>         ├── metadata.opf</span>
<span id="cb4-13"><a href="#cb4-13"></a><span class="ex">│&nbsp;&nbsp;</span>         └── style.css</span>
<span id="cb4-14"><a href="#cb4-14"></a><span class="ex">├──</span> main.py</span>
<span id="cb4-15"><a href="#cb4-15"></a><span class="ex">├──</span> notebooks</span>
<span id="cb4-16"><a href="#cb4-16"></a><span class="ex">│&nbsp;&nbsp;</span> ├── extract_text.ipynb</span>
<span id="cb4-17"><a href="#cb4-17"></a><span class="ex">│&nbsp;&nbsp;</span> └── vectorize_text.ipynb</span>
<span id="cb4-18"><a href="#cb4-18"></a><span class="ex">├──</span> poetry.lock</span>
<span id="cb4-19"><a href="#cb4-19"></a><span class="ex">├──</span> pyproject.toml</span>
<span id="cb4-20"><a href="#cb4-20"></a><span class="ex">├──</span> requirements.txt</span>
<span id="cb4-21"><a href="#cb4-21"></a><span class="ex">├──</span> .env-example</span>
<span id="cb4-22"><a href="#cb4-22"></a><span class="ex">└──</span> .venv/</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is your project’s structure. Let me explain the purpose of the most important files and directories:</p>
<ul>
<li><code>config.py</code>: This file contains project configuration specifications such as Qdrant’s host, port, and API key (read from a <code>.env</code> file)</li>
<li><code>data/</code>: This directory contains the project’s data. It contains Meditations as originally downloaded from <a href="https://en.wikisource.org/wiki/Main_Page">Wikisource</a> as well as the processed file that you will use in the project.</li>
<li><code>main.py</code>: This file contains the code of the FastAPI application.</li>
<li><code>notebooks/</code>: This directory contains Jupyter notebooks for extracting, vectorizing, and indexing the data. <code>extract_text.ipynb</code> contains code to parse the HTML file and <code>vectorize_text.ipynb</code> contains code to vectorize and index the data.</li>
<li><code>poetry.lock</code> and <code>pyproject.toml</code>: These files contain information about the project’s dependencies and are used by Poetry to replicate the environment.</li>
<li><code>requirements.txt</code>: This file contains a list of Python packages required by the project and their respective versions.</li>
<li><code>.env-example</code>: This file is an example of the environment variables you must provide.</li>
<li><code>.venv/</code>: This directory contains the project’s virtual environment.</li>
</ul>
<p>That’s it! You’re now ready to get started.</p>
</section>
<section id="configure-qdrant-and-openai" class="level2">
<h2 class="anchored" data-anchor-id="configure-qdrant-and-openai">Configure Qdrant and OpenAI</h2>
<p>Start by renaming <code>.env-example</code> to <code>.env</code>. Don’t worry about filling in the values in <code>.env</code>. After you’ve created a cluster and the API keys for Qdrant and OpenAI, you’ll fill in the blanks.</p>
<section id="qdrant" class="level3">
<h3 class="anchored" data-anchor-id="qdrant">Qdrant</h3>
<p>Create an account at <a href="https://cloud.qdrant.io/">Qdrant</a>, if you don’t already have one. Then, on your account page go to <em>Clusters &gt; Create,</em> and create a cluster of 1GB of RAM, 0.5 vCPU, and 20GB Disk. Qdrant has a generous free tier, and it’s free to run a cluster with those specifications.</p>
<p><a href="images/ai-search-engine-fastapi-qdrant-chatgpt/image-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="images/ai-search-engine-fastapi-qdrant-chatgpt/image-1.png" class="img-fluid"></a></p>
<p>Next, paste the host and API key you obtained when you created your cluster into <code>.env</code>:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1"></a><span class="va">QDRANT_PORT</span><span class="op">=</span>6333</span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="va">QDRANT_HOST</span><span class="op">=&lt;</span>your_qdrant_host<span class="op">&gt;</span></span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="va">QDRANT_API_KEY</span><span class="op">=&lt;</span>your_qdrant_api_key<span class="op">&gt;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If you didn’t copy the key, you can still create a new one in <em>Access.</em></p>
<p>Finally, you can test that everything went well by running the first three cells in <code>vectorize_data.ipynb</code>.</p>
</section>
<section id="openai" class="level3">
<h3 class="anchored" data-anchor-id="openai">OpenAI</h3>
<p>If you don’t have an OpenAI account, <a href="https://platform.openai.com/login">create one</a>. After that, go to <em>Manage account &gt; API keys &gt; &nbsp;+ Create new secret key.</em></p>
<p><a href="images/ai-search-engine-fastapi-qdrant-chatgpt/image-4.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="images/ai-search-engine-fastapi-qdrant-chatgpt/image-4.png" class="img-fluid"></a></p>
<p>Then, paste the generated key in <code>.env</code>:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1"></a><span class="va">QDRANT_PORT</span><span class="op">=</span>6333</span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="va">QDRANT_HOST</span><span class="op">=&lt;</span>your_qdrant_host<span class="op">&gt;</span></span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="va">QDRANT_API_KEY</span><span class="op">=&lt;</span>your_qdrant_api_key<span class="op">&gt;</span></span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="va">OPENAI_API_KEY</span><span class="op">=&lt;</span>your_openai_api_key<span class="op">&gt;</span> <span class="co"># new</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>That’s it! Let’s continue.</p>
</section>
</section>
<section id="extract-data" class="level2">
<h2 class="anchored" data-anchor-id="extract-data">Extract Data</h2>
<p>The data extraction pipeline will vary greatly between projects, so I won’t go into too much detail here.<br>
Here are some useful guidelines to keep in mind when doing so:</p>
<ol type="1">
<li><strong>Garbage in, garbage out:</strong> The quality of your data will heavily influence your search results, so take your time with this step.</li>
<li><strong>Splitting documents:</strong> When you do semantic search, you need to divide documents into smaller chunks so that you can compare the similarity of each chunk to the user’s query. There is no right or wrong way to do this. In this case, I took a straightforward approach: divide the text into paragraphs, and if the paragraph is above a certain number of characters, divide it into multiple sentences.</li>
<li><strong>Production:</strong> For real-world scenarios, you should think about how frequently you’ll be extracting and ingesting data, adapting your pipeline for different data sources (e.g., scraping, APIs), and building pipeline monitors, among other things. In this example, because data extraction is a one-time event, I’m using jupyter notebooks, which isn’t always a good idea.</li>
</ol>
<p>Here’s a sneak peek at the <a href="https://github.com/dylanjcastillo/ai-search-fastapi-qdrant-gpt3/blob/main/data/processed/Marcus_Aurelius_Antoninus_-_His_Meditations_concerning_himselfe/Marcus_Aurelius_Antoninus_-_His_Meditations_concerning_himselfe.json">data</a> from this tutorial:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource json number-lines code-with-copy"><code class="sourceCode json"><span id="cb7-1"><a href="#cb7-1"></a><span class="fu">{</span></span>
<span id="cb7-2"><a href="#cb7-2"></a>    <span class="dt">"book_title"</span><span class="fu">:</span> <span class="st">"Meditations by Marcus Aurelius"</span><span class="fu">,</span></span>
<span id="cb7-3"><a href="#cb7-3"></a>    <span class="dt">"url"</span><span class="fu">:</span> <span class="st">"https://en.wikisource.org/wiki/Marcus_Aurelius_Antoninus_-_His_Meditations_concerning_himselfe"</span><span class="fu">,</span></span>
<span id="cb7-4"><a href="#cb7-4"></a>    <span class="dt">"data"</span><span class="fu">:</span> <span class="ot">[</span></span>
<span id="cb7-5"><a href="#cb7-5"></a>        <span class="fu">{</span></span>
<span id="cb7-6"><a href="#cb7-6"></a>            <span class="dt">"title"</span><span class="fu">:</span> <span class="st">"THE FIRST BOOK"</span><span class="fu">,</span></span>
<span id="cb7-7"><a href="#cb7-7"></a>            <span class="dt">"url"</span><span class="fu">:</span> <span class="st">"https://en.wikisource.org/wiki/Marcus_Aurelius_Antoninus_-_His_Meditations_concerning_himselfe#THE_FIRST_BOOK"</span><span class="fu">,</span></span>
<span id="cb7-8"><a href="#cb7-8"></a>            <span class="dt">"sentences"</span><span class="fu">:</span> <span class="ot">[</span></span>
<span id="cb7-9"><a href="#cb7-9"></a>                <span class="st">"I. Of my grandfather Verus I have learned to be gentle and meek..."</span><span class="ot">,</span></span>
<span id="cb7-10"><a href="#cb7-10"></a>                <span class="st">"II. Of him that brought me up, not to be fondly addicted..."</span><span class="ot">,</span></span>
<span id="cb7-11"><a href="#cb7-11"></a>                <span class="st">"III. Of Diognetus, not to busy myself about vain things..."</span><span class="ot">,</span></span>
<span id="cb7-12"><a href="#cb7-12"></a>                <span class="st">"IV. To Rusticus I am beholding, that I first entered into the..."</span><span class="ot">,</span></span>
<span id="cb7-13"><a href="#cb7-13"></a>    <span class="er">...</span></span>
<span id="cb7-14"><a href="#cb7-14"></a>        <span class="er">}</span></span>
<span id="cb7-15"><a href="#cb7-15"></a>    <span class="ot">]</span></span>
<span id="cb7-16"><a href="#cb7-16"></a> <span class="fu">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The tutorial’s data includes general metadata such as the book title and source URL, as well as information from each chapter with the sentences you’ll index.</p>
<p>If you want to take a look at how I extracted the data used in this tutorial, check out the <code>[extract_data.ipynb](https://github.com/dylanjcastillo/ai-search-fastapi-qdrant-gpt3/blob/main/notebooks/extract_text.ipynb)</code>.</p>
</section>
<section id="vectorize-and-index-data" class="level2">
<h2 class="anchored" data-anchor-id="vectorize-and-index-data">Vectorize and Index Data</h2>
<p>Once you’ve extracted the data, you’ll want to index it in your vector database.</p>
<p>The process consists of two steps:</p>
<ol type="1">
<li>Generate vectors for each sentence you extracted earlier.</li>
<li>Insert those vectors in a <a href="https://qdrant.tech/documentation/collections/"><strong>collection</strong></a>(the set of vectors you can search in the vector database).</li>
</ol>
<p>You can find the code for this section in <code>notebooks/vectorize_data.ipynb</code>.</p>
<p>As usual, you start by importing the required libraries:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="im">import</span> json</span>
<span id="cb8-2"><a href="#cb8-2"></a></span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-4"><a href="#cb8-4"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb8-5"><a href="#cb8-5"></a><span class="im">import</span> torch</span>
<span id="cb8-6"><a href="#cb8-6"></a></span>
<span id="cb8-7"><a href="#cb8-7"></a><span class="im">from</span> qdrant_client <span class="im">import</span> QdrantClient</span>
<span id="cb8-8"><a href="#cb8-8"></a><span class="im">from</span> qdrant_client.http <span class="im">import</span> models</span>
<span id="cb8-9"><a href="#cb8-9"></a><span class="im">from</span> sentence_transformers <span class="im">import</span> SentenceTransformer</span>
<span id="cb8-10"><a href="#cb8-10"></a></span>
<span id="cb8-11"><a href="#cb8-11"></a><span class="im">from</span> tqdm.notebook <span class="im">import</span> tqdm</span>
<span id="cb8-12"><a href="#cb8-12"></a></span>
<span id="cb8-13"><a href="#cb8-13"></a><span class="im">from</span> config <span class="im">import</span> QDRANT_HOST, QDRANT_PORT, QDRANT_API_KEY, DATA, COLLECTION_NAME</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This code imports all the libraries and configuration variables you need to vectorize and index the data. Here are some things worth mentioning:</p>
<ul>
<li><code>qdrant_client</code> and <code>qdrant_client.http</code> let you interact with Qdrant’s client, so that you can insert and retrieve data from the <strong>collection.</strong></li>
<li><code>sentence_transformers</code> let you generate the vectors from text, using pretrained models.</li>
</ul>
<p>Next, you read the data as follows:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a>BOOK_FILENAME <span class="op">=</span> <span class="st">"Marcus_Aurelius_Antoninus_-_His_Meditations_concerning_himselfe"</span></span>
<span id="cb9-2"><a href="#cb9-2"></a></span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="ss">f"</span><span class="sc">{</span>DATA<span class="sc">}</span><span class="ss">/processed/</span><span class="sc">{</span>BOOK_FILENAME<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>BOOK_FILENAME<span class="sc">}</span><span class="ss">.json"</span>, <span class="st">"r"</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb9-4"><a href="#cb9-4"></a>    meditations_json <span class="op">=</span> json.load(<span class="bu">file</span>)</span>
<span id="cb9-5"><a href="#cb9-5"></a></span>
<span id="cb9-6"><a href="#cb9-6"></a>rows <span class="op">=</span> []</span>
<span id="cb9-7"><a href="#cb9-7"></a><span class="cf">for</span> chapter <span class="kw">in</span> tqdm(meditations_json[<span class="st">"data"</span>]):</span>
<span id="cb9-8"><a href="#cb9-8"></a>    <span class="cf">for</span> sentence <span class="kw">in</span> chapter[<span class="st">"sentences"</span>]:</span>
<span id="cb9-9"><a href="#cb9-9"></a>        rows.append(</span>
<span id="cb9-10"><a href="#cb9-10"></a>            (</span>
<span id="cb9-11"><a href="#cb9-11"></a>                chapter[<span class="st">"title"</span>],</span>
<span id="cb9-12"><a href="#cb9-12"></a>                chapter[<span class="st">"url"</span>],</span>
<span id="cb9-13"><a href="#cb9-13"></a>                sentence,</span>
<span id="cb9-14"><a href="#cb9-14"></a>            )</span>
<span id="cb9-15"><a href="#cb9-15"></a>        )</span>
<span id="cb9-16"><a href="#cb9-16"></a></span>
<span id="cb9-17"><a href="#cb9-17"></a>df <span class="op">=</span> pd.DataFrame(</span>
<span id="cb9-18"><a href="#cb9-18"></a>    data<span class="op">=</span>rows, columns<span class="op">=</span>[<span class="st">"title"</span>, <span class="st">"url"</span>, <span class="st">"sentence"</span>]</span>
<span id="cb9-19"><a href="#cb9-19"></a>)</span>
<span id="cb9-20"><a href="#cb9-20"></a></span>
<span id="cb9-21"><a href="#cb9-21"></a>df <span class="op">=</span> df[df[<span class="st">"sentence"</span>].<span class="bu">str</span>.split().<span class="bu">str</span>.<span class="bu">len</span>() <span class="op">&gt;</span> <span class="dv">15</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This code reads the previously processed data and removes short sentences. It works as follows:</p>
<ul>
<li><strong>Lines 1 to 4</strong> read the JSON file you generated for Meditations, and save it as <code>meditations_json</code>.</li>
<li><strong>Lines 6 to 15</strong> go through all the chapters of the book stored in the <code>data</code> key from <code>meditations_json</code> and for each chapter, it extracts the relevant data (title of chapter, URL of chapter, and sentence), and adds it to <code>rows</code>.</li>
<li><strong>Lines 17 to 21</strong> create a DataFrame with the data from <code>rows</code> and removes the sentences with less than 15 words.</li>
</ul>
<p>Next, you create a collection in your vector database:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="co"># Create collection</span></span>
<span id="cb10-2"><a href="#cb10-2"></a>client <span class="op">=</span> QdrantClient(</span>
<span id="cb10-3"><a href="#cb10-3"></a>    host<span class="op">=</span>QDRANT_HOST, port<span class="op">=</span>QDRANT_PORT, api_key<span class="op">=</span>QDRANT_API_KEY</span>
<span id="cb10-4"><a href="#cb10-4"></a>)</span>
<span id="cb10-5"><a href="#cb10-5"></a>client.recreate_collection(</span>
<span id="cb10-6"><a href="#cb10-6"></a>    collection_name<span class="op">=</span>COLLECTION_NAME,</span>
<span id="cb10-7"><a href="#cb10-7"></a>    vectors_config<span class="op">=</span>models.VectorParams(</span>
<span id="cb10-8"><a href="#cb10-8"></a>        size<span class="op">=</span><span class="dv">384</span>,</span>
<span id="cb10-9"><a href="#cb10-9"></a>        distance<span class="op">=</span>models.Distance.COSINE</span>
<span id="cb10-10"><a href="#cb10-10"></a>    ),</span>
<span id="cb10-11"><a href="#cb10-11"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This code connects to your Qdrant’s cluster and creates a collection based on the name and settings you provide. In this case, you set the <code>size</code> to 384 based on the needs of the <a href="https://huggingface.co/sentence-transformers/msmarco-MiniLM-L-6-v3">model</a> you’ll use for vectorizing the sentences. You also set <code>distance</code> to use <a href="https://en.wikipedia.org/wiki/Cosine_similarity">Cosine distance</a>, which will define how the similarity between vectors is computed.</p>
<p>The next step is to generate the vectors (embeddings) from the text. You’ll use a pretrained model from Sentence Transformers to generate them instead of OpenAI-based embeddings. <a href="https://medium.com/@nils_reimers/openai-gpt-3-text-embeddings-really-a-new-state-of-the-art-in-dense-text-embeddings-6571fe3ec9d9">The latter are more expensive and not necessarily better.</a></p>
<p>To accomplish this, you load the pretrained model, generate the embeddings from the DataFrame sentences, and insert them into the collection you created:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>model <span class="op">=</span> SentenceTransformer(</span>
<span id="cb11-2"><a href="#cb11-2"></a>    <span class="st">"msmarco-MiniLM-L-6-v3"</span>,</span>
<span id="cb11-3"><a href="#cb11-3"></a>    device<span class="op">=</span><span class="st">"cuda"</span></span>
<span id="cb11-4"><a href="#cb11-4"></a>    <span class="cf">if</span> torch.cuda.is_available()</span>
<span id="cb11-5"><a href="#cb11-5"></a>    <span class="cf">else</span> <span class="st">"mps"</span></span>
<span id="cb11-6"><a href="#cb11-6"></a>    <span class="cf">if</span> torch.backends.mps.is_available()</span>
<span id="cb11-7"><a href="#cb11-7"></a>    <span class="cf">else</span> <span class="st">"cpu"</span>,</span>
<span id="cb11-8"><a href="#cb11-8"></a>)</span>
<span id="cb11-9"><a href="#cb11-9"></a></span>
<span id="cb11-10"><a href="#cb11-10"></a>vectors <span class="op">=</span> []</span>
<span id="cb11-11"><a href="#cb11-11"></a>batch_size <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb11-12"><a href="#cb11-12"></a>batch <span class="op">=</span> []</span>
<span id="cb11-13"><a href="#cb11-13"></a></span>
<span id="cb11-14"><a href="#cb11-14"></a><span class="cf">for</span> doc <span class="kw">in</span> tqdm(df[<span class="st">"sentence"</span>].to_list()):</span>
<span id="cb11-15"><a href="#cb11-15"></a>    batch.append(doc)</span>
<span id="cb11-16"><a href="#cb11-16"></a></span>
<span id="cb11-17"><a href="#cb11-17"></a>    <span class="cf">if</span> <span class="bu">len</span>(batch) <span class="op">&gt;=</span> batch_size:</span>
<span id="cb11-18"><a href="#cb11-18"></a>        vectors.append(model.encode(batch))</span>
<span id="cb11-19"><a href="#cb11-19"></a>        batch <span class="op">=</span> []</span>
<span id="cb11-20"><a href="#cb11-20"></a></span>
<span id="cb11-21"><a href="#cb11-21"></a><span class="cf">if</span> <span class="bu">len</span>(batch) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb11-22"><a href="#cb11-22"></a>    vectors.append(model.encode(batch))</span>
<span id="cb11-23"><a href="#cb11-23"></a>    batch <span class="op">=</span> []</span>
<span id="cb11-24"><a href="#cb11-24"></a></span>
<span id="cb11-25"><a href="#cb11-25"></a>vectors <span class="op">=</span> np.concatenate(vectors)</span>
<span id="cb11-26"><a href="#cb11-26"></a></span>
<span id="cb11-27"><a href="#cb11-27"></a>book_name <span class="op">=</span> meditations_json[<span class="st">"book_title"</span>]</span>
<span id="cb11-28"><a href="#cb11-28"></a></span>
<span id="cb11-29"><a href="#cb11-29"></a>client.upsert(</span>
<span id="cb11-30"><a href="#cb11-30"></a>    collection_name<span class="op">=</span>COLLECTION_NAME,</span>
<span id="cb11-31"><a href="#cb11-31"></a>    points<span class="op">=</span>models.Batch(</span>
<span id="cb11-32"><a href="#cb11-32"></a>        ids<span class="op">=</span>[i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(df.shape[<span class="dv">0</span>])],</span>
<span id="cb11-33"><a href="#cb11-33"></a>        payloads<span class="op">=</span>[</span>
<span id="cb11-34"><a href="#cb11-34"></a>            {</span>
<span id="cb11-35"><a href="#cb11-35"></a>                <span class="st">"text"</span>: row[<span class="st">"sentence"</span>],</span>
<span id="cb11-36"><a href="#cb11-36"></a>                <span class="st">"title"</span>: row[<span class="st">"title"</span>] <span class="op">+</span> <span class="ss">f", </span><span class="sc">{</span>book_name<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb11-37"><a href="#cb11-37"></a>                <span class="st">"url"</span>: row[<span class="st">"url"</span>],</span>
<span id="cb11-38"><a href="#cb11-38"></a>            }</span>
<span id="cb11-39"><a href="#cb11-39"></a>            <span class="cf">for</span> _, row <span class="kw">in</span> df.iterrows()</span>
<span id="cb11-40"><a href="#cb11-40"></a>        ],</span>
<span id="cb11-41"><a href="#cb11-41"></a>        vectors<span class="op">=</span>[v.tolist() <span class="cf">for</span> v <span class="kw">in</span> vectors],</span>
<span id="cb11-42"><a href="#cb11-42"></a>    ),</span>
<span id="cb11-43"><a href="#cb11-43"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This code loads the model, generates vectors from the sentences in the DataFrame, and inserts them into the collection you created. Here’s how it works:</p>
<ul>
<li><strong>Lines 1 to 8</strong> load the <code>msmarco-MiniLM-L-6-v3</code> sentence transformer model, and set the correct device in case you have a GPU available.</li>
<li><strong>Lines 10 to 23</strong> generate an array of vectors using the model you loaded. Each vector is a numerical representation of the sentences from your DataFrame.</li>
<li><strong>Lines 29 to 43</strong> insert the vectors and the additional data (actual sentence, book and chapter title, and URL) into the collection in your vector database.</li>
</ul>
</section>
<section id="create-a-server-with-fastapi" class="level2">
<h2 class="anchored" data-anchor-id="create-a-server-with-fastapi">Create a Server with FastAPI</h2>
<p>Next, you will create the FastAPI application that will let the user interact with your vector database and ChatGPT. The code for this section is in <code>main.py</code>.</p>
<p>You start by importing the required dependencies, setting up your Qdrant client, and loading the model:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="im">import</span> openai</span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="im">from</span> fastapi <span class="im">import</span> FastAPI</span>
<span id="cb12-3"><a href="#cb12-3"></a><span class="im">from</span> qdrant_client <span class="im">import</span> QdrantClient</span>
<span id="cb12-4"><a href="#cb12-4"></a><span class="im">from</span> sentence_transformers <span class="im">import</span> SentenceTransformer</span>
<span id="cb12-5"><a href="#cb12-5"></a></span>
<span id="cb12-6"><a href="#cb12-6"></a><span class="im">from</span> config <span class="im">import</span> (</span>
<span id="cb12-7"><a href="#cb12-7"></a>    COLLECTION_NAME,</span>
<span id="cb12-8"><a href="#cb12-8"></a>    OPENAI_API_KEY,</span>
<span id="cb12-9"><a href="#cb12-9"></a>    QDRANT_API_KEY,</span>
<span id="cb12-10"><a href="#cb12-10"></a>    QDRANT_HOST,</span>
<span id="cb12-11"><a href="#cb12-11"></a>    QDRANT_PORT,</span>
<span id="cb12-12"><a href="#cb12-12"></a>)</span>
<span id="cb12-13"><a href="#cb12-13"></a></span>
<span id="cb12-14"><a href="#cb12-14"></a>openai.api_key <span class="op">=</span> OPENAI_API_KEY</span>
<span id="cb12-15"><a href="#cb12-15"></a></span>
<span id="cb12-16"><a href="#cb12-16"></a>qdrant_client <span class="op">=</span> QdrantClient(</span>
<span id="cb12-17"><a href="#cb12-17"></a>    host<span class="op">=</span>QDRANT_HOST,</span>
<span id="cb12-18"><a href="#cb12-18"></a>    port<span class="op">=</span>QDRANT_PORT,</span>
<span id="cb12-19"><a href="#cb12-19"></a>    api_key<span class="op">=</span>QDRANT_API_KEY,</span>
<span id="cb12-20"><a href="#cb12-20"></a>)</span>
<span id="cb12-21"><a href="#cb12-21"></a></span>
<span id="cb12-22"><a href="#cb12-22"></a>retrieval_model <span class="op">=</span> SentenceTransformer(<span class="st">"msmarco-MiniLM-L-6-v3"</span>)</span>
<span id="cb12-23"><a href="#cb12-23"></a></span>
<span id="cb12-24"><a href="#cb12-24"></a>app <span class="op">=</span> FastAPI()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This code imports the libraries and configuration settings, initializes the Qdrant client, and loads the model to memory (the same one you used for vectorizing the sentences). You load your model globally so that you don’t slow down requests by loading it each time someone asks a question.</p>
<p>Next, you define a function to help you create the prompt that you’ll use to get ChatGPT to generate a coherent answer based on the most relevant passages from Meditations:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="kw">def</span> build_prompt(question: <span class="bu">str</span>, references: <span class="bu">list</span>) <span class="op">-&gt;</span> <span class="bu">tuple</span>[<span class="bu">str</span>, <span class="bu">str</span>]:</span>
<span id="cb13-2"><a href="#cb13-2"></a>    prompt <span class="op">=</span> <span class="ss">f"""</span></span>
<span id="cb13-3"><a href="#cb13-3"></a><span class="ss">    You're Marcus Aurelius, emperor of Rome. You're giving advice to a friend who has asked you the following question: '</span><span class="sc">{</span>question<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb13-4"><a href="#cb13-4"></a></span>
<span id="cb13-5"><a href="#cb13-5"></a><span class="ss">    You've selected the most relevant passages from your writings to use as source for your answer. Cite them in your answer.</span></span>
<span id="cb13-6"><a href="#cb13-6"></a></span>
<span id="cb13-7"><a href="#cb13-7"></a><span class="ss">    References:</span></span>
<span id="cb13-8"><a href="#cb13-8"></a><span class="ss">    """</span>.strip()</span>
<span id="cb13-9"><a href="#cb13-9"></a></span>
<span id="cb13-10"><a href="#cb13-10"></a>    references_text <span class="op">=</span> <span class="st">""</span></span>
<span id="cb13-11"><a href="#cb13-11"></a></span>
<span id="cb13-12"><a href="#cb13-12"></a>    <span class="cf">for</span> i, reference <span class="kw">in</span> <span class="bu">enumerate</span>(references, start<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb13-13"><a href="#cb13-13"></a>        text <span class="op">=</span> reference.payload[<span class="st">"text"</span>].strip()</span>
<span id="cb13-14"><a href="#cb13-14"></a>        references_text <span class="op">+=</span> <span class="ss">f"</span><span class="ch">\n</span><span class="ss">[</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">]: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb13-15"><a href="#cb13-15"></a></span>
<span id="cb13-16"><a href="#cb13-16"></a>    prompt <span class="op">+=</span> (</span>
<span id="cb13-17"><a href="#cb13-17"></a>        references_text</span>
<span id="cb13-18"><a href="#cb13-18"></a>        <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">How to cite a reference: This is a citation [1]. This one too [3]. And this is sentence with many citations [2][3].</span><span class="ch">\n</span><span class="st">Answer:"</span></span>
<span id="cb13-19"><a href="#cb13-19"></a>    )</span>
<span id="cb13-20"><a href="#cb13-20"></a>    <span class="cf">return</span> prompt, references_text</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This code will combine a prompt to make ChatGPT “simulate” Marcus Aurelius answering a user-supplied question with a list of references previously obtained from your vector database. Then it will return the generated prompt, and a list of the references to add to the answer sent to the user.</p>
<p>Then you create two endpoints as follows:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="at">@app.get</span>(<span class="st">"/"</span>)</span>
<span id="cb14-2"><a href="#cb14-2"></a><span class="kw">def</span> read_root():</span>
<span id="cb14-3"><a href="#cb14-3"></a>    <span class="cf">return</span> {</span>
<span id="cb14-4"><a href="#cb14-4"></a>        <span class="st">"message"</span>: <span class="st">"Make a post request to /ask to ask a question about Meditations by Marcus Aurelius"</span></span>
<span id="cb14-5"><a href="#cb14-5"></a>    }</span>
<span id="cb14-6"><a href="#cb14-6"></a></span>
<span id="cb14-7"><a href="#cb14-7"></a></span>
<span id="cb14-8"><a href="#cb14-8"></a><span class="at">@app.post</span>(<span class="st">"/ask"</span>)</span>
<span id="cb14-9"><a href="#cb14-9"></a><span class="kw">def</span> ask(question: <span class="bu">str</span>):</span>
<span id="cb14-10"><a href="#cb14-10"></a>    similar_docs <span class="op">=</span> qdrant_client.search(</span>
<span id="cb14-11"><a href="#cb14-11"></a>        collection_name<span class="op">=</span>COLLECTION_NAME,</span>
<span id="cb14-12"><a href="#cb14-12"></a>        query_vector<span class="op">=</span>retrieval_model.encode(question),</span>
<span id="cb14-13"><a href="#cb14-13"></a>        limit<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb14-14"><a href="#cb14-14"></a>        append_payload<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb14-15"><a href="#cb14-15"></a>    )</span>
<span id="cb14-16"><a href="#cb14-16"></a></span>
<span id="cb14-17"><a href="#cb14-17"></a>    prompt, references <span class="op">=</span> build_prompt(question, similar_docs)</span>
<span id="cb14-18"><a href="#cb14-18"></a></span>
<span id="cb14-19"><a href="#cb14-19"></a>    response <span class="op">=</span> openai.ChatCompletion.create(</span>
<span id="cb14-20"><a href="#cb14-20"></a>        model<span class="op">=</span><span class="st">"gpt-3.5-turbo"</span>,</span>
<span id="cb14-21"><a href="#cb14-21"></a>        messages<span class="op">=</span>[</span>
<span id="cb14-22"><a href="#cb14-22"></a>            {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: prompt},</span>
<span id="cb14-23"><a href="#cb14-23"></a>        ],</span>
<span id="cb14-24"><a href="#cb14-24"></a>        max_tokens<span class="op">=</span><span class="dv">250</span>,</span>
<span id="cb14-25"><a href="#cb14-25"></a>        temperature<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb14-26"><a href="#cb14-26"></a>    )</span>
<span id="cb14-27"><a href="#cb14-27"></a></span>
<span id="cb14-28"><a href="#cb14-28"></a>    <span class="cf">return</span> {</span>
<span id="cb14-29"><a href="#cb14-29"></a>        <span class="st">"response"</span>: response[<span class="st">"choices"</span>][<span class="dv">0</span>][<span class="st">"text"</span>],</span>
<span id="cb14-30"><a href="#cb14-30"></a>        <span class="st">"references"</span>: references,</span>
<span id="cb14-31"><a href="#cb14-31"></a>    }</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>These are the two endpoints that you’ll use in your app. Here’s what each line does:</p>
<ul>
<li><strong>Lines 1 to 5</strong> set up an endpoint that accepts <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/GET">GET</a> requests on “/”. It returns a JSON response with a <code>message</code> key telling the user to use the “/ask” endpoint.</li>
<li><strong>Lines 8 to 17</strong> define an endpoint that takes accepts <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/POST">POST</a> requests on “/ask”, with a single parameter, <code>question</code>of <code>string</code> type. Once the user submits a request, you vectorize the question using the model you loaded previously, then you get the 3 most similar documents from your vector database.</li>
<li><strong>Lines 19 to 32</strong> combine the documents you got from the vector database with your prompt and make a request to the ChatGPT API. You set <code>max_tokens=250</code> to keep answers short and set <code>temperature=0.2</code>, to prevent the model from getting “too creative” with its responses. Finally, you extract the answer from the ChatGPT API response and return it to the user, along with the references.</li>
</ul>
<p>If you want to test it locally, type the following command into a terminal (inside the project’s virtual environment):</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>uvicorn main:app <span class="op">--</span><span class="bu">reload</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In your browser, navigate to <code>localhost:8000/docs</code> to test your <code>/ask</code> endpoint:</p>
<p><a href="images/ai-search-engine-fastapi-qdrant-chatgpt/image-5.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="images/ai-search-engine-fastapi-qdrant-chatgpt/image-5.png" class="img-fluid"></a></p>
<p>A successful response will look as follows:</p>
<p><a href="images/ai-search-engine-fastapi-qdrant-chatgpt/image-6.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="images/ai-search-engine-fastapi-qdrant-chatgpt/image-6.png" class="img-fluid"></a></p>
<p>That’s it! You have a working version of your AI search engine. Next, I’ll mention some ideas about deployment.</p>
</section>
<section id="deploy-your-app" class="level2">
<h2 class="anchored" data-anchor-id="deploy-your-app">Deploy Your App</h2>
<p>There are many different ways to deploy an app, so you choose whatever approach you prefer. In my case, I like to use a VPS with NGINX acting as a reverse proxy and using Gunicorn as a process manager with Uvicorn workers. If you’d like to follow that approach, check a <a href="https://dylancastillo.co/fastapi-nginx-gunicorn/">tutorial</a> I wrote about it.</p>
<p>If you choose that route, keep the following points in mind:</p>
<ul>
<li>You should use <code>[--preload](https://dylancastillo.co/fastapi-nginx-gunicorn/)</code> if you want to share the same model across all the processes and use less RAM memory.</li>
<li>There are memory leak issues when serving some types of <a href="https://github.com/tiangolo/fastapi/issues/2425">models</a>. A workaround that has worked for me is setting <code>--max-requests</code> and <code>--max-requests-jitter</code> to low numbers.</li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Way to go! In this tutorial, you’ve built a (tiny) AI search engine. You’ve learned:</p>
<ul>
<li>How to structure the project.</li>
<li>How to set up a vector database using Qdrant.</li>
<li>How to vectorize your data using Sentence Transformers.</li>
<li>How to use ChatGPT to combine references into a coherent answer.</li>
</ul>
<p>Hope you found this tutorial useful. Let me know if you have any questions!</p>
<p>All the code for this tutorial is <a href="https://github.com/dylanjcastillo/ai-search-fastapi-qdrant-chatgpt">available on GitHub</a>.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{castillo2023,
  author = {Castillo, Dylan},
  title = {Build an {AI} {Search} {Engine} {Using} {FastAPI,} {Qdrant,}
    and {ChatGPT}},
  date = {2023-03-03},
  url = {https://dylancastillo.co/posts/ai-search-engine-fastapi-qdrant-chatgpt.html},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-castillo2023" class="csl-entry quarto-appendix-citeas" role="listitem">
Castillo, Dylan. 2023. <span>“Build an AI Search Engine Using FastAPI,
Qdrant, and ChatGPT.”</span> March 3, 2023. <a href="https://dylancastillo.co/posts/ai-search-engine-fastapi-qdrant-chatgpt.html">https://dylancastillo.co/posts/ai-search-engine-fastapi-qdrant-chatgpt.html</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/dylancastillo\.co");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="dylanjcastillo/blog_comments" issue-term="pathname" theme="dark-blue" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2024, Dylan Castillo</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>