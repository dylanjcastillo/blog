---
title: "The good, the bad, and the ugly of Gemini's structured outputs"
date: "12/27/2024"
description-meta: "This article will go over Gemini's structured outputs and how they compare to unstructured outputs."
categories:
  - llm
  - gemini
  - pydantic
  - python
---

Because structured outputs made my life so much easier when working with LLMs, it took me a long time to realize that maybe they could be hurting the performance of my applications.

After reading [Let Me Speak Freely?](https://arxiv.org/abs/2408.02442) and [.txt's rebuttal](https://blog.dottxt.co/say-what-you-mean.html), I decided to run some benchmarks myself using popular proprietary models.

I found that for [_GPT-4o-mini_](https://dylancastillo.co/posts/say-what-you-mean-sometimes.html), structured outputs could decrease performance in some tasks. So I was curious to see if the same was true for _Gemini 1.5 Flash_.

In this article, I'll share the results of running the same benchmarks for _Gemini 1.5 Flash_ and a few interesting things I learned about the Gemini API along the way. I've got good, bad, and ugly news for you.

You can find the code in this [GitHub repository](https://github.com/dylanjcastillo/blog/tree/main/_extras/gemini-structured-outputs).

## Results (TLDR)

If you're in a hurry, here are my key findings:

1. The good news: Gemini's structured outputs performed on par with unstructured outputs.^[Assuming “structured outputs” refers to any method that generates LLM output adhering to a JSON schema.]
2. The bad news: This only holds for the less rigid interpretation of "structured outputs". When testing constrained decoding specifically, Gemini’s structured outputs performed worse than unstructured outputs.
3. The ugly news: Function calling and constrained decoding have a big design flaw. The order of the keys specified in the schema is not preserved when using the [Generative AI Python SDK](https://ai.google.dev/). This will break chain-of-thought reasoning in your applications unless you use a workaround (that only works for constrained decoding).

The figure below shows the overall results for _Gemini 1.5 Flash_:

```{python}
#| label: fig-gemini-flash-best
#| fig-cap: "Best results for Gemini 1.5 Flash."
#| echo: false

import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Data
tasks = ['GSM8k', 'Last Letter', 'Shuffled Objects']
natural_language = [94.84, 82.67, 97.15]
json_prompt = [94.16, 82.00, 98.37]
json_schema = [93.63, 81.33, 86.18]

# Create figure
fig = go.Figure()

# Add bars
fig.add_trace(go.Bar(
    x=tasks,
    y=natural_language,
    name='Natural Language',
    text=natural_language,
    textposition='outside',
    texttemplate='%{text:.2f}%',
    textfont=dict(size=10),
    hoverinfo='skip'
))
fig.add_trace(go.Bar(
    x=tasks,
    y=json_schema,
    name='JSON-Schema',
    text=json_schema,
    textposition='outside',
    texttemplate='%{text:.2f}%',
    textfont=dict(size=10),
    hoverinfo='skip',
))
fig.add_trace(go.Bar(
    x=tasks,
    y=json_prompt,
    name='JSON-Prompt',
    text=json_prompt,
    textposition='outside',
    texttemplate='%{text:.2f}%',
    textfont=dict(size=10),
    hoverinfo='skip',
))


# Update layout
fig.update_layout(
    template='plotly_dark',
    barmode='group',
    height=400,  # Increased height to accommodate legend
    margin=dict(
        l=50,
        r=50,
        t=50,
        b=50,  # Reduced bottom margin
        pad=10
    ),
    yaxis=dict(
        title='Score (%)',
        range=[0, 105],
        fixedrange=True  # Prevent y-axis zooming
    ),
    xaxis=dict(
        title='Task',
        fixedrange=True  # Prevent x-axis zooming
    ),
    legend=dict(
        orientation='h',
        yanchor='bottom',
        y=1.05,  # Moved legend above plot
        xanchor='center',
        x=0.5
    ),
    showlegend=True,
    modebar=dict(
        remove=["autoScale2d", "autoscale",
            "editInChartStudio", "editinchartstudio",
            "hoverCompareCartesian", "hovercompare", "lasso",
            "lasso2d", "orbitRotation", "orbitrotation", "pan",
            "pan2d", "pan3d", "reset", "resetCameraDefault3d",
            "resetCameraLastSave3d", "resetGeo",
            "resetSankeyGroup", "resetScale2d", "resetViewMap",
            "resetViewMapbox", "resetViews", "resetcameradefault",
            "resetcameralastsave", "resetsankeygroup",
            "resetscale", "resetview", "resetviews", "select",
            "select2d", "sendDataToCloud", "senddatatocloud",
            "tableRotation", "tablerotation", "toImage",
            "toggleHover", "toggleSpikelines", "togglehover",
            "togglespikelines", "toimage", "zoom", "zoom2d",
            "zoom3d", "zoomIn2d", "zoomInGeo", "zoomInMap",
            "zoomInMapbox", "zoomOut2d", "zoomOutGeo",
            "zoomOutMap", "zoomOutMapbox", "zoomin", "zoomout"],
    )
)

fig.show()
```

The figure above compares the performance of Gemini's structured outputs to unstructured outputs. **NL** stands for _Natural Language_, which means the model writes the output in a free-form manner. In contrast, **JSON-Prompt** and **JSON-Schema** involve structured outputs that follow a predefined JSON schema.

For **JSON-Prompt**, the JSON schema is included in the prompt, instructing the model to generate JSON formatted output based on its MIME type configuration. **JSON-Schema** works similarly, but the schema is set directly in the model's configuration instead of being included in the prompt.

When considering both **JSON-Prompt** and **JSON-Schema**, Gemini's structured outputs performed comparably to unstructured outputs. However, with **JSON-Schema** alone (i.e., constrained decoding), performance drops compared to unstructured outputs. This difference is most evident in the _Shuffled Objects_ task, where **NL** achieved a score of 97.15%, while **JSON-Schema** scored 86.18%.

## Study design

In Let Me Speak Freely?, Tam et al. evaluated structured and unstructured outputs across three reasoning tasks and six classification tasks. They used exact match to evaluate the reasoning tasks and accuracy to evaluate the classification tasks. They ran the experiments using the following models:

1.  **Proprietary models**: _gpt-3.5-turbo-0125_, _claude-3-haiku-20240307_, _gemini-1.5-flash_, and _gpt-4o-mini-2024-07-18_.
2.  **Open-weight models**: _LLaMA3-8B-Instruct_, and _Gemma-2-9B-Instruct_.

For this article, I just focused on _Gemini 1.5 Flash_ and the reasoning tasks. I already ran the benchmarks for _GPT-4o-mini_ and _Llama3-8B-Instruct_ in my [previous post](https://dylancastillo.co/posts/say-what-you-mean-sometimes.html).

I excluded the classification tasks because I believe structured outputs perform better in classification tasks, and this is also in line with the study's original findings. So I just focused on the three reasoning tasks:

1.  [GSM8K](https://huggingface.co/datasets/openai/gsm8k): A dataset from of grade school math word problems.
2.  [Last Letter](https://huggingface.co/datasets/ChilleD/LastLetterConcat): A dataset of simple word puzzles that require concatening the last letters of a list of names.
3.  [Shuffled Objects](https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/tracking_shuffled_objects): A dataset that requires reasoning about the state of a system after a sequence of shuffling operations.

The rest of the article details the process of re-evaluating these benchmarks using _Gemini-1.5-Flash_.

## Generating structured outputs with Gemini

Gemini has three ways of generating structured outputs:

1. [**Forced function calling (FC)**](https://ai.google.dev/gemini-api/tutorials/extract_structured_data): You force the model to call a "function" and that makes the model generate a JSON schema that matches the function's arguments.
2. [**Schema in prompt (JSON-Prompt)**](https://ai.google.dev/gemini-api/docs/structured-output?lang=python#supply-schema-in-prompt): You include a JSON schema in the prompt, specify `mime_type='application/json'` and the model generates a response that matches the schema.
3. [**Schema in model configuration (JSON-Schema)**](https://ai.google.dev/gemini-api/docs/structured-output?lang=python#supply-schema-in-config): You provide a JSON schema in the model configuration, specify `mime_type='application/json'` in the request and the model generates a response that matches the schema. This is the only method that seems to use [controlled generation](https://developers.googleblog.com/en/mastering-controlled-generation-with-gemini-15-schema-adherence/).

I've included _JSON-Prompt_ and _JSON-Schema_ in the analysis, but had to exclude _FC_ because it's not possible to use it for chain-of-thought reasoning, which is a requirement for the benchmarks.

## Figuring out Gemini's flawed structured outputs

When running the three benchmarks, I quickly ran into a performance issue with _FC_ and _JSON-Schema_. In all tasks, both showed double-digit performance drops compared to _NL_.

This didn't make a lot of sense, so I started investigating.

I was using the following response schema for all structured outputs:

```python
class Response(BaseModel):
    reasoning: str
    answer: str
```

The prompts were similar to the one below, adjusted according to the task:

> You are an expert in solving grade school math tasks. You will be presented with a grade-school math word problem and be asked to solve it.
>
> You will always respond with JSON matching the following schema:
>
> [RESPONSE_SCHEMA]
>
> First, provide your step by step reasoning in the "reasoning" field. Then, in the "answer" field, provide your final answer. Don't include any other text in the "answer" field.

I eventually realized that the performance drop in _JSON-Schema_ was due to the keys in the schema being reversed when generating the response. I then noticed that Tam et al. briefly mentioned in Let Me Speak Freely? that _JSON-Schema_ responses failed to produce valid JSON due to this exact problem, so they did not include it in their analysis.

I didn't want to exclude it from the analysis, so I started looking for a way to control the order of the keys in the schema. I found that the order of the keys in the schema gets sorted alphabetically before the response is generated. To confirm this, I ran a test with 100 randomly generated schemas. Every resulting output had its keys sorted alphabetically, so it's clear this is not by chance.

I also found that the Vertex AI documentation mentions a `propertyOrdering` parameter, which should allow control over the order of keys. However, this feature [doesn’t appear](https://discuss.ai.google.dev/t/issue-with-key-ordering-in-controlled-generation-on-gemini-1-5/41737/3) [to work](https://github.com/google-gemini/generative-ai-python/issues/533) with the Generative AI Python SDK.

Unable to use the `propertyOrdering` parameter, I resorted to a quick workaround: I named the keys in a way that forced the desired order alphabetically. Instead of using `reasoning` and `answer`, I used `reasoning` and `solution`. This preserved the chain-of-thought reasoning in the responses, and resolved the performance drop in _JSON-Schema_.

But _FC_ was a different story. Unlike _JSON-Schema_, the order of the keys follow a less predictable pattern, and I couldn’t find a way to control it. So I decided to exclude _FC_ from the analysis.

## Evaluating Gemini 1.5 Flash

After applying the key ordering workaround, and [additional improvements](https://dylancastillo.co/posts/say-what-you-mean-sometimes.html#replicating-.txts-rebuttal.html) discussed in my analysis of GPT-4o-mini's structured outputs, I recomputed the benchmarks.

The table below shows the results for Gemini 1.5 Flash compared to the original results from Tam et al.

```{python}
#| label: tbl-gemini-1.5-flash
#| tbl-cap: "Results for Gemini 1.5 Flash."
#| echo: false

import pandas as pd

# Define MultiIndex for Task and Method
index = pd.MultiIndex.from_product(
    [['GSM8K', 'Last Letter', 'Shuffled Obj.'],
     ['Tam et al.', 'Me, 0-shot', 'Me, 3-shot']],
    names=['Task', 'Method']
)

# Data without explicit Task and Method columns
data = {
    'NL': [
        89.33, 93.71, 94.84,
        65.45, 82.67, 80.00,
        58.21, 97.15, 92.68
    ],
    'JSON-Prompt': [
        89.21, 93.78, 94.16,
        77.02, 80.00, 82.00,
        65.07, 92.28, 98.37
    ],
    'JSON-Schema': [
        47.78, 93.03, 93.63,
        0.67, 81.33, 80.67,
        pd.NA, 86.18, 84.96
    ]
}

# Create DataFrame with MultiIndex
df = pd.DataFrame(data, index=index)

# Style the DataFrame
table = (
  df.style
    .format(precision=2, na_rep='N/A')
)

table
```

Using a 0-shot prompt and 3-shot prompts, I was able to improve all the metrics on the tasks and methods Tam et al. used for their benchmarks. Which is great!

_NL_ and _JSON-Prompt_ behave similarly, without a clear winner between them. Each method got a slight edge over the other half the time. On the other hand, _JSON-Schema_ performed worst than _NL_ in 5 out of 6 tasks. Plus, in _Shuffled Objects_, it did so with a gap of more than 10 percentage points: 97.15% for _NL_ vs. 86.18% for _JSON-Schema_.

Tam et al. defined structured outputs as any method that "involves providing output in standardized formats like JSON or XML through format restriction." Using this definition, the results show no performance gap between structured and unstructured outputs. This directly contradicts the study's original claim.

But if you take the also common interpretation that constrained decoding is the only form of structured generation, then the study's original conclusion still applies: there is indeed a significant performance gap between structured and unstructured outputs.

Weird, I know. But that's the way it is.

## Conclusion and recommendations

Similar to my [previous post](https://dylancastillo.co/posts/say-what-you-mean-sometimes.html), the results are also mixed for Gemini 1.5 Flash.

The good news is that structured outputs perform on par with unstructured ones. The bad news is that this only holds if you adopt a more flexible definition of “structured outputs.” And the ugly news is that they have a major issue in how they handle the order of keys in the schema you provide to the model.

You shouldn't be disappointed by these results. In my case, it only reinforces my position that you shouldn’t blindly trust random posts or papers on the internet. If you want to improve your LLM workflows, you need to test things for yourself.

Structured outputs are great. They make my life easier when working with LLMs. But there are tasks where they might perform worse (or better!) than unstructured outputs, and that's something you need to be aware of.

It sucks when the answer is "it depends", but that's better than a false sense of security. So, run your own evals, and choose the approach that works best for your task.

If you want to run the benchmarks yourself, you can find the code in this [GitHub repository](https://github.com/dylanjcastillo/blog/tree/main/_extras/gemini-structured-outputs).
