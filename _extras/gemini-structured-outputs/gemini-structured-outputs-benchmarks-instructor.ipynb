{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18dc8ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65549cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33194d54-582b-4b81-a195-8f1269a438a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T20:37:59.673511Z",
     "iopub.status.busy": "2024-11-17T20:37:59.673328Z",
     "iopub.status.idle": "2024-11-17T20:38:07.222035Z",
     "shell.execute_reply": "2024-11-17T20:38:07.221730Z",
     "shell.execute_reply.started": "2024-11-17T20:37:59.673491Z"
    }
   },
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from asyncio import Semaphore\n",
    "from enum import Enum\n",
    "from functools import partial\n",
    "from textwrap import dedent\n",
    "from typing import Callable, Dict, List, Literal\n",
    "\n",
    "import google.generativeai as genai\n",
    "import instructor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from instructor import Mode\n",
    "from langsmith import traceable\n",
    "from pydantic import BaseModel, Field\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_NAME = \"gemini-1.5-flash-002\"\n",
    "USE_SAMPLE = True\n",
    "SAMPLE_SIZE = 200\n",
    "MAX_CONCURRENCY = 50\n",
    "\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "client_tools = instructor.from_gemini(\n",
    "    client=genai.GenerativeModel(\n",
    "        model_name=MODEL_NAME,\n",
    "    ),\n",
    "    mode=Mode.GEMINI_TOOLS,\n",
    "    use_async=True,\n",
    ")\n",
    "\n",
    "client_json = instructor.from_gemini(\n",
    "    client=genai.GenerativeModel(\n",
    "        model_name=MODEL_NAME,\n",
    "    ),\n",
    "    mode=Mode.GEMINI_JSON,\n",
    "    use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7967c968",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39c919d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "\n",
    "class PromptType(Enum):\n",
    "    WITH_TOOL_CALLS = \"with_so_tool_calls\"\n",
    "    WITH_STRICT_TOOL_CALLS = \"with_so_strict_tool_calls\"\n",
    "\n",
    "\n",
    "class ClientConfig(BaseModel):\n",
    "    name: str\n",
    "    col_name: str\n",
    "    score_col_name: str\n",
    "\n",
    "\n",
    "CONFIGS = [\n",
    "    ClientConfig(\n",
    "        name=PromptType.WITH_TOOL_CALLS.value,\n",
    "        col_name=f\"response_{PromptType.WITH_TOOL_CALLS.value}\",\n",
    "        score_col_name=f\"score_{PromptType.WITH_TOOL_CALLS.value}\",\n",
    "    ),\n",
    "    ClientConfig(\n",
    "        name=PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "        col_name=f\"response_{PromptType.WITH_STRICT_TOOL_CALLS.value}\",\n",
    "        score_col_name=f\"score_{PromptType.WITH_STRICT_TOOL_CALLS.value}\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eb319a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "\n",
    "class LLMEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        configs: List[ClientConfig],\n",
    "        create_prompt_fn: Callable,\n",
    "        response_model: BaseModel,\n",
    "        concurrency: int = MAX_CONCURRENCY,\n",
    "    ):\n",
    "        self.configs = configs\n",
    "        self.create_prompt_fn = create_prompt_fn\n",
    "        self.response_model = response_model\n",
    "        self.concurrency = concurrency\n",
    "\n",
    "        self.error_counts: Dict[str, int] = {config.name: 0 for config in self.configs}\n",
    "        self.key_order_errors: Dict[str, int] = {\n",
    "            config.name: 0 for config in self.configs\n",
    "        }\n",
    "        self.key_missing_errors: Dict[str, int] = {\n",
    "            config.name: 0 for config in self.configs\n",
    "        }\n",
    "\n",
    "    @traceable(run_type=\"prompt\")\n",
    "    def create_prompt(\n",
    "        self,\n",
    "        question: str,\n",
    "        prompt_type: str,\n",
    "    ) -> List[dict]:\n",
    "        return self.create_prompt_fn(\n",
    "            question=question,\n",
    "            prompt_type=prompt_type,\n",
    "            response_model=self.response_model,\n",
    "        )\n",
    "\n",
    "    @traceable(run_type=\"parser\")\n",
    "    def parse_response(\n",
    "        self,\n",
    "        response: BaseModel,\n",
    "        prompt_type: str,\n",
    "    ) -> str | int:\n",
    "        if prompt_type == PromptType.WITH_TOOL_CALLS.value:\n",
    "            raw_response = str(\n",
    "                response._raw_response.candidates[0]\n",
    "                .content.parts[0]\n",
    "                .function_call.args.pb\n",
    "            )\n",
    "        elif prompt_type == PromptType.WITH_STRICT_TOOL_CALLS.value:\n",
    "            raw_response = response._raw_response.candidates[0].content.parts[0].text\n",
    "\n",
    "        reasoning_index = raw_response.find(\"reasoning\")\n",
    "        answer_index = raw_response.find(\"answer\")\n",
    "\n",
    "        if reasoning_index == -1 or answer_index == -1:\n",
    "            self.key_missing_errors[prompt_type] += 1\n",
    "        elif reasoning_index > answer_index:\n",
    "            self.key_order_errors[prompt_type] += 1\n",
    "\n",
    "        return response.final_answer\n",
    "\n",
    "    @traceable(run_type=\"chain\")\n",
    "    async def call_llm(\n",
    "        self,\n",
    "        config: ClientConfig,\n",
    "        question: str,\n",
    "    ) -> BaseModel:\n",
    "        if config.name == PromptType.WITH_TOOL_CALLS.value:\n",
    "            response = await client_tools.chat.completions.create(\n",
    "                messages=self.create_prompt(question=question, prompt_type=config.name),\n",
    "                response_model=self.response_model,\n",
    "            )\n",
    "            return response\n",
    "        elif config.name == PromptType.WITH_STRICT_TOOL_CALLS.value:\n",
    "            response = await client_json.chat.completions.create(\n",
    "                messages=self.create_prompt(question=question, prompt_type=config.name),\n",
    "                response_model=self.response_model,\n",
    "            )\n",
    "            return response\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid config name: {config.name}\")\n",
    "\n",
    "    async def process_question(\n",
    "        self,\n",
    "        question: str,\n",
    "        config: ClientConfig,\n",
    "        semaphore: Semaphore,\n",
    "        max_attempts: int = 3,\n",
    "    ) -> str | int | None:\n",
    "        async with semaphore:\n",
    "            for _ in range(max_attempts):\n",
    "                try:\n",
    "                    answer = await self.call_llm(\n",
    "                        config=config,\n",
    "                        question=question,\n",
    "                    )\n",
    "                    parsed_answer = self.parse_response(answer, config.name)\n",
    "                    if not parsed_answer:\n",
    "                        self.error_counts[config.name] += 1\n",
    "                    return parsed_answer\n",
    "                except Exception:\n",
    "                    self.error_counts[config.name] += 1\n",
    "                    print(f\"{config.name}, {question[:10]}: Retrying...\")\n",
    "                    await asyncio.sleep(1)\n",
    "                    continue\n",
    "            print(\n",
    "                f\"{config.name}, {question[:10]}: Failed to process question after {max_attempts} attempts. Set answer to null.\"\n",
    "            )\n",
    "        return None\n",
    "\n",
    "    @traceable(run_type=\"chain\")\n",
    "    async def process_questions(\n",
    "        self,\n",
    "        run_name: str,\n",
    "        questions: List[dict],\n",
    "        config: ClientConfig,\n",
    "    ) -> List[str | int | None]:\n",
    "        semaphore = Semaphore(self.concurrency)\n",
    "        tasks = [\n",
    "            self.process_question(\n",
    "                question=question[\"question\"],\n",
    "                config=config,\n",
    "                semaphore=semaphore,\n",
    "            )\n",
    "            for question in questions\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        return results\n",
    "\n",
    "    def generate_outputs(self, questions: List[dict]) -> pd.DataFrame:\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"id\": [i for i in range(len(questions))],\n",
    "                \"question\": [question[\"question\"] for question in questions],\n",
    "                \"answer\": [question[\"answer\"] for question in questions],\n",
    "            }\n",
    "        )\n",
    "        for config in self.configs:\n",
    "            responses = asyncio.run(\n",
    "                self.process_questions(\n",
    "                    run_name=config.name,\n",
    "                    questions=questions,\n",
    "                    config=config,\n",
    "                )\n",
    "            )\n",
    "            df[config.col_name] = responses\n",
    "        return df\n",
    "\n",
    "    def evaluate_outputs(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df_copy = df.copy()\n",
    "        for config in self.configs:\n",
    "            df_copy[config.score_col_name] = (\n",
    "                df_copy[\"answer\"] == df_copy[config.col_name]\n",
    "            ) * 1\n",
    "        return df_copy\n",
    "\n",
    "    def calculate_confidence_intervals(\n",
    "        self, df: pd.DataFrame, conf_level: float = 0.95\n",
    "    ) -> None:\n",
    "        print(\n",
    "            f\"Calculating confidence intervals ({conf_level}) with {len(df)} observations:\"\n",
    "        )\n",
    "        for config in self.configs:\n",
    "            score_col = config.score_col_name\n",
    "            scores = df[score_col]\n",
    "\n",
    "            if len(scores) == 0:\n",
    "                print(f\"No scores available for {score_col}\")\n",
    "                continue\n",
    "\n",
    "            mean_score = scores.mean()\n",
    "            se_score = scores.std() / np.sqrt(len(scores))\n",
    "\n",
    "            z_score = stats.norm.ppf((1 + conf_level) / 2)\n",
    "            margin_error = z_score * se_score\n",
    "            ci = [\n",
    "                max(0.0, mean_score - margin_error),\n",
    "                min(1.0, mean_score + margin_error),\n",
    "            ]\n",
    "            print(\n",
    "                f\"{score_col} - Mean: {mean_score * 100:.2f}% CI: {ci[0] * 100:.2f}% - {ci[1] * 100:.2f}%\"\n",
    "            )\n",
    "        print()\n",
    "\n",
    "    def run_paired_t_test(self, df: pd.DataFrame) -> None:\n",
    "        scores = {}\n",
    "\n",
    "        for config in self.configs:\n",
    "            score_col = config.score_col_name\n",
    "            scores[score_col] = df[score_col] * 1\n",
    "\n",
    "        for score_col_1, score_col_2 in [\n",
    "            (\"score_without_so\", \"score_with_so_tool_calls\"),\n",
    "            (\"score_without_so\", \"score_with_so_strict_tool_calls\"),\n",
    "        ]:\n",
    "            if score_col_1 in scores and score_col_2 in scores:\n",
    "                t_stat, p_value = stats.ttest_rel(\n",
    "                    scores[score_col_1], scores[score_col_2]\n",
    "                )\n",
    "                print(f\"{score_col_1} vs {score_col_2}\")\n",
    "                print(f\"t-statistic: {t_stat}, p-value: {p_value}\")\n",
    "\n",
    "    def report_error_counts(self) -> None:\n",
    "        print(\"Error counts:\")\n",
    "        for config in self.configs:\n",
    "            name = config.name\n",
    "            errors = self.error_counts.get(name, 0)\n",
    "            key_order = self.key_order_errors.get(name, 0)\n",
    "            print(f\"- {name}: {errors} processing errors, {key_order} key order errors\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13cccb1",
   "metadata": {},
   "source": [
    "## GSM8K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656a5248",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ff85876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "\n",
    "class ResponseGSM8K(BaseModel):\n",
    "    reasoning: str = Field(description=\"step by step reasoning about the answer\")\n",
    "    answer: int = Field(description=\"final answer\")\n",
    "\n",
    "\n",
    "def create_prompt_gsm8k(\n",
    "    prompt_type: str,\n",
    "    question: str,\n",
    "    response_model: ResponseGSM8K | None = None,\n",
    "    zero_shot: bool = False,\n",
    ") -> str:\n",
    "    system_prompt = dedent(\"\"\"\n",
    "    You are an expert in solving grade school math tasks. You will be presented with a grade-school math word problem and be asked to solve it.\n",
    "\n",
    "    First, provide your step by step reasoning in the \"reasoning\" field. Then, in the \"answer\" field, provide an integer that corresponds to the correct answer to the question. Don't include any other text in the \"answer\" field. \n",
    "    \"\"\")\n",
    "\n",
    "    examples = [\n",
    "        (\n",
    "            \"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\",\n",
    "            \"There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6.\",\n",
    "            6,\n",
    "        ),\n",
    "        (\n",
    "            \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\",\n",
    "            \"There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5.\",\n",
    "            5,\n",
    "        ),\n",
    "        (\n",
    "            \"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\",\n",
    "            \"Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39.\",\n",
    "            39,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    if not zero_shot:\n",
    "        system_prompt += \"\\nExamples:\" if examples else \"\"\n",
    "        for i, (example_q, example_reason, example_ans) in enumerate(examples):\n",
    "            system_prompt += f\"\\n\\n**{i+1}**\\nQuestion: {example_q}\"\n",
    "            if prompt_type in [\n",
    "                PromptType.WITH_TOOL_CALLS.value,\n",
    "                PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "            ]:\n",
    "                response = (\n",
    "                    f'{{\"reasoning\": \"{example_reason}\", \"answer\": {example_ans}}}'\n",
    "                )\n",
    "            else:\n",
    "                response = f\"{example_reason}\\nANSWER: {example_ans}\"\n",
    "            system_prompt += f\"\\nAssistant Response:\\n{response}\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question: {question}\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "\n",
    "create_prompt_gsm8k_zero_shot = partial(create_prompt_gsm8k, zero_shot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8bb4c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "evals = [\n",
    "    {\n",
    "        \"question\": d[\"question\"],\n",
    "        \"answer\": int(d[\"answer\"].split(\"#### \")[1].replace(\",\", \"\").strip()),\n",
    "    }\n",
    "    for d in dataset[\"test\"]\n",
    "]\n",
    "\n",
    "if USE_SAMPLE:\n",
    "    evals = evals[:SAMPLE_SIZE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0409fd08",
   "metadata": {},
   "source": [
    "### Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86cd8588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error counts:\n",
      "- with_so_tool_calls: 1 processing errors, 0 key order errors\n",
      "- with_so_strict_tool_calls: 2 processing errors, 0 key order errors\n",
      "\n",
      "Calculating confidence intervals (0.95) with 200 observations:\n",
      "score_with_so_tool_calls - Mean: 39.50% CI: 32.71% - 46.29%\n",
      "score_with_so_strict_tool_calls - Mean: 95.00% CI: 91.97% - 98.03%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "evaluator = LLMEvaluator(\n",
    "    configs=CONFIGS,\n",
    "    create_prompt_fn=create_prompt_gsm8k_zero_shot,\n",
    "    response_model=ResponseGSM8K,\n",
    ")\n",
    "\n",
    "df = evaluator.generate_outputs(evals)\n",
    "df_results = evaluator.evaluate_outputs(df)\n",
    "\n",
    "evaluator.report_error_counts()\n",
    "evaluator.calculate_confidence_intervals(df_results)\n",
    "evaluator.run_paired_t_test(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
