{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9548f597",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"How to use synthetic data to bootstrap your RAG system evals\"\n",
    "date: 2025-07-11\n",
    "date-modified: 2025-07-11\n",
    "description-meta: \"How to use synthetic data to build a RAG system\"\n",
    "categories:\n",
    "  - llm\n",
    "  - python\n",
    "  - rag \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb7288c",
   "metadata": {},
   "source": [
    "I recently took part in Hamel and Shreya's [AI Evals for Engineers & PMs](https://maven.com/parlance-labs/evals). I couldn't go through the lessons live, but I've been catching up on the recordings and the course materials in the last few weeks.\n",
    "\n",
    "It's a course packed with high-quality knowledge that you won't find elsewhere. If you want to learn how to improve your LLM systems, I don't think you can do better than this course.\n",
    "\n",
    "I've worked in many AI projects, but I still hadn't had the chance to generate synthetic data to bootstrap my Retrieval Augmented Generation (RAG) system evals. So I thought it'd be fun to try it out.\n",
    "\n",
    "In this article, I'll walk you through the steps you need to take to generate synthetic data for your RAG system evals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3aeba9",
   "metadata": {},
   "source": [
    "## How it works\n",
    "\n",
    "The process goes as follows:\n",
    "\n",
    "1. Index your documents in a vector database.\n",
    "2. Sample a few documents from the vector database.\n",
    "3. Using the sampled documents extract a unambiguous **fact** and make a **question** about it. \n",
    "4. Filter the generated questions to remove the ones that don't seem realistic.\n",
    "5. Use the filtered dataset to calculate the evals of your RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb11674",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "If you plan to follow along, you'll need to:\n",
    "\n",
    "1. Sign up and generate an API key in [OpenAI](https://platform.openai.com/docs/overview).\n",
    "2. Set the API key as an environment variable called `OPENAI_API_KEY`.\n",
    "3. Create a virtual environment in Python (I use [`uv`](https://docs.astral.sh/uv/)) and install the following packages: `langchain`, `langchain-openai`, `langchain-community`, `jupyter`, `chromadb`, `python-dotenv`, `nest_asyncio`, and `sentence-transformers`.\n",
    "4. Download the People Group's section from GitLab's [handbook](https://gitlab.com/gitlab-com/content-sites/handbook/-/tree/main/content/handbook/people-group).\n",
    "\n",
    "Also, I'm assuming you're familiar with the basics of RAG systems and how to use vector databases. If you need a refresher, you can check out my [RAG post](https://dylancastillo.co/posts/what-is-rag.html).\n",
    "\n",
    "Then, you'll be able to run the code from this article. If you don't want to copy and paste the code, you can  download this [notebook](https://github.com/dylanjcastillo/blog/tree/main/posts/synthetic-data-rag.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6458947",
   "metadata": {},
   "source": [
    "## Setup and loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48152517",
   "metadata": {},
   "source": [
    "We'll use `asyncio` in some of the code snippets, so you'll need to enable `nest_asyncio` to run this notebook: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e2a708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b0d002",
   "metadata": {},
   "source": [
    "Then, you can proceed as usual, importing the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f58b9d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import random\n",
    "from textwrap import dedent\n",
    "\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_text_splitters import MarkdownTextSplitter\n",
    "from langsmith import Client, traceable\n",
    "from pydantic import BaseModel\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aea2fa",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "source": [
    "These are the main libraries you'll need for this article:\n",
    "\n",
    "- **chromadb**: Vector database for storing and retrieving document embeddings\n",
    "- **langchain**: Framework for building LLM applications\n",
    "- **langchain-openai**: Wrapper for OpenAI's API, providing access to LLMs and embeddings\n",
    "- **pydantic**: Provides models for generating structured data and validating types\n",
    "- **sentence-transformers**: In the last section of the article, we'll use this library to rerank the retrieved documents. \n",
    "\n",
    "The rest of the libraries are used for typical Python tasks, such as reading files, managing environment variables, etc. \n",
    "\n",
    "Next, you can read the data from the People Group's section of GitLab's handbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a8c9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DirectoryLoader(\n",
    "    \"../data/synthetic-data-rag/people-group/\", glob=\"**/*.md\", loader_cls=TextLoader\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054c3e88",
   "metadata": {},
   "source": [
    "If you set everything up correctly, you should see the number of documents in the `docs` variable. Depending on when you read this article, the number of documents may change, as the handbook is updated regularly. When I got the data, there were 99 documents in the `docs` variable.\n",
    "\n",
    "Then, you must add the data to the vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2fe2a3",
   "metadata": {},
   "source": [
    "## Index data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97203439",
   "metadata": {},
   "source": [
    "A **vector database** is a database designed to efficiently store and query data as vector embeddings (numerical representations). Provided with a user query, it's the engine you use to find the most similar data in your database.\n",
    "\n",
    "For this tutorial, we'll use [ChromaDB](https://www.trychroma.com/). So, let's set it up: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7df6e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_ef = OpenAIEmbeddingFunction(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "client = chromadb.PersistentClient(\n",
    "    path=\"../data/synthetic-data-rag/chroma\",\n",
    ")\n",
    "collection = client.get_or_create_collection(\n",
    "    \"gitlab-handbook\", embedding_function=openai_ef\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7ec784",
   "metadata": {},
   "source": [
    "This will do three things:\n",
    "\n",
    "1. Define an embedding function that uses OpenAI's API to generate embeddings for the documents.\n",
    "2. Create a ChromaDB client to interact with the vector database.\n",
    "3. Create a collection in the vector database to store the document embeddings and set the embedding function to use the one we defined earlier.\n",
    "\n",
    "Then, you must split the documents into smaller chunks. This isn't strictly necessary if you have short documents (depending on your embedding model), but it's a good practice to ensure that the embeddings are more meaningful and that the vector database can retrieve relevant chunks of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10523d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = MarkdownTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4o\",\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c21e26",
   "metadata": {},
   "source": [
    "Luckily for us, GitLab's handbook is already in Markdown, so we can use the `MarkdownTextSplitter` to split the documents. This will use the headings in the files for the splitting in addition to the length. This generally results in better chunks, as they will be more likely to contain complete thoughts or sections of the document.\n",
    "\n",
    "We'll use the `tiktoken` encoder to do the splits based on the number of tokens (not characters) and set a chunk size of 400 tokens, with no overlap.\n",
    "\n",
    "Then, because you'll likely get a significant number of chunks, let's define a utility function that lets you add the chunks to the vector database in batches. This will help you avoid hitting the API rate limits: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5cc250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(ids, documents, metadatas, batch_size=100):\n",
    "    batches = []\n",
    "    for i in range(0, len(ids), batch_size):\n",
    "        batch_ids = ids[i : i + batch_size]\n",
    "        batch_documents = documents[i : i + batch_size]\n",
    "        batch_metadatas = metadatas[i : i + batch_size]\n",
    "        batches.append((batch_ids, batch_metadatas, batch_documents))\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33702c69",
   "metadata": {},
   "source": [
    "Using this function, you can add the chunks to the vector database in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3287eb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection already exists, skipping creation.\n"
     ]
    }
   ],
   "source": [
    "ids = [f\"{str(i)}\" for i in range(len(splits))]\n",
    "documents = [doc.page_content for doc in splits]\n",
    "metadatas = [doc.metadata for doc in splits]\n",
    "\n",
    "\n",
    "if collection.count() > 0:\n",
    "    print(\"Collection already exists, skipping creation.\")\n",
    "else:\n",
    "    print(\"Adding documents...\")\n",
    "    batches = create_batches(ids=ids, documents=documents, metadatas=metadatas)\n",
    "    for i, batch in enumerate(batches):\n",
    "        print(f\"Adding batch {i} of size {len(batch[0])}\")\n",
    "        collection.add(ids=batch[0], metadatas=batch[1], documents=batch[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c757db8",
   "metadata": {},
   "source": [
    "This should take a few seconds to run. Once it's done, you're vector database will be ready to use.\n",
    "\n",
    "Let's define a couple of functions to query the vector database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb0baae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievedDoc(BaseModel):\n",
    "    id: str\n",
    "    path: str\n",
    "    page_content: str\n",
    "\n",
    "\n",
    "def get_similar_docs(text: str, top_k: int = 5) -> list[RetrievedDoc]:\n",
    "    results = collection.query(query_texts=[text], n_results=top_k)\n",
    "    docs = [results[\"documents\"][0][i] for i in range(top_k)]\n",
    "    metadatas = [results[\"metadatas\"][0][i] for i in range(top_k)]\n",
    "    ids = [results[\"ids\"][0][i] for i in range(top_k)]\n",
    "    return [\n",
    "        RetrievedDoc(id=id_, path=m[\"source\"], page_content=d)\n",
    "        for d, m, id_ in zip(docs, metadatas, ids)\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_doc_by_id(doc_id: str) -> RetrievedDoc:\n",
    "    results = collection.get(ids=[doc_id])\n",
    "    doc = results[\"documents\"][0]\n",
    "    metadata = results[\"metadatas\"][0]\n",
    "    return RetrievedDoc(id=doc_id, path=metadata[\"source\"], page_content=doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115847fd",
   "metadata": {},
   "source": [
    "These functions will let you either get the most similar documents to a user query or retrieve a document by its ID.\n",
    "\n",
    "With this in place, we can now generate a sample of documents we'll use to generate the synthetic data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4db2a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_docs_idx = random.sample(range(len(splits)), 200)\n",
    "golden_docs = [get_doc_by_id(str(i)) for i in golden_docs_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a583132",
   "metadata": {},
   "source": [
    "This will result in 200 documents that we'll use to generate the synthetic data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf323e",
   "metadata": {},
   "source": [
    "## Generate QA Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe436fe1",
   "metadata": {},
   "source": [
    "Then, using the documents you just sampled, you can generate the synthetic data. For each document, you'll extract a fact and generate a question from it. Hamel and Shreya mention that this might result in questions that are too easy to answer, which might not be ideal for evaluating your RAG system.\n",
    "\n",
    "To create more challenging synthetic queries, they recommend adding similar chunks to the generation process, so that it can generate questions that are uniquely answered by the target chunk but also include themes or keywords that are present in other chunks.\n",
    "\n",
    "Here's an example of how this works:\n",
    "\n",
    "**Target chunk:** \"George Orwell's masterpiece, *Nineteen Eighty-Four*, was published in June 1949 and introduced the concept of 'Big Brother' to a global audience.\"\n",
    "\n",
    "**Similar chunks:** \n",
    "\n",
    "    1. \"Aldous Huxley's *Brave New World*, another influential work of dystopian fiction, was first released in 1932 and explores themes of social conditioning and control.\"\n",
    "    2. \"Ray Bradbury's *Fahrenheit 451*, published in 1953, depicts a future society where books are banned and 'firemen' burn any that are found.\"\n",
    "\n",
    "**Synthetic Question:** \"In what year was the dystopian novel that introduced the concept of 'Big Brother' published?\"\n",
    "\n",
    "The target chunk helps the generator come up with a synthetic question. The similar chunks provide distractors that help the generator include themes or keywords that are also present in other chunks (e.g., dystopian fiction), making the question more challenging.\n",
    "\n",
    "To do this, you can use the following prompts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582a5b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_generate = dedent(\n",
    "    \"\"\"\n",
    "    You are a helpful assistant generating synthetic QA pairs for retrieval evaluation.\n",
    "\n",
    "    Given a target chunk of text and a set of confounding chunks, you must extract a specific, self-contained fact from the target chunk that is not included in the confounding chunks. Then write a question that is directly and unambiguously answered by that fact. The question should only be answered by the fact extracted from the target chunk (and not by any of the confounding chunks) but it should also use themes or terminology that is present in the confounding chunks.\n",
    "\n",
    "    Always respond with a JSON object with the following keys (in that exact order):\n",
    "    1. \"fact\": \"<the fact extracted from the target chunk>\",\n",
    "    2. \"confounding_terms\": \"<a list of terms or themes from the confounding chunks that are relevant to the question>\",\n",
    "    3. \"question\": \"<the question that is directly and unambiguously answered by the fact>\",\n",
    "    \n",
    "    You should write the questions as if you're an employee looking for information in the handbook. The question should be as realistic and natural as possible, reflecting the kind of queries an employee might actually make when searching for information in the handbook.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "user_prompt_generate = dedent(\n",
    "    \"\"\"\n",
    "    TARGET CHUNK:\n",
    "    {target_chunk}\n",
    "\n",
    "    CONFOUNDING CHUNKS:\n",
    "    {confounding_chunks} \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c0808f",
   "metadata": {},
   "source": [
    "These prompts will be used to generate the synthetic data. The `system_prompt_generate` defines the generation process in the same way we defined earlier, and `user_prompt_generate` provides the required context: target and confounding chunks. \n",
    "\n",
    "Then, you initialize the LLM, set up the response model, and define a function to format the documents for the LLM: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aab841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response(BaseModel):\n",
    "    fact: str\n",
    "    confounding_terms: list[str] = []\n",
    "    question: str\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=1)\n",
    "llm_with_structured_output = llm.with_structured_output(Response)\n",
    "\n",
    "messages = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt_generate), (\"user\", user_prompt_generate)]\n",
    ")\n",
    "\n",
    "def format_docs(chunks: list[RetrievedDoc]) -> str:\n",
    "    return \"\\n\".join(\n",
    "        [f\"*** Filepath: {chunk.path} ***\\n{chunk.page_content}\\n\" for chunk in chunks]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9e04fb",
   "metadata": {},
   "source": [
    "Finally, you can define a function to generate the synthetic data. This function will take a chunk, retrieve the most similar chunks, and generate a question from the target chunk using the similar chunks as distrctors: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248ec536",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_qa_pair(chunk):\n",
    "    similar_chunks = get_similar_docs(chunk.page_content)\n",
    "    compiled_messages = await messages.ainvoke(\n",
    "        {\n",
    "            \"target_chunk\": format_docs([similar_chunks[0]]),\n",
    "            \"confounding_chunks\": format_docs(similar_chunks[1:]),\n",
    "        }\n",
    "    )\n",
    "    output = await llm_with_structured_output.ainvoke(compiled_messages)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c92bc8",
   "metadata": {},
   "source": [
    "To speed up question generation, you can run this concurrently using asyncio.\n",
    "\n",
    "Here's how you can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3cf7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [generate_qa_pair(random_split) for random_split in golden_docs]\n",
    "qa_pairs = await asyncio.gather(*tasks)\n",
    "\n",
    "df = pd.DataFrame([qa_pair.dict() for qa_pair in qa_pairs])\n",
    "df.to_excel(\"../data/synthetic-data-rag/files/qa_pairs.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1264cd",
   "metadata": {},
   "source": [
    "Here are some of the resulting QA pairs:\n",
    "\n",
    "1. Example 1:\n",
    "   - **Question:** How soon should managers send the results after the 360 feedback cycle closes to prepare for the feedback meeting?\n",
    "   - **Answer:** Managers should send the results of 360 feedback within 48 hours of the feedback cycle closing so they can prepare and come to the meeting with questions and discussion points.\n",
    "\n",
    "2. Example 2:\n",
    "   - **Question:** At what point in the hiring process must candidates disclose outside employment or side projects for GitLab to assess potential conflicts with their job obligations?\n",
    "   - **Answer:** Candidates at a certain stage in the recruiting process are asked to disclose outside employment, side projects, or other activities so GitLab can determine if a conflict exists with their ability to fulfill obligations to GitLab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1410de0d",
   "metadata": {},
   "source": [
    "Even though the questions seem relevant, you might've noticed that they might not really reflect how real user queries are structured.\n",
    "\n",
    "To improve that, you can provide few shot examples of real or adjusted queries. But before that, you can generate a filter that will help you remove the questions that don't seem realistic enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38ffcc1",
   "metadata": {},
   "source": [
    "## Filtering QA pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455b2b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeec315",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_rate = dedent(\n",
    "    \"\"\"\n",
    "    You are an AI assistant helping us curate a high-quality dataset of questions for evaluating an company's internal handbook. We have generated synthetic questions and need to filter out those that are unrealistic or not representative of typical user queries.\n",
    "\n",
    "    Here are examples of realistic and unrealistic user queries we have manually rated:\n",
    "\n",
    "    ### Realistic Queries (Good Examples)\n",
    "\n",
    "    * **Query:** \"What is the required process for creating a new learning hub for your team in Level Up at GitLab?\"\n",
    "        * **Explanation:** Very realistic user query. It's concise, information-seeking, and process-oriented.\n",
    "        * **Rating:** 5\n",
    "    * **Query:** \"Where is the People Operations internal handbook hosted, and how can someone gain access to it?\"\n",
    "        * **Explanation:** Realistic query but might be a bit too detailed for a typical user.\n",
    "        * **Rating:** 4\n",
    "    * **Query:** \"Who controls access to People Data in the data warehouse at GitLab, and what approvals are required for Analytics Engineers and Data Analysts to obtain access?\"\n",
    "        * **Explanation:** Seems reasonable but too lengthy for a typical user query. \n",
    "        * **Rating:** 3\n",
    "\n",
    "    ### Unrealistic Queries (Bad Examples)\n",
    "\n",
    "    * **Query:** \"If a GitLab team member has been with the company for over 3 months and is interested in participating in the Onboarding Buddy Program, what should they do to express their interest?\"\n",
    "        * **Explanation:** Overly specific and unnatural. No real user would ask this.\n",
    "        * **Rating:** 1\n",
    "    * **Query:** \"On what date did the 'Managing Burnout with Time Off with John Fitch' session occur as part of the FY21 Learning Speaker Series?\"\n",
    "        * **Explanation:** Irrelevant and overly specific. Not a typical user query. \n",
    "        * **Rating:** 2\n",
    "\n",
    "    ### Your Task\n",
    "\n",
    "    For the following generated question, please:\n",
    "\n",
    "    1.  Rate its realism as a typical user query for an internal handbook application on a scale of 1 to 5 (1 = Very Unrealistic, 3 = Neutral/Somewhat Realistic, 5 = Very Realistic).\n",
    "    2.  Provide a brief explanation for your rating, comparing it to the examples above if helpful.\n",
    "\n",
    "    ### Output Format\n",
    "\n",
    "    **Explanation:** `[Your brief explanation]`\n",
    "    **Rating:** `[Your 1–5 rating]`\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "user_prompt_rate = dedent(\n",
    "    \"\"\"\n",
    "    **Generated Question to Evaluate:**\n",
    "    `{question_to_evaluate}`\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0929e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseFiltering(BaseModel):\n",
    "    explanation: str\n",
    "    rating: int\n",
    "\n",
    "\n",
    "llm_with_structured_output_filtering = llm.with_structured_output(ResponseFiltering)\n",
    "\n",
    "messages_filtering = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt_rate), (\"user\", user_prompt_rate)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5b953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def rate_qa_pair(qa_pair):\n",
    "    compiled_messages = await messages_filtering.ainvoke(\n",
    "        {\"question_to_evaluate\": qa_pair.question}\n",
    "    )\n",
    "    output = await llm_with_structured_output_filtering.ainvoke(compiled_messages)\n",
    "    return output\n",
    "\n",
    "\n",
    "tasks = [rate_qa_pair(qa_pair) for qa_pair in qa_pairs]\n",
    "results = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a7e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rated_qa_pairs = [\n",
    "    {\n",
    "        \"rating\": result.rating,\n",
    "        \"explanation\": result.explanation,\n",
    "        \"question\": qa_pair.question,\n",
    "        \"answer\": qa_pair.fact,\n",
    "    }\n",
    "    for (result, qa_pair) in zip(results, qa_pairs)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1921d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rated_qa_pairs = pd.DataFrame(\n",
    "    rated_qa_pairs, columns=[\"Rating\", \"Explanation\", \"Question\"]\n",
    ")\n",
    "\n",
    "df_rated_qa_pairs.to_excel(\n",
    "    \"../data/synthetic-data-rag/files/rated_qa_pairs.xlsx\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa79d22",
   "metadata": {},
   "source": [
    "## Evaluating RAG system "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7ad169",
   "metadata": {},
   "source": [
    "Now that we have the filtered QA pairs, it's time to evaluate your RAG system.\n",
    "\n",
    "You'll evaluate two parts of the RAG system: retrieval and generation.\n",
    "\n",
    "Start by creating a dataset on LangSmith:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfdfe231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists, skipping creation.\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "\n",
    "langsmith_client = Client()\n",
    "dataset_name = \"Gitlab Handbook QA Evaluation 2\"\n",
    "\n",
    "try:\n",
    "    dataset = langsmith_client.create_dataset(dataset_name=dataset_name)\n",
    "    examples = [\n",
    "        {\n",
    "            \"inputs\": {\n",
    "                \"question\": h[\"question\"],\n",
    "            },\n",
    "            \"outputs\": {\n",
    "                \"answer\": h[\"answer\"],\n",
    "                \"doc\": {\n",
    "                    \"id\": chunk.id,\n",
    "                    \"path\": chunk.path,\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "        for h, chunk in zip(rated_qa_pairs, golden_docs)\n",
    "        if h[\"rating\"] >= 5\n",
    "    ]\n",
    "    langsmith_client.create_examples(dataset_id=dataset.id, examples=examples)\n",
    "except Exception:\n",
    "    print(\"Dataset already exists, skipping creation.\")\n",
    "    dataset = langsmith_client.read_dataset(dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a43d34",
   "metadata": {},
   "source": [
    "### Retrieval Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88b0b84",
   "metadata": {},
   "source": [
    "To evaluate the retrieval part of the RAG system, you can use metrics such as recall@k, precision@k, Mean Reciprocal Rank (MRR), and Normalized Discounted Cumulative Gain (NDCG).\n",
    "\n",
    "For this tutorial, you'll use two metrics: MRR and recall@k.\n",
    "\n",
    "**MRR** (Mean Reciprocal Rank) is a measure of how well the RAG system retrieves relevant documents. It calculates the average of the reciprocal ranks of the first relevant document for each query. It essentially measures how quickly the system retrieves the first relevant document for a given query.\n",
    "\n",
    "For example, if a relevant document appears at position 1, the MRR contribution is 1/1 = 1. If it appears at position 3, the contribution is 1/3 ≈ 0.33. The formula is:\n",
    "\n",
    "$$MRR = \\frac{1}{|Q|} \\sum_{i=1}^{|Q|} \\frac{1}{rank_i}$$\n",
    "\n",
    "where $|Q|$ is the number of queries and $rank_i$ is the position of the first relevant document for query $i$.\n",
    "\n",
    "**Recall@k** measures the proportion of relevant documents retrieved in the top k results. It helps you understand how many relevant documents are retrieved by the RAG system.\n",
    "\n",
    "For example, if you retrieve 5 documents and 3 of them are relevant, recall@5 is 3/5 = 0.6. The formula is: \n",
    "\n",
    "$$Recall@k = \\frac{|\\text{relevant documents in top k}|}{|\\text{total relevant documents}|}$$\n",
    "\n",
    "where $|\\text{relevant documents in top k}|$ is the number of relevant documents retrieved in the top k results, and $|\\text{total relevant documents}|$ is the total number of relevant documents for the query.\n",
    "\n",
    "In our case, given that we only have one relevant document per query, recall@k will be 1 if the relevant document is in the top k results, and 0 otherwise.\n",
    "\n",
    "You can define two LangSmith evaluators to calculate these metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07239efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr(inputs: dict, outputs: dict, reference_outputs: dict) -> float:\n",
    "    reference_docs = [str(reference_outputs[\"doc\"][\"id\"])]\n",
    "    docs = outputs.get(\"docs\", [])\n",
    "    if not docs:\n",
    "        return 0.0\n",
    "    rank = next((i + 1 for i, doc in enumerate(docs) if doc in reference_docs), None)\n",
    "    return 1.0 / rank if rank else 0.0\n",
    "\n",
    "\n",
    "def recall(inputs: dict, outputs: dict, reference_outputs: dict) -> float:\n",
    "    reference_docs = [str(reference_outputs[\"doc\"][\"id\"])]\n",
    "    docs = outputs.get(\"docs\", [])\n",
    "    if not docs:\n",
    "        return 0.0\n",
    "    return float(any(doc in reference_docs for doc in docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc8c45d",
   "metadata": {},
   "source": [
    "LangSmith evaluators take `inputs`, `outputs`, and `reference_outputs` as arguments. The `inputs` are the user query and the retrieved documents, the `outputs` are the generated answers, and the `reference_outputs` are the target chunks.\n",
    "\n",
    "Then using those values, you can calculate the MRR and recall@k metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c074a4",
   "metadata": {},
   "source": [
    "### Generation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd7f776",
   "metadata": {},
   "source": [
    "To measure the generation quality, Hamel and Shreya recommend using [ARES](https://github.com/stanford-futuredata/ARES) or [RAGAS](https://github.com/explodinggradients/ragas).\n",
    "\n",
    "ARES requires a human preference validation set of at least 50 examples and the standard RAGAS metrics consume tons of tokens. So for the sake of simplicity, we'll build 3 simple metrics using an LLM judge, similar to RAGAS' [Nvidia metrics](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/nvidia_metrics/).\n",
    "\n",
    "- **Answer accuracy**: Measures how accurate the generated answer is compared to the target chunk.\n",
    "- **Context relevance**: Measures if the context provided to the LLM is relevant to the user query.\n",
    "- **Grounded**: Measures if the generated answer is grounded in the provided context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604a2d2d",
   "metadata": {},
   "source": [
    "#### Answer accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd0b5cf",
   "metadata": {},
   "source": [
    "We'll use an LLM judge that evaluates the generated answer against a reference answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07f60491",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_answer_accuracy = dedent(\n",
    "    \"\"\"\n",
    "    You are an expert evaluator. Your task is to evaluate the accuracy of a User Answer against a Reference Answer, given a Question.\n",
    "\n",
    "    Here's the grading scale you must use:\n",
    "\n",
    "    0 - If User Answer is not contained in Reference Answer or not accurate in all terms, topics, numbers, metrics, dates and units or the User Answer do not answer the question.\n",
    "    2 - If User Answer is full contained and equivalent to Reference Answer in all terms, topics, numbers, metrics, dates and units.\n",
    "    1 - If User Answer is partially contained and almost equivalent to Reference Answer in all terms, topics, numbers, metrics, dates and units.\n",
    "\n",
    "    Your rating must be only 0, 1 or 2 according to the instructions above.\n",
    "\n",
    "    Your answer must be a JSON object with the following keys:\n",
    "    1. \"explanation\": \"<a brief explanation of your rating>\",\n",
    "    2. \"rating\": \"<your rating, which must be one of the following: 0, 1, 2>\"\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "user_prompt_answer_accuracy = dedent(\n",
    "    \"\"\"\n",
    "    **Question:** `{question}`\n",
    "    **User Answer:** `{user_answer}`\n",
    "    **Reference Answer:** `{reference_answer}`\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "messages_answer_accuracy = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt_answer_accuracy), (\"user\", user_prompt_answer_accuracy)]\n",
    ")\n",
    "\n",
    "\n",
    "class ResponseAnswerAccuracy(BaseModel):\n",
    "    explanation: str\n",
    "    rating: int\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "\n",
    "llm_with_structured_output_answer_accuracy = llm.with_structured_output(\n",
    "    ResponseAnswerAccuracy\n",
    ")\n",
    "\n",
    "\n",
    "async def answer_accuracy(\n",
    "    inputs: dict, outputs: dict, reference_outputs: dict\n",
    ") -> float:\n",
    "    compiled_messages = await messages_answer_accuracy.ainvoke(\n",
    "        {\n",
    "            \"question\": inputs[\"question\"],\n",
    "            \"user_answer\": outputs[\"answer\"],\n",
    "            \"reference_answer\": reference_outputs[\"answer\"],\n",
    "        }\n",
    "    )\n",
    "    output = await llm_with_structured_output_answer_accuracy.ainvoke(compiled_messages)\n",
    "    return output.rating / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa93718c",
   "metadata": {},
   "source": [
    "#### Context relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9381dd",
   "metadata": {},
   "source": [
    "Then, we'll create another LLM judge that evaluates if the context provided to the LLM is relevant to the user query.\n",
    "\n",
    "These are the prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82c8f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_context_relevance = dedent(\n",
    "    \"\"\"\n",
    "    You are an expert evaluator. Your task is to evaluate the relevance of a Context in order to answer a Question. \n",
    "\n",
    "    Do not rely on your previous knowledge about the Question. Use only what is written in the Context and in the Question.\n",
    "\n",
    "    Here's the grading scale you must use:\n",
    "\n",
    "    0 - If the context does not contain any relevant information to answer the question.\n",
    "    1 - If the context partially contains relevant information to answer the question.\n",
    "    2 - If the context contains relevant information to answer the question.\n",
    "\n",
    "    You must always provide the relevance score of 0, 1, or 2, nothing else.\n",
    "\n",
    "    Your answer must be a JSON object with the following keys:\n",
    "    1. \"explanation\": \"<a brief explanation of your rating>\",\n",
    "    2. \"rating\": \"<your rating, which must be one of the following: 0, 1, 2>\"\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "user_prompt_context_relevance = dedent(\n",
    "    \"\"\"\n",
    "    **Question:** `{question}`\n",
    "    **Context:** `{context}`\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "messages_context_relevance = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt_context_relevance),\n",
    "        (\"user\", user_prompt_context_relevance),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class ResponseContextRelevance(BaseModel):\n",
    "    explanation: str\n",
    "    rating: int\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "llm_with_structured_output_context_relevance = llm.with_structured_output(\n",
    "    ResponseContextRelevance\n",
    ")\n",
    "\n",
    "\n",
    "async def context_relevance(\n",
    "    inputs: dict, outputs: dict, reference_outputs: dict\n",
    ") -> float:\n",
    "    compiled_messages = await messages_context_relevance.ainvoke(\n",
    "        {\n",
    "            \"question\": inputs[\"question\"],\n",
    "            \"context\": outputs[\"context\"],\n",
    "        }\n",
    "    )\n",
    "    output = await llm_with_structured_output_context_relevance.ainvoke(\n",
    "        compiled_messages\n",
    "    )\n",
    "    return output.rating / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d5f4b6",
   "metadata": {},
   "source": [
    "#### Groundedness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901f135c",
   "metadata": {},
   "source": [
    "Finally, we'll create a judge that evaluates if the generated answer is grounded in the provided context.\n",
    "\n",
    "These are the prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32cef827",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_groundedness = dedent(\n",
    "    \"\"\"\n",
    "    You are an expert evaluator. Your task is to evaluate the groundedness of an assertion against a context. \n",
    "\n",
    "    Do not rely on your previous knowledge about the assertion or context. Use only what is written in the assertion and in the context.\n",
    "\n",
    "    Here's the grading scale you must use:\n",
    "\n",
    "    0 - If the assertion is not supported by the context. Or, if the context or assertion is empty.\n",
    "    1 - If the context partially contains relevant information to support the assertion.\n",
    "    2 - If the context fully supports the assertion.\n",
    "\n",
    "    You must always provide the relevance score of 0, 1, or 2, nothing else.\n",
    "\n",
    "    Your answer must be a JSON object with the following keys:\n",
    "\n",
    "    1. \"explanation\": \"<a brief explanation of your rating>\",\n",
    "    2. \"rating\": \"<your rating, which must be one of the following: 0, 1, 2>\"\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "user_prompt_groundedness = dedent(\n",
    "    \"\"\"\n",
    "    **Assertion:** `{answer}`\n",
    "    **Context:** `{context}`\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "messages_groundedness = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt_groundedness), (\"user\", user_prompt_groundedness)]\n",
    ")\n",
    "\n",
    "\n",
    "class ResponseGroundedness(BaseModel):\n",
    "    explanation: str\n",
    "    rating: int\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "llm_with_structured_output_groundedness = llm.with_structured_output(\n",
    "    ResponseGroundedness\n",
    ")\n",
    "\n",
    "\n",
    "async def groundedness(inputs: dict, outputs: dict, reference_outputs: dict) -> float:\n",
    "    compiled_messages = await messages_groundedness.ainvoke(\n",
    "        {\n",
    "            \"answer\": outputs[\"answer\"],\n",
    "            \"context\": outputs[\"context\"],\n",
    "        }\n",
    "    )\n",
    "    output = await llm_with_structured_output_groundedness.ainvoke(compiled_messages)\n",
    "    return output.rating / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f760b10",
   "metadata": {},
   "source": [
    "### Run evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21db4338",
   "metadata": {},
   "source": [
    "Now we can run the full RAG pipeline and evaluate its results using the LangSmith evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddb7581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_generation = dedent(\n",
    "    \"\"\"\n",
    "    You're a helpful assistant. Provided with a question and the most relevant documents, you must generate a concise and accurate answer based on the information in those documents.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "user_prompt_generation = dedent(\n",
    "    \"\"\"\n",
    "    QUESTION: {question}\n",
    "\n",
    "    RELEVANT DOCUMENTS:\n",
    "    {documents}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "messages_generation = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt_generation), (\"user\", user_prompt_generation)]\n",
    ")\n",
    "\n",
    "llm_generation = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91e5b3f",
   "metadata": {},
   "source": [
    "For example, you could evaluate different values for the number of retrieved documents. I'll run this code for number of retrieved documents equal to 3, 5, and 10, and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ddd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3\n",
    "\n",
    "@traceable\n",
    "async def target(inputs: dict) -> dict:\n",
    "    relevant_docs = get_similar_docs(inputs[\"question\"], top_k=K)\n",
    "    formatted_docs = format_docs(relevant_docs)\n",
    "    messages = await messages_generation.ainvoke(\n",
    "        {\n",
    "            \"question\": inputs[\"question\"],\n",
    "            \"documents\": formatted_docs,\n",
    "        }\n",
    "    )\n",
    "    response = await llm_generation.ainvoke(messages)\n",
    "    return {\n",
    "        \"answer\": response.content,\n",
    "        \"docs\": [doc.id for doc in relevant_docs],\n",
    "        \"context\": formatted_docs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af2580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = await langsmith_client.aevaluate(\n",
    "    target,\n",
    "    data=dataset_name,\n",
    "    evaluators=[recall, mrr, answer_accuracy, context_relevance, groundedness],\n",
    "    max_concurrency=50,\n",
    "    experiment_prefix=f\"top-{K}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f196606e",
   "metadata": {},
   "source": [
    "## Improving retrieval with a reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b26aa3",
   "metadata": {},
   "source": [
    "A quick way to improve your RAG system is to rerank the retrieved documents. In addition to doing retrieval using vector similarity or keyword search, you do a reranking step that uses a slow and expensive model to rerank the retrieve documents based on their relevance to the user query. \n",
    "\n",
    "You can use the `sentence-transformers` library to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ef76069",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = CrossEncoder(\"mixedbread-ai/mxbai-rerank-xsmall-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704f4720",
   "metadata": {},
   "source": [
    "Here's an example of how to use the `sentence-transformers` library to rerank the retrieved documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "481b2e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the process for creating a new learning hub for your team in Level Up at GitLab?\"\n",
    "hits = get_similar_docs(query, top_k=50)\n",
    "cross_inp = [[query, h.page_content] for h in hits]\n",
    "reranker_scores = cross_encoder.predict(cross_inp)\n",
    "sorted_hits = sorted(hits, key=lambda x: reranker_scores[hits.index(x)], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba39e57",
   "metadata": {},
   "source": [
    "Now you can do the same evaluation as before, but using the reranked documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3411cd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "\n",
    "def get_reranked_docs(\n",
    "    query: str, similar_docs: list[RetrievedDoc]\n",
    ") -> list[RetrievedDoc]:\n",
    "    cross_inp = [[query, doc.page_content] for doc in similar_docs]\n",
    "    reranker_scores = cross_encoder.predict(cross_inp)\n",
    "    sorted_docs = sorted(\n",
    "        similar_docs, key=lambda x: reranker_scores[similar_docs.index(x)], reverse=True\n",
    "    )\n",
    "    return sorted_docs\n",
    "\n",
    "\n",
    "@traceable\n",
    "async def target_with_reranking(inputs: dict) -> dict:\n",
    "    relevant_docs = get_similar_docs(inputs[\"question\"], top_k=75)\n",
    "    reranked_docs = get_reranked_docs(inputs[\"question\"], relevant_docs)[:K]\n",
    "    formatted_docs = format_docs(reranked_docs)\n",
    "    messages = await messages_generation.ainvoke(\n",
    "        {\n",
    "            \"question\": inputs[\"question\"],\n",
    "            \"documents\": formatted_docs,\n",
    "        }\n",
    "    )\n",
    "    response = await llm.ainvoke(messages)\n",
    "    return {\n",
    "        \"answer\": response,\n",
    "        \"docs\": [doc.id for doc in reranked_docs],\n",
    "        \"context\": formatted_docs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29acba15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'top-5-reranked-9a0c8901' at:\n",
      "https://smith.langchain.com/o/197ee903-f183-50ac-ae2c-929c6a09833a/datasets/0d1ea85c-d251-4104-ae51-844b6564e7ad/compare?selectedSessions=846548f7-1a2c-48fb-a646-918bab167425\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ad63819b3146d996445b0a2cbb50e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_results = await langsmith_client.aevaluate(\n",
    "    target_with_reranking,\n",
    "    data=dataset_name,\n",
    "    evaluators=[recall, mrr, answer_accuracy, context_relevance, groundedness],\n",
    "    max_concurrency=50,\n",
    "    experiment_prefix=f\"top-{K}-reranked\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40649786",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
