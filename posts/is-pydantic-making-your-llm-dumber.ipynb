{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Is Pydantic making your LLM dumber?\"\n",
    "date: \"11/10/2024\"\n",
    "date-modified: last-modified\n",
    "description-meta: \"Structured outputs make it easy to work with LLMs, but are they impacting their performance?\"\n",
    "toc: true\n",
    "toc-depth: 3\n",
    "lightbox: true\n",
    "fig-cap-location: margin\n",
    "categories:\n",
    "  - llm\n",
    "  - openai\n",
    "  - pydantic\n",
    "  - python\n",
    "author:\n",
    "  - name: Dylan Castillo\n",
    "    url: https://dylancastillo.co\n",
    "    affiliation: Iwana Labs\n",
    "    affiliation-url: https://iwanalabs.com\n",
    "citation: true\n",
    "comments:\n",
    "  utterances:\n",
    "    repo: dylanjcastillo/blog_comments\n",
    "    theme: dark-blue\n",
    "    issue-term: pathname\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the [Let me speak freely?](https://arxiv.org/abs/2408.02442) paper was published, there's been a lot of talk about the impact of structured outputs on the performance of LLMs.\n",
    "\n",
    "The [.txt](https://dottxt.co) team wrote a very [compelling rebuttal](https://blog.dottxt.co/say-what-you-mean.html). But they focused on open-weight models, which is not what I tend to use. So, I'm not sure if their results directly translate to proprietary models. \n",
    "\n",
    "Plus, I wanted to see the performance on benchmarks that are [less likely to be part of the training data](https://arxiv.org/abs/2405.00332) of the models, such as [LiveBench](https://arxiv.org/abs/2406.19314).\n",
    "\n",
    "So that's what I'm going to do in this post!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dcast/Documents/GitHub/blog/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "\n",
    "import asyncio\n",
    "import difflib\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from asyncio import Semaphore\n",
    "from enum import Enum\n",
    "from itertools import permutations\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import instructor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from openai import AsyncOpenAI\n",
    "from openai.types.chat import ChatCompletion\n",
    "from pydantic import BaseModel, Field\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "langsmith_client = wrap_openai(AsyncOpenAI())\n",
    "instructor_client = instructor.from_openai(langsmith_client, mode=instructor.Mode.TOOLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This imports the required libraries. Two things to note:\n",
    "\n",
    "1. I'm using [LangSmith](https://smith.langchain.com/) to trace the calls to the OpenAI API. This makes it easier to debug and see the prompts that are being used.\n",
    "2. I use instructor to compare the performance of `structured outputs` using tool calls and JSON mode.\n",
    "\n",
    "Next, load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path().absolute().parent / \"data\" / \"live_bench\"\n",
    "reasoning_dir = data_dir / \"reasoning\"\n",
    "math_dir = data_dir / \"math\"\n",
    "language_dir = data_dir / \"language\"\n",
    "\n",
    "df_reasoning = pd.read_json(reasoning_dir / \"updated_questions.jsonl\", lines=True)\n",
    "df_language = pd.read_json(language_dir / \"updated_questions.jsonl\", lines=True)\n",
    "df_math = pd.read_json(math_dir / \"updated_questions.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using a slightly modified version of the reasoning, language, and math question (excluding some categories from the latter).\n",
    "\n",
    "I had to modify them because the original ones included formatting requirements that would have made the comparison less fair.\n",
    "\n",
    "You can see the changes I did [here](../scripts/is-pydantic-making-your-model-dumber/update_livebench_questions.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response(BaseModel):\n",
    "    reasoning: str = Field(description=\"Your reasoning explaining your answer.\")\n",
    "    answer: str = Field(description=\"Your answer, don't include any other text.\")\n",
    "\n",
    "\n",
    "class PromptType(Enum):\n",
    "    WITHOUT_STRUCTURED_OUTPUT = \"without_structured_output\"\n",
    "    WITH_TOOL_CALLS = \"with_structured_output_tool_calls\"\n",
    "    WITH_JSON_MODE = \"with_structured_output_json_mode\"\n",
    "\n",
    "\n",
    "SYSTEM_MESSAGE_MAPPING = {\n",
    "    PromptType.WITHOUT_STRUCTURED_OUTPUT.value: (\n",
    "        \"You're a helpful assistant. You will help me answer a question.\"\n",
    "        \"\\nYou must respond using the following format:\"\n",
    "        \"\\nREASONING: <your reasoning explaining your answer>\"\n",
    "        \"\\nANSWER: <your answer, don't include any other text>\"\n",
    "    ),\n",
    "    PromptType.WITH_TOOL_CALLS.value: (\n",
    "        \"You're a helpful assistant. You will help me answer a question.\"\n",
    "    ),\n",
    "    PromptType.WITH_JSON_MODE.value: (\n",
    "        \"You're a helpful assistant. You will help me answer a question.\"\n",
    "        + \"\\nYou must respond using the following JSON schema:\"\n",
    "        + json.dumps(Response.model_json_schema())\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I defined the three types of prompts I'm going to use:\n",
    "\n",
    "1. Without structured outputs: The LLM is given a system message that includes the format of the response and a user message with the question. But the instructions do not require a **too-rigid** structure.\n",
    "2. With structured outputs using tool calls: The LLM is given a system message without a specific format, and later on it is asked to use the `Response` pydantic model to parse its response.\n",
    "3. With structured outputs using JSON mode: The LLM is given a system message that includes the format of the response and a user message with the question. But the instructions ask the LLM to return a JSON object in the `Response` pydantic model.\n",
    "\n",
    "Finally, I defined a helper function to parse the responses from the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(\n",
    "    response: ChatCompletion | Response, response_type: PromptType\n",
    ") -> str:\n",
    "    if isinstance(response, Response):\n",
    "        return response.answer\n",
    "    elif (\n",
    "        isinstance(response, ChatCompletion)\n",
    "        and response_type == PromptType.WITHOUT_STRUCTURED_OUTPUT\n",
    "    ):\n",
    "        return response.choices[0].message.content.split(\"\\nANSWER:\")[1].strip()\n",
    "    elif (\n",
    "        isinstance(response, ChatCompletion)\n",
    "        and response_type == PromptType.WITH_JSON_MODE\n",
    "    ):\n",
    "        return Response.model_validate_json(response.choices[0].message.content).answer\n",
    "    raise ValueError(f\"Invalid response type: {type(response)}\")\n",
    "\n",
    "\n",
    "@traceable\n",
    "async def call_model(\n",
    "    client,\n",
    "    prompt_type: PromptType,\n",
    "    user_message: str,\n",
    "    timeout: int = 120,\n",
    ") -> Response:\n",
    "    params = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_MESSAGE_MAPPING[prompt_type.value]},\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "        ],\n",
    "        \"timeout\": timeout,\n",
    "    }\n",
    "    if prompt_type == PromptType.WITH_JSON_MODE:\n",
    "        params.update({\"response_format\": {\"type\": \"json_object\"}})\n",
    "    if prompt_type == PromptType.WITH_TOOL_CALLS:\n",
    "        params.update(\n",
    "            {\n",
    "                \"response_model\": Response,\n",
    "            }\n",
    "        )\n",
    "    response = await client.chat.completions.create(**params)\n",
    "    return parse_response(response, prompt_type)\n",
    "\n",
    "\n",
    "@traceable\n",
    "async def process_row(\n",
    "    row: pd.Series,\n",
    "    prompt_type: PromptType,\n",
    "    semaphore: Semaphore,\n",
    ") -> str:\n",
    "    async with semaphore:\n",
    "        for _ in range(3):\n",
    "            try:\n",
    "                answer = await call_model(\n",
    "                    client=(\n",
    "                        instructor_client\n",
    "                        if prompt_type == PromptType.WITH_TOOL_CALLS\n",
    "                        else langsmith_client\n",
    "                    ),\n",
    "                    prompt_type=prompt_type,\n",
    "                    user_message=f\"Question:\\n{row.updated_question}\",\n",
    "                )\n",
    "                return answer\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {row.name}: {e}\")\n",
    "                continue\n",
    "        raise Exception(f\"Failed to process row {row.name}, after 3 attempts\")\n",
    "\n",
    "\n",
    "@traceable\n",
    "async def process_df(\n",
    "    df: pd.DataFrame,\n",
    "    prompt_type: PromptType,\n",
    "    concurrency: int = 100,\n",
    ") -> List[str]:\n",
    "    semaphore = Semaphore(concurrency)\n",
    "    tasks = [process_row(row, prompt_type, semaphore) for _, row in df.iterrows()]\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://github.com/LiveBench/LiveBench/blob/main/livebench/process_results/writing/plot_unscrambling/utils.py\n",
    "def levenshtein_distance(A, B):\n",
    "    N, M = len(A), len(B)\n",
    "    # Create an array of size NxM\n",
    "    dp = [[0 for i in range(M + 1)] for j in range(N + 1)]\n",
    "\n",
    "    # Base Case: When N = 0\n",
    "    for j in range(M + 1):\n",
    "        dp[0][j] = j\n",
    "    # Base Case: When M = 0\n",
    "    for i in range(N + 1):\n",
    "        dp[i][0] = i\n",
    "    # Transitions\n",
    "    for i in range(1, N + 1):\n",
    "        for j in range(1, M + 1):\n",
    "            if A[i - 1] == B[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(\n",
    "                    dp[i - 1][j],  # Insertion\n",
    "                    dp[i][j - 1],  # Deletion\n",
    "                    dp[i - 1][j - 1],  # Replacement\n",
    "                )\n",
    "\n",
    "    return dp[N][M]\n",
    "\n",
    "\n",
    "def plot_unscrambling_process_results(ground_truth: str, llm_answer: str) -> float:\n",
    "    gt_sentences = [s.strip() for s in ground_truth.split(\".\")]\n",
    "    ans_sentences = [s.strip() for s in llm_answer.split(\".\")]\n",
    "\n",
    "    gt_sentences = [s for s in gt_sentences if s]\n",
    "    ans_sentences = [s for s in ans_sentences if s]\n",
    "\n",
    "    ans_ordering = []\n",
    "    for x in gt_sentences:\n",
    "        best_match = difflib.get_close_matches(x, ans_sentences, n=1, cutoff=0.0)\n",
    "        if best_match:\n",
    "            ans_ordering.append(ans_sentences.index(best_match[0]))\n",
    "\n",
    "    n_sentences_gt = len(gt_sentences)\n",
    "    raw_distance = levenshtein_distance(list(range(len(gt_sentences))), ans_ordering)\n",
    "    score = 1 - (raw_distance / n_sentences_gt)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def evaluate_language_task(ground_truth: str, task_type: str, response: str):\n",
    "    if task_type == \"connections\":\n",
    "        objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in response.split(\",\")\n",
    "        ]\n",
    "        gt_objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in ground_truth.split(\",\")\n",
    "        ]\n",
    "\n",
    "        groups = [set(objects[i : i + 4]) for i in range(0, len(objects), 4)]\n",
    "        gt_groups = [set(gt_objects[i : i + 4]) for i in range(0, len(gt_objects), 4)]\n",
    "\n",
    "        max_correct = 0\n",
    "        for perm in permutations(groups):\n",
    "            correct_groups = sum(g1 == g2 for g1, g2 in zip(perm, gt_groups))\n",
    "            max_correct = max(max_correct, correct_groups)\n",
    "        return max_correct / len(gt_groups)\n",
    "    elif task_type == \"plot_unscrambling\":\n",
    "        return plot_unscrambling_process_results(ground_truth, response)\n",
    "    elif task_type == \"typos\":\n",
    "        return ground_truth in response\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid task type: {task_type}\")\n",
    "\n",
    "\n",
    "def evaluate_reasoning_task(ground_truth: str, task_type: str, response: str):\n",
    "    if task_type == \"web_of_lies_v2\":\n",
    "        response_objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in response.split(\",\")\n",
    "        ]\n",
    "        gt_objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in ground_truth.split(\",\")\n",
    "        ]\n",
    "        return response_objects == gt_objects\n",
    "    elif task_type in (\"spatial\", \"zebra_puzzle\"):\n",
    "        response = response.rstrip(\".\")\n",
    "        return ground_truth.lower().strip() == response.lower().strip()\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid task type: {task_type}\")\n",
    "\n",
    "\n",
    "def evaluate_math_task(ground_truth: str, task_type: str, response: str):\n",
    "    if task_type == \"olympiad\":\n",
    "        response_objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in response.split(\",\")\n",
    "        ]\n",
    "        gt_objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in ground_truth.split(\",\")\n",
    "        ]\n",
    "        return response_objects == gt_objects\n",
    "    elif task_type == \"math_comp\":\n",
    "        return ground_truth == response\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid task type: {task_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "def generate_outputs(df):\n",
    "    df_copy = df.copy()\n",
    "    responses_without_so = asyncio.run(\n",
    "        process_df(df_copy, PromptType.WITHOUT_STRUCTURED_OUTPUT)\n",
    "    )\n",
    "    responses_with_so_tool_calls = asyncio.run(\n",
    "        process_df(df_copy, PromptType.WITH_TOOL_CALLS)\n",
    "    )\n",
    "    responses_with_so_json_mode = asyncio.run(\n",
    "        process_df(df_copy, PromptType.WITH_JSON_MODE)\n",
    "    )\n",
    "    df_copy[\"response_without_so\"] = responses_without_so\n",
    "    df_copy[\"response_with_so_tool_calls\"] = responses_with_so_tool_calls\n",
    "    df_copy[\"response_with_so_json_mode\"] = responses_with_so_json_mode\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def evaluate_outputs(df, evaluator):\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"score_without_so\"] = df_copy.apply(\n",
    "        lambda row: evaluator(\n",
    "            row[\"ground_truth\"], row[\"task\"], row[\"response_without_so\"]\n",
    "        ) * 1,\n",
    "        axis=1,\n",
    "    )\n",
    "    df_copy[\"score_with_so_tool_calls\"] = df_copy.apply(\n",
    "        lambda row: evaluator(\n",
    "            row[\"ground_truth\"], row[\"task\"], row[\"response_with_so_tool_calls\"]\n",
    "        ) * 1,\n",
    "        axis=1,\n",
    "    )\n",
    "    df_copy[\"score_with_so_json_mode\"] = df_copy.apply(\n",
    "        lambda row: evaluator(\n",
    "            row[\"ground_truth\"], row[\"task\"], row[\"response_with_so_json_mode\"]\n",
    "        ) * 1,\n",
    "        axis=1,\n",
    "    )\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reasoning_results = generate_outputs(df_reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reasoning_results = evaluate_outputs(df_reasoning_results, evaluate_reasoning_task)\n",
    "df_reasoning_results.to_csv(data_dir / \"reasoning\" / \"reasoning_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_questions</th>\n",
       "      <th>score_without_so</th>\n",
       "      <th>score_with_so_tool_calls</th>\n",
       "      <th>score_with_so_json_mode</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spatial</th>\n",
       "      <td>50</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>web_of_lies_v2</th>\n",
       "      <td>50</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zebra_puzzle</th>\n",
       "      <td>50</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                n_questions  score_without_so  score_with_so_tool_calls  \\\n",
       "task                                                                      \n",
       "spatial                  50              0.44                      0.44   \n",
       "web_of_lies_v2           50              0.40                      0.44   \n",
       "zebra_puzzle             50              0.36                      0.26   \n",
       "\n",
       "                score_with_so_json_mode  \n",
       "task                                     \n",
       "spatial                            0.46  \n",
       "web_of_lies_v2                     0.40  \n",
       "zebra_puzzle                       0.36  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "df_reasoning_results.groupby(\"task\").agg(\n",
    "    n_questions=(\"question_id\", \"count\"),\n",
    "    score_without_so=(\"score_without_so\", \"mean\"),\n",
    "    score_with_so_tool_calls=(\"score_with_so_tool_calls\", \"mean\"),\n",
    "    score_with_so_json_mode=(\"score_with_so_json_mode\", \"mean\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_language_results = generate_outputs(df_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_language_results = evaluate_outputs(df_language_results, evaluate_language_task)\n",
    "df_language_results.to_csv(data_dir / \"language\" / \"language_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_questions</th>\n",
       "      <th>score_without_so</th>\n",
       "      <th>score_with_so_tool_calls</th>\n",
       "      <th>score_with_so_json_mode</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>connections</th>\n",
       "      <td>50</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.441667</td>\n",
       "      <td>0.431667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plot_unscrambling</th>\n",
       "      <td>40</td>\n",
       "      <td>0.365219</td>\n",
       "      <td>0.335338</td>\n",
       "      <td>0.389255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>typos</th>\n",
       "      <td>50</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.460000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   n_questions  score_without_so  score_with_so_tool_calls  \\\n",
       "task                                                                         \n",
       "connections                 50          0.458333                  0.441667   \n",
       "plot_unscrambling           40          0.365219                  0.335338   \n",
       "typos                       50          0.640000                  0.500000   \n",
       "\n",
       "                   score_with_so_json_mode  \n",
       "task                                        \n",
       "connections                       0.431667  \n",
       "plot_unscrambling                 0.389255  \n",
       "typos                             0.460000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "df_language_results.groupby(\"task\").agg(\n",
    "    n_questions=(\"question_id\", \"count\"),\n",
    "    score_without_so=(\"score_without_so\", \"mean\"),\n",
    "    score_with_so_tool_calls=(\"score_with_so_tool_calls\", \"mean\"),\n",
    "    score_with_so_json_mode=(\"score_with_so_json_mode\", \"mean\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing row 6: list index out of range\n",
      "Error processing row 95: list index out of range\n",
      "Error processing row 84: 1 validation error for Response\n",
      "answer\n",
      "  Field required [type=missing, input_value={'reasoning': 'First, we ...rule solved amplifier.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/missing\n"
     ]
    }
   ],
   "source": [
    "df_math_results = generate_outputs(df_math)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_math_results = evaluate_outputs(df_math_results, evaluate_math_task)\n",
    "df_math_results.to_csv(data_dir / \"math\" / \"math_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_questions</th>\n",
       "      <th>score_without_so</th>\n",
       "      <th>score_with_so_tool_calls</th>\n",
       "      <th>score_with_so_json_mode</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>math_comp</th>\n",
       "      <td>96</td>\n",
       "      <td>0.427083</td>\n",
       "      <td>0.427083</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>olympiad</th>\n",
       "      <td>36</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.277778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           n_questions  score_without_so  score_with_so_tool_calls  \\\n",
       "task                                                                 \n",
       "math_comp           96          0.427083                  0.427083   \n",
       "olympiad            36          0.305556                  0.277778   \n",
       "\n",
       "           score_with_so_json_mode  \n",
       "task                                \n",
       "math_comp                 0.416667  \n",
       "olympiad                  0.277778  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "df_math_results.groupby(\"task\").agg(\n",
    "    n_questions=(\"question_id\", \"count\"),\n",
    "    score_without_so=(\"score_without_so\", \"mean\"),\n",
    "    score_with_so_tool_calls=(\"score_with_so_tool_calls\", \"mean\"),\n",
    "    score_with_so_json_mode=(\"score_with_so_json_mode\", \"mean\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "def calculate_confidence_intervals(df):\n",
    "    mean_score_without_so = df[\"score_without_so\"].mean()\n",
    "    mean_score_with_so_tool_calls = df[\"score_with_so_tool_calls\"].mean()\n",
    "    mean_score_with_so_json_mode = df[\"score_with_so_json_mode\"].mean()\n",
    "\n",
    "    n = len(df)\n",
    "    se_score_without_so = df[\"score_without_so\"].std() / np.sqrt(n)\n",
    "    se_score_with_so_tool_calls = df[\"score_with_so_tool_calls\"].std() / np.sqrt(n)\n",
    "    se_score_with_so_json_mode = df[\"score_with_so_json_mode\"].std() / np.sqrt(n)\n",
    "\n",
    "    ci_score_without_so = [\n",
    "        mean_score_without_so - 1.96 * se_score_without_so,\n",
    "        mean_score_without_so + 1.96 * se_score_without_so,\n",
    "    ]\n",
    "    ci_score_with_so_tool_calls = [\n",
    "        mean_score_with_so_tool_calls - 1.96 * se_score_with_so_tool_calls,\n",
    "        mean_score_with_so_tool_calls + 1.96 * se_score_with_so_tool_calls,\n",
    "    ]\n",
    "    ci_score_with_so_json_mode = [\n",
    "        mean_score_with_so_json_mode - 1.96 * se_score_with_so_json_mode,\n",
    "        mean_score_with_so_json_mode + 1.96 * se_score_with_so_json_mode,\n",
    "    ]\n",
    "\n",
    "    print(\n",
    "        f\"Response format without SO - Mean: {mean_score_without_so * 100:.2f}% CI: {ci_score_without_so[0] * 100:.2f}% - {ci_score_without_so[1] * 100:.2f}%\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Response format with SO tool calls - Mean: {mean_score_with_so_tool_calls * 100:.2f}% CI: {ci_score_with_so_tool_calls[0] * 100:.2f}% - {ci_score_with_so_tool_calls[1] * 100:.2f}%\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Response format with SO JSON mode - Mean: {mean_score_with_so_json_mode * 100:.2f}% CI: {ci_score_with_so_json_mode[0] * 100:.2f}% - {ci_score_with_so_json_mode[1] * 100:.2f}%\"\n",
    "    )\n",
    "\n",
    "\n",
    "def run_paired_t_test(df):\n",
    "    score_without_so = df[\"score_without_so\"] * 1\n",
    "    score_with_so_tool_calls = df[\"score_with_so_tool_calls\"] * 1\n",
    "    score_with_so_json_mode = df[\"score_with_so_json_mode\"] * 1\n",
    "\n",
    "    t_stat_without_so_tool_calls, p_value_without_so_tool_calls = stats.ttest_rel(\n",
    "        score_without_so, score_with_so_tool_calls\n",
    "    )\n",
    "    print(\"Without SO vs With SO Tool Calls\")\n",
    "    print(\n",
    "        f\"t-statistic: {t_stat_without_so_tool_calls}, p-value: {p_value_without_so_tool_calls}\"\n",
    "    )\n",
    "\n",
    "    t_stat_without_so_json_mode, p_value_without_so_json_mode = stats.ttest_rel(\n",
    "        score_without_so, score_with_so_json_mode\n",
    "    )\n",
    "    print(\"Without SO vs With SO JSON Mode\")\n",
    "    print(\n",
    "        f\"t-statistic: {t_stat_without_so_json_mode}, p-value: {p_value_without_so_json_mode}\"\n",
    "    )\n",
    "\n",
    "    print(\"With SO Tool Calls vs With SO JSON Mode\")\n",
    "    t_stat_with_so_tool_calls_json_mode, p_value_with_so_tool_calls_json_mode = (\n",
    "        stats.ttest_rel(score_with_so_tool_calls, score_with_so_json_mode)\n",
    "    )\n",
    "    print(\n",
    "        f\"t-statistic: {t_stat_with_so_tool_calls_json_mode}, p-value: {p_value_with_so_tool_calls_json_mode}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response format without SO - Mean: 40.00% CI: 32.13% - 47.87%\n",
      "Response format with SO tool calls - Mean: 38.00% CI: 30.21% - 45.79%\n",
      "Response format with SO JSON mode - Mean: 40.67% CI: 32.78% - 48.55%\n",
      "Without SO vs With SO Tool Calls\n",
      "t-statistic: 0.4189279042128261, p-value: 0.6758721078960683\n",
      "Without SO vs With SO JSON Mode\n",
      "t-statistic: -0.14238984278693576, p-value: 0.8869644448921145\n",
      "With SO Tool Calls vs With SO JSON Mode\n",
      "t-statistic: -0.5239540698784948, p-value: 0.6010889880390796\n"
     ]
    }
   ],
   "source": [
    "calculate_confidence_intervals(df_reasoning_results)\n",
    "run_paired_t_test(df_reasoning_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response format without SO - Mean: 49.66% CI: 42.82% - 56.50%\n",
      "Response format with SO tool calls - Mean: 43.21% CI: 36.32% - 50.11%\n",
      "Response format with SO JSON mode - Mean: 42.97% CI: 36.24% - 49.69%\n",
      "Without SO vs With SO Tool Calls\n",
      "t-statistic: 2.0126012234688253, p-value: 0.04608825957394367\n",
      "Without SO vs With SO JSON Mode\n",
      "t-statistic: 2.2091509685326236, p-value: 0.028801264351177552\n",
      "With SO Tool Calls vs With SO JSON Mode\n",
      "t-statistic: 0.07865871714837198, p-value: 0.9374172057953019\n"
     ]
    }
   ],
   "source": [
    "calculate_confidence_intervals(df_language_results)\n",
    "run_paired_t_test(df_language_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response format without SO - Mean: 39.39% CI: 31.03% - 47.76%\n",
      "Response format with SO tool calls - Mean: 38.64% CI: 30.30% - 46.97%\n",
      "Response format with SO JSON mode - Mean: 37.88% CI: 29.57% - 46.19%\n",
      "Without SO vs With SO Tool Calls\n",
      "t-statistic: 0.21742895818757638, p-value: 0.828212355010325\n",
      "Without SO vs With SO JSON Mode\n",
      "t-statistic: 0.40695595328491374, p-value: 0.6847044945111276\n",
      "With SO Tool Calls vs With SO JSON Mode\n",
      "t-statistic: 0.22859065962748135, p-value: 0.8195435412874758\n"
     ]
    }
   ],
   "source": [
    "calculate_confidence_intervals(df_math_results)\n",
    "run_paired_t_test(df_math_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the mean scores with confidence intervals:\n",
    "\n",
    "| **Category** | **Response Format**              | **Mean (CI %)**         |\n",
    "|--------------|----------------------------------|-------------------------|\n",
    "| **Reasoning** | Without SO                      | 42.67% (34.73 - 50.61)  |\n",
    "|               | With SO Tool Calls               | 39.33% (31.49 - 47.18)  |\n",
    "|               | With SO JSON Mode                | 44.00% (36.03 - 51.97)  |\n",
    "| **Language**  | Without SO                      | 48.91% (42.05 - 55.76)  |\n",
    "|               | With SO Tool Calls               | 44.78% (37.96 - 51.59)  |\n",
    "|               | With SO JSON Mode                | 46.66% (40.03 - 53.30)  |\n",
    "| **Math**      | Without SO                      | 34.85% (26.69 - 43.01)  |\n",
    "|               | With SO Tool Calls               | 37.12% (28.85 - 45.39)  |\n",
    "|               | With SO JSON Mode                | 34.09% (25.97 - 42.21)  |\n",
    "\n",
    "And these are the results of the paired t-tests:\n",
    "\n",
    "### 2. T-Test Results\n",
    "\n",
    "| **Category** | **Comparison**                         | **t-Statistic** | **p-Value** |\n",
    "|--------------|----------------------------------------|-----------------|-------------|\n",
    "| **Reasoning** | Without SO vs With SO Tool Calls       | 0.7442          | 0.4579      |\n",
    "|               | Without SO vs With SO JSON Mode        | -0.2878         | 0.7739      |\n",
    "|               | With SO Tool Calls vs With SO JSON Mode| -1.0000         | 0.3189      |\n",
    "| **Language**  | Without SO vs With SO Tool Calls       | 1.2362          | 0.2185      |\n",
    "|               | Without SO vs With SO JSON Mode        | 0.7979          | 0.4263      |\n",
    "|               | With SO Tool Calls vs With SO JSON Mode| -0.5662         | 0.5721      |\n",
    "| **Math**      | Without SO vs With SO Tool Calls       | -0.5985         | 0.5505      |\n",
    "|               | Without SO vs With SO JSON Mode        | 0.1917          | 0.8482      |\n",
    "|               | With SO Tool Calls vs With SO JSON Mode| 0.7833          | 0.4349      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
