{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "title: \"Japanese is the most expensive language in terms of input tokens\"\n",
    "date: 2025-06-27\n",
    "description-meta: \"Counting tokens in different languages, and their implications for costs.\"\n",
    "categories:\n",
    "  - til\n",
    "  - openai \n",
    "  - tiktoken \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI mentions in their documentation that [1 token corresponds to roughly 4 characters](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them). \n",
    "\n",
    "I was curious how this would work for different languages, so: \n",
    "\n",
    "1. I took a small section of Paul Graham's [How to Do Great Work](https://www.paulgraham.com/greatwork.html)\n",
    "2. Translated it into 7 different languages: English, Spanish, French, German, Japanese, Chinese, and Hindi\n",
    "3. Counted the tokens\n",
    "4. compared the results.\n",
    "\n",
    "## Code\n",
    "\n",
    "Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def read_text(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return file.read()\n",
    "\n",
    "text_en = read_text(\"../_extras/counting-tokens/en.md\")\n",
    "text_es = read_text(\"../_extras/counting-tokens/es.md\")\n",
    "text_fr = read_text(\"../_extras/counting-tokens/fr.md\")\n",
    "text_de = read_text(\"../_extras/counting-tokens/de.md\")\n",
    "text_jp = read_text(\"../_extras/counting-tokens/jp.md\")\n",
    "text_zh = read_text(\"../_extras/counting-tokens/zh.md\")\n",
    "text_hi = read_text(\"../_extras/counting-tokens/hi.md\")\n",
    "text_ru = read_text(\"../_extras/counting-tokens/ru.md\")\n",
    "text_pt = read_text(\"../_extras/counting-tokens/pt.md\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(tiktoken.encoding_for_model(\"gpt-4o\").encode(text))\n",
    "\n",
    "chars_count = {\n",
    "    \"en\": len(text_en),\n",
    "    \"es\": len(text_es),\n",
    "    \"fr\": len(text_fr),\n",
    "    \"de\": len(text_de),\n",
    "    \"jp\": len(text_jp),\n",
    "    \"zh\": len(text_zh),\n",
    "    \"hi\": len(text_hi),\n",
    "    \"ru\": len(text_ru),\n",
    "    \"pt\": len(text_pt),\n",
    "}\n",
    "\n",
    "tokens_count = {\n",
    "    \"en\": count_tokens(text_en),\n",
    "    \"es\": count_tokens(text_es),\n",
    "    \"fr\": count_tokens(text_fr),\n",
    "    \"de\": count_tokens(text_de),\n",
    "    \"jp\": count_tokens(text_jp),\n",
    "    \"zh\": count_tokens(text_zh),\n",
    "    \"hi\": count_tokens(text_hi),\n",
    "    \"ru\": count_tokens(text_ru),\n",
    "    \"pt\": count_tokens(text_pt),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reads the text from the file, and uses `tiktoken` to count the tokens. I also counted the number of characters in the text.\n",
    "\n",
    "Then I calculated the ratio of tokens to characters for each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en: 4.75 chars per token, 2053 chars, 432 tokens\n",
      "es: 4.56 chars per token, 2271 chars, 498 tokens\n",
      "fr: 4.69 chars per token, 2689 chars, 573 tokens\n",
      "de: 4.46 chars per token, 2479 chars, 556 tokens\n",
      "jp: 1.41 chars per token, 1081 chars, 767 tokens\n",
      "zh: 1.33 chars per token, 707 chars, 531 tokens\n",
      "hi: 3.51 chars per token, 2194 chars, 625 tokens\n",
      "ru: 4.02 chars per token, 2275 chars, 566 tokens\n",
      "pt: 4.63 chars per token, 2200 chars, 475 tokens\n"
     ]
    }
   ],
   "source": [
    "for lang in [\"en\", \"es\", \"fr\", \"de\", \"jp\", \"zh\", \"hi\", \"ru\", \"pt\"]:\n",
    "    chars = chars_count[lang]\n",
    "    tokens = tokens_count[lang]\n",
    "    print(f\"{lang}: {chars / tokens:.2f} chars per token, {chars} chars, {tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "I found this interesting:\n",
    "\n",
    "- English is the most efficient language in terms of characters per token, with 4.75 characters per token.\n",
    "- Mandarin Chinese (1.33 characters per token) is the least efficient language in terms of characters per token, followed by Japanese (1.41 characters per token).\n",
    "- The same text in Japanese uses 77% more tokens than in English, making it the most expensive language in terms of input tokens.\n",
    "- Even though Chinese is less efficient than Japanese in terms of characters per token, it's more efficient in terms of information conveyed per character. The article took 2053 characters in English, 707 characters in Chinese, and 1081 characters in Japanese. This explains why Chinese isn't also the most expensive language. \n",
    "- Languages that use a latin alphabet (English, Spanish, French, German, Portuguese) are more efficient than languages that use a non-latin alphabet (Japanese, Chinese, Hindi, Russian). Russian is the most efficient language of these, with 4.02 characters per token.\n",
    "\n",
    "## Limitations\n",
    "\n",
    "This analysis has some clear limitations: \n",
    "\n",
    "1. The text might not be a good example of the types of texts you're working with. \n",
    "2. The translations might not be good enough to truly reflect the information conveyed per character."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
