---
title: "Key parameters for LLMs"
date: 2025-06-26
description-meta: "How to get the best out of your LLMs."
categories:
  - llm
  - openai
  - anthropic
  - gemini
  - python
---

I recently did a workshop about building agents. During the workshop I discussed the key parameters for LLMs. I thought it would be useful to write a short post about it.

After a couple of years building LLM-based products, I've found that the most frequently used parameters are:

- Model
- Messages/prompts
- Temperature
- Seed
- Top-P Sampling
- Logprobs
- Logit biases
- Max completion tokens
- Response format
- Streaming
- Tools

## Model

When choosing a model, consider the following factors:

- Complexity of task: Am I solving a problem that requires advanced reasoning capabilities?
- Speed: Is it for real-time interaction or for batch processing?
- Cost: How much do I want to spend on this task?
- Provider: Which providers do I have access to? Do I need to self-host?

These days my go-to model is Gemini 2.5 Pro or Claude 4 for complex tasks. For simpler tasks, I use Gemini 2.5 Flash or OpenAI's gpt-4.1 family.

The best way to pick a model is to start with the most capable models and then scale down to the simplest models that still capable of solving the task. Otherwise, you could end up spending a lot of time trying to solve an issue that might simply not be possible with current models.

## Messages/prompts

The messages/prompts you send to the LLM will determine the context and instructions for the LLM to follow. I wrote a guide on [prompt engineering](https://dylancastillo.co/posts/prompt-engineering-101.html) that covers the basics of how to write good prompts.

## Temperature

The temperature parameter controls the randomness of the model's output. A temperature of 0 will make the model more deterministic, while a temperature of 1 will make the model more random.

I wrote a [post](https://dylancastillo.co/posts/seed-temperature-llms.html) about how temperature affects the output of LLMs.

## Seed

The seed parameter is used to initialize the random number generator that is then used to sample the next token. This is only available in OpenAI and open-weight models. Check my [post](https://dylancastillo.co/posts/seed-temperature-llms.html) for more details.

## Top-P Sampling

Top-p sampling is a technique that limits the number of tokens that can be selected from the vocabulary by first selecting the smallest group of tokens whose combined probability â‰¥ P. For example, top P = 0.9 picks the next token from the smallest group of tokens that together cover at least 90% probability.

## Logit biases

Logits are the raw scores that the model assigns to each token. You can use biases to change the odds of a token being selected. Positive biases increase the odds of the sequence being selected, while negative biases do the opposite.

This is often used for document classification tasks or [LLM-based rerankers](https://cookbook.openai.com/examples/search_reranking_with_cross-encoders).

## Logprobs

Logprobs are the logaritmic probabilities of the tokens. It's defined as:

$$logprob(w_i) = ln(P(w_i)) = ln(\frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}) $$

Where:

- $w_i$ is the $i$-th token in the vocabulary.
- $P(w_i)$ is the probability of the $i$-th token.
- $z_i$ is the logit of the $i$-th token.
- $n$ is the number of tokens in the vocabulary.

This available for OpenAI models. Anthropic doesn't provide them. Gemini provides a [single request with logprobs per day](https://discuss.ai.google.dev/t/get-logprobs-at-output-token-level/54418/15) (I'm not kidding ðŸ˜…).

Open-weight models don't provide logprobs. They provide logits instead, which you can use to calculate the probabilities of the tokens.

## Max completion tokens

This parameter limits the number of tokens that the model can generate. This is useful to control costs and length of the output.

## Response format

You can use this parameter to specify the format of the response. Anthropic, OpenAI, and Gemini support structured outputs in the form of JSON schemas. I've written multiple posts on this topic:

- [Structured outputs can hurt the performance of LLMs](https://dylancastillo.co/posts/say-what-you-mean-sometimes.html)
- [The good, the bad, and the ugly of Gemini's structured outputs](https://dylancastillo.co/posts/gemini-structured-outputs.html)
- [Structured outputs: donâ€™t put the cart before the horse](https://dylancastillo.co/posts/llm-pydantic-order-matters.html)

Open-weight models allow for more flexibility in the structured output format. [outlines](https://dottxt-ai.github.io/outlines/latest/) is a great tool for this.

## Streaming

This parameter is used to stream the response from the model. This improves the user experience as it allows you to see the output as it's being generated.

## Tools

This parameter is used to specify the tools that the model can use. This is useful to control the capabilities of the model.

## Conclusion

This was a short post about the key parameters for LLMs. I hope you found it useful.

Let me know if you have any questions or feedback.
