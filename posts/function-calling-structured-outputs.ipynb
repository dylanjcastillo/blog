{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "title: \"Function Calling and Structured Outputs in LLMs with LangChain and OpenAI\"\n",
    "date: 2025-06-30\n",
    "description-meta: \"Understand how to use function calling and structured outputs in LLMs with LangChain and OpenAI\"\n",
    "categories:\n",
    "  - llm\n",
    "  - function-calling\n",
    "  - structured-outputs\n",
    "  - openai\n",
    "---\n",
    "\n",
    "## Function calling\n",
    "\n",
    "Function calling or tool use refers to the ability to get LLMs to invoke external tools or functions. This works by giving the LLM access to a set of predefined tools that it can invoke at any point.\n",
    "\n",
    "It's relevant because it gives LLMs more capabilities, allows them to integrate with systems, and enables complex task automation. This was one of the key features that unlocked agents.\n",
    "\n",
    "The usual flow is:\n",
    "1. The user asks a question\n",
    "2. The LLM decides if it needs to use a tool\n",
    "3. If it does, it invokes the tool and gets the output from the tool\n",
    "4. The LLM then uses the output to answer the user's question\n",
    "\n",
    "Here's a diagram that illustrates how function calling works:\n",
    "\n",
    "![Function calling flow](./images/function-calling-structured-outputs/diagram.png)\n",
    "\n",
    "AI developers are increasingly using function calling to build more complex systems. You can use it to:\n",
    "\n",
    "- Get information from a CRM, DB, etc\n",
    "- Perform calculations (e.g., generate an estimae for a variable, financial calculations)\n",
    "- Manipulate data (e.g., data cleaning, data transformation)\n",
    "- Interact with external systems (e.g., booking a flight, sending an email)\n",
    "\n",
    "## Structured outputs\n",
    "\n",
    "Structured outputs are a group of methods that “ensure that model outputs adhere to a specific structure”^[[\"We Need Structured Output\": Towards User-centered Constraints on LLM Output. MX Liu et al. 2024](https://arxiv.org/abs/2404.07362)]. With proprietary models, this usually means a JSON schema. Most propietary providers give you different methods to define a JSON schema.\n",
    "\n",
    "In open-weight models, a structure can mean anything from a JSON schema to a specific regex pattern. You can use [outlines](https://dottxt-ai.github.io/outlines/latest/) for this.\n",
    "\n",
    "Structured outputs are very useful to create agentic systems, as they greatly simplify the communication between components. It's a lot easier to parse the output of a JSON object than a free-form text. But as with any other component on your system, you should measure its impact. It might [impact the performance](https://dylancastillo.co/posts/say-what-you-mean-sometimes.html) of your task.\n",
    "\n",
    "In the next sections, I'll show you how to use function calling and structured outputs with OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To follow this tutorial you'll need to:\n",
    "\n",
    "1. Sign up and generate an API key in [OpenAI](https://platform.openai.com/docs/overview).\n",
    "2. Set the API key as an environment variable called `OPENAI_API_KEY`.\n",
    "3. Create a virtual environment in Python and install the requirements:\n",
    "\n",
    "```bash\n",
    "python -m venv venv\n",
    "source venv/bin/activate\n",
    "pip install langchain langchain-openai python-dotenv jupyter\n",
    "```\n",
    "\n",
    "Once you've completed the steps above, you can run copy and paste the code from the next sections. You can also download the notebook from [here](function-calling-structured-outputs.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: false\n",
    "import os\n",
    "from typing import Literal\n",
    "\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import traceable\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model_name=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def find_weather(latitude: float, longitude: float):\n",
    "    \"\"\"Get the weather of a given latitude and longitude\"\"\"\n",
    "    response = requests.get(\n",
    "        f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\"\n",
    "    )\n",
    "    data = response.json()\n",
    "    return data[\"current\"][\"temperature_2m\"]\n",
    "\n",
    "\n",
    "tools_mapping = {\n",
    "    \"find_weather\": find_weather,\n",
    "}\n",
    "\n",
    "model_with_tools = model.bind_tools([find_weather])\n",
    "\n",
    "\n",
    "@traceable\n",
    "def get_response(question: str):\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            \"You're a helpful assistant. Use the tools provided when relevant.\"\n",
    "        ),\n",
    "        HumanMessage(question),\n",
    "    ]\n",
    "    ai_message = model_with_tools.invoke(messages)\n",
    "    messages.append(ai_message)\n",
    "\n",
    "    for tool_call in ai_message.tool_calls:\n",
    "        selected_tool = tools_mapping[tool_call[\"name\"]]\n",
    "        tool_msg = selected_tool.invoke(tool_call)\n",
    "        messages.append(tool_msg)\n",
    "\n",
    "    ai_message = model_with_tools.invoke(messages)\n",
    "    messages.append(ai_message)\n",
    "\n",
    "    return ai_message.content\n",
    "\n",
    "\n",
    "response = get_response(\"What's the weather in Tokyo?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4.1-mini\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;129m@tool\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_weather\u001b[39m(latitude: \u001b[38;5;28mfloat\u001b[39m, longitude: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[32m      6\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the weather of a given latitude and longitude\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/blog/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/blog/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:743\u001b[39m, in \u001b[36mBaseChatOpenAI.validate_environment\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    736\u001b[39m         \u001b[38;5;28mself\u001b[39m.http_client = httpx.Client(\n\u001b[32m    737\u001b[39m             proxy=\u001b[38;5;28mself\u001b[39m.openai_proxy, verify=global_ssl_context\n\u001b[32m    738\u001b[39m         )\n\u001b[32m    739\u001b[39m     sync_specific = {\n\u001b[32m    740\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttp_client\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.http_client\n\u001b[32m    741\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _get_default_httpx_client(\u001b[38;5;28mself\u001b[39m.openai_api_base, \u001b[38;5;28mself\u001b[39m.request_timeout)\n\u001b[32m    742\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m743\u001b[39m     \u001b[38;5;28mself\u001b[39m.root_client = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msync_specific\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    744\u001b[39m     \u001b[38;5;28mself\u001b[39m.client = \u001b[38;5;28mself\u001b[39m.root_client.chat.completions\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_client:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/blog/.venv/lib/python3.13/site-packages/openai/_client.py:126\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    124\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    127\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m     )\n\u001b[32m    129\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "model = ChatOpenAI(model_name=\"gpt-4.1-mini\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(latitude: float, longitude: float):\n",
    "    \"\"\"Get the weather of a given latitude and longitude\"\"\"\n",
    "    response = requests.get(\n",
    "        f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\"\n",
    "    )\n",
    "    data = response.json()\n",
    "    return data[\"current\"][\"temperature_2m\"]\n",
    "\n",
    "\n",
    "@tool\n",
    "def check_guidelines(drafted_response: str) -> str:\n",
    "    \"\"\"Check if a given response follows the company guidelines\"\"\"\n",
    "    model = ChatOpenAI(model_name=\"gpt-4.1-mini\")\n",
    "    response = model.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                \"You're a helpful assistant. Your task is to check if a given response follows the company guidelines. The company guidelines are that responses should be written in the style of a haiku. You should reply with 'OK' or 'REQUIRES FIXING' and a short explanation.\"\n",
    "            ),\n",
    "            HumanMessage(f\"Current response: {drafted_response}\"),\n",
    "        ]\n",
    "    )\n",
    "    return response.content\n",
    "\n",
    "\n",
    "tools_mapping = {\n",
    "    \"get_weather\": get_weather,\n",
    "    \"check_guidelines\": check_guidelines,\n",
    "}\n",
    "\n",
    "model_with_tools = model.bind_tools([get_weather, check_guidelines])\n",
    "\n",
    "\n",
    "@traceable\n",
    "def get_response(question: str):\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            \"You're a helpful assistant. Use the tools provided when relevant. Then draft a response and check if it follows the company guidelines. Only respond to the user after you've validated and modified the response if needed.\"\n",
    "        ),\n",
    "        HumanMessage(question),\n",
    "    ]\n",
    "    ai_message = model_with_tools.invoke(messages)\n",
    "    messages.append(ai_message)\n",
    "\n",
    "    while ai_message.tool_calls:\n",
    "        for tool_call in ai_message.tool_calls:\n",
    "            selected_tool = tools_mapping[tool_call[\"name\"]]\n",
    "            tool_msg = selected_tool.invoke(tool_call)\n",
    "            messages.append(tool_msg)\n",
    "        ai_message = model_with_tools.invoke(messages)\n",
    "        messages.append(ai_message)\n",
    "\n",
    "    return ai_message.content\n",
    "\n",
    "\n",
    "response = get_response(\"What is the temperature in Madrid?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentInfo(BaseModel):\n",
    "    category: Literal[\"financial\", \"legal\", \"marketing\", \"pets\", \"other\"]\n",
    "    summary: str\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "\n",
    "def get_document_info(document: str) -> DocumentInfo:\n",
    "    model_with_structure = model.with_structured_output(DocumentInfo)\n",
    "    response = model_with_structure.invoke(document)\n",
    "    return response\n",
    "\n",
    "\n",
    "document_text = dedent(\"\"\"\n",
    "This is a document about cats. Very important document. It explain how cats will take over the world in 20230.\n",
    "\"\"\"\n",
    ")\n",
    "document_info = get_document_info(\"I'm a document about a cat\")\n",
    "print(document_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
