{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b9ac1d5",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Prompt chaining workflow with Pydantic AI\"\n",
    "date: 2025-07-08\n",
    "description-meta: \"Using Pydantic AI to build the prompt chaining agentic workflow\"\n",
    "categories:\n",
    "  - til\n",
    "  - llm\n",
    "  - pydantic-ai\n",
    "  - workflows \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5c3bca",
   "metadata": {},
   "source": [
    "To get more familiar with Pydantic AI, I've been re-implementing typical patterns for building [agentic](https://dylancastillo.co/til/react-agent-pydantic-ai.html) [systems](https://dylancastillo.co/posts/agentic-workflows-langgraph.html). \n",
    "\n",
    "In this post, I'll explore how to build a [prompt chaining](https://www.anthropic.com/engineering/building-effective-agents#workflow-prompt-chaining). I won't cover the basics of agentic workflows, so if you're not familiar with the concept, I recommend you to read [this post](https://dylancastillo.co/posts/agentic-workflows-langgraph.html) first.\n",
    "\n",
    "I've also written other TILs about Pydantic AI:\n",
    "- [Routing](https://dylancastillo.co/til/routing-pydantic-ai.html)\n",
    "- [Evaluator-optimizer](https://dylancastillo.co/til/evaluator-optimizer-pydantic-ai.html)\n",
    "- [ReAct agent](https://dylancastillo.co/til/react-agent-pydantic-ai.html)\n",
    "- [Parallelization and Orchestrator-workers](https://dylancastillo.co/til/parallelization-orchestrator-workers-pydantic-ai.html)\n",
    "\n",
    "You can download this notebook [here](https://github.com/dcastillo/blog/blob/main/til/prompt-chaining-pydantic-ai.ipynb).\n",
    "\n",
    "## What is prompt chaining?\n",
    "\n",
    "Prompt chaining is a workflow pattern that splits a complex task into multiple subtasks. This gives you better results, but at the cost of longer completion times (higher latency).\n",
    "\n",
    "It looks like this:\n",
    "\n",
    "```{mermaid}\n",
    "flowchart LR\n",
    "    In --> LLM1[\"LLM Call 1\"]\n",
    "    LLM1 -- \"Output 1\" --> Gate{Gate}\n",
    "    Gate -- Pass --> LLM2[\"LLM Call 2\"]\n",
    "    Gate -- Fail --> Exit[Exit]\n",
    "    LLM2 -- \"Output 2\" --> LLM3[\"LLM Call 3\"]\n",
    "    LLM3 --> Out\n",
    "```\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "- Generating content in a pipeline by generating table of contents, content, revisions, translations, etc.\n",
    "- Generating a text through a multi-step process to evaluate if it matches certain criteria\n",
    "\n",
    "Let's get to it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce750c5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a10e8e",
   "metadata": {},
   "source": [
    "I went with a simple example to implement a content generation workflow composed of three steps:\n",
    "\n",
    "1. Generate a table of contents for the article\n",
    "2. Generate the content of the article\n",
    "3. Update the content of the article if it's too long\n",
    "\n",
    "Because Pydantic AI uses `asyncio` under the hood, you need to enable `nest_asyncio` to use it in a notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a607486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5193cf3",
   "metadata": {},
   "source": [
    "Then, you need to import the required libraries. **[Logfire](https://logfire.pydantic.dev/)** is part of the Pydantic ecosystem, so I thought it'd be good to use it for observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4081c493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "\n",
    "import logfire\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from pydantic_ai import Agent, RunContext\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10e5695",
   "metadata": {},
   "source": [
    "**PydanticAI** is compatible with OpenTelemetry (OTel). So it's pretty easy to use it with Logfire or with any other OTel-compatible observability tool (e.g., [Langfuse](https://langfuse.com/)).\n",
    "\n",
    "To enable tracking, create a project in Logfire, generate a `Write token` and add it to the `.env` file. Then, you just need to run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "014a1f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mLogfire\u001b[0m project URL: \u001b]8;id=13458;https://logfire-us.pydantic.dev/dylanjcastillo/blog\u001b\\\u001b[4;36mhttps://logfire-us.pydantic.dev/dylanjcastillo/blog\u001b[0m\u001b]8;;\u001b\\\n"
     ]
    }
   ],
   "source": [
    "logfire.configure(\n",
    "    token=os.getenv('LOGFIRE_TOKEN'),\n",
    ")\n",
    "logfire.instrument_pydantic_ai()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a02dc0",
   "metadata": {},
   "source": [
    "The first time you run this, it will ask you to create a project in Logfire. From it, it will generate a `logfire_credentials.json` file in your working directory. In following runs, it will automatically use the credentials from the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c8cc51",
   "metadata": {},
   "source": [
    "## Prompt chaining workflow "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edbf064",
   "metadata": {},
   "source": [
    "As mentioned before, the workflow is composed of three steps: generate a table of contents, generate the content of the article and update the content if it's too long.\n",
    "\n",
    "So I created three `Agent` instances. Each one takes care of one of the steps.\n",
    "\n",
    "Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97be7efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:54:00.987 Run workflow\n",
      "18:54:00.988   toc_agent run\n",
      "18:54:00.989     chat gpt-4.1-mini\n",
      "18:54:02.911   article_agent run\n",
      "18:54:02.911     chat gpt-4.1-mini\n",
      "18:54:18.621   editor_agent run\n",
      "18:54:18.622     chat gpt-4.1-mini\n"
     ]
    }
   ],
   "source": [
    "toc_agent = Agent(\n",
    "    'openai:gpt-4.1-mini',\n",
    "    system_prompt=(\n",
    "        \"You are an expert writer specialized in SEO. Provided with a topic, you will generate the table of contents for a short article.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "article_agent = Agent(\n",
    "    'openai:gpt-4.1-mini',\n",
    "    system_prompt=(\n",
    "        \"You are an expert writer specialized in SEO. Provided with a topic and a table of contents, you will generate the content of the article.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "editor_agent = Agent(\n",
    "    'openai:gpt-4.1-mini',\n",
    "    system_prompt=(\n",
    "        \"You are an expert writer specialized in SEO. Provided with a topic, a table of contents and a content, you will revise the content of the article to make it less than 1000 characters.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "@logfire.instrument(\"Run workflow\")\n",
    "def run_workflow(topic: str) -> str:\n",
    "    toc = toc_agent.run_sync(f\"Generate the table of contents of an article about {topic}\")\n",
    "    content = article_agent.run_sync(f\"Generate the content of an article about {topic} with the following table of contents: {toc.output}\")\n",
    "    if len(content.output) > 1000:\n",
    "        revised_content = editor_agent.run_sync(f\"Revise the content of an article about {topic} with the following table of contents: {toc.output} and the following content: {content.output}\")\n",
    "        return revised_content.output\n",
    "    return content.output\n",
    "\n",
    "output = run_workflow(\"Artificial Intelligence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db850415",
   "metadata": {},
   "source": [
    "This code creates the agents and puts them together in a workflow. I used `@logfire.instrument` to make sure all the traces related to the workflow are logged within the same span. See example below:\n",
    "\n",
    "![Prompt chaining workflow](./images/prompt-chaining-pydantic-ai.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c618be",
   "metadata": {},
   "source": [
    "And here's the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ee5bd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Intelligence (AI) simulates human intelligence in machines capable of learning, problem-solving, and decision-making. Originating as a formal discipline in the 1950s, AI evolved from rule-based systems to advanced machine learning and deep learning, now integral to daily life. AI types include Narrow AI for specific tasks, General AI with human-level cognition, and theoretical Superintelligent AI. Key technologies include machine learning, deep learning, natural language processing, and computer vision. AI transforms sectors like healthcare, finance, transportation, and education by automating tasks and improving decisions. Benefits include increased efficiency and innovation, while challenges involve data privacy, bias, job displacement, and transparency. Future trends highlight explainable AI, edge computing, and human-AI collaboration. Ethical concerns focus on accountability, fairness, and user privacy. Responsible AI development promises a transformative, inclusive future.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be7735b",
   "metadata": {},
   "source": [
    "That's all!\n",
    "\n",
    "You can access this notebook [here](https://github.com/dylanjcastillo/blog/tree/main/til/prompt-chaining-pydantic-ai.ipynb).\n",
    "\n",
    "If you have any questions or feedback, please let me know in the comments below."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
