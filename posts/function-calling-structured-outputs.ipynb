{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "title: \"Function calling and structured outputs in LLMs with LangChain and OpenAI\"\n",
    "date: 2025-07-01\n",
    "description-meta: \"Understand how to use function calling and structured outputs in LLMs with LangChain and OpenAI\"\n",
    "categories:\n",
    "  - llm\n",
    "  - function-calling\n",
    "  - structured-outputs\n",
    "  - openai\n",
    "---\n",
    "\n",
    "Function calling and structured outputs let you go from chatbots that just talk to agents that interact with the world. They're two of the most important techniques for building LLM applications.\n",
    "\n",
    "Function calling let LLMs access external tools and services. Structured outputs ensure that the data coming back from your models is ready to integrate \n",
    "\n",
    "These are two of the most important techniques for building LLM applications. I can tell you that mastering them will make your applications better and easier to maintain.\n",
    "\n",
    "In this tutorial, you'll learn:\n",
    "\n",
    "- How function calling and structured outputs work and when to use them\n",
    "- How to implement both techniques using LangChain and OpenAI\n",
    "- Practical examples you can run and adapt for your own projects.\n",
    "\n",
    "Let's get started.\n",
    "\n",
    "## Function calling\n",
    "\n",
    "Function calling refers to the ability to get LLMs to use external tools or functions. It matters because it gives LLMs more capabilities, allows them to talk to external systems, and enables complex task automation. This is one of the key features that unlocked agents.\n",
    "\n",
    "The usual flow is:\n",
    "\n",
    "1. The developer sets up an LLM with a set of predefined tools\n",
    "2. The user asks a question\n",
    "3. The LLM decides if it needs to use a tool\n",
    "4. If it does, it invokes the tool and gets the output from the tool.\n",
    "5. The LLM then uses the output to answer the user's question\n",
    "\n",
    "Here's a diagram that illustrates how function calling works:\n",
    "\n",
    "![Function calling flow](./images/function-calling-structured-outputs/diagram.png)\n",
    "\n",
    "AI developers are increasingly using function calling to build more complex systems. You can use it to:\n",
    "\n",
    "- Get information from a CRM, DB, etc\n",
    "- Perform calculations (e.g., generate an estimate for a variable, financial calculations)\n",
    "- Manipulate data (e.g., data cleaning, data transformation)\n",
    "- Interact with external systems (e.g., booking a flight, sending an email)\n",
    "\n",
    "## Structured outputs\n",
    "\n",
    "Structured outputs are a group of methods that “ensure that model outputs adhere to a specific structure”^[[\"We Need Structured Output\": Towards User-centered Constraints on LLM Output. MX Liu et al. 2024](https://arxiv.org/abs/2404.07362)]. With proprietary models, this usually means a JSON schema. With open-weight models, a structure can mean anything from a JSON schema to a specific regex pattern. You can use [outlines](https://dottxt-ai.github.io/outlines/latest/) for this.\n",
    "\n",
    "Structured outputs are very useful to create agentic systems, as they simplify the communication between components. As you can imagine, it's a lot easier to parse the output of a JSON object than a free-form text. Note, however, that as with other things in life, there's no free lunch. Using this technique might [impact the performance](https://dylancastillo.co/posts/say-what-you-mean-sometimes.html) of your task, so you should have evals in place. \n",
    "\n",
    "In the next sections, I'll show you how to use function calling and structured outputs with OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To follow this tutorial you'll need to:\n",
    "\n",
    "1. Sign up and generate an API key in [OpenAI](https://platform.openai.com/docs/overview).\n",
    "2. Sign up and generate an API key in [LangSmith](https://smith.langchain.com/signup).\n",
    "3. Create an `.env` file with the following variables: \n",
    "```bash\n",
    "OPENAI_API_KEY=sk-...\n",
    "LANGCHAIN_TRACING_V2=true\n",
    "LANGSMITH_API_KEY=lsv2_...\n",
    "LANGCHAIN_PROJECT=\"my-project\"\n",
    "```\n",
    "4. Create a virtual environment in Python and install the requirements:\n",
    "\n",
    "```bash\n",
    "python -m venv venv\n",
    "source venv/bin/activate\n",
    "pip install langchain langsmith pydantic langchain-openai python-dotenv jupyter\n",
    "```\n",
    "\n",
    "Once you've completed the steps above, you can run copy and paste the code from the next sections. You can also download the notebook from [here](function-calling-structured-outputs.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, you'll start by importing the necessary libraries.\n",
    "\n",
    "You'll use LangChain to interact with the OpenAI API and Pydantic for data validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: false\n",
    "from textwrap import dedent\n",
    "from typing import Literal\n",
    "\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import traceable\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've imported the libraries, we'll work on three examples: \n",
    "\n",
    "1. Providing a model with a single tool\n",
    "2. Providing a model with multiple tools\n",
    "3. Generating a structured output from a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function calling with a single tool\n",
    "\n",
    "First you start by defining the model and the tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model_name=\"gpt-4.1-mini\")\n",
    "\n",
    "@tool\n",
    "def find_weather(latitude: float, longitude: float):\n",
    "    \"\"\"Get the weather of a given latitude and longitude\"\"\"\n",
    "    response = requests.get(\n",
    "        f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\"\n",
    "    )\n",
    "    data = response.json()\n",
    "    return data[\"current\"][\"temperature_2m\"]\n",
    "\n",
    "\n",
    "tools_mapping = {\n",
    "    \"find_weather\": find_weather,\n",
    "}\n",
    "\n",
    "model_with_tools = model.bind_tools([find_weather])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up a `gpt-4.1-mini` model with a single tool. To define a tool, you must define a function and use the `@tool` decorator. This function must necessarily have a docstring because this will be used to describe the tool to the model. In this case, the tool is a function that takes latitude and longitude values and returns the weather for that location by making a call to the Open Meteo API.\n",
    "\n",
    "Next, you need to tell your code how to find and use your tools. This is the purpose of `tools_mapping`. It is a common point of confusion. The LLM doesn't run the tools on its own. It only decides if a tool should be used. After the model makes its decision, your own code must make the actual tool call.\n",
    "\n",
    "In this situation, since you only have one tool, a mapping isn't really necessary. But if you were using multiple tools, which is often the case, you would need to create a \"map\" that links each tool's name to its corresponding function. This lets you call the right tool when the model decides to use it.\n",
    "\n",
    "Finally, you need to _bind_ the tool to the model. The binding makes the model aware of the tool, so that it can use it. \n",
    "\n",
    "Then, let's define a function that lets you call the model with the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature in Tokyo is approximately 25.5°C. If you want more detailed weather information, please let me know!\n"
     ]
    }
   ],
   "source": [
    "@traceable\n",
    "def get_response(question: str):\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            \"You're a helpful assistant. Use the tools provided when relevant.\"\n",
    "        ),\n",
    "        HumanMessage(question),\n",
    "    ]\n",
    "    ai_message = model_with_tools.invoke(messages)\n",
    "    messages.append(ai_message)\n",
    "\n",
    "    for tool_call in ai_message.tool_calls:\n",
    "        selected_tool = tools_mapping[tool_call[\"name\"]]\n",
    "        tool_msg = selected_tool.invoke(tool_call)\n",
    "        messages.append(tool_msg)\n",
    "\n",
    "    ai_message = model_with_tools.invoke(messages)\n",
    "    messages.append(ai_message)\n",
    "\n",
    "    return ai_message.content\n",
    "\n",
    "\n",
    "response = get_response(\"What's the weather in Tokyo?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a city name and returns the weather for that city. It uses the `find_weather` tool to get the weather data.\n",
    "\n",
    "It works as follows:\n",
    "\n",
    "1. **Line 1** adds a LangSmith's `traceable` decorator to the function, so that you can see the trace of the function in the LangSmith UI. If you prefer to not use LangSmith, you can remove this line.\n",
    "1. **Lines 2 to 10** set up the [prompts](https://dylancastillo.co/posts/prompt-engineering-101.html) and call the model.\n",
    "2. **Lines 12 to 16** is where the magic happens. This is a loop that will check if there's been a tool call in the response from the model. If there is, it will call (invoke) the tool and add the result to the messages.\n",
    "3. **Lines 17 to 18** the model is called again to get the final response.\n",
    "\n",
    "When you run this code, you'll get a text response with the weather for the city you asked for. If you check the trace, you can see how the whole process works:\n",
    "\n",
    "![Function calling trace](./images/function-calling-structured-outputs/trace-spans.png)\n",
    "\n",
    "There are three steps in the process: \n",
    "\n",
    "1. **Initial model call** with the question from the user.\n",
    "2. **Tool call** to get the weather data.\n",
    "3. **Final model call** to get the response.\n",
    "\n",
    "If you dig deeper into the first model call, you'll see how the tool is provided to the model and how the model decides to use it:\n",
    "\n",
    "![Model calls tool](./images/function-calling-structured-outputs/model-calls-tool.png)\n",
    "\n",
    "The tools is provided by describing it to the model using the docstring of the function. The parameter and their types are also provided. Then the model responds specifying the name of the tool it wants to use and the parameters it wants to pass to it. This tool is then called:\n",
    "\n",
    "![Tool call](./images/function-calling-structured-outputs/tool-call.png)\n",
    "\n",
    "The results of the tool call are then passed to the model again. The model then uses the result to generate the final response:\n",
    "\n",
    "![Tool call result](./images/function-calling-structured-outputs/tool-call-result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. This how you provide a model with tools. In the next section, you'll see how to use multiple tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function calling with multiple tools\n",
    "\n",
    "Similar to the previous example, you start by defining the tools (using the `@tool` decorator) and binding them to the model.\n",
    "\n",
    "In addition to `get_weather`, you'll also define a tool to check if a response follows the company guidelines. In this case, the company guidelines are that responses should be written in the style of a haiku.^[Please don't judge me. Companies do all sorts of weird things these days.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model_name=\"gpt-4.1-mini\")\n",
    "\n",
    "@tool\n",
    "def get_weather(latitude: float, longitude: float):\n",
    "    \"\"\"Get the weather of a given latitude and longitude\"\"\"\n",
    "    response = requests.get(\n",
    "        f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\"\n",
    "    )\n",
    "    data = response.json()\n",
    "    return data[\"current\"][\"temperature_2m\"]\n",
    "\n",
    "\n",
    "@tool\n",
    "def check_guidelines(drafted_response: str) -> str:\n",
    "    \"\"\"Check if a given response follows the company guidelines\"\"\"\n",
    "    model = ChatOpenAI(model_name=\"gpt-4.1-mini\")\n",
    "    response = model.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                \"You're a helpful assistant. Your task is to check if a given response follows the company guidelines. The company guidelines are that responses should be written in the style of a haiku. You should reply with 'OK' or 'REQUIRES FIXING' and a short explanation.\"\n",
    "            ),\n",
    "            HumanMessage(f\"Current response: {drafted_response}\"),\n",
    "        ]\n",
    "    )\n",
    "    return response.content\n",
    "\n",
    "\n",
    "tools_mapping = {\n",
    "    \"get_weather\": get_weather,\n",
    "    \"check_guidelines\": check_guidelines,\n",
    "}\n",
    "\n",
    "model_with_tools = model.bind_tools([get_weather, check_guidelines])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines the tools and binds them to the model. Just like we did before, you also need to define a mapping of the tools, so that you can call the right tool when the model decides to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunny Madrid basks,  \n",
      "Thirty-six degrees embrace,  \n",
      "Summer's warm caress.\n"
     ]
    }
   ],
   "source": [
    "@traceable\n",
    "def get_response(question: str):\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            \"You're a helpful assistant. Use the tools provided when relevant. Then draft a response and check if it follows the company guidelines. Only respond to the user after you've validated and modified the response if needed.\"\n",
    "        ),\n",
    "        HumanMessage(question),\n",
    "    ]\n",
    "    ai_message = model_with_tools.invoke(messages)\n",
    "    messages.append(ai_message)\n",
    "\n",
    "    while ai_message.tool_calls:\n",
    "        for tool_call in ai_message.tool_calls:\n",
    "            selected_tool = tools_mapping[tool_call[\"name\"]]\n",
    "            tool_msg = selected_tool.invoke(tool_call)\n",
    "            messages.append(tool_msg)\n",
    "        ai_message = model_with_tools.invoke(messages)\n",
    "        messages.append(ai_message)\n",
    "\n",
    "    return ai_message.content\n",
    "\n",
    "\n",
    "response = get_response(\"What is the temperature in Madrid?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is pretty much the same as the previous example, but with two tools. There's also a one small difference. \n",
    "\n",
    "Previously, we checked for tool calls once. Now, we'll use a while loop that keeps checking. So, instead of the model having to provide the final answer after one turn, it can now ask for tools multiple times in a row until it has all the information it needs.\n",
    "\n",
    "This is the core idea behind how agents work. So, congratulations, you've just built a simple agent! If you check the process in LangSmith, you'll see how these turns play out.\n",
    "\n",
    "Next, let's see how to use structured outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structured outputs are a set of methods used to get model outputs that follow a specific structure. This is useful when you want to get a specific type of output, such as a JSON object.\n",
    "\n",
    "It's easy to set up with proprietary models. With LangChain, you can define a `dict` or a [Pydantic model](https://docs.pydantic.dev/latest/concepts/models/) to describe the output. I recommend using Pydantic models.\n",
    "\n",
    "For example, let's define a Pydantic model that will help us classify document into categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentInfo(BaseModel):\n",
    "    category: Literal[\"financial\", \"legal\", \"marketing\", \"pets\", \"other\"] = Field(\n",
    "        description=\"The category of the document\"\n",
    "    )\n",
    "    summary: str = Field(description=\"A short summary of the document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model defines the structured output we'll get from the model. It has two fields: `category` and `summary`. \n",
    "\n",
    "Then, you can use the `with_structured_output` method to create a model that will return the structured output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category='pets' summary='A document about a cat.'\n"
     ]
    }
   ],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "def get_document_info(document: str) -> DocumentInfo:\n",
    "    model_with_structure = model.with_structured_output(DocumentInfo)\n",
    "    response = model_with_structure.invoke(document)\n",
    "    return response\n",
    "\n",
    "\n",
    "document_text = dedent(\"\"\"\n",
    "This is a document about cats. Very important document. It explain how cats will take over the world in 20230.\n",
    "\"\"\"\n",
    ")\n",
    "document_info = get_document_info(\"I'm a document about a cat\")\n",
    "print(document_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this code, you'll get a structured output with the category and summary of the document that you can then use in further steps of your workflow.\n",
    "\n",
    "Depending on the provider, you'll have different options to get structured outputs. OpenAI offers three different methods:\n",
    "\n",
    "- `function_calling`: This uses the tool calling mechanism to get the structured output.\n",
    "- `json_mode`: This method ensures you get a valid JSON object, but it's not clear how it works under the hood.\n",
    "- `json_schema`: This is default method in LangChain. It ensures that the output is a valid JSON object and that it matches the schema you provide using [constrained decoding](https://openai.com/index/introducing-structured-outputs-in-the-api/).\n",
    "\n",
    "[Gemini](https://ai.google.dev/gemini-api/docs/structured-output) and [Anthropic](https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/increase-consistency) provide their own methods to get structured outputs.\n",
    "\n",
    "One thing to keep in mind is that structured outputs can impact performance. I've [written](https://dylancastillo.co/posts/llm-pydantic-order-matters.html) [multiple](https://dylancastillo.co/posts/say-what-you-mean-sometimes.html) [posts](https://dylancastillo.co/posts/gemini-structured-outputs.html) about this topic, so I won't go into detail here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function calling and structured outputs are powerful tools that help build more capable AI systems. They're also the foundation of agents.\n",
    "\n",
    "Function calling is a way to provide LLMs with tools to use. It lets you go from building a chatbot that can only talk to building an AI assistant that can actually interact with the world. It opens up a world of possibilities, from connecting to databases, calling APIs, or automating workflows.\n",
    "\n",
    "Structured outputs are just as important. They're critical to integrating LLMs into existing systems. Instead of struggling with parsing free-form text, you get clean, predictable data structures that you can use in your code.\n",
    "\n",
    "The examples in this tutorial should give you a sense of how to use these methods. But as usual, the real learning happens when you start applying these concepts to your own problems. Pick a task you're working on, see if any of these methods can help you, and give it a try.\n",
    "\n",
    "If you have any questions or comments, let me know in the comments below."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
