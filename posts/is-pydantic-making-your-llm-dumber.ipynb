{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Is Pydantic making your LLM dumber?\"\n",
    "date: \"11/10/2024\"\n",
    "date-modified: last-modified\n",
    "description-meta: \"Structured outputs make it easy to work with LLMs, but are they impacting their performance?\"\n",
    "toc: true\n",
    "toc-depth: 3\n",
    "lightbox: true\n",
    "fig-cap-location: margin\n",
    "categories:\n",
    "  - llm\n",
    "  - openai\n",
    "  - pydantic\n",
    "  - python\n",
    "author:\n",
    "  - name: Dylan Castillo\n",
    "    url: https://dylancastillo.co\n",
    "    affiliation: Iwana Labs\n",
    "    affiliation-url: https://iwanalabs.com\n",
    "citation: true\n",
    "comments:\n",
    "  utterances:\n",
    "    repo: dylanjcastillo/blog_comments\n",
    "    theme: dark-blue\n",
    "    issue-term: pathname\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the [Let me speak freely?](https://arxiv.org/abs/2408.02442) was published, there's been a lot of talk^[By a lot, I mean just a couple of randos on my corner of the internet.] about the impact of structured outputs on the performance of LLMs. \n",
    "\n",
    "The [.txt](https://dottxt.co) team wrote a very [compelling rebuttal](https://blog.dottxt.co/say-what-you-mean.html). But they focused on open-weight models, which is not what I tend to use, and I don't know how well their results translate to proprietary models. Plus, I also missed seeing the performance of structured outputs on benchmarks that are [less likely to be part of the training data](https://arxiv.org/abs/2405.00332) of the models, such as [LiveBench](https://arxiv.org/abs/2406.19314).\n",
    "\n",
    "So, I decided to run my own experiment to evaluate the impact of structured outputs on the performance of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment design\n",
    "\n",
    "I gathered the published reasoning, language, and math^[I only included the olympiad and math_comp tasks from the math category.] questions from [LiveBench](https://huggingface.co/livebench), and then I modified them to remove any formatting instructions.\n",
    "\n",
    "Using these questions, I created three types of prompts:\n",
    "\n",
    "1. Without structured outputs^[By structured outputs, I refer to the generation of JSON or XML objects.]: The LLM is given a system message that includes the format of the response and a user message with the question. But the instructions do not require a **too-rigid** structure.\n",
    "2. With structured outputs using tool calls: The LLM is given a system message without a specific format, and later on it is asked to use the `Response` pydantic model to parse its response.\n",
    "3. With structured outputs using JSON mode: The LLM is given a system message that includes the format of the response and a user message with the question. But the instructions ask the LLM to return a JSON object in the `Response` pydantic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "\n",
    "import asyncio\n",
    "import difflib\n",
    "import json\n",
    "import re\n",
    "from asyncio import Semaphore\n",
    "from enum import Enum\n",
    "from itertools import permutations\n",
    "from pathlib import Path\n",
    "from typing import Any, List\n",
    "\n",
    "import instructor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from openai import AsyncOpenAI\n",
    "from openai.types.chat import ChatCompletion\n",
    "from pydantic import BaseModel, Field\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This imports the required libraries. Two things to note:\n",
    "\n",
    "1. I'm using [LangSmith](https://smith.langchain.com/) to trace the calls to the OpenAI API. This makes it easier to debug and see the prompts that are being used.\n",
    "2. I use instructor to compare the performance of `structured outputs` using tool calls and JSON mode.\n",
    "\n",
    "Next, load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path().absolute().parent / \"data\" / \"live_bench\"\n",
    "reasoning_dir = data_dir / \"reasoning\"\n",
    "math_dir = data_dir / \"math\"\n",
    "language_dir = data_dir / \"language\"\n",
    "\n",
    "df_reasoning = pd.read_json(\n",
    "    reasoning_dir / \"updated_questions.jsonl\", lines=True\n",
    ")\n",
    "df_language = pd.read_json(language_dir / \"updated_questions.jsonl\", lines=True)\n",
    "df_math = pd.read_json(math_dir / \"updated_questions.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using a slightly modified version of the reasoning, language, and math question (excluding some categories from the latter).\n",
    "\n",
    "I had to modify them because the original ones included formatting requirements that would have made the comparison less fair.\n",
    "\n",
    "You can see the changes I did [here](../scripts/is-pydantic-making-your-model-dumber/update_livebench_questions.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "langsmith_client = wrap_openai(AsyncOpenAI())\n",
    "tool_calls_client = instructor.from_openai(langsmith_client, mode=instructor.Mode.TOOLS)\n",
    "json_mode_client = instructor.from_openai(langsmith_client, mode=instructor.Mode.JSON)\n",
    "strict_tool_calls_client = instructor.from_openai(\n",
    "    langsmith_client, mode=instructor.Mode.TOOLS_STRICT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response(BaseModel):\n",
    "    reasoning: str = Field(description=\"Your reasoning explaining your answer.\")\n",
    "    answer: str = Field(description=\"Your answer, don't include any other text.\")\n",
    "\n",
    "\n",
    "class PromptType(Enum):\n",
    "    WITHOUT_STRUCTURED_OUTPUT = \"no_so\"\n",
    "    WITH_TOOL_CALLS = \"so_tool_calls\"\n",
    "    WITH_JSON_MODE = \"so_json_mode\"\n",
    "    WITH_STRICT_TOOL_CALLS = \"so_strict_tool_calls\"\n",
    "\n",
    "\n",
    "class ClientConfig(BaseModel):\n",
    "    client: Any\n",
    "    system_message: str\n",
    "\n",
    "\n",
    "CONFIG_MAPPING = {\n",
    "    PromptType.WITHOUT_STRUCTURED_OUTPUT.value: ClientConfig(\n",
    "        client=langsmith_client,\n",
    "        system_message=(\n",
    "            \"You're a helpful assistant. You will help me answer a question.\"\n",
    "            \"\\nYou must respond using the following format:\"\n",
    "            \"\\nREASONING: <your reasoning explaining your answer>\"\n",
    "            \"\\nANSWER: <your answer, don't include any other text>\"\n",
    "        ),\n",
    "    ),\n",
    "    PromptType.WITH_TOOL_CALLS.value: ClientConfig(\n",
    "        client=tool_calls_client,\n",
    "        system_message=(\n",
    "            \"You're a helpful assistant. You will help me answer a question.\"\n",
    "        ),\n",
    "    ),\n",
    "    PromptType.WITH_JSON_MODE.value: ClientConfig(\n",
    "        client=json_mode_client,\n",
    "        system_message=(\n",
    "            \"You're a helpful assistant. You will help me answer a question.\"\n",
    "            + \"\\nYou must respond using the following JSON schema:\"\n",
    "            + json.dumps(Response.model_json_schema())\n",
    "        ),\n",
    "    ),\n",
    "    PromptType.WITH_STRICT_TOOL_CALLS.value: ClientConfig(\n",
    "        client=strict_tool_calls_client,\n",
    "        system_message=(\n",
    "            \"You're a helpful assistant. You will help me answer a question.\"\n",
    "        ),\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I defined the three types of prompts I'm going to use:\n",
    "\n",
    "1. Without structured outputs: The LLM is given a system message that includes the format of the response and a user message with the question. But the instructions do not require a **too-rigid** structure.\n",
    "2. With structured outputs using tool calls: The LLM is given a system message without a specific format, and later on it is asked to use the `Response` pydantic model to parse its response.\n",
    "3. With structured outputs using JSON mode: The LLM is given a system message that includes the format of the response and a user message with the question. But the instructions ask the LLM to return a JSON object in the `Response` pydantic model.\n",
    "\n",
    "Finally, I defined a helper function to parse the responses from the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(\n",
    "    response: ChatCompletion | Response, response_type: PromptType\n",
    ") -> str:\n",
    "    if isinstance(response, Response):\n",
    "        return response.answer\n",
    "    elif (\n",
    "        isinstance(response, ChatCompletion)\n",
    "        and response_type == PromptType.WITHOUT_STRUCTURED_OUTPUT\n",
    "    ):\n",
    "        return response.choices[0].message.content.split(\"\\nANSWER:\")[1].strip()\n",
    "    raise ValueError(f\"Invalid response type: {type(response)}\")\n",
    "\n",
    "\n",
    "@traceable\n",
    "async def call_model(\n",
    "    client,\n",
    "    prompt_type: PromptType,\n",
    "    user_message: str,\n",
    "    timeout: int = 120,\n",
    ") -> Response:\n",
    "    config = CONFIG_MAPPING[prompt_type.value]\n",
    "    params = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": config.system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "        ],\n",
    "        \"timeout\": timeout,\n",
    "    }\n",
    "    if prompt_type in (\n",
    "        PromptType.WITH_TOOL_CALLS,\n",
    "        PromptType.WITH_STRICT_TOOL_CALLS,\n",
    "        PromptType.WITH_JSON_MODE,\n",
    "    ):\n",
    "        params.update(\n",
    "            {\n",
    "                \"response_model\": Response,\n",
    "            }\n",
    "        )\n",
    "    response = await client.chat.completions.create(**params)\n",
    "    return parse_response(response, prompt_type)\n",
    "\n",
    "\n",
    "@traceable\n",
    "async def process_row(\n",
    "    row: pd.Series,\n",
    "    prompt_type: PromptType,\n",
    "    semaphore: Semaphore,\n",
    ") -> str:\n",
    "    client = CONFIG_MAPPING[prompt_type.value].client\n",
    "    async with semaphore:\n",
    "        for _ in range(3):\n",
    "            try:\n",
    "                answer = await call_model(\n",
    "                    client=client,\n",
    "                    prompt_type=prompt_type,\n",
    "                    user_message=f\"Question:\\n{row.updated_question}\",\n",
    "                )\n",
    "                return answer\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {row.name}: {e}\")\n",
    "                continue\n",
    "        raise Exception(f\"Failed to process row {row.name}, after 3 attempts\")\n",
    "\n",
    "\n",
    "@traceable\n",
    "async def process_df(\n",
    "    df: pd.DataFrame,\n",
    "    prompt_type: PromptType,\n",
    "    concurrency: int = 100,\n",
    ") -> List[str]:\n",
    "    semaphore = Semaphore(concurrency)\n",
    "    tasks = [process_row(row, prompt_type, semaphore) for _, row in df.iterrows()]\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://github.com/LiveBench/LiveBench/blob/main/livebench/process_results/writing/plot_unscrambling/utils.py\n",
    "def levenshtein_distance(A, B):\n",
    "    N, M = len(A), len(B)\n",
    "    # Create an array of size NxM\n",
    "    dp = [[0 for i in range(M + 1)] for j in range(N + 1)]\n",
    "\n",
    "    # Base Case: When N = 0\n",
    "    for j in range(M + 1):\n",
    "        dp[0][j] = j\n",
    "    # Base Case: When M = 0\n",
    "    for i in range(N + 1):\n",
    "        dp[i][0] = i\n",
    "    # Transitions\n",
    "    for i in range(1, N + 1):\n",
    "        for j in range(1, M + 1):\n",
    "            if A[i - 1] == B[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(\n",
    "                    dp[i - 1][j],  # Insertion\n",
    "                    dp[i][j - 1],  # Deletion\n",
    "                    dp[i - 1][j - 1],  # Replacement\n",
    "                )\n",
    "\n",
    "    return dp[N][M]\n",
    "\n",
    "\n",
    "def plot_unscrambling_process_results(ground_truth: str, llm_answer: str) -> float:\n",
    "    gt_sentences = [s.strip() for s in ground_truth.split(\".\")]\n",
    "    ans_sentences = [s.strip() for s in llm_answer.split(\".\")]\n",
    "\n",
    "    gt_sentences = [s for s in gt_sentences if s]\n",
    "    ans_sentences = [s for s in ans_sentences if s]\n",
    "\n",
    "    ans_ordering = []\n",
    "    for x in gt_sentences:\n",
    "        best_match = difflib.get_close_matches(x, ans_sentences, n=1, cutoff=0.0)\n",
    "        if best_match:\n",
    "            ans_ordering.append(ans_sentences.index(best_match[0]))\n",
    "\n",
    "    n_sentences_gt = len(gt_sentences)\n",
    "    raw_distance = levenshtein_distance(list(range(len(gt_sentences))), ans_ordering)\n",
    "    score = 1 - (raw_distance / n_sentences_gt)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def evaluate_language_task(ground_truth: str, task_type: str, response: str):\n",
    "    if task_type == \"connections\":\n",
    "        objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in response.split(\",\")\n",
    "        ]\n",
    "        gt_objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in ground_truth.split(\",\")\n",
    "        ]\n",
    "\n",
    "        groups = [set(objects[i : i + 4]) for i in range(0, len(objects), 4)]\n",
    "        gt_groups = [set(gt_objects[i : i + 4]) for i in range(0, len(gt_objects), 4)]\n",
    "\n",
    "        max_correct = 0\n",
    "        for perm in permutations(groups):\n",
    "            correct_groups = sum(g1 == g2 for g1, g2 in zip(perm, gt_groups))\n",
    "            max_correct = max(max_correct, correct_groups)\n",
    "        return max_correct / len(gt_groups)\n",
    "    elif task_type == \"plot_unscrambling\":\n",
    "        return plot_unscrambling_process_results(ground_truth, response)\n",
    "    elif task_type == \"typos\":\n",
    "        return ground_truth in response\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid task type: {task_type}\")\n",
    "\n",
    "\n",
    "def evaluate_reasoning_task(ground_truth: str, task_type: str, response: str):\n",
    "    if task_type == \"web_of_lies_v2\":\n",
    "        response_objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in response.split(\",\")\n",
    "        ]\n",
    "        gt_objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in ground_truth.split(\",\")\n",
    "        ]\n",
    "        return response_objects == gt_objects\n",
    "    elif task_type in (\"spatial\", \"zebra_puzzle\"):\n",
    "        response = response.rstrip(\".\")\n",
    "        return ground_truth.lower().strip() == response.lower().strip()\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid task type: {task_type}\")\n",
    "\n",
    "\n",
    "def evaluate_math_task(ground_truth: str, task_type: str, response: str):\n",
    "    if task_type == \"olympiad\":\n",
    "        response_objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in response.split(\",\")\n",
    "        ]\n",
    "        gt_objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in ground_truth.split(\",\")\n",
    "        ]\n",
    "        return response_objects == gt_objects\n",
    "    elif task_type == \"math_comp\":\n",
    "        return ground_truth == response\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid task type: {task_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "def generate_outputs(df):\n",
    "    df_copy = df.copy()\n",
    "    responses_without_so = asyncio.run(\n",
    "        process_df(df_copy, PromptType.WITHOUT_STRUCTURED_OUTPUT)\n",
    "    )\n",
    "    responses_with_so_tool_calls = asyncio.run(\n",
    "        process_df(df_copy, PromptType.WITH_TOOL_CALLS)\n",
    "    )\n",
    "    responses_with_so_json_mode = asyncio.run(\n",
    "        process_df(df_copy, PromptType.WITH_JSON_MODE)\n",
    "    )\n",
    "    responses_with_so_strict_tool_calls = asyncio.run(\n",
    "        process_df(df_copy, PromptType.WITH_STRICT_TOOL_CALLS)\n",
    "    )\n",
    "    df_copy[\"response_without_so\"] = responses_without_so\n",
    "    df_copy[\"response_with_so_tool_calls\"] = responses_with_so_tool_calls\n",
    "    df_copy[\"response_with_so_json_mode\"] = responses_with_so_json_mode\n",
    "    df_copy[\"response_with_so_strict_tool_calls\"] = responses_with_so_strict_tool_calls\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def evaluate_outputs(df, evaluator):\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"score_without_so\"] = df_copy.apply(\n",
    "        lambda row: evaluator(\n",
    "            row[\"ground_truth\"], row[\"task\"], row[\"response_without_so\"]\n",
    "        )\n",
    "        * 1,\n",
    "        axis=1,\n",
    "    )\n",
    "    df_copy[\"score_with_so_tool_calls\"] = df_copy.apply(\n",
    "        lambda row: evaluator(\n",
    "            row[\"ground_truth\"], row[\"task\"], row[\"response_with_so_tool_calls\"]\n",
    "        )\n",
    "        * 1,\n",
    "        axis=1,\n",
    "    )\n",
    "    df_copy[\"score_with_so_json_mode\"] = df_copy.apply(\n",
    "        lambda row: evaluator(\n",
    "            row[\"ground_truth\"], row[\"task\"], row[\"response_with_so_json_mode\"]\n",
    "        )\n",
    "        * 1,\n",
    "        axis=1,\n",
    "    )\n",
    "    df_copy[\"score_with_so_strict_tool_calls\"] = df_copy.apply(\n",
    "        lambda row: evaluator(\n",
    "            row[\"ground_truth\"], row[\"task\"], row[\"response_with_so_strict_tool_calls\"]\n",
    "        )\n",
    "        * 1,\n",
    "        axis=1,\n",
    "    )\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reasoning_results = generate_outputs(df_reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reasoning_results = evaluate_outputs(df_reasoning_results, evaluate_reasoning_task)\n",
    "df_reasoning_results.to_csv(data_dir / \"reasoning\" / \"reasoning_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "df_reasoning_results.groupby(\"task\").agg(\n",
    "    n_questions=(\"question_id\", \"count\"),\n",
    "    score_without_so=(\"score_without_so\", \"mean\"),\n",
    "    score_with_so_tool_calls=(\"score_with_so_tool_calls\", \"mean\"),\n",
    "    score_with_so_json_mode=(\"score_with_so_json_mode\", \"mean\"),\n",
    "    score_with_so_strict_tool_calls=(\"score_with_so_strict_tool_calls\", \"mean\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_language_results = generate_outputs(df_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_language_results = evaluate_outputs(df_language_results, evaluate_language_task)\n",
    "df_language_results.to_csv(data_dir / \"language\" / \"language_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "df_language_results.groupby(\"task\").agg(\n",
    "    n_questions=(\"question_id\", \"count\"),\n",
    "    score_without_so=(\"score_without_so\", \"mean\"),\n",
    "    score_with_so_tool_calls=(\"score_with_so_tool_calls\", \"mean\"),\n",
    "    score_with_so_json_mode=(\"score_with_so_json_mode\", \"mean\"),\n",
    "    score_with_so_strict_tool_calls=(\"score_with_so_strict_tool_calls\", \"mean\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_math_results = generate_outputs(df_math)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_math_results = evaluate_outputs(df_math_results, evaluate_math_task)\n",
    "df_math_results.to_csv(data_dir / \"math\" / \"math_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "df_math_results.groupby(\"task\").agg(\n",
    "    n_questions=(\"question_id\", \"count\"),\n",
    "    score_without_so=(\"score_without_so\", \"mean\"),\n",
    "    score_with_so_tool_calls=(\"score_with_so_tool_calls\", \"mean\"),\n",
    "    score_with_so_json_mode=(\"score_with_so_json_mode\", \"mean\"),\n",
    "    score_with_so_strict_tool_calls=(\"score_with_so_strict_tool_calls\", \"mean\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "def calculate_confidence_intervals(df):\n",
    "    for score_col in [\n",
    "        \"score_without_so\",\n",
    "        \"score_with_so_tool_calls\",\n",
    "        \"score_with_so_json_mode\",\n",
    "        \"score_with_so_strict_tool_calls\",\n",
    "    ]:\n",
    "        mean_score = df[score_col].mean()\n",
    "        se_score = df[score_col].std() / np.sqrt(len(df))\n",
    "        ci = [\n",
    "            mean_score - 1.96 * se_score,\n",
    "            mean_score + 1.96 * se_score,\n",
    "        ]\n",
    "        print(\n",
    "            f\"{score_col} - Mean: {mean_score * 100:.2f}% CI: {ci[0] * 100:.2f}% - {ci[1] * 100:.2f}%\"\n",
    "        )\n",
    "    print()\n",
    "\n",
    "\n",
    "def run_paired_t_test(df):\n",
    "    scores = {}\n",
    "\n",
    "    for score_col in [\n",
    "        \"score_without_so\",\n",
    "        \"score_with_so_tool_calls\",\n",
    "        \"score_with_so_json_mode\",\n",
    "        \"score_with_so_strict_tool_calls\",\n",
    "    ]:\n",
    "        scores[score_col] = df[score_col] * 1\n",
    "\n",
    "    for score_col_1, score_col_2 in [\n",
    "        (\"score_without_so\", \"score_with_so_tool_calls\"),\n",
    "        (\"score_without_so\", \"score_with_so_json_mode\"),\n",
    "        (\"score_without_so\", \"score_with_so_strict_tool_calls\"),\n",
    "    ]:\n",
    "        t_stat, p_value = stats.ttest_rel(scores[score_col_1], scores[score_col_2])\n",
    "        print(f\"{score_col_1} vs {score_col_2}\")\n",
    "        print(f\"t-statistic: {t_stat}, p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_confidence_intervals(df_reasoning_results)\n",
    "run_paired_t_test(df_reasoning_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_confidence_intervals(df_language_results)\n",
    "run_paired_t_test(df_language_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_confidence_intervals(df_math_results)\n",
    "run_paired_t_test(df_math_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the mean scores with confidence intervals:\n",
    "\n",
    "| **Category** | **Response Format**              | **Mean (CI %)**         |\n",
    "|--------------|----------------------------------|-------------------------|\n",
    "| **Reasoning** | Without SO                      | 42.67% (34.73 - 50.61)  |\n",
    "|               | With SO Tool Calls               | 39.33% (31.49 - 47.18)  |\n",
    "|               | With SO JSON Mode                | 44.00% (36.03 - 51.97)  |\n",
    "| **Language**  | Without SO                      | 48.91% (42.05 - 55.76)  |\n",
    "|               | With SO Tool Calls               | 44.78% (37.96 - 51.59)  |\n",
    "|               | With SO JSON Mode                | 46.66% (40.03 - 53.30)  |\n",
    "| **Math**      | Without SO                      | 34.85% (26.69 - 43.01)  |\n",
    "|               | With SO Tool Calls               | 37.12% (28.85 - 45.39)  |\n",
    "|               | With SO JSON Mode                | 34.09% (25.97 - 42.21)  |\n",
    "\n",
    "And these are the results of the paired t-tests:\n",
    "\n",
    "### 2. T-Test Results\n",
    "\n",
    "| **Category** | **Comparison**                         | **t-Statistic** | **p-Value** |\n",
    "|--------------|----------------------------------------|-----------------|-------------|\n",
    "| **Reasoning** | Without SO vs With SO Tool Calls       | 0.7442          | 0.4579      |\n",
    "|               | Without SO vs With SO JSON Mode        | -0.2878         | 0.7739      |\n",
    "|               | With SO Tool Calls vs With SO JSON Mode| -1.0000         | 0.3189      |\n",
    "| **Language**  | Without SO vs With SO Tool Calls       | 1.2362          | 0.2185      |\n",
    "|               | Without SO vs With SO JSON Mode        | 0.7979          | 0.4263      |\n",
    "|               | With SO Tool Calls vs With SO JSON Mode| -0.5662         | 0.5721      |\n",
    "| **Math**      | Without SO vs With SO Tool Calls       | -0.5985         | 0.5505      |\n",
    "|               | Without SO vs With SO JSON Mode        | 0.1917          | 0.8482      |\n",
    "|               | With SO Tool Calls vs With SO JSON Mode| 0.7833          | 0.4349      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
