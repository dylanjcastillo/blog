{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Is Pydantic making your model dumber?\"\n",
    "date: \"11/10/2024\"\n",
    "date-modified: last-modified\n",
    "description-meta: \"\"\n",
    "toc: true\n",
    "toc-depth: 3\n",
    "lightbox: true\n",
    "fig-cap-location: margin\n",
    "categories:\n",
    "  - llm\n",
    "  - openai\n",
    "  - pydantic\n",
    "  - python\n",
    "author:\n",
    "  - name: Dylan Castillo\n",
    "    url: https://dylancastillo.co\n",
    "    affiliation: Iwana Labs\n",
    "    affiliation-url: https://iwanalabs.com\n",
    "citation: true\n",
    "comments:\n",
    "  utterances:\n",
    "    repo: dylanjcastillo/blog_comments\n",
    "    theme: dark-blue\n",
    "    issue-term: pathname\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from asyncio import Semaphore\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "\n",
    "import instructor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from openai import AsyncOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "langsmith_client = wrap_openai(AsyncOpenAI())\n",
    "instructor_client = instructor.from_openai(langsmith_client, mode=instructor.Mode.TOOLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path().absolute().parent / \"data\" / \"live_bench\"\n",
    "reasoning_dir = data_dir / \"reasoning\"\n",
    "math_dir = data_dir / \"math\"\n",
    "language_dir = data_dir / \"language\"\n",
    "\n",
    "df_reasoning = (\n",
    "    pd.read_json(reasoning_dir / \"question.jsonl\", lines=True)\n",
    "    # .query(\"livebench_release_date == '2024-06-24'\")\n",
    "    .assign(turns_str=lambda x: x.turns.str[0])\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"data_point_id\"})\n",
    "    # .sample(15)\n",
    ")\n",
    "\n",
    "# assert there is a single turn per row\n",
    "assert df_reasoning.turns.str.len().eq(1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormatType(Enum):\n",
    "    T1 = \"Bold formatting for a single phrase. Example: Think step by step, and then put your answer in **bold** as a single phrase (for example, **sphere**).\"\n",
    "    T2 = \"Bold formatting for a list of three words. Example: Think step by step, and then put your answer in **bold** as a list of three words, yes or no (for example, **yes, no, yes**).\"\n",
    "    T3 = \"Bold formatting for a single integer. Example: Think step by step, and then put your answer in **bold** as a single integer (for example, **0**).\"\n",
    "    T4 = \"Return a single digit number. Example: Return a single digit number, in the following format: **N**, where N is the position.\"\n",
    "    T5 = \"Return your answer as a single word. Example: Return your answer as a single word, in the following format: **X**, where X is the answer.\"\n",
    "    OTHER = \"Other formatting requirements. Example: Other formatting requirements (if none of the above apply)\"\n",
    "\n",
    "\n",
    "class FormatClassification(BaseModel):\n",
    "    classification: FormatType = Field(\n",
    "        description=f\"The formatting requirements of the output of the provided question. Only allowed types: {[t.value for t in FormatType]}, should be used\",\n",
    "    )\n",
    "\n",
    "\n",
    "system_prompt_classification_reasoning = \"You're a helpful assistant. I will provide you with a question and you will classify the formatting requirements of the output of the provided question into the most appropriate category.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_mapping = {\n",
    "    FormatType.T1.name: \"Put your answer as a single phrase (for example, sphere).\",\n",
    "    FormatType.T2.name: \"Put your answer as a list of three words, yes or no (for example, yes, no, yes).\",\n",
    "    FormatType.T3.name: \"Put your answer as a single integer (for example, 0).\",\n",
    "    FormatType.T4.name: \"Return a single digit number, in the following format: N, where N is the position.\",\n",
    "    FormatType.T5.name: \"Return your answer as a single word, in the following format: X, where X is the answer.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "async def process_row(\n",
    "    system_message: str,\n",
    "    user_message: str,\n",
    "    response_model: BaseModel,\n",
    "    semaphore: Semaphore,\n",
    ") -> dict:\n",
    "    async with semaphore:\n",
    "        return await instructor_client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "            ],\n",
    "            response_model=response_model,\n",
    "        )\n",
    "\n",
    "\n",
    "@traceable\n",
    "async def classify_reasoning_questions(df, concurrency: int = 30):\n",
    "    semaphore = Semaphore(concurrency)\n",
    "    tasks = [\n",
    "        process_row(\n",
    "            system_message=system_prompt_classification_reasoning,\n",
    "            user_message=f\"Question:\\n{row.turns_str}\",\n",
    "            response_model=FormatClassification,\n",
    "            semaphore=semaphore,\n",
    "        )\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    return [r.classification for r in responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_reasoning = asyncio.run(classify_reasoning_questions(df_reasoning))\n",
    "df_reasoning[\"classification\"] = classified_reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdatedQuestion(BaseModel):\n",
    "    updated_question: str\n",
    "\n",
    "\n",
    "system_prompt_replace_reasoning_format = (\n",
    "    \"You're a helpful assistant. I will provide you with a question and the old formatting requirements. Your task is to replace the old formatting requirements with the new ones.\"\n",
    "    \"Please return the full text of the question with the new formatting requirements. Don't include any other text. Don't include 'Question:' or 'Old formatting:' or 'New formatting:'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "async def replace_reasoning_questions_format(df, concurrency: int = 30):\n",
    "    semaphore = Semaphore(concurrency)\n",
    "    tasks = [\n",
    "        process_row(\n",
    "            system_message=system_prompt_replace_reasoning_format,\n",
    "            user_message=f\"Question:\\n{row.turns_str}\\nOld formatting:\\n{row.classification.value}\\nNew formatting:{format_mapping[row.classification.name]}\\n\",\n",
    "            response_model=UpdatedQuestion,\n",
    "            semaphore=semaphore,\n",
    "        )\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    return [r.updated_question for r in responses]\n",
    "\n",
    "\n",
    "replaced_reasoning = asyncio.run(replace_reasoning_questions_format(df_reasoning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reasoning[\"replaced_question\"] = replaced_reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reasoning.to_csv(reasoning_dir / \"updated_questions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_language = (\n",
    "    pd.read_json(language_dir / \"question.jsonl\", lines=True)\n",
    "    .assign(\n",
    "        turns_str=lambda x: x.turns.str[0],\n",
    "        ground_truth=lambda x: x.ground_truth.str.strip(),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"data_point_id\"})\n",
    ")\n",
    "\n",
    "assert df_language.turns.str.len().eq(1).all()\n",
    "\n",
    "df_language[\"replaced_question\"] = (\n",
    "    df_language.turns_str.str.replace(\"Begin the plot summary with <PLOT_SUMMARY>\", \"\")\n",
    "    .str.replace(\"in **bold**\", \"\")\n",
    "    .str.replace(\"**\", \"\")\n",
    "    .str.strip()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the system prompt and the Pydantic models you'll use to format the responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_without_structured_output = (\n",
    "    \"You're a helpful assistant. You will help me answer a question.\"\n",
    "    \"\\nPlease respond as follows:\"\n",
    "    \"\\nREASONING: <your reasoning here>\"\n",
    "    \"\\nANSWER: <your answer here>\"\n",
    ")\n",
    "\n",
    "system_prompt_with_structured_output = (\n",
    "    \"You're a helpful assistant. You will help me answer a question.\"\n",
    "    \"\\nYou will use this JSON schema for your response:\"\n",
    "    \"\\n{response_format}\"\n",
    ")\n",
    "\n",
    "\n",
    "class ResponseFormat(BaseModel):\n",
    "    reasoning: str\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the system prompt you send to the LLM, you'll replace `{response_format}` with the JSON schema of the response format you want to use.\n",
    "\n",
    "Then, let's define a few helper functions to run the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_response(response_text: str, type: str):\n",
    "    if type == \"without_structured_output\":\n",
    "        answer = response_text.split(\"\\nANSWER:\")[1].strip()\n",
    "        return answer\n",
    "    else:\n",
    "        return ResponseFormat.model_validate_json(response_text).answer\n",
    "\n",
    "\n",
    "@traceable\n",
    "async def process_row(row: pd.Series, type: str, semaphore: Semaphore) -> dict:\n",
    "    if type == \"without_structured_output\":\n",
    "        system_prompt = system_prompt_without_structured_output\n",
    "    else:\n",
    "        system_prompt = system_prompt_with_structured_output.format(\n",
    "            response_format=ResponseFormat.model_json_schema()\n",
    "        )\n",
    "    async with semaphore:\n",
    "        for _ in range(3):\n",
    "            try:\n",
    "                response = await langsmith_client.chat.completions.create(\n",
    "                    model=\"gpt-4o\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": f\"Question:\\n{row.replaced_question}\",\n",
    "                        },\n",
    "                    ],\n",
    "                    response_format={\"type\": \"json_object\"}\n",
    "                    if type == \"with_structured_output\"\n",
    "                    else None,\n",
    "                    timeout=30,\n",
    "                )\n",
    "                return validate_response(response.choices[0].message.content, type)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to generate a valid response for row {row.name}: {e}\")\n",
    "        raise Exception(\"Failed to generate a valid response and ran out of retries.\")\n",
    "\n",
    "\n",
    "@traceable\n",
    "async def main(df, response_format, concurrency: int = 50):\n",
    "    semaphore = Semaphore(concurrency)\n",
    "    tasks = [process_row(row, response_format, semaphore) for _, row in df.iterrows()]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, `validate_response` is used to check if the response is valid (i.e. it matches the JSON schema in the same order). If it is, it returns the response. Otherwise, it raises an exception.\n",
    "\n",
    "`process_row` is used to process a single row of the DataFrame. It sends the system prompt to the LLM and validates the response. It includes a simple retry mechanism in case the validation fails. Each run is tracked in LangSmith.\n",
    "\n",
    "Finally, `main` is used to run the experiment. It runs the `process_row` function concurrently for each row in the DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the experiment\n",
    "\n",
    "Now, you can run the experiment using the two response formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "df_copy = df_language.copy()\n",
    "\n",
    "responses_without_so = asyncio.run(main(df_copy, \"without_structured_output\"))\n",
    "responses_with_so = asyncio.run(main(df_copy, \"with_structured_output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "import re\n",
    "from itertools import permutations\n",
    "\n",
    "\n",
    "def levenshtein_distance(A, B):\n",
    "    N, M = len(A), len(B)\n",
    "    # Create an array of size NxM\n",
    "    dp = [[0 for i in range(M + 1)] for j in range(N + 1)]\n",
    "\n",
    "    # Base Case: When N = 0\n",
    "    for j in range(M + 1):\n",
    "        dp[0][j] = j\n",
    "    # Base Case: When M = 0\n",
    "    for i in range(N + 1):\n",
    "        dp[i][0] = i\n",
    "    # Transitions\n",
    "    for i in range(1, N + 1):\n",
    "        for j in range(1, M + 1):\n",
    "            if A[i - 1] == B[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(\n",
    "                    dp[i - 1][j],  # Insertion\n",
    "                    dp[i][j - 1],  # Deletion\n",
    "                    dp[i - 1][j - 1],  # Replacement\n",
    "                )\n",
    "\n",
    "    return dp[N][M]\n",
    "\n",
    "\n",
    "def plot_unscrambling_process_results(ground_truth: str, llm_answer: str) -> float:\n",
    "    gt_sentences = [s.strip() for s in ground_truth.split(\".\")]\n",
    "    ans_sentences = [s.strip() for s in llm_answer.split(\".\")]\n",
    "\n",
    "    gt_sentences = [s for s in gt_sentences if s]\n",
    "    ans_sentences = [s for s in ans_sentences if s]\n",
    "\n",
    "    ans_ordering = []\n",
    "    for x in gt_sentences:\n",
    "        best_match = difflib.get_close_matches(x, ans_sentences, n=1, cutoff=0.0)\n",
    "        if best_match:\n",
    "            ans_ordering.append(ans_sentences.index(best_match[0]))\n",
    "\n",
    "    n_sentences_gt = len(gt_sentences)\n",
    "    raw_distance = levenshtein_distance(list(range(len(gt_sentences))), ans_ordering)\n",
    "    score = 1 - (raw_distance / n_sentences_gt)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def evaluate_language_task(ground_truth: str, task_type: str, response: str):\n",
    "    if task_type == \"connections\":\n",
    "        objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in response.split(\",\")\n",
    "        ]\n",
    "        gt_objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in ground_truth.split(\",\")\n",
    "        ]\n",
    "\n",
    "        groups = [set(objects[i : i + 4]) for i in range(0, len(objects), 4)]\n",
    "        gt_groups = [set(gt_objects[i : i + 4]) for i in range(0, len(gt_objects), 4)]\n",
    "\n",
    "        max_correct = 0\n",
    "        for perm in permutations(groups):\n",
    "            correct_groups = sum(g1 == g2 for g1, g2 in zip(perm, gt_groups))\n",
    "            max_correct = max(max_correct, correct_groups)\n",
    "        return max_correct / len(gt_groups)\n",
    "    elif task_type == \"plot_unscrambling\":\n",
    "        return plot_unscrambling_process_results(ground_truth, response)\n",
    "    elif task_type == \"typos\":\n",
    "        return ground_truth in response\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid task type: {task_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy[\"response_without_so\"] = responses_without_so\n",
    "df_copy[\"is_correct_without_so\"] = df_copy.apply(\n",
    "    lambda row: evaluate_language_task(\n",
    "        row[\"ground_truth\"], row[\"task\"], row[\"response_without_so\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "df_copy[\"response_with_so\"] = responses_with_so\n",
    "df_copy[\"is_correct_with_so\"] = df_copy.apply(\n",
    "    lambda row: evaluate_language_task(\n",
    "        row[\"ground_truth\"], row[\"task\"], row[\"response_with_so\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_questions</th>\n",
       "      <th>accuracy_without_so</th>\n",
       "      <th>accuracy_with_so</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>connections</th>\n",
       "      <td>50</td>\n",
       "      <td>0.423333</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plot_unscrambling</th>\n",
       "      <td>40</td>\n",
       "      <td>0.30545</td>\n",
       "      <td>0.369002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>typos</th>\n",
       "      <td>50</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   n_questions accuracy_without_so accuracy_with_so\n",
       "task                                                               \n",
       "connections                 50            0.423333             0.55\n",
       "plot_unscrambling           40             0.30545         0.369002\n",
       "typos                       50                 0.6             0.52"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.groupby(\"task\").agg(\n",
    "    n_questions=(\"data_point_id\", \"count\"),\n",
    "    accuracy_without_so=(\"is_correct_without_so\", \"mean\"),\n",
    "    accuracy_with_so=(\"is_correct_with_so\", \"mean\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context(\"display.max_colwidth\", None):   \n",
    "#     display(\n",
    "#         df_copy.query(\"task == 'connections' and is_correct_without_so == False\")[\n",
    "#             [\n",
    "#         \"ground_truth\",\n",
    "#         \"replaced_question\",\n",
    "#         \"response_without_so\",\n",
    "#         \"response_with_so\",\n",
    "#             ]\n",
    "#         ]\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the experiment multiple times with the same inputs to account for the randomness in the LLM's responses. Ideally, we should run it more than three times, but I'm poor. So, we'll just do it 3 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response format without SO - Mean: 45.27% CI: 38.55% - 52.19%\n",
      "Response format with SO - Mean: 48.04% CI: 41.20% - 54.90%\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "n_bootstraps = 10_000\n",
    "bootstrap_accuracies_without_so = []\n",
    "bootstrap_accuracies_with_so = []\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    bootstrap_sample = df_copy.sample(n=len(df_copy), replace=True)\n",
    "\n",
    "    mean_accuracy_without_so = bootstrap_sample[\"is_correct_without_so\"].mean()\n",
    "    mean_accuracy_with_so = bootstrap_sample[\"is_correct_with_so\"].mean()\n",
    "\n",
    "    bootstrap_accuracies_without_so.append(mean_accuracy_without_so)\n",
    "    bootstrap_accuracies_with_so.append(mean_accuracy_with_so)\n",
    "\n",
    "ci_without_so = np.percentile(bootstrap_accuracies_without_so, [2.5, 97.5])\n",
    "ci_with_so = np.percentile(bootstrap_accuracies_with_so, [2.5, 97.5])\n",
    "\n",
    "mean_accuracy_without_so = df_copy[\"is_correct_without_so\"].mean()\n",
    "mean_accuracy_with_so = df_copy[\"is_correct_with_so\"].mean()\n",
    "\n",
    "print(\n",
    "    f\"Response format without SO - Mean: {mean_accuracy_without_so * 100:.2f}% CI: {ci_without_so[0] * 100:.2f}% - {ci_without_so[1] * 100:.2f}%\"\n",
    ")\n",
    "print(\n",
    "    f\"Response format with SO - Mean: {mean_accuracy_with_so * 100:.2f}% CI: {ci_with_so[0] * 100:.2f}% - {ci_with_so[1] * 100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can build bootstrap confidence intervals for the accuracies of the two response formats. Given that I'm asking the LLM the same question multiple times, I went with an approach called [cluster bootstrapping](https://pmc.ncbi.nlm.nih.gov/articles/PMC5965657/), which accounts for the fact that the data points are not independent.\n",
    "\n",
    "It should take a few minutes to run. Once it's done, you should see output like the following:\n",
    "\n",
    "| Response Format | Mean (95% CI)           |\n",
    "|-----------------|-------------------------|\n",
    "| A        | 46.67% (34.67% – 58.67%) |\n",
    "| B        | 30.67% (20.67% – 41.33%) |\n",
    "\n",
    "These results suggest that the order of the fields in the JSON schema does matter.\n",
    "\n",
    "But if you're still unsure, you can perform a t-test to see if the two response formats are statistically different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic: -0.7477145847774228, p-value: 0.45589544260063786\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "accuracies_without_so = df_copy[\"is_correct_without_so\"]\n",
    "accuracies_with_so = df_copy[\"is_correct_with_so\"]\n",
    "\n",
    "t_stat, p_value = stats.ttest_rel(accuracies_without_so, accuracies_with_so)\n",
    "\n",
    "print(f\"t-statistic: {t_stat}, p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got a p-value <0.001, meaning I can reject the null hypothesis that the two response formats are the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Based on the results of the experiment, we can safely say that `ResponseFormatA` is better than `ResponseFormatB`.\n",
    "\n",
    "But why?\n",
    "\n",
    "In this case, it's simple. \n",
    "\n",
    "These response formats are meant to help the LLM reason step by step to arrive at the answer. This is known as [chain of thought reasoning](https://en.wikipedia.org/wiki/Chain_of_thought_reasoning). However, for it to work, we need the LLM to first provide us with the reasoning of how it arrived at the answer and then the answer.\n",
    "\n",
    "In `ResponseFormatB`, we defined our Pydantic model with the answer first and the reasoning second. This means that the LLM will give us the answer first, and then adjust the reasoning to match that answer. `ResponseFormatA` does the opposite, which is why it performs better.\n",
    "\n",
    "So, to summarize, when using structured outputs, don't put the cart before the horse.\n",
    "\n",
    "That's all! Let me know if you have any questions in the comments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
