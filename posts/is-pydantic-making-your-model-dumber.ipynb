{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Is Pydantic making your model dumber?\"\n",
    "date: \"11/10/2024\"\n",
    "date-modified: last-modified\n",
    "description-meta: \"\"\n",
    "toc: true\n",
    "toc-depth: 3\n",
    "lightbox: true\n",
    "fig-cap-location: margin\n",
    "categories:\n",
    "  - llm\n",
    "  - openai\n",
    "  - pydantic\n",
    "  - python\n",
    "author:\n",
    "  - name: Dylan Castillo\n",
    "    url: https://dylancastillo.co\n",
    "    affiliation: Iwana Labs\n",
    "    affiliation-url: https://iwanalabs.com\n",
    "citation: true\n",
    "comments:\n",
    "  utterances:\n",
    "    repo: dylanjcastillo/blog_comments\n",
    "    theme: dark-blue\n",
    "    issue-term: pathname\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import difflib\n",
    "import json\n",
    "import re\n",
    "from asyncio import Semaphore\n",
    "from enum import Enum\n",
    "from itertools import permutations\n",
    "from pathlib import Path\n",
    "\n",
    "import instructor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from openai import AsyncOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "langsmith_client = wrap_openai(AsyncOpenAI())\n",
    "instructor_client = instructor.from_openai(langsmith_client, mode=instructor.Mode.TOOLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path().absolute().parent / \"data\" / \"live_bench\"\n",
    "reasoning_dir = data_dir / \"reasoning\"\n",
    "math_dir = data_dir / \"math\"\n",
    "language_dir = data_dir / \"language\"\n",
    "\n",
    "df_reasoning = pd.read_json(reasoning_dir / \"updated_questions.jsonl\", lines=True)\n",
    "df_language = pd.read_json(language_dir / \"updated_questions.jsonl\", lines=True)\n",
    "df_math = pd.read_json(math_dir / \"updated_questions.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_without_structured_output = (\n",
    "    \"You're a helpful assistant. You will help me answer a question.\"\n",
    "    \"\\nYou must respond using the following format:\"\n",
    "    \"\\nREASONING: <your reasoning here>\"\n",
    "    \"\\nANSWER: <your answer here, don't include any other text>\"\n",
    ")\n",
    "\n",
    "system_prompt_with_structured_output = (\n",
    "    \"You're a helpful assistant. You will help me answer a question.\"\n",
    "    \"\\nYou must respond using the following JSON schema:\"\n",
    "    \"\\n{response_format}\"\n",
    ")\n",
    "\n",
    "\n",
    "class ResponseFormat(BaseModel):\n",
    "    reasoning: str = Field(description=\"Your reasoning explaining your answer.\")\n",
    "    answer: str = Field(description=\"Your answer, don't include any other text.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_response(response_text: str, type: str):\n",
    "    if type == \"without_structured_output\":\n",
    "        answer = response_text.split(\"\\nANSWER:\")[1].strip()\n",
    "        return answer\n",
    "    else:\n",
    "        return ResponseFormat.model_validate_json(response_text).answer\n",
    "\n",
    "\n",
    "@traceable\n",
    "async def process_row(row: pd.Series, type: str, semaphore: Semaphore) -> dict:\n",
    "    if type == \"without_structured_output\":\n",
    "        system_prompt = system_prompt_without_structured_output\n",
    "    else:\n",
    "        system_prompt = system_prompt_with_structured_output.format(\n",
    "            response_format=ResponseFormat.model_json_schema()\n",
    "        )\n",
    "    async with semaphore:\n",
    "        for _ in range(3):\n",
    "            try:\n",
    "                response = await langsmith_client.chat.completions.create(\n",
    "                    model=\"gpt-4o\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": f\"Question:\\n{row.updated_question}\",\n",
    "                        },\n",
    "                    ],\n",
    "                    response_format={\"type\": \"json_object\"}\n",
    "                    if type == \"with_structured_output\"\n",
    "                    else None,\n",
    "                    timeout=120,\n",
    "                )\n",
    "                return validate_response(response.choices[0].message.content, type)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to generate a valid response for row {row.name}: {e}\")\n",
    "        raise Exception(\"Failed to generate a valid response and ran out of retries.\")\n",
    "\n",
    "\n",
    "@traceable\n",
    "async def process_df(df, response_format, concurrency: int = 50):\n",
    "    semaphore = Semaphore(concurrency)\n",
    "    tasks = [process_row(row, response_format, semaphore) for _, row in df.iterrows()]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://github.com/LiveBench/LiveBench/blob/main/livebench/process_results/writing/plot_unscrambling/utils.py\n",
    "def levenshtein_distance(A, B):\n",
    "    N, M = len(A), len(B)\n",
    "    # Create an array of size NxM\n",
    "    dp = [[0 for i in range(M + 1)] for j in range(N + 1)]\n",
    "\n",
    "    # Base Case: When N = 0\n",
    "    for j in range(M + 1):\n",
    "        dp[0][j] = j\n",
    "    # Base Case: When M = 0\n",
    "    for i in range(N + 1):\n",
    "        dp[i][0] = i\n",
    "    # Transitions\n",
    "    for i in range(1, N + 1):\n",
    "        for j in range(1, M + 1):\n",
    "            if A[i - 1] == B[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(\n",
    "                    dp[i - 1][j],  # Insertion\n",
    "                    dp[i][j - 1],  # Deletion\n",
    "                    dp[i - 1][j - 1],  # Replacement\n",
    "                )\n",
    "\n",
    "    return dp[N][M]\n",
    "\n",
    "\n",
    "def plot_unscrambling_process_results(ground_truth: str, llm_answer: str) -> float:\n",
    "    gt_sentences = [s.strip() for s in ground_truth.split(\".\")]\n",
    "    ans_sentences = [s.strip() for s in llm_answer.split(\".\")]\n",
    "\n",
    "    gt_sentences = [s for s in gt_sentences if s]\n",
    "    ans_sentences = [s for s in ans_sentences if s]\n",
    "\n",
    "    ans_ordering = []\n",
    "    for x in gt_sentences:\n",
    "        best_match = difflib.get_close_matches(x, ans_sentences, n=1, cutoff=0.0)\n",
    "        if best_match:\n",
    "            ans_ordering.append(ans_sentences.index(best_match[0]))\n",
    "\n",
    "    n_sentences_gt = len(gt_sentences)\n",
    "    raw_distance = levenshtein_distance(list(range(len(gt_sentences))), ans_ordering)\n",
    "    score = 1 - (raw_distance / n_sentences_gt)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def evaluate_language_task(ground_truth: str, task_type: str, response: str):\n",
    "    if task_type == \"connections\":\n",
    "        objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in response.split(\",\")\n",
    "        ]\n",
    "        gt_objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in ground_truth.split(\",\")\n",
    "        ]\n",
    "\n",
    "        groups = [set(objects[i : i + 4]) for i in range(0, len(objects), 4)]\n",
    "        gt_groups = [set(gt_objects[i : i + 4]) for i in range(0, len(gt_objects), 4)]\n",
    "\n",
    "        max_correct = 0\n",
    "        for perm in permutations(groups):\n",
    "            correct_groups = sum(g1 == g2 for g1, g2 in zip(perm, gt_groups))\n",
    "            max_correct = max(max_correct, correct_groups)\n",
    "        return max_correct / len(gt_groups)\n",
    "    elif task_type == \"plot_unscrambling\":\n",
    "        return plot_unscrambling_process_results(ground_truth, response)\n",
    "    elif task_type == \"typos\":\n",
    "        return ground_truth in response\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid task type: {task_type}\")\n",
    "\n",
    "def evaluate_reasoning_task(ground_truth: str, task_type: str, response: str):\n",
    "    if task_type == \"web_of_lies_v2\":\n",
    "        response_objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in response.split(\",\")\n",
    "        ]\n",
    "        gt_objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in ground_truth.split(\",\")\n",
    "        ]\n",
    "        return response_objects == gt_objects\n",
    "    elif task_type in (\"spatial\", \"zebra_puzzle\"):\n",
    "        response = response.rstrip(\".\")\n",
    "        return ground_truth.lower().strip() == response.lower().strip()\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid task type: {task_type}\")\n",
    "\n",
    "def evaluate_math_task(ground_truth: str, task_type: str, response: str):\n",
    "    if task_type == \"olympiad\":\n",
    "        response_objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in response.split(\",\")\n",
    "        ]\n",
    "        gt_objects = [\n",
    "            re.sub(r\"[^\\w\\s]\", \"\", o.strip().lower()) for o in ground_truth.split(\",\")\n",
    "        ]\n",
    "        return response_objects == gt_objects\n",
    "    elif task_type == \"math_comp\":\n",
    "        return ground_truth == response\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid task type: {task_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "def generate_outputs(df):\n",
    "    df_copy = df.copy()\n",
    "    responses_without_so = asyncio.run(process_df(df_copy, \"without_structured_output\"))\n",
    "    responses_with_so = asyncio.run(process_df(df_copy, \"with_structured_output\"))\n",
    "    df_copy[\"response_without_so\"] = responses_without_so\n",
    "    df_copy[\"response_with_so\"] = responses_with_so\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def evaluate_outputs(df, evaluator):\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"is_correct_without_so\"] = df_copy.apply(\n",
    "        lambda row: evaluator(row[\"ground_truth\"], row[\"task\"], row[\"response_without_so\"]),\n",
    "        axis=1,\n",
    "    )\n",
    "    df_copy[\"is_correct_with_so\"] = df_copy.apply(\n",
    "        lambda row: evaluator(row[\"ground_truth\"], row[\"task\"], row[\"response_with_so\"]),\n",
    "        axis=1,\n",
    "    )\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate a valid response for row 76: Request timed out.\n",
      "Failed to generate a valid response for row 75: 1 validation error for ResponseFormat\n",
      "answer\n",
      "  Field required [type=missing, input_value={'reasoning': \"Let's reas...s:', answer='manager'}\"}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/missing\n"
     ]
    }
   ],
   "source": [
    "df_reasoning_results = generate_outputs(df_reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reasoning_results = evaluate_outputs(df_reasoning_results, evaluate_reasoning_task)\n",
    "df_reasoning_results.to_csv(data_dir / \"reasoning\" / \"reasoning_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_questions</th>\n",
       "      <th>accuracy_without_so</th>\n",
       "      <th>accuracy_with_so</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spatial</th>\n",
       "      <td>50</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>web_of_lies_v2</th>\n",
       "      <td>50</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zebra_puzzle</th>\n",
       "      <td>50</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                n_questions  accuracy_without_so  accuracy_with_so\n",
       "task                                                              \n",
       "spatial                  50                 0.36              0.44\n",
       "web_of_lies_v2           50                 0.38              0.60\n",
       "zebra_puzzle             50                 0.44              0.44"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reasoning_results.groupby(\"task\").agg(\n",
    "    n_questions=(\"question_id\", \"count\"),\n",
    "    accuracy_without_so=(\"is_correct_without_so\", \"mean\"),\n",
    "    accuracy_with_so=(\"is_correct_with_so\", \"mean\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_language_results = generate_outputs(df_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_language_results = evaluate_outputs(df_language_results, evaluate_language_task)\n",
    "df_language_results.to_csv(data_dir / \"language\" / \"language_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_questions</th>\n",
       "      <th>accuracy_without_so</th>\n",
       "      <th>accuracy_with_so</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>connections</th>\n",
       "      <td>50</td>\n",
       "      <td>0.446667</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plot_unscrambling</th>\n",
       "      <td>40</td>\n",
       "      <td>0.337397</td>\n",
       "      <td>0.37747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>typos</th>\n",
       "      <td>50</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   n_questions accuracy_without_so accuracy_with_so\n",
       "task                                                               \n",
       "connections                 50            0.446667             0.43\n",
       "plot_unscrambling           40            0.337397          0.37747\n",
       "typos                       50                0.66             0.64"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_language_results.groupby(\"task\").agg(\n",
    "    n_questions=(\"question_id\", \"count\"),\n",
    "    accuracy_without_so=(\"is_correct_without_so\", \"mean\"),\n",
    "    accuracy_with_so=(\"is_correct_with_so\", \"mean\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate a valid response for row 14: 1 validation error for ResponseFormat\n",
      "answer\n",
      "  Field required [type=missing, input_value={'reasoning': 'To solve t...= Math-refundable: \\\\)'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/missing\n",
      "Failed to generate a valid response for row 17: 1 validation error for ResponseFormat\n",
      "answer\n",
      "  Field required [type=missing, input_value={'reasoning': 'To solve t...n \\'answer\\': \\'025\\'}'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/missing\n"
     ]
    }
   ],
   "source": [
    "df_math_results = generate_outputs(df_math)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_math_results = evaluate_outputs(df_math_results, evaluate_math_task)\n",
    "df_math_results.to_csv(data_dir / \"math\" / \"math_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_questions</th>\n",
       "      <th>accuracy_without_so</th>\n",
       "      <th>accuracy_with_so</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>math_comp</th>\n",
       "      <td>96</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.427083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>olympiad</th>\n",
       "      <td>36</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.305556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           n_questions  accuracy_without_so  accuracy_with_so\n",
       "task                                                         \n",
       "math_comp           96             0.375000          0.427083\n",
       "olympiad            36             0.277778          0.305556"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_math_results.groupby(\"task\").agg(\n",
    "    n_questions=(\"question_id\", \"count\"),\n",
    "    accuracy_without_so=(\"is_correct_without_so\", \"mean\"),\n",
    "    accuracy_with_so=(\"is_correct_with_so\", \"mean\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "def calculate_confidence_intervals(df):\n",
    "    mean_accuracy_without_so = df[\"is_correct_without_so\"].mean()\n",
    "    mean_accuracy_with_so = df[\"is_correct_with_so\"].mean()\n",
    "\n",
    "    n = len(df)\n",
    "    se_without_so = df[\"is_correct_without_so\"].std() / np.sqrt(n)\n",
    "    se_with_so = df[\"is_correct_with_so\"].std() / np.sqrt(n)\n",
    "\n",
    "    ci_without_so = [\n",
    "        mean_accuracy_without_so - 1.96 * se_without_so,\n",
    "        mean_accuracy_without_so + 1.96 * se_without_so,\n",
    "    ]\n",
    "    ci_with_so = [\n",
    "        mean_accuracy_with_so - 1.96 * se_with_so,\n",
    "        mean_accuracy_with_so + 1.96 * se_with_so,\n",
    "    ]\n",
    "\n",
    "    print(\n",
    "        f\"Response format without SO - Mean: {mean_accuracy_without_so * 100:.2f}% CI: {ci_without_so[0] * 100:.2f}% - {ci_without_so[1] * 100:.2f}%\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Response format with SO - Mean: {mean_accuracy_with_so * 100:.2f}% CI: {ci_with_so[0] * 100:.2f}% - {ci_with_so[1] * 100:.2f}%\"\n",
    "    )\n",
    "\n",
    "def run_paired_t_test(df):\n",
    "    accuracies_without_so = df[\"is_correct_without_so\"] * 1\n",
    "    accuracies_with_so = df[\"is_correct_with_so\"] * 1\n",
    "    t_stat, p_value = stats.ttest_rel(accuracies_without_so, accuracies_with_so)\n",
    "    print(f\"t-statistic: {t_stat}, p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response format without SO - Mean: 39.33% CI: 31.49% - 47.18%\n",
      "Response format with SO - Mean: 49.33% CI: 41.31% - 57.36%\n",
      "t-statistic: -2.2667004730406615, p-value: 0.024847877331910594\n"
     ]
    }
   ],
   "source": [
    "calculate_confidence_intervals(df_reasoning_results)\n",
    "run_paired_t_test(df_reasoning_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response format without SO - Mean: 49.16% CI: 42.39% - 55.94%\n",
      "Response format with SO - Mean: 49.00% CI: 42.07% - 55.92%\n",
      "t-statistic: 0.05090289868160946, p-value: 0.9594759715752339\n"
     ]
    }
   ],
   "source": [
    "calculate_confidence_intervals(df_language_results)\n",
    "run_paired_t_test(df_language_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response format without SO - Mean: 34.85% CI: 26.69% - 43.01%\n",
      "Response format with SO - Mean: 39.39% CI: 31.03% - 47.76%\n",
      "t-statistic: -1.227088882859258, p-value: 0.22198988572802872\n"
     ]
    }
   ],
   "source": [
    "calculate_confidence_intervals(df_math_results)\n",
    "run_paired_t_test(df_math_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
