{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3b314c8",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Can you really 'say what you mean'?\"\n",
    "date: \"12/01/2024\"\n",
    "date-modified: last-modified\n",
    "description-meta: \"A look at the impact of structured outputs on the performance of proprietary LLMs.\"\n",
    "toc: true\n",
    "toc-depth: 3\n",
    "lightbox: true\n",
    "fig-cap-location: margin\n",
    "categories:\n",
    "  - llm\n",
    "  - openai\n",
    "  - pydantic\n",
    "  - python\n",
    "author:\n",
    "  - name: Dylan Castillo\n",
    "    url: https://dylancastillo.co\n",
    "    affiliation: Iwana Labs\n",
    "    affiliation-url: https://iwanalabs.com\n",
    "citation: true\n",
    "comments:\n",
    "  utterances:\n",
    "    repo: dylanjcastillo/blog_comments\n",
    "    theme: dark-blue\n",
    "    issue-term: pathname\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18dc8ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65549cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33194d54-582b-4b81-a195-8f1269a438a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T20:37:59.673511Z",
     "iopub.status.busy": "2024-11-17T20:37:59.673328Z",
     "iopub.status.idle": "2024-11-17T20:38:07.222035Z",
     "shell.execute_reply": "2024-11-17T20:38:07.221730Z",
     "shell.execute_reply.started": "2024-11-17T20:37:59.673491Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dcast/Documents/GitHub/blog/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "\n",
    "import asyncio\n",
    "import re\n",
    "from asyncio import Semaphore\n",
    "from enum import Enum\n",
    "from textwrap import dedent\n",
    "from typing import Callable, List, Literal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from openai import AsyncOpenAI\n",
    "from openai.types.chat import ChatCompletion\n",
    "from pydantic import BaseModel, ConfigDict, ValidationError\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "\n",
    "client = wrap_openai(AsyncOpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39c919d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseGSM8K(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\", title=\"Response\")\n",
    "    reasoning: str\n",
    "    answer: int\n",
    "\n",
    "\n",
    "class ResponseLastLetter(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\", title=\"Response\")\n",
    "    reasoning: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "class ResponseShuffledObjects(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\", title=\"Response\")\n",
    "    reasoning: str\n",
    "    answer: Literal[\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "\n",
    "\n",
    "class PromptType(Enum):\n",
    "    WITHOUT_STRUCTURED_OUTPUT = \"without_so\"\n",
    "    WITH_TOOL_CALLS = \"with_so_tool_calls\"\n",
    "    WITH_JSON_MODE = \"with_so_json_mode\"\n",
    "    WITH_STRICT_TOOL_CALLS = \"with_so_strict_tool_calls\"\n",
    "\n",
    "\n",
    "class ClientConfig(BaseModel):\n",
    "    name: str\n",
    "    use_json_format: bool\n",
    "    col_name: str\n",
    "    score_col_name: str\n",
    "\n",
    "\n",
    "CONFIGS = [\n",
    "    ClientConfig(\n",
    "        name=PromptType.WITHOUT_STRUCTURED_OUTPUT.value,\n",
    "        use_json_format=False,\n",
    "        col_name=f\"response_{PromptType.WITHOUT_STRUCTURED_OUTPUT.value}\",\n",
    "        score_col_name=f\"score_{PromptType.WITHOUT_STRUCTURED_OUTPUT.value}\",\n",
    "    ),\n",
    "    ClientConfig(\n",
    "        name=PromptType.WITH_TOOL_CALLS.value,\n",
    "        use_json_format=True,\n",
    "        col_name=f\"response_{PromptType.WITH_TOOL_CALLS.value}\",\n",
    "        score_col_name=f\"score_{PromptType.WITH_TOOL_CALLS.value}\",\n",
    "    ),\n",
    "    ClientConfig(\n",
    "        name=PromptType.WITH_JSON_MODE.value,\n",
    "        use_json_format=True,\n",
    "        col_name=f\"response_{PromptType.WITH_JSON_MODE.value}\",\n",
    "        score_col_name=f\"score_{PromptType.WITH_JSON_MODE.value}\",\n",
    "    ),\n",
    "    ClientConfig(\n",
    "        name=PromptType.WITH_STRICT_TOOL_CALLS.value,\n",
    "        use_json_format=True,\n",
    "        col_name=f\"response_{PromptType.WITH_STRICT_TOOL_CALLS.value}\",\n",
    "        score_col_name=f\"score_{PromptType.WITH_STRICT_TOOL_CALLS.value}\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eb319a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        configs: List[ClientConfig],\n",
    "        create_prompt_fn: Callable,\n",
    "        parse_response_fn: Callable,\n",
    "        response_model: ResponseGSM8K | ResponseLastLetter | ResponseShuffledObjects,\n",
    "        concurrency: int = 100,\n",
    "    ):\n",
    "        self.configs = configs\n",
    "        self.create_prompt_fn = create_prompt_fn\n",
    "        self.parse_response_fn = parse_response_fn\n",
    "        self.response_model = response_model\n",
    "        self.concurrency = concurrency\n",
    "\n",
    "    def _create_tool_call_schema(\n",
    "        self,\n",
    "        strict: bool = False,\n",
    "    ) -> dict:\n",
    "        model_schema = self.response_model.model_json_schema()\n",
    "        if strict:\n",
    "            return {\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": model_schema[\"title\"],\n",
    "                    \"schema\": model_schema,\n",
    "                    \"strict\": True,\n",
    "                },\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": model_schema[\"title\"],\n",
    "                    \"description\": f\"Correctly extracted `{model_schema['title']}` with all the required parameters with correct types\",\n",
    "                    \"parameters\": model_schema,\n",
    "                },\n",
    "            }\n",
    "            \n",
    "\n",
    "    @traceable(run_type=\"prompt\")\n",
    "    def create_prompt(self, question: str, use_json_format: bool) -> List[dict]:\n",
    "        return self.create_prompt_fn(question, use_json_format)\n",
    "\n",
    "    @traceable(run_type=\"parser\")\n",
    "    def parse_response(\n",
    "        self,\n",
    "        response: ChatCompletion\n",
    "        | ResponseGSM8K\n",
    "        | ResponseLastLetter\n",
    "        | ResponseShuffledObjects,\n",
    "    ) -> str | int:\n",
    "        return self.parse_response_fn(response)\n",
    "\n",
    "    @traceable(run_type=\"llm\")\n",
    "    async def call_llm(\n",
    "        self,\n",
    "        prompt_type: str,\n",
    "        use_json_format: bool,\n",
    "        question: str,\n",
    "    ) -> ChatCompletion | ResponseGSM8K | ResponseLastLetter | ResponseShuffledObjects:\n",
    "        params = {\n",
    "            \"messages\": self.create_prompt_fn(\n",
    "                question=question, use_json_format=use_json_format\n",
    "            ),\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"timeout\": 120,\n",
    "        }\n",
    "\n",
    "        if prompt_type == PromptType.WITHOUT_STRUCTURED_OUTPUT.value:\n",
    "            pass\n",
    "        elif prompt_type == PromptType.WITH_JSON_MODE.value:\n",
    "            params.update({\"response_format\": {\"type\": \"json_object\"}})\n",
    "        elif prompt_type == PromptType.WITH_TOOL_CALLS.value:\n",
    "            params.update(\n",
    "                {\n",
    "                    \"tools\": [self._create_tool_call_schema(strict=False)]\n",
    "                }\n",
    "            )\n",
    "        elif prompt_type == PromptType.WITH_STRICT_TOOL_CALLS.value:\n",
    "            params.update(\n",
    "                {\n",
    "                    \"response_format\": self._create_tool_call_schema(strict=True)\n",
    "                }\n",
    "            )\n",
    "        response = await client.chat.completions.create(**params)\n",
    "        return response\n",
    "\n",
    "    @traceable(run_type=\"chain\")\n",
    "    async def process_question(\n",
    "        self,\n",
    "        question: str,\n",
    "        prompt_type: str,\n",
    "        use_json_format: bool,\n",
    "        semaphore: Semaphore,\n",
    "        max_attempts: int = 3,\n",
    "    ) -> str | int | None:\n",
    "        async with semaphore:\n",
    "            for _ in range(max_attempts):\n",
    "                try:\n",
    "                    answer = await self.call_llm(\n",
    "                        prompt_type=prompt_type,\n",
    "                        use_json_format=use_json_format,\n",
    "                        question=question,\n",
    "                    )\n",
    "                    parsed_answer = self.parse_response(answer)\n",
    "                    return parsed_answer\n",
    "                except Exception as e:\n",
    "                    print(f\"{prompt_type}: Error processing question {question}: {e}. Will try again.\")\n",
    "                    continue\n",
    "            raise Exception(\n",
    "                f\"{prompt_type}: Failed to process question {question}, after 3 attempts\"\n",
    "            )\n",
    "\n",
    "    @traceable(run_type=\"chain\")\n",
    "    async def process_questions(\n",
    "        self,\n",
    "        questions: List[dict],\n",
    "        prompt_type: str,\n",
    "        use_json_format: bool,\n",
    "    ) -> List[str | int | None]:\n",
    "        semaphore = Semaphore(self.concurrency)\n",
    "        tasks = [\n",
    "            self.process_question(\n",
    "                question=question[\"question\"],\n",
    "                prompt_type=prompt_type,\n",
    "                use_json_format=use_json_format,\n",
    "                semaphore=semaphore,\n",
    "            )\n",
    "            for question in questions\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        return results\n",
    "\n",
    "    def generate_outputs(self, questions: List[dict]) -> pd.DataFrame:\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"id\": [i for i in range(len(questions))],\n",
    "                \"question\": [question[\"question\"] for question in questions],\n",
    "                \"answer\": [question[\"answer\"] for question in questions],\n",
    "            }\n",
    "        )\n",
    "        for config in self.configs:\n",
    "            responses = asyncio.run(\n",
    "                self.process_questions(\n",
    "                    questions=questions,\n",
    "                    prompt_type=config.name,\n",
    "                    use_json_format=config.use_json_format,\n",
    "                )\n",
    "            )\n",
    "            df[config.col_name] = responses\n",
    "        return df\n",
    "\n",
    "    def evaluate_outputs(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df_copy = df.copy()\n",
    "        for config in self.configs:\n",
    "            df_copy[config.score_col_name] = (\n",
    "                df_copy[\"answer\"] == df_copy[config.col_name]\n",
    "            ) * 1\n",
    "        return df_copy\n",
    "\n",
    "    def calculate_confidence_intervals(\n",
    "        self, df: pd.DataFrame, conf_level: float = 0.95\n",
    "    ) -> None:\n",
    "        print(\n",
    "            f\"Calculating confidence intervals ({conf_level}) with {len(df)} observations:\"\n",
    "        )\n",
    "        for config in self.configs:\n",
    "            score_col = config.score_col_name\n",
    "            scores = df[score_col]\n",
    "\n",
    "            if len(scores) == 0:\n",
    "                print(f\"No scores available for {score_col}\")\n",
    "                continue\n",
    "\n",
    "            mean_score = scores.mean()\n",
    "            se_score = scores.std() / np.sqrt(len(scores))\n",
    "\n",
    "            z_score = stats.norm.ppf((1 + conf_level) / 2)\n",
    "            margin_error = z_score * se_score\n",
    "            ci = [mean_score - margin_error, mean_score + margin_error]\n",
    "\n",
    "            print(\n",
    "                f\"{score_col} - Mean: {mean_score * 100:.2f}% CI: {ci[0] * 100:.2f}% - {ci[1] * 100:.2f}%\"\n",
    "            )\n",
    "        print()\n",
    "\n",
    "    def run_paired_t_test(self, df: pd.DataFrame) -> None:\n",
    "        scores = {}\n",
    "\n",
    "        for config in self.configs:\n",
    "            score_col = config.score_col_name\n",
    "            scores[score_col] = df[score_col] * 1\n",
    "\n",
    "        for score_col_1, score_col_2 in [\n",
    "            (\"score_without_so\", \"score_with_so_tool_calls\"),\n",
    "            (\"score_without_so\", \"score_with_so_json_mode\"),\n",
    "            (\"score_without_so\", \"score_with_so_strict_tool_calls\"),\n",
    "        ]:\n",
    "            if score_col_1 in scores and score_col_2 in scores:\n",
    "                t_stat, p_value = stats.ttest_rel(\n",
    "                    scores[score_col_1], scores[score_col_2]\n",
    "                )\n",
    "                print(f\"{score_col_1} vs {score_col_2}\")\n",
    "                print(f\"t-statistic: {t_stat}, p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8896e2c6",
   "metadata": {},
   "source": [
    "## GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ff85876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_gsm8k(question, use_json_format=True):\n",
    "    json_system_prompt = dedent(\"\"\"\n",
    "        You are an expert in solving grade school math tasks. You will be presented with a grade-school math word problem and be asked to solve it.\n",
    "\n",
    "        You will always respond with JSON in the format described below:\n",
    "        \n",
    "        {\"reasoning\": <str, step by step reasoning about the answer>, \"answer\": <int, final answer>}\n",
    "\n",
    "        First, provide your step by step reasoning in the \"reasoning\" field. Then, in the \"answer\" field, provide an integer that corresponds to the correct answer to the question. Don't include any other text in the \"answer\" field. Both fields are required.\n",
    "        \"\"\")\n",
    "\n",
    "    explanation_system_prompt = dedent(\"\"\"\n",
    "        You are an expert in solving grade school math tasks. You will be presented with a grade-school math word problem and be asked to solve it.\n",
    "        \n",
    "        You will always respond in the following format:\n",
    "        \n",
    "        <str, step by step reasoning about the answer>\n",
    "        ANSWER: <int, final answer>\n",
    "        \n",
    "        First, provide your step by step reasoning. Then, in ANSWER, provide an integer that corresponds to the correct answer to the question. Don't include any other text in ANSWER.\n",
    "        \"\"\")\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": json_system_prompt\n",
    "            if use_json_format\n",
    "            else explanation_system_prompt,\n",
    "        },\n",
    "    ]\n",
    "    example_question = [\n",
    "        \"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\",\n",
    "        \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\",\n",
    "        \"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\",\n",
    "    ]\n",
    "\n",
    "    example_explanation = [\n",
    "        \"There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6.\",\n",
    "        \"There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5.\",\n",
    "        \"Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39.\",\n",
    "    ]\n",
    "\n",
    "    example_answer = [6, 5, 39]\n",
    "\n",
    "    for example_q, example_reason, example_ans in zip(\n",
    "        example_question, example_explanation, example_answer\n",
    "    ):\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Question: {example_q}\"})\n",
    "\n",
    "        if use_json_format:\n",
    "            response = f'{{\"reasoning\": \"{example_reason}\", \"answer\": {example_ans}}}'\n",
    "        else:\n",
    "            response = f\"{example_reason}\\nANSWER: {example_ans}\"\n",
    "\n",
    "        messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"Question: {question}\"})\n",
    "\n",
    "    return messages\n",
    "\n",
    "\n",
    "def parse_response_gsm8k(response: ChatCompletion) -> int | None:\n",
    "    try:\n",
    "        return ResponseGSM8K.model_validate_json(\n",
    "            response.choices[0].message.content\n",
    "        ).answer\n",
    "    except ValidationError:\n",
    "        response_text = (\n",
    "            response.choices[0].message.content.split(\"\\nANSWER:\")[1].replace(\",\", \"\").strip()\n",
    "        )\n",
    "        try:\n",
    "            return int(response_text)\n",
    "        except ValueError:\n",
    "            print(f\"Could not parse response: {response}. Set to null.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77ab558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "evals = [\n",
    "    {\"question\": d[\"question\"], \"answer\": int(d[\"answer\"].split(\"#### \")[1].replace(\",\", \"\").strip())}\n",
    "    for d in dataset[\"test\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8bb4c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not parse response: ChatCompletion(id='chatcmpl-AZkoY5iHOkAaWgM6p5wDsUY5kELgg', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='For the sandcastle, the top level has a square footage of 16. Each level below it will have half the square footage of the level above it. \\n\\n1. The top level (Level 1) has 16 square feet.\\n2. The second level (Level 2) has 16 / 2 = 8 square feet.\\n3. The third level (Level 3) has 8 / 2 = 4 square feet.\\n4. The fourth level (Level 4) has 4 / 2 = 2 square feet.\\n\\nNow, we can add the square footage of all four levels together: \\n16 + 8 + 4 + 2 = 30 square feet.\\n\\nTo find the average square footage per level, we divide the total square footage by the number of levels:\\nAverage = Total square footage / Number of levels = 30 / 4 = 7.5.\\n\\nANSWER: 7.5', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733084798, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_3de1288069', usage=CompletionUsage(completion_tokens=194, prompt_tokens=410, total_tokens=604, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))). Set to null.\n",
      "without_so: Error processing question Cole wanted to buy new jeans for a dance contest. At the store, he couldn't decide between tattered jeans and jogger jeans. Since the jeans were on sale, he decided to buy them both. The tattered jeans cost $28 while the jogger jeans cost $6 less than the tattered jeans. He saved a total of $6. If he saved 1/3 of the total savings from the jogger jeans and the rest from the tattered jeans, how much more do jogger jeans originally cost than the tattered jeans?: list index out of range. Will try again.\n",
      "Could not parse response: ChatCompletion(id='chatcmpl-AZkopMjjb0N9ZdKLNSTarVtmUcZWl', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Let's denote the age of one twin as T and the age of one triplet as R. According to the problem: \\n\\n1. T = R + 7 (because one twin is 7 years older than one triplet).\\n2. The combined age is 44, so we have T + R = 44.\\n\\nNow, substituting the first equation into the second equation:\\n\\n(R + 7) + R = 44.\\n\\nThis simplifies to:\\n\\n2R + 7 = 44.\\n\\nNow, subtract 7 from both sides:\\n\\n2R = 37.\\n\\nNext, divide by 2:\\n\\nR = 18.5.\\n\\nUsing R to find T:\\n\\nT = 18.5 + 7 = 25.5.\\n\\nSince ages are typically whole numbers, we need to evaluate if the problem implies whole number ages or if a rounding context might apply. Given the mathematical process, we'll take T as the age of one of the twins being 25.5, but rounding is usually not done for ages.\\n\\nFrom the context, we round the values of T and R appropriately. Therefore, it appears reasonable that one twin's age can round up potentially to 26 depending on contextual interpretation.\\n\\nBut since we are sticking to strict values derived:\\n\\nUsing those interpretations, since the actual derived output points us to:\\n\\nT = T (8) hence: the inference is usually one twin is ... \\n\\nSo referring back: \\n\\nBy final numbers, given the adult context:\\n\\nThe closest whole number for realistic interpretation suggestions would be utilized.\\n\\nFinal figure under constraints led but was taken from the basic assignment noting original context challenges:\\n\\nEspecially if deemed as asked for ages counting.\\n\\nFinal take should lead us into account:\\n\\nGiven net response takes realistically to below numerical records upon coins:\\n\\nANSWER: 25.5. \\n\\nFinal round resolutions lead it into above discussions typically led 25 would drive total:\\n\\nConfirming direct lead majorities yield no emerge thus follow-ups typically aged cut leads 14 + 10 thus driving fair net deduction clarified were at hand introductory at full dynamics.\\n\\nSo known here say deviations under isolations enable clarifications unique here:\\n\\nSo shall keep our tight listings here now led for response reassembled neatly:\\n\\nIn closure, by rounding towards practical outputs hence layering contexts:\\n\\nUpon findings iterations directs proper rounding towards …\\n\\nANSWER: 25.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733084815, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_3de1288069', usage=CompletionUsage(completion_tokens=470, prompt_tokens=385, total_tokens=855, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))). Set to null.\n",
      "Could not parse response: ChatCompletion(id='chatcmpl-AZkooPY03iV5tZbYZb1MHGFC5GRzy', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"John initially received $100,000 for the first 5 months. This means he received $100,000 / 5 = $20,000 per month for those first 5 months.\\n\\nSince his research took 10 times longer, it lasted a total of 5 months * 10 = 50 months.\\n\\nThis means he has already spent $100,000 for the first 5 months. \\n\\nThe remaining months are 50 - 5 = 45 months.\\n\\nEach month after the first 5 costs 50% more than the previous month, so we start with $20,000 and increase each subsequent month by 50%. The costs for these 45 months will be calculated as follows:\\n\\n- Month 6: $20,000\\n- Month 7: $20,000 + (0.5 * $20,000) = $30,000\\n- Month 8: $30,000 + (0.5 * $30,000) = $45,000\\n- Month 9: $45,000 + (0.5 * $45,000) = $67,500\\n- Month 10: $67,500 + (0.5 * $67,500) = $101,250\\n- Month 11: $101,250 + (0.5 * $101,250) = $151,875\\n- Month 12: $151,875 + (0.5 * $151,875) = $227,812.50\\n- Month 13: $227,812.50 + (0.5 * $227,812.50) = $341,718.75\\n- Month 14: $341,718.75 + (0.5 * $341,718.75) = $512,578.13\\n- Month 15: $512,578.13 + (0.5 * $512,578.13) = $768,867.19\\n- Month 16: $768,867.19 + (0.5 * $768,867.19) = $1,153,300.79\\n\\nContinuing this pattern…\\n\\nThis resembles a geometric sequence where the first term for month 6 is $20,000 and the common ratio is 1.5 over the remaining months.\\n\\nUsing the formula for the sum of a geometric series, we can calculate the total cost for the research.\\n\\nThe sum can be calculated as follows:\\n\\nTotal Cost for months 6 to 50 = 20,000 * (1.5^45 - 1) / (1.5 - 1)\\n\\nBut calculating this will provide an overwhelming number. It's clear that this is a vastly increasing value.\\n\\nBut since we already have $100,000 for the first 5 months, once all additional funds are calculated, we end up knowing the total will largely exceed that figure.\\n\\nWithout exactormath on each month, broadly outlining the continuation, the total is overwhelmingly numerous via the increase leading towards totals into hundreds of thousands, essentially at least tens of millions after the complete evaluation of this increase.\\n\\nHowever, for easier access on the base considered from 35 months out, it convergently forms over distinct checks.\\n\\nDetermining the direct outputs multiple can side as towards over calculating intricacies is the core, leading towards numerous high tally under significant length, where evaluations remain million plus based in round metrics over timelines regards total caps.\\n\\nTotal cost can cease at comforting assurances due to the consistency presented within structures...\\n\\nTotal research cost thus stated will reside beyond the initially expense struct under direct sums.\\n\\nThe clear way will remain thus leads at least towards well figures about:\\nANSWER: 1000000 (for maximized estimates consistent towards capping totals upwards in standard form beyond edge.) \\n\\nThis towering it through sums finals thus remains as place due to checks over outputs distinctly.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733084814, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0705bf87c0', usage=CompletionUsage(completion_tokens=790, prompt_tokens=411, total_tokens=1201, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))). Set to null.\n",
      "Could not parse response: ChatCompletion(id='chatcmpl-AZkoz4HfgzLrsELst6PysvDHMGio1', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='First, we need to find out how many total days the team worked for. There are 3 weeks, and each week has 7 days, so they worked for 3 x 7 = 21 days. \\n\\nNext, the painters worked for 3/8ths of a day each day. So in total, the hours they worked is 21 days x (3/8) days = (21 * 3)/8 = 63/8 days.\\n\\nNow, to find out how many hours each painter worked, we need to convert this into hours. Since there are 8 hours in a full workday (assuming an 8-hour workday), we need to multiply the total days of work by 8. \\n\\nSo, the total hours worked by the team is (63/8) * 8 = 63 hours for the entire team. \\n\\nGiven that there are 4 painters, we need to divide the total hours by the number of painters to find out how many hours each one worked:\\n63 hours / 4 painters = 15.75 hours.\\n\\nTherefore, each painter worked 15.75 hours in total.\\nANSWER: 15.75', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733084825, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0705bf87c0', usage=CompletionUsage(completion_tokens=240, prompt_tokens=381, total_tokens=621, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))). Set to null.\n",
      "with_so_json_mode: Error processing question Jackie grew 3\" over the summer.  She is now 2\" shorter than Anne, who is twice the size of Albert.  If Albert is 36\" tall, how tall was Jackie before summer?: list index out of range. Will try again.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "evals = [\n",
    "    {\"question\": d[\"question\"], \"answer\": int(d[\"answer\"].split(\"#### \")[1].replace(\",\", \"\").strip())}\n",
    "    for d in dataset[\"test\"]\n",
    "]\n",
    "\n",
    "evaluator = LLMEvaluator(\n",
    "    configs=CONFIGS,\n",
    "    create_prompt_fn=create_prompt_gsm8k,\n",
    "    parse_response_fn=parse_response_gsm8k,\n",
    "    response_model=ResponseGSM8K,\n",
    ")\n",
    "\n",
    "df = evaluator.generate_outputs(evals)\n",
    "df_results = evaluator.evaluate_outputs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86cd8588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating confidence intervals (0.95) with 1319 observations:\n",
      "score_without_so - Mean: 93.56% CI: 92.23% - 94.88%\n",
      "score_with_so_tool_calls - Mean: 92.72% CI: 91.32% - 94.12%\n",
      "score_with_so_json_mode - Mean: 93.40% CI: 92.06% - 94.74%\n",
      "score_with_so_strict_tool_calls - Mean: 91.81% CI: 90.33% - 93.29%\n",
      "\n",
      "score_without_so vs score_with_so_tool_calls\n",
      "t-statistic: 1.4089315111766563, p-value: 0.1590912930784272\n",
      "score_without_so vs score_with_so_json_mode\n",
      "t-statistic: 0.2499111342792166, p-value: 0.8026950178465436\n",
      "score_without_so vs score_with_so_strict_tool_calls\n",
      "t-statistic: 3.056039757888458, p-value: 0.002287892607879198\n"
     ]
    }
   ],
   "source": [
    "evaluator.calculate_confidence_intervals(df_results)\n",
    "evaluator.run_paired_t_test(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7072f4eb",
   "metadata": {},
   "source": [
    "## Last Letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2ae3a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_last_letter(question, use_json_format=True):\n",
    "    json_system_prompt = dedent(\"\"\"\n",
    "        You are an expert in string manipulation tasks. You will be given a sequence of words and need to create a new string made from the last letter of each word. Before answering, explain your reasoning about how you'll extract and concatenate the letters.\n",
    "          \n",
    "        You will always respond with JSON in the format described below:\n",
    "        \n",
    "        {\"reasoning\": <str, step by step reasoning about the answer>, \"answer\": <str, lowercase concatenated last letters>}\n",
    "\n",
    "        First, provide your step by step reasoning in the \"reasoning\" field. Then, in the \"answer\" field, provide only the lowercase concatenated letters without any additional text.\n",
    "        \"\"\")\n",
    "\n",
    "    explanation_system_prompt = dedent(\"\"\"\n",
    "        You are an expert in string manipulation tasks. You will be given a sequence of words and need to create a new string made from the last letter of each word. Before answering, explain your reasoning about how you'll extract and concatenate the letters.\n",
    "        \n",
    "        You will always respond in the following format:\n",
    "        \n",
    "        <str, step by step reasoning about the answer>\n",
    "        ANSWER: <str, lowercase concatenated last letters>\n",
    "        \n",
    "        First, provide your step by step reasoning. Then, in ANSWER, provide only the lowercase concatenated letters without any additional text.\n",
    "        \"\"\")\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": json_system_prompt\n",
    "            if use_json_format\n",
    "            else explanation_system_prompt,\n",
    "        }\n",
    "    ]\n",
    "    fewshot_examples = [\n",
    "        (\n",
    "            \"Ian Peter Bernard Stephen\",\n",
    "            \"The last letter of 'Ian' is 'n'. The last letter of 'Peter' is 'r'. The last letter of 'Bernard' is 'd'. The last letter of 'Stephen' is 'N'. Concatenating them is 'nrdn'.\",\n",
    "            \"nrdn\",\n",
    "        ),\n",
    "        (\n",
    "            \"Larry Page\",\n",
    "            \"The last letter of 'Larry' is 'y'. The last letter of 'Page' is 'e'. Concatenating them is 'ye'.\",\n",
    "            \"ye\",\n",
    "        ),\n",
    "        (\n",
    "            \"Sergey Brin\", \n",
    "            \"The last letter of 'Sergey' is 'y'. The last letter of 'Brin' is 'n'. Concatenating them is 'yn'.\",\n",
    "            \"yn\",\n",
    "        ),\n",
    "        (\n",
    "            \"Bill Gates\",\n",
    "            \"The last letter of 'Bill' is 'l'. The last letter of 'Gates' is 's'. Concatenating them is 'ls'.\",\n",
    "            \"ls\",\n",
    "        ),\n",
    "        (\n",
    "            \"Jason Wei\",\n",
    "            \"The last letter of 'Jason' is 'n'. The last letter of 'Wei' is 'i'. Concatenating them is 'ni'.\",\n",
    "            \"ni\",\n",
    "        ),\n",
    "        (\n",
    "            \"François Chollet\",\n",
    "            \"The last letter of 'François' is 's'. The last letter of 'Chollet' is 't'. Concatenating them is 'st'.\",\n",
    "            \"st\",\n",
    "        ),\n",
    "        (\n",
    "            \"Yann LeCun\",\n",
    "            \"The last letter of 'Yann' is 'n'. The last letter of 'LeCun' is 'n'. Concatenating them is 'nn'.\",\n",
    "            \"nn\",\n",
    "        ),\n",
    "        (\n",
    "            \"Eliezer Yudkowsky\",\n",
    "            \"The last letter of 'Eliezer' is 'r'. The last letter of 'Yudkowsky' is 'y'. Concatenating them is 'ry'.\",\n",
    "            \"ry\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    for example_q, example_reason, example_ans in fewshot_examples:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Question: Take the last letters of the words in '{example_q}' and concatenate them.\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if use_json_format:\n",
    "            response = f'{{\"reasoning\": \"{example_reason}\", \"answer\": \"{example_ans}\"}}'\n",
    "        else:\n",
    "            response = f\"{example_reason}\\nANSWER: {example_ans}\"\n",
    "\n",
    "        messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"Question: {question}\"})\n",
    "\n",
    "    return messages\n",
    "\n",
    "\n",
    "def parse_response_last_letter(response: ChatCompletion) -> str | None:\n",
    "    try:\n",
    "        return ResponseLastLetter.model_validate_json(\n",
    "            response.choices[0].message.content\n",
    "        ).answer\n",
    "    except ValidationError:\n",
    "        try:\n",
    "            response_text = (\n",
    "                response.choices[0].message.content.split(\"\\nANSWER:\")[1].strip()\n",
    "            )\n",
    "            return response_text\n",
    "        except ValueError:\n",
    "            print(f\"Could not parse response: {response}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bfacd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with_so_tool_calls: Error processing question Take the last letters of each words in \"Billie Paloma Tanner Raul\" and concatenate them.: 'NoneType' object has no attribute 'split'. Will try again.\n",
      "with_so_tool_calls: Error processing question Take the last letters of each words in \"Dustin Luiz Rolando Connor\" and concatenate them.: 'NoneType' object has no attribute 'split'. Will try again.\n",
      "with_so_tool_calls: Error processing question Take the last letters of each words in \"Tucker Daniel Hernandez Alison\" and concatenate them.: 'NoneType' object has no attribute 'split'. Will try again.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"ChilleD/LastLetterConcat\")\n",
    "evals = [\n",
    "    {\"question\": d[\"question\"], \"answer\": d[\"answer\"].lower()} for d in dataset[\"test\"]\n",
    "]\n",
    "\n",
    "evaluator = LLMEvaluator(\n",
    "    configs=CONFIGS,\n",
    "    create_prompt_fn=create_prompt_last_letter,\n",
    "    parse_response_fn=parse_response_last_letter,\n",
    "    response_model=ResponseLastLetter,\n",
    ")\n",
    "\n",
    "df = evaluator.generate_outputs(evals)\n",
    "df_results = evaluator.evaluate_outputs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "042dfdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating confidence intervals (0.95) with 150 observations:\n",
      "score_without_so - Mean: 86.00% CI: 80.43% - 91.57%\n",
      "score_with_so_tool_calls - Mean: 87.33% CI: 81.99% - 92.67%\n",
      "score_with_so_json_mode - Mean: 83.33% CI: 77.35% - 89.32%\n",
      "score_with_so_strict_tool_calls - Mean: 87.33% CI: 81.99% - 92.67%\n",
      "\n",
      "score_without_so vs score_with_so_tool_calls\n",
      "t-statistic: -0.4071114230647015, p-value: 0.6845104035260943\n",
      "score_without_so vs score_with_so_json_mode\n",
      "t-statistic: 0.893827507934847, p-value: 0.3728556534698112\n",
      "score_without_so vs score_with_so_strict_tool_calls\n",
      "t-statistic: -0.44601783508317583, p-value: 0.6562318183397802\n"
     ]
    }
   ],
   "source": [
    "evaluator.calculate_confidence_intervals(df_results)\n",
    "evaluator.run_paired_t_test(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d0a1d",
   "metadata": {},
   "source": [
    "## Shuffled objects   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00b7f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_shuffled_objects(question, use_json_format=True):\n",
    "    json_system_prompt = dedent(\"\"\"\n",
    "        You are an expert in performing common sense tasks involving the ordering of a sequence of events.\n",
    "        Each question will present you with a logic puzzle over a sequence of events.\n",
    "          \n",
    "        You will always respond with JSON in the format described below:\n",
    "        \n",
    "        {\"reasoning\": <str, step by step reasoning about the answer>, \"answer\": <str, final answer>}\n",
    "\n",
    "        First, provide your step by step reasoning in the \"reasoning\" field. Then, in the \"answer\" field, provide only the single letter representing the correct choice you are presented with. Don't include any other text in the \"answer\" field. Both fields are required.\n",
    "        \"\"\")\n",
    "\n",
    "    explanation_system_prompt = dedent(\"\"\"\n",
    "        You are an expert in performing common sense tasks involving the ordering of a sequence of events.\n",
    "        Each question will present you with a logic puzzle over a sequence of events.\n",
    "        \n",
    "        You will always respond in the following format:\n",
    "        \n",
    "        <str, step by step reasoning about the answer>\n",
    "        ANSWER: <str, final answer>\n",
    "        \n",
    "        First, provide your step by step reasoning. Then, in ANSWER, provide only the single letter representing the correct choice you are presented with. Don't include any other text in ANSWER.\n",
    "        \"\"\")\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": json_system_prompt\n",
    "            if use_json_format\n",
    "            else explanation_system_prompt,\n",
    "        }\n",
    "    ]\n",
    "    fewshot_examples = [\n",
    "        (\n",
    "            \"Alice, Bob, Claire, Dave, and Eve are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Patrick, Bob is dancing with Sam, Claire is dancing with Jamie, Dave is dancing with Lola, and Eve is dancing with Melissa.\\nThroughout the song, the dancers often trade partners. First, Dave and Eve switch partners. Then, Dave and Alice switch partners. Then, Eve and Alice switch partners. Then, Claire and Bob switch partners. Finally, Dave and Alice switch partners. At the end of the dance, Alice is dancing with\\nOptions:\\n(A) Patrick\\n(B) Sam\\n(C) Jamie\\n(D) Lola\\n(E) Melissa\",\n",
    "            \"Dave and Eve switch partners, so Dave's partner is now Melissa and Eve's partner is now Patrick. Then Dave and Alice switch partners so Dave's partner is now Patrick and Alice's partner is now Melissa. Then Eve and Alice switch partners so Eve's partner is now Melissa and Alice's partner is now Lola. Then Claire and Bob switch patners so Claire's partner is now Sam, and Bob's partner is now Jamie. Finally, Dave and Alice switch partners so Dave's new partner is Lola, and Alice's new partner is Patrick. Alice is dance in with Patrick, choice A.\",\n",
    "            \"A\",\n",
    "        ),\n",
    "        (\n",
    "            \"Alice, Bob, Claire, Dave, and Eve are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Ophelia, Bob is dancing with Jamie, Claire is dancing with Melissa, Dave is dancing with Rodrigo, and Eve is dancing with Patrick.\\nThroughout the song, the dancers often trade partners. First, Claire and Bob switch partners. Then, Claire and Eve switch partners. Then, Claire and Bob switch partners. Then, Eve and Dave switch partners. Finally, Claire and Alice switch partners. At the end of the dance, Alice is dancing with\\nOptions:\\n(A) Ophelia\\n(B) Jamie\\n(C) Melissa\\n(D) Rodrigo\\n(E) Patrick\",\n",
    "            \"Claire and Bob switch partners, so Claire's partner is now Jamie and Bob's partner is now Melissa. Then, Claire and Eve switch partners, so Claire's partner becomes Patrick and Eve's partner becomes Jamie. Next, Claire and Bob switch partners again, making Claire's partner Melissa and Bob's partner Patrick. After that, Eve and Dave switch partners, resulting in Eve's partner being Rodrigo and Dave's partner being Jamie. Finally, Claire and Alice switch partners, so Claire's partner is now Ophelia and Alice's partner becomes Melissa. Alice is dancing with Melissa, which is choice C.\",\n",
    "            \"C\",\n",
    "        ),\n",
    "        (\n",
    "            \"Alice, Bob, Claire, Dave, and Eve are friends and avid readers who occasionally trade books. At the start of the semester, they each buy one new book: Alice gets Catch-22, Bob gets Hound of the Baskervilles, Claire gets Frankenstein, Dave gets The Pearl, and Eve gets The Fellowship of the Ring.\\nAs the semester proceeds, they start trading around the new books. First, Eve and Alice swap books. Then, Alice and Claire swap books. Then, Alice and Bob swap books. Then, Dave and Alice swap books. Finally, Dave and Claire swap books. At the end of the semester, Dave has\\nOptions:\\n(A) Catch-22\\n(B) Hound of the Baskervilles\\n(C) Frankenstein\\n(D) The Pearl\\n(E) The Fellowship of the Ring\",\n",
    "            \"Dave and Eve start by swapping their books, so Dave now has The Fellowship of the Ring and Eve has The Pearl. Next, Dave and Alice swap books, meaning Dave takes Alice's original book Catch-22 while Alice receives The Fellowship of the Ring. Then, Alice and Claire swap books, so Alice now holds Frankenstein and Claire gets The Fellowship of the Ring. After that, Alice and Bob swap books, resulting in Alice having Hound of the Baskervilles and Bob taking Frankenstein. Finally, Dave and Claire swap books, which means Dave ends up with The Fellowship of the Ring. Therefore, the correct answer is E.\",\n",
    "            \"E\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    for example_q, example_reason, example_ans in fewshot_examples:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Question: {example_q}\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if use_json_format:\n",
    "            response = f'{{\"reasoning\": \"{example_reason}\", \"answer\": \"{example_ans}\"}}'\n",
    "        else:\n",
    "            response = f\"{example_reason}\\nANSWER: {example_ans}\"\n",
    "\n",
    "        messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"Question: {question}\"})\n",
    "\n",
    "    return messages\n",
    "\n",
    "\n",
    "def parse_response_shuffled_objects(response: ChatCompletion) -> str:\n",
    "    try:\n",
    "        return ResponseShuffledObjects.model_validate_json(\n",
    "            response.choices[0].message.content\n",
    "        ).answer\n",
    "    except ValidationError:\n",
    "        response_text = (\n",
    "            response.choices[0].message.content.split(\"\\nANSWER:\")[1].strip()\n",
    "        )\n",
    "        try:\n",
    "            return response_text\n",
    "        except ValueError:\n",
    "            print(f\"Could not parse response: {response}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11d87fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"openeval/BIG-Bench-Hard\", data_files=\"tracking_shuffled_objects_five_objects.json\"\n",
    ")\n",
    "evals = [\n",
    "    {\n",
    "        \"question\": d[\"input\"],\n",
    "        \"answer\": d[\"target\"].replace(\"(\", \"\").replace(\")\", \"\").strip(),\n",
    "    }\n",
    "    for d in dataset[\"train\"][\"examples\"][0][4:]  # first 3 are few-shot examples\n",
    "]\n",
    "\n",
    "evaluator = LLMEvaluator(\n",
    "    configs=CONFIGS,\n",
    "    create_prompt_fn=create_prompt_shuffled_objects,\n",
    "    parse_response_fn=parse_response_shuffled_objects,\n",
    "    response_model=ResponseShuffledObjects,\n",
    ")\n",
    "\n",
    "df = evaluator.generate_outputs(evals)\n",
    "df_results = evaluator.evaluate_outputs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ef07430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating confidence intervals (0.95) with 246 observations:\n",
      "score_without_so - Mean: 82.93% CI: 78.22% - 87.64%\n",
      "score_with_so_tool_calls - Mean: 64.63% CI: 58.65% - 70.62%\n",
      "score_with_so_json_mode - Mean: 65.85% CI: 59.92% - 71.79%\n",
      "score_with_so_strict_tool_calls - Mean: 66.67% CI: 60.76% - 72.57%\n",
      "\n",
      "score_without_so vs score_with_so_tool_calls\n",
      "t-statistic: 5.125509719126267, p-value: 6.022145641276423e-07\n",
      "score_without_so vs score_with_so_json_mode\n",
      "t-statistic: 4.980333222613169, p-value: 1.1987851854079184e-06\n",
      "score_without_so vs score_with_so_strict_tool_calls\n",
      "t-statistic: 4.932513776454719, p-value: 1.4990535155322099e-06\n"
     ]
    }
   ],
   "source": [
    "evaluator.calculate_confidence_intervals(df_results)\n",
    "evaluator.run_paired_t_test(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
