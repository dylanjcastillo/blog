---
title: "Prompt engineering 101"
date: 2025-06-26
description-meta: "How to convince LLMs to do what you want them to do."
categories:
  - llm
  - openai
  - anthropic
  - python
---

I've tried every trick in the book to get Large Language Models (LLMs) to do what I want them to do.

From threatening them with violence or bribing them with money to solve logic puzzles, to asking them to call me [big daddy](https://www.reddit.com/r/cursor/comments/1joapwk/comment/mkqg8aw) to ensure they follow my Cursor rules (while making me feel special!).

Prompt engineering is a key part of using LLMs, but it's also one of the most hyped and abused techniques. Nowadays, it's closer to astrology than to engineering.

This article is a short guide to help you get the most important things right when it comes to writing prompts.

Let's get started!

## What is a prompt?

Prompts are instructions sent as text to the model. Most models work with two types of instructions:

1. **System/developer prompt**: Sets the “big picture” or high-level rules for the entire conversation. Examples: “You are a helpful assistant.”; “Always answer in haiku.”
2. **User prompt**: The actual question end-user types and any additional context. Examples: “What’s today’s weather in Dublin?”; “Summarize the following documents”

The system prompt gives the assistant a “role”, while the user prompt requests specific content within that framework.

Prompts are usually provided as messages to the model. The system prompt correspond to the system message, and the user prompt corresponds to the user message. There's also also assistant messages, which are the model's responses and tool calls, which are the model's calls to external tools.

## Components of a good prompt

There are many useful free resources online you can use to learn more about prompt engineering. I recommend [this article](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview#prompt-engineering-tutorial) by Anthropic or [this one](https://platform.openai.com/docs/guides/text?api-mode=responses) by OpenAI.

Most of the advice boils down to a few key ideas.

You should:

1. Be clear and specific
2. Provide examples
3. Let models think
4. Structure prompts into sections and use clear delimiters
5. Split complex tasks into smaller steps
6. Repeat instructions when the context is long

### Be clear and specific

This is the most important principle. If you cannot describe in detail the task you want to perform, the model will not be able to perform it. Whenever you write a prompt, ask yourself: "If I didn't have any background knowledge in this domain, could I complete this task based on the text that I just wrote in this prompt?"

In addition to describing the task, you should also provide a role for the model. For example, if you're classifying documents you can use a role like "You're an expert in document classification" or if you're dealing with financial data you can use a role like "You're an expert in financial analysis".

❌ Don't do this:

> Please fix this email.
> <EMAIL CONTENT>

✅ Do this:

> You're an expert in business writing. Please review and improve this email by addressing the following issues:
>
> - Fix any grammatical errors and typos
> - Improve clarity and conciseness
> - Ensure professional tone throughout
> - Strengthen the subject line to be more specific
> - Add a clear call-to-action if missing
> - Format for better readability (bullets, paragraphs, etc.)
>
> <EMAIL CONTENT>

In most cases, you will provide this in the system prompt.

### Provide examples

One of the lowest hanging fruit in prompt engineering is to provide examples. It's as simple as showing the model a few input and output pairs.

This technique is formally known as "few shot prompt". It has proved in practice to be a powerful way to improve the [performance of LLMs](https://arxiv.org/abs/2009.03300) in many tasks.

✅ Do this:

> You are an expert in solving simple word puzzles using reasoning steps. Provided with a list of 4 names, you will concatenate the last letters into a word.

> Examples:
>
> **Example 1**:
>
> Input: 'Ian Peter Bernard Stephen'
>
> Output: 'NRDN'
>
> **Example 2**:
>
> Input: 'Javier Dylan Christopher Joseph'
>
> Output: 'RNRH'

In my experience, it's better to provide these examples directly in the system prompt. However, some people prefer to use assistant messages to provide examples. Both methods are valid, I prefer to use the system prompt because I find it cleaner.

### Let models think

Models think in tokens. If you want them to achieve better results, you should let them think.

This process is formally known as "chain of thought" (CoT) prompt. Similar to few shot prompts, it has proved in practice to be a powerful way to improve the [performance of LLMs](https://arxiv.org/abs/2201.11903) in many tasks.

You can use a 0-shot CoT prompt, which essentially means that you explicitly ask the model to think step by step to solve the problem but don't provide any examples of how it should reason about it. Or a few-shot CoT prompt, which means that you provide examples of how the model should reason about the problem.

Here are two examples of CoT prompts:

#### 0-Shot CoT Prompt

> **Question:** A cinema sold 120 tickets at $8 each. What was the total revenue?
>
> **Note:** think about your answer step by step

#### 1-Shot CoT Prompt

> **<example>**  
> **Question:** Emily buys 3 notebooks at $4 each and 2 pens at $1.50 each. What's her total cost?  
> **Reasoning:**
>
> 1. Cost of notebooks = 3 × $4 = $12
> 2. Cost of pens = 2 × $1.50 = $3
> 3. Total cost = $12 + $3 = $15  
>    **Answer:** $15  
>    **</example>**
>
> **Question:** A cinema sold 120 tickets at $8 each. What was the total revenue?

Most providers now allow you to let models think without explicitly asking them to do so. With OpenAI models you can use models from the o-family. For Anthropic and Gemini, you can configure the model to use thinking tokens by setting a specific parameter. Note, however, that only Gemini lets you extract the thinking token from the model, which is often useful for debugging purposes. For OpenAI and Anthropic, you will only get the final answer.

### Structure prompts into sections

You should structure your system and user prompt into sections. Some people like to use markdown formatting to make the prompt more readable, others use xml tags. Both work, just make sure to do it consistently.

You should also reverse backticks (```) to delimit code blocks.

#### System prompt

> **Role and objective**
> You’re an expert document classifier. Your goal is to classify this document…
> **Rules**
>
> 1. Documents that contain information about medical treatments should be classified as …
> 2. Do not classify documents into multiple categories
> 3. …
>    **Examples**
>
> Input: [document text]
> Classification: [document category]
> …
> **Output**
>
> You should generate a JSON object with this structure: [JSON schema]
>
> **(Optional) Reiterate objective and elicit thinking**
>
> Your goal is to XYZ… Before writing your answers, write your reasoning step by step.

#### User prompt

> **Briefly reiterate objective**
> Please classify this document into a category.
> **Context**
> Input: [document text]

### Split complex tasks into smaller steps

LLMs often get confused when the context is too long and the instructions are complex.

For example, you might have a document classifier with complicated entity extraction logic that you want to implement. Instead of doing a single LLM call with a prompt that does the document classifiacation and the conditional entity extraction, you can split the task into two steps:

1. First, you classify the document into a category.
2. Then, you extract the entities from the document, based on the category.

This way, you avoid overwhelming the model with a complex prompt.

❌ Don't do this:

> You're an expert document classifier. First, classify the document into the following categories:
>
> - Medical
> - Financial
> - Legal
>
> Then, if the document is classified as "Medical", extract the following entities:
> ...
>
> If the document is classified as "Financial", extract the following entities:
> ...
>
> If the document is classified as "Legal", extract the following entities:
> ...

✅ Do this:

**Prompt 1: Document classification**

> You're an expert document classifier. First, classify the document into the following categories:
>
> - Medical
> - Financial
> - Legal

**Prompt N: Entity extraction (one for each category)**

> You're an expert in entity extraction in the <CATEGORY> domain. Your goal is to extract the entities from the document.
> ...

### With long contexts, repeat instructions

LLMs often get confused when the context is too long and the instructions are complex. When context gets above a certain length, it's better to repeat the instructions at the bottom of the prompt.

❌ Don't do this:

> You're an expert in entity extraction in the <CATEGORY> domain. Your goal is to classify the document into the following categories: "Medical", "Financial", "Legal".
> <VERY LONG CONTEXT>

✅ Do this:

> You're an expert in entity extraction in the <CATEGORY> domain. Your goal is to classify the document into the following categories: "Medical", "Financial", "Legal".
> <VERY LONG CONTEXT>
> Remember, your goal is to classify the document into the following categories: "Medical", "Financial", "Legal".

## Conclusion

This article is a short guide to help you write better prompts.

Hope you found this article useful. If you have any questions or feedback, leave a comment below.
